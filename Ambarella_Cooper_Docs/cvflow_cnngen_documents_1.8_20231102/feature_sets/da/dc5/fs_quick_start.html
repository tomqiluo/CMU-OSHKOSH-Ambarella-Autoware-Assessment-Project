<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: Quick Start</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('da/dc5/fs_quick_start.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Quick Start </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Before reading the following documentation and quickly getting started using the CNNGen toolchain, an important note is that users are advised to use pruning test tools to test performance improvements in CVflow.</p><ul>
<li>For detail, please refer to <a class="el" href="../../d1/d82/fs_workflow.html#sub_sec_1_model_prune">2.1 Pruning</a>.</li>
<li>For how to do the test, please refer to <a class="el" href="../../da/dc5/fs_quick_start.html#subsec_eazyai_gsg_perf">3.1 Quick Performance Evaluation</a>.</li>
</ul>
<h1><a class="anchor" id="sec_qs_prepare"></a>
1 Prepare</h1>
<p>Before starting, please refer to below sections to see what are needed.</p>
<h2><a class="anchor" id="sub_qs_pre_package"></a>
1.1 Packages</h2>
<p>User should have two SDK packges and two toolchain pacakges as below.</p><ol type="1">
<li>CVflow CNNGen SDK<ol type="a">
<li><code>cvflow_cnngen_samples_&lt;version&gt;_&lt;date&gt;.tar.xz</code> + <code>cvflow_cnngen_samples_&lt;version&gt;_&lt;date&gt;.tar</code><ul>
<li>Easy tools for convert, accuracy debug, and inference test with x86 Simulator or real chip on EVK. Users needs an EVK if CVflow inference is needed. Please refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_hw">1.2 HW</a> for detail.</li>
<li>Lots of network examples.</li>
</ul>
</li>
<li><code>cvflow_cnngen_documents_&lt;VERSION&gt;_&lt;date&gt;.tar.bz2</code>, which is the prebuild doxygen documents for converting and deployment, also users can generate it from above SDK packqge directly.</li>
<li><code>Ambarella_Toolchain_CNNGen_&lt;version&gt;_&lt;date&gt;.tar.xz</code>, which is the CNNGen toolchain and EazyAI CFlite Python tools for network convert and compile. It also provides quick shell installation script and docker file installation both for CPU and GPU, only for Ubuntu2004. And there should be some simple tool guides in this package.</li>
</ol>
</li>
<li><p class="startli">Cooper SDK</p><ol type="a">
<li><code>cooper_linux_sdk_&lt;version&gt;_&lt;date&gt;.tar.xz</code> + <code>cooper_linux_&lt;version&gt;_&lt;date&gt;.tar</code>, which provids the MP level APIs for the final deployment.<ul>
<li>Cavalry Driver API, official low level APIs</li>
<li>Cavalry_Mem Library API, official low-level APIs</li>
<li>NNCtrl Library API, official low-level APIs</li>
<li>VPROC Library API, official low-level APIs</li>
<li>EazyAI Library API, easy-to-use high-level APIs for running neural networks on CV platforms of Ambarella, this library is just for demos, not a official APIs, of course, users can use it if they need. It encapsulates low-level modules in SDK including IAV driver, Cavalry_Mem lib, VPROC lib, NNCtrl lib and SmartFB lib, then arrange functions in high-level with general conceptions in CV ground, like preprocess, forward and post process.</li>
</ul>
</li>
<li><code>cooper_linux_sdk_&lt;version&gt;_&lt;arch&gt;_doxygen_&lt;date&gt;.tar.xz</code>, which is the prebuild doxygen documents, also users can generate it from above SDK package directly.</li>
<li><code>Ambarella_CortexA76_Toolchain_&lt;version&gt;_&lt;date&gt;.tar.xz</code>, which is the linaro toolchain for cross compile for ARM. It also provides quick shell installation script.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>CV2x / CV5x are using Cavalry v2 for Cavalry Driver, NNCtrl, and Vproc, but CV72 is using Cavalry v3 for these libraries. EazyAI library is compatible for both CV2x, CV5x, and CV7x.</li>
<li>CV5x and CV7x will use the unify Linux SDK pacakge which is named cooper_linux_sdk_&lt;version&gt;_&lt;date&gt;.tar.xz. Even in future, CV2x will be included in it.</li>
</ul>
</dd>
<dd>
EazyAI source code are both included in above two pacakges.</dd></dl>
</li>
</ol>
<h2><a class="anchor" id="sub_qs_pre_hw"></a>
1.2 HW</h2>
<p>User can refer to the follow hardware list which are needed for network inference.</p>
<ul>
<li>It is suggested that users can use IMX274_MIPI which can support lots of different VIN resolutions.</li>
<li>It is suggested that users can use <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_yuv_input">15 HDMI YUV Input</a> to feed videos to EVK for algorithm evaluation.</li>
<li>It is suggested to use below EVK for CVflow inference with <code>eazyai_inf</code>.<ul>
<li>CV2_Chestnut</li>
<li>CV22_Walnut</li>
<li>CV25_Hazenut</li>
<li>CV28M_Cashewnut</li>
<li>CV5_Timn</li>
<li>CV72_Gage</li>
<li>CV3_DK_Mini</li>
</ul>
</li>
<li>One TV Screen with HDMI.</li>
<li>One computer which has installed VLC to receive RTSP streaming.</li>
<li>It is suggested to prepare one SDCARD as NAND is too small, may not be enough to save the big models.</li>
</ul>
<h2><a class="anchor" id="sub_qs_pre_install"></a>
1.3 Installation</h2>
<p>There are two CNNGen packages for CVflow_v2 and CVflow_v3, both of which use Ubuntu 2004 system:</p><ul>
<li><code>Ambarella_Toolchain_CNNGen_2.*.*_2023****.tar.xz</code>: used is for CVflow_v2 (CV2x and CV5x) conversion;</li>
<li><code>Ambarella_Toolchain_CNNGen_cv7x.3.*.*_2023****.tar.xz</code>: used for CVflow_v3 (CV7x) conversion;</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Since Ubuntu2004, AmbaCaffe will not be provided in above toolchain package, user can find it in the old CNNGen toolchain package.</dd></dl>
<p>Before running the installation script for the toolchain, run the following command: </p><div class="fragment"><div class="line">uild $ sudo apt-get update</div>
</div><!-- fragment --><p>Users should regularly upgrade the operating system as shown below, as the toolchain is developed using the latest version OS. </p><div class="fragment"><div class="line">Build $ sudo apt-get upgrade</div>
</div><!-- fragment --><p>Although the command above is included in the installation script, Ambarella recommends noting it and assigning it to an independent machine to avoid a possible build conflict.</p>
<p>Install the toolchain as shown below. For Ubuntu Linux, the commands are as follows: </p><div class="fragment"><div class="line">build $ cd Ubuntu-20.04</div>
<div class="line">build $ sudo chmod +x ubuntuToolChain-&lt;version&gt;.ubuntu-20.04</div>
<div class="line">build $ ./ubuntuToolChain-&lt;version&gt;.ubuntu-20.04</div>
<div class="line">Run [sudo apt-get update] and [sudo apt-get upgrade] <span class="keywordflow">if</span> needed? (Y/n):y</div>
<div class="line">[sudo] password <span class="keywordflow">for</span> user: xxx</div>
<div class="line">Reading package lists... Done</div>
<div class="line">Building dependency tree</div>
<div class="line">Reading state information... Done</div>
<div class="line">The following packages were automatically installed and are no longer required:</div>
<div class="line">  libavcodec-dev libavformat-dev ......</div>
<div class="line">Do you want to <span class="keywordflow">continue</span>? [Y/n]y</div>
<div class="line">......</div>
<div class="line">Remove ppa:komfamedia/backports successfully!</div>
<div class="line">Hit:1 http:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/ubuntu focal InRelease</span></div>
<div class="line">Hit:2 http:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates InRelease</span></div>
<div class="line">Hit:3 http:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/ubuntu focal-backports InRelease</span></div>
<div class="line">Hit:4 http:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/ubuntu focal-security InRelease</span></div>
<div class="line">Reading package lists... Done</div>
<div class="line">Run [sudo pip3 install --upgrade pip] <span class="keywordflow">if</span> needed? (Y/n):y</div>
<div class="line">......</div>
<div class="line">TOOLCHAIN PATH [/usr/local]:</div>
<div class="line">Install amba-cv-tools-*qa.2023-10-08.261.ubuntu-20.04.tar.xz into /usr/local? (Y/n):y</div>
<div class="line">Install caffe-cpu package from Ubuntu software repo? (Y/n)y</div>
<div class="line">Install thirdparty tensorflow Python <span class="keyword">package </span>from Python repo? (Y/n)y</div>
<div class="line">Install onnx-cpu and pytorch-cpu package from Python repo? (Y/n)y</div>
<div class="line">Install EazyAI Python CFlite Library from WHL file? (Y/n)y</div>
<div class="line">Installation is complete!</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li><code>installToolChainONLY-qa.2023-10-08.261.ubuntu-20.04</code>, which will install toolchain only.</li>
<li><code>installWithNormalUser-qa.2023-10-08.261.ubuntu-20.04</code>, which will install python libraries and toolchain with normal user.</li>
<li>Also there is docker file to help users to install the docker environment.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_qs_pre_dm"></a>
1.4 Download Models</h2>
<p>As models and images are too big to be included in CNNGen Samples package, since CVflow CNNGen Samples 1.8, users can download these models, DRA images, and dataset in <a href="https://amba.egnyte.com/fl/pnAckpTfWI">https://amba.egnyte.com/fl/pnAckpTfWI</a> with password <code>Amba_2023</code> to get the access permission.</p>
<p>Then users can get all the files from below page, users can download all the folders, or download what they need. </p><div class="image">
<img src="../../download.jpg" alt=""/>
<div class="caption">
Figure 1-4. Download Web.</div></div>
<p>After downloading, user can get below folder structure. and copy them to CNNGen samples folders as below. </p><div class="fragment"><div class="line">build $ tree ./ -L 1</div>
<div class="line">amba_cnngen_models</div>
<div class="line">├── caffe (Caffe network models and DRA)</div>
<div class="line">├── data (Dataset <span class="keywordflow">for</span> Accuracy test in @ref fs_accuracy_tool_test)</div>
<div class="line">├── license (License files <span class="keywordflow">for</span> the models)</div>
<div class="line">├── onnx (ONNX network models and DRA)</div>
<div class="line">├── prebuild (Prebuild cavalry binary <span class="keywordflow">for</span> quick live demos in @ref sec_qs_live_demos)</div>
<div class="line">└── tensorflow (Tensorflow network models and DRA)</div>
<div class="line">build $ cp -rf .<span class="comment">/* &lt;CNNGen Samples Package Path&gt;/</span></div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>The folder structure between CNNGen Samples Package and this web are totally the same.</dd></dl>
<p>Also user can only copy the network they want as below. </p><div class="fragment"><div class="line">build $ cp -rf ./onnx/test_networks/mobilenet_v2<span class="comment">/* &lt;CNNGen Samples Package Path&gt;/onnx/test_networks/mobilenet_v2/</span></div>
</div><!-- fragment --><p>To reproduce the accuracy test examples, first, users should prepare the test dataset with the script in <code>data</code> folder. </p><div class="fragment"><div class="line">build $ tree data/</div>
<div class="line">data/</div>
<div class="line">├── COCO_2014</div>
<div class="line">│   └── get_coco_2014.sh</div>
<div class="line">├── ILSVRC_2012</div>
<div class="line">│   └── gen_from_downloaded_ilsvrc2012.sh</div>
<div class="line">└── VOC_07</div>
<div class="line">    └── get_voc07.sh</div>
</div><!-- fragment --><p> For how to use it, please refer to <a class="el" href="../../d1/d82/fs_workflow.html#sub_sec_accuracy_download_dataset">4.2.3 Download the Test Dataset</a>.</p>
<h2><a class="anchor" id="sub_qs_pre_evk_conn"></a>
1.5 EVK Connection</h2>
<p>Users should make sure the build server and EVK are in the same LAN, then users can do belows on server.</p><ol type="1">
<li>Use FTP to get files from EVK and upload files to EVK</li>
<li>Use Telnet to connect to the EVK</li>
<li>Use SSH to connect to the EVK</li>
<li>Use <code>eazyai_inf</code>, <code>eazyai_inf_simple_dummy</code>, and <code>eazyai_inf_simple_live</code> to do the inference on EVK directly.</li>
</ol>
<h2><a class="anchor" id="sub_qs_pre_ref_files"></a>
1.6 Reference Materials</h2>
<p>There are some training videos and documents which are not included in doxygen documents.</p>
<ol type="1">
<li><b>CNNGen Training Video EN / CN</b> and <b>CNNGen Training PDF</b>, if users need, please ask Ambarella Support for assistance.</li>
<li><b>SDK PDF Documents</b>, there are lots of documents, for CVfLow related, please refer to the followings.<ul>
<li><em>Ambarella_Cooper_Linux_SDK_Code_Building</em> which help users to install the CNNGen Toolchain and setup build environment.</li>
<li><em>Ambarella_Cooper_Linux_SDK_Getting_Started_Guide</em> which help users to run some simple demos in Ambarella EVK and <b>instruct them how to generated doxygen documents</b>.</li>
<li><em>Ambarella_CV*_DG_Flexible_Linux_SDK*.*_CVflow_FAQ</em> which lists the answers of some common questions.</li>
</ul>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_qs_live_demos"></a>
2 Live Demos</h1>
<p>Users can refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_dm">1.4 Download Models</a> to download the prebuild cavalry binaries, and refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_install">1.3 Installation</a> to install CFlite, also prepare below EVKs.</p><ul>
<li>CV2_Chestnut</li>
<li>CV22_Walnut</li>
<li>CV25_Hazenut</li>
<li>CV28M_Cashewnut</li>
<li>CV5_Timn</li>
<li>CV72_Gage</li>
</ul>
<p>Then to run demos as below.</p><ol type="1">
<li>caffe mobilenetv1_ssd <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 -cb cvx/cv22_cavalry_mobilenetv1_ssd.bin -pn ssd -pl caffe/demo_networks/mobilenetv1_ssd/config/ssd_caffe.lua -dm 0 -lp caffe/demo_networks/mobilenetv1_ssd/config/label_voc_with_bg.txt --fsync_off -dd STREAM1 -ei caffe/demo_networks/mobilenetv1_ssd/models/mobilenet_priorbox_fp32.bin</div>
</div><!-- fragment --></li>
<li>onnx_ddrnet <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 -cb cvx/cv22_cavalry_onnx_ddrnet.bin -pn deeplabv3 -pl onnx/demo_networks/DDRNet/config/ddrnet.lua -dm 1 -dd STREAM1</div>
</div><!-- fragment --></li>
<li>caffe yolov3_fastest_xl <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 -cb cvx_cavalry_yolov3_fastest_xl.bin  -pn yolov3 -pl caffe/demo_networks/light_yolo/config/yolov3_fastest.lua -dm 0 -lp caffe/demo_networks/light_yolo/config/label_voc.txt --fsync_off -dd STREAM1</div>
</div><!-- fragment --></li>
<li>tf_hfnet <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 2 -cb cvx/cv22_cavalry_tf_hfnet.bin -pn hfnet -pl tensorflow/demo_networks/hfnet/config/hfnet.lua -dm 1 -dd STREAM1</div>
</div><!-- fragment --></li>
<li>RetinaFace <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 -cb cvx_cavalry_onnx_retinaface.bin -pn retinaface -pl onnx/demo_networks/retinaface/config/retinaface.lua -dm 0 -dd STREAM1</div>
</div><!-- fragment --></li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_eazyai_gsg"></a>
3 EazyAI CFlite Python Tools</h1>
<p>Please refer to the following development flow for CVflow development, before that, please refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_install">1.3 Installation</a> to install CFlite.</p>
<p>And refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_helper">2.1 EazyAI Helper Tool</a> for quick workflow and usage. </p><div class="fragment"><div class="line">build $ eazyai_helper</div>
<div class="line">build $ eazyai_helper --show_tools</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_gsg_perf"></a>
3.1 Quick Performance Evaluation</h2>
<p>User can use <code>eazyai_cvt</code> dummy mode for quick performance roughly evaluation for different networks.</p>
<p>Below commands will print the theory performacne at the end. </p><div class="fragment"><div class="line">build $ eazyai_cvt -mp model_file --dummy</div>
<div class="line">build $ eazyai_cvt -mp model_file --dummy --density 0.2 (Only keep 20% non-zero weights)</div>
</div><!-- fragment --><p>Such as below example. </p><div class="fragment"><div class="line">build $ eazyai_cvt -mp onnx/demo_networks/FGFD/models/version-RFB-320.onnx --dummy</div>
<div class="line"><span class="preprocessor">###version-RFB-320: TheoryPerf Result: [0.1299 ms]</span></div>
<div class="line"><span class="preprocessor">### Conversion completed !</span></div>
</div><!-- fragment --><p>Please make sure the network binary if CVflow friendly. Users can use below tools to check it.</p><ol type="1">
<li><code>tf_print_graph_summary.py</code></li>
<li><code>onnx_print_graph_summary.py</code></li>
<li><code>caffe_print_graph_summary.py</code></li>
</ol>
<p>If there is some warning message, please fix it with <code>graph_surgery.py</code>.</p>
<p>User can use <code>eazyai_inf_simple_dummy</code> for quick network performance evaluation on real chip based on the model which is generated with <code>eazyai_cvt</code>. </p><div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb cavalry_model.bin -t 100 (Loop 100 times to get the average inference time)</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Please make sure build server can conect EVK board, modify the <code>-ip</code> for EVK IP is needed.</dd></dl>
<h2><a class="anchor" id="subsec_eazyai_gsg_basic_acc"></a>
3.2 Basic Accuracy Evaluation</h2>
<ol type="1">
<li>Use <code>eazyai_cfg</code> to generate a correct network yaml configuration file. <div class="fragment"><div class="line">build $ eazyai_cfg -mp model_file    (Generate ea_cvt_cfg.yaml <span class="keyword">using</span> the user interface)</div>
<div class="line">build $ ea_cvt_cfg.yaml</div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: take_name</div>
<div class="line"><a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a>: ./out/onnx/demo_networks</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>: /user_path/model_file</div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path: /user_path/dra_img/</div>
<div class="line">    in_file_ext: jpg / png / ...</div>
<div class="line">    out_shape: N,C,H,W</div>
<div class="line">    out_data_format: uint8</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color: RGB / BGR / ...</div>
<div class="line">   input_nodes:</div>
<div class="line">     images:</div>
<div class="line">       data_prepare: dra_data_1</div>
<div class="line">       mean: xxx,xxx,xxx</div>
<div class="line">       std: xxx,xxx,xxx # (X - mean) / std</div>
<div class="line">output_nodes:</div>
<div class="line">  output:</div>
<div class="line">    data_format: fp32 ( For CV7x, please use FP16 as it does not support FP32)</div>
<div class="line">build $ eazyai_cfg -mp model_file -fs (Generate templates and modify them manually)</div>
</div><!-- fragment --> For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Use <code>eazyai_cvt</code> to convert the nerwotk to Ambarella format, and run layer_compare. <div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac       (With Layer_compare)</div>
<div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac -lc   (Without Layer_compare)</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="a">
<li>For CVflow v3, such as CV7x, please use FP16 for input and output data format, users can modify the input and output data format in <code>ea_cvt_cfg.yaml</code> manually, or use below <code>--adapt_chip / -ac</code> to adapt unsupported parameters. For CVflow v2, user do not need to use it.</li>
<li>The command will generate <code>cflite_cvt_summary.yaml</code> for below inference.</li>
<li>For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
If the accuracy is not so good in the excel file which is generated in above command. Users can modify the DRA parameters to do some fine tuning, or to use a custom sideband file to define the layer format. <div class="fragment"><div class="line">cnngen_convert:</div>
<div class="line">  dra_option:</div>
<div class="line">    version: 2 or 3</div>
<div class="line">    device: -2</div>
<div class="line">    strategy: <span class="keyword">auto</span></div>
<div class="line">    dra_v2:</div>
<div class="line">      coverage_th:</div>
<div class="line">    dra_v3:</div>
<div class="line">      slope_psnr_end:</div>
<div class="line">sideband:</div>
</div><!-- fragment --> For detailed meaning of the parameters, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsubsec_eazyai_python_tools_cfg_adv">2.2.2 Full Convert Parameters</a>.</li>
<li><p class="startli">Use <code>eazyai_inf</code> to run inference to check the real result with JPG / PNG file mode input. </p><div class="fragment"><div class="line">build $ eazyai_inf -cy cflite_cvt_summary.yaml --platform ades                 (Without postprocess and generate binary only)</div>
<div class="line">build $ eazyai_inf -cy cflite_cvt_summary.yaml -iy ea_inf.yaml --platform ades (Without postprocess and generate <span class="keyword">final</span> result)</div>
<div class="line">build $ cat ea_inf.yaml</div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: network_name</div>
<div class="line">data_prepare:</div>
<div class="line">    test_data_1:</div>
<div class="line">         in_path: /user_path/image_folder</div>
<div class="line">         in_file_ext: jpg</div>
<div class="line">input_nodes:</div>
<div class="line">    data:</div>
<div class="line">        data_prepare: test_data_1</div>
<div class="line">post_process:</div>
<div class="line">    <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: <span class="stringliteral">&quot;network_postprocess_name&quot;</span></div>
<div class="line">    lua: /user_path/network.lua</div>
</div><!-- fragment --><p class="startli">Then user can compare its network result between simulator and original framework.</p><ol type="a">
<li>For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a>.</li>
<li>For the differece between Ades and Acinference, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_x86_simulator">10 x86 Simulator</a>.</li>
<li>Use <code>--platform ades / acinf / cvflow / orig</code> to switch the platform, and compare its result<ul>
<li>For <code>ea_inf.yaml</code>, pelase refer to the exmaples in the config folder of every network, such as <code>tensorflow/demo_networks/deeplab_v3/config/ea_inf_tf_deeplab_v3.yaml</code> in CNNGen Samples Package</li>
</ul>
</li>
<li>For <code>--platform cvflow</code>, please run below command first <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Use <code>eazyai_inf_simple_live</code> to run live demo with EVK, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a> for detail.</li>
</ol>
<h2><a class="anchor" id="subsec_eazyai_gsg_full_acc"></a>
3.3 Full Accuracy Evaluation</h2>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac (Convert and generate cflite_cvt_summary.yaml)</div>
<div class="line">build $ eazyai_video -ip 10.0.0.2</div>
<div class="line">build $ eazyai_inf -ip 10.0.0.2 --cy cflite_cvt_summary.yaml -iy ea_acc_inf.yaml --platform cvflow</div>
<div class="line">build $ cat ea_acc_inf.yaml</div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: network_name</div>
<div class="line">data_prepare:</div>
<div class="line">    test_data_1:</div>
<div class="line">        in_path: tensorflow/demo_networks/deeplab_v3/dra_img</div>
<div class="line">        in_file_ext: jpg</div>
<div class="line">input_nodes:</div>
<div class="line">    data:</div>
<div class="line">        data_prepare: test_data_1</div>
<div class="line">post_process:</div>
<div class="line">    <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: <span class="stringliteral">&quot;network_postprocess_name&quot;</span></div>
<div class="line">    lua: /user_path/network.lua</div>
<div class="line">eval_info:</div>
<div class="line">    test_mode: mAP</div>
<div class="line">    dataset: COCO_2014</div>
<div class="line">    classnum: 80</div>
<div class="line">    groundtruth: /user_path/coco_val2014_label.csv</div>
</div><!-- fragment --><p>For detail, please refer to fs_accuracy_tool.</p>
<p>If the accuracy is not OK, please go step 3 again to do DRA fine tuning again, if DRA cannot help anymore, please apply Ambarella quantization tool.</p>
<p>Also if accuracy is OK, but performance is not OK, please apply Ambarella Pruning tools.</p>
<p>For detail, please refer to <a class="el" href="../../d1/d82/fs_workflow.html#sec_model_opt">2 Network Optimization</a>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For <code>ea_acc_inf.yaml</code>, pelase refer to the exmaples in the config folder of every network, such as <code>tensorflow/demo_networks/deeplab_v3/config/ea_inf_acc_tf_deeplab_v3.yaml</code> in CNNGen Samples Package.</li>
<li>There are lots of configuration files for users' reference, please find them in <code>cvflow_cnngen_samples/&lt;framework&gt;/&lt;xxx_networks&gt;/&lt;network_name&gt;/config</code>. Also please refer to the examples in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html">Caffe Demos</a>, <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html">Tensorflow Demos</a>, and <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html">ONNX Demos</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="subsec_eazyai_gsg_efe"></a>
3.4 Efficiency Evaluation</h2>
<p>Profiler tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_profiler">2.7 CVflow Layer Profiler Tool</a> can help user to find the operators which has poor efficiency with CVflow. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac (Convert and generate cflite_cvt_summary.yaml)</div>
<div class="line">build $ eazyai_video -ip 10.0.0.2</div>
<div class="line">build $ eazyai_profiler -cy cflite_cvt_summary.yaml -op &lt;profiler result output path&gt;</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_gsg_mp"></a>
3.5 Product Deployment</h2>
<p>Above are all for debug stage, after everything is OK in above steps, users need write real application to deploy the converted network.</p>
<p>How to transfer debug stage to real development stage?</p><ol type="1">
<li>Get the <code>test_eazyai</code> full command from <code>eazyai_inf</code> or <code>eazyai_inf_simple_live</code>. <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf -cy cflite_cvt_summary.yaml -iy ea_inf.yaml --platform cvflow -gs</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 -cb network_cavalry.bin -pn network_name -pl network.lua -dm 1 -dd STREAM1 -gs</div>
</div><!-- fragment --></li>
<li>Follow the full command and sourece code of <code>test_eazyai</code> to write users' own application.<ol type="a">
<li>Refer to "EazyAI Inference Engine" in <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_modules">2 CFlite Modules</a> for how <code>eazyai_inf</code> calls <code>test_eazyai</code></li>
<li>Refer to below sections for EazyAI APIs and Unit test.<ol type="i">
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
</li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Above debug tools are calling the real C deployed APIs to do the test, so if debug stage does not have problems, there will be no risk when to write users' own application.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngui"></a>
4 CNNGUI</h1>
<p>CNNGUI is a user-friendly interface (UI) to convert and evaluate CNN models. It is located in the folder <em>cvflow_cnngen_samples_&lt;version&gt;/CNNGUI</em>.</p>
<p>In current, only platforms below can be supported, more and more platforms will be added in future.</p>
<ul>
<li>cv2_chestnut</li>
<li>cv22_walnut</li>
<li>cv25_hazelnut</li>
<li>cv25m_pinenut</li>
<li>cv28m_cashewnut</li>
<li>cv5_timn</li>
<li>cv52_crco</li>
<li>cv72_gage</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>In old CV72 SDK, CV72 board is named "cv72_ga", please modify it to "cv72_gage" in below two files.<ul>
<li>If firmware will be rebuilt for the EVK, please modify <code>ambarella/packages/eazyai/unit_test/cnngui/cnngui.json</code>.</li>
<li>If only need to modify directly in EVK, please modify <code>/usr/share/ambarella/eazyai/cnngui.json</code>.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_cnngui_env_setting"></a>
4.1 Environment Setting</h2>
<p>CNNGUI depends on Ambarella CNNGen Toolchain and PyQt5, also Ubuntu 18.04 is recommended.</p>
<ol type="1">
<li>Install Ambarella CNNGen Toolchain, users should install it before using CNNGUI, for installation information, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_install">2 Installation</a>.</li>
<li>Install PyQt5 as below. <pre class="fragment">     Sudo User:
     build $ sudo pip3 install --upgrade pip
     build $ sudo pip3 install PyQt5
     Normal User:
     build $ pip3 install --upgrade pip --user
     build $ pip3 install PyQt5 --user
</pre></li>
<li><p class="startli">Run PyQt5 with debug mode.</p>
<p class="startli">As the PyQt5 may run fail for the lack of some other libraries, users can run PyQt5 with debug mode to debug this problem. </p><pre class="fragment">    build $ export QT_DEBUG_PLUGINS=1
    build $ python3 CNNGUI.py
</pre><p class="startli">The missing libraries will be listed, then use <em>apt-get</em> to install it.</p>
<p class="startli">For example, when there is a lack of <b>libxcb-xinerama.so</b>, users can debug the problem with the steps below.</p>
<p class="startli">a. Run CNNGUI with debug mode, and it shows that "libxcb-xinerama.so" is missing: </p><pre class="fragment">    build $ export QT_DEBUG_PLUGINS=1
    build $ python3 CNNGUI.py

    ...
    Got keys from plugin meta data ("xcb")
    QFactoryLoader::QFactoryLoader() checking directory path "/usr/bin/platforms" ...
    Cannot load library /home/user/.local/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)
    QLibraryPrivate::loadPlugin failed on "/home/user/.local/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so" : "Cannot load library /home/user/.local/lib/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)"
    qt.qpa.plugin: could not load the Qt platform plugin "xcb" in "" even though it was found.
</pre><p class="startli">b. Search the missing library by apt-cache: </p><pre class="fragment">    build $ apt-cache search libxcb-xinerama
    libxcb-xinerama0 - X C Binding, xinerama extension
</pre><p class="startli">c. Install the missing library: </p><pre class="fragment">    build $ sudo apt-get install libxcb-xinerama0
</pre><p class="startli">Below lists some libraries for Ubuntu 1804 server, every user's environment is different, please follow the steps above if there are some problems. </p><pre class="fragment">    build $ sudo apt install libxcb-icccm4
    build $ sudo apt install libxcb-image0
    build $ sudo apt install libxcb-keysyms1
    build $ sudo apt install libxcb-randr0
    build $ sudo apt install libxcb-render-util0
    build $ sudo apt install libxcb-shape0
    build $ sudo apt install libxcb-xkb1
    build $ sudo apt install libxkbcommon-x11-0
    build $ sudo apt-get install libxcb-xinerama0
</pre></li>
<li><p class="startli">Run on a remote host Qt UI.</p>
<p class="startli">When CNNGUI is running on a remote host, users must need to set the remote display enviroment with the command below. </p><pre class="fragment">    build $ export DISPLAY=&lt;IP adress of remote machine&gt;:0.0
</pre></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Users should close the debug model after it works well, or else, there will be some debug messages. <pre class="fragment">    build $ export QT_DEBUG_PLUGINS=0
    build $ python3 CNNGUI.py
</pre></dd></dl>
<h2><a class="anchor" id="sub_sec_cnngui_usage"></a>
4.2 How To Use</h2>
<p>There are six pages in CNNGUI.</p>
<ol type="1">
<li><b>Choose Model</b>: Select CNNGen toolchain version, CNN framework, Model and so on.</li>
<li><b>Port Information</b>: The Network Information for convert, most parameters are generated automatically.</li>
<li><b>Convert Model</b>: Convert Process, Debug and so on.</li>
<li><b>Simulation Tools</b>: Simulate on PC by ADES and Acinference.</li>
<li><b>Deploy Model</b>: Deploy converted model to EVK easily.</li>
<li><b>Accuracy Tools</b>: Check the accurcy of the converted model.</li>
</ol>
<p>Steps to convert and evaluate a NN model by CNNGUI are as below.</p>
<ol type="1">
<li>Specify model framework, model file, model name, and env file in <b>"Choose Model"</b> page.</li>
<li>Set parameters of input and output nodes in <b>"Port Information"</b> page.</li>
<li>Convert model in <b>"Convert Model"</b> page.</li>
<li>Evaluate model on PC in <b>"Simulation Tools"</b> page.</li>
<li>Evaluate model on EVK in <b>"Deploy Model"</b> page.</li>
</ol>
<p>This section provides the mainly usage of each pages. For more details, users can refer to the help information in each component displayed in CNNGUI with <b>moving the mouse to the component</b>.</p>
<h2><a class="anchor" id="sub_sec_cnngui_model_page"></a>
4.3 Choose Model Page</h2>
<p>In this page, users can specify the NN framework, env file, NN model file, and model name.</p>
<div class="image">
<img src="../../cnngui_choose_model_page.jpg" alt=""/>
<div class="caption">
Choose Model</div></div>
   <dl class="section note"><dt>Note</dt><dd><ul>
<li>For NN framework, when Caffe is chosen, "Export Private Env" button will display, which could be used to specify the environment values for user own built Caffe.</li>
<li>Before entering the second page, users can click "Print Graph Summary" button to check the model to see if it is a VP friendly model.</li>
</ul>
</dd></dl>
<p>In addition, users can use two pre-process model tools: graph_surgery and prune_model by clicking the list "Choose Pre-process Model Tool If Needed":</p>
<ul>
<li><p class="startli"><b>Graph Surgery</b></p>
<p class="startli">In this page, users can surgery the model to make it VP friendly.</p>
<div class="image">
<img src="../../cnngui_graph_surgery.jpg" alt=""/>
<div class="caption">
Graph Surgery</div></div>
   <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users need to specify the input option, output nodes, and the output file path. To get the input and output nodes information, click the "Model Info" button, which will dispaly the graph summary.</li>
<li>It is recommended to use "Print Surgeried Model Info" button to check the output model to see if it is surgeried correctly.</li>
<li>CNNGUI will use the default transform method to surgery the model, users can click "More" button to specify other methods.</li>
</ul>
</dd></dl>
</li>
<li><p class="startli"><b>Prune Model</b></p>
<p class="startli">In this page, users can prune the model with the specified sparsity threshold, which means the fraction of non-zero coefficients in each kernel that will be set to 0. For example, “0.8” means 20% density, just left 20% valid weights.</p>
<div class="image">
<img src="../../cnngui_prune_model.jpg" alt=""/>
<div class="caption">
Prune Model</div></div>
   <p class="startli">For more detailed information about these tools, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sub_sec_graph_surgery">13.2 Graph Surgery</a> and sec_deep_learning_dg_prune.</p>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_cnngui_port_page"></a>
4.4 Port Information Page</h2>
<p>In this page, users can set all needed parameters for convert, most parameters are generated automatically.</p>
<div class="image">
<img src="../../cnngui_port_information_page.jpg" alt=""/>
<div class="caption">
Port Information</div></div>
   <dl class="section note"><dt>Note</dt><dd><ul>
<li>It is recommended that clicking "Show All Info" button to check the parameters is correct or not.</li>
<li>The output folder path is specified in "Output Params" tab as well, which will save all of the output files.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_cnngui_convert_page"></a>
4.5 Convert Model Page</h2>
<p>In this page, users can convert and evaluate the NN model by CV toolchain with specified env file and cavalry version.</p>
<div class="image">
<img src="../../cnngui_convert_model_page.jpg" alt=""/>
<div class="caption">
Convert Model</div></div>
   <dl class="section note"><dt>Note</dt><dd>There are three levels for the displayed logs: error, notice, and verbose.<ul>
<li>When "Error" is chosen, it will display the stages of conversion.</li>
<li>When "Notice" is chosen, it will display the stages and commands of conversion.</li>
<li>When "Verbose" is chosen, it will display the stages, commands, and logs of conversion.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_cnngui_simulation_page"></a>
4.6 Simulation Tools page</h2>
<p>In this page, users can simulate the converted model and evaluate the performance on PC.</p>
<ul>
<li>ADES</li>
<li>Acinference</li>
<li>Estimated Performance</li>
</ul>
<div class="image">
<img src="../../cnngui_simulation_tools_page.jpg" alt=""/>
<div class="caption">
Simulation Tools</div></div>
   <dl class="section note"><dt>Note</dt><dd>To simulate on PC, users should install the 3rd party libs first by below commands:<ul>
<li>Install json-c: build $ cd {CNNGen Sample Package}/library/eazyai/3rd_party/ &amp;&amp; ./readme.sh</li>
<li>Install lua and eigen: build $ cd {CNNGen Sample Package}/library/eazyai/unit_test/3rd_party/ &amp;&amp; ./readme.sh</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_cnngui_deploy_page"></a>
4.7 Deploy Model Page</h2>
<p>In this page, users can deploy the converted model to EVK.</p>
<ul>
<li>File Mode</li>
<li>Live Mode</li>
<li>Dummy Mode</li>
<li>Check the EVK and IAV status in the "EVK / IAV" Status window</li>
</ul>
<div class="image">
<img src="../../cnngui_run_on_board_page.jpg" alt=""/>
<div class="caption">
Run on Board</div></div>
   <dl class="section note"><dt>Note</dt><dd><ul>
<li>To evaluate on EVK, users should ensure that the EVK and the host machine are in the same local network for Telnet and FTP connection, then specify the IP adress and port number of EVK and click "Connect" button to build the connection between CNNGUI and the EVK.</li>
<li>"Work Dir" shows the work folder on the EVK which can be specified by users. SD card will be selected by default.</li>
</ul>
</dd></dl>
<p>The <b>"EVK / IAV Status"</b> window is as below.</p>
<p>In this window, users can check the information and status of the EVK, IAV and VP to evaluate the demo's performance, including IAV driver, NNCtrl version, CMA, DRAM, top, dmesg and so on.</p>
<div class="image">
<img src="../../cnngui_iav_status.jpg" alt=""/>
<div class="caption">
IAV / VP Status</div></div>
   <h2><a class="anchor" id="sub_sec_cnngui_accuracy_page"></a>
4.8 Accuracy Tools page</h2>
<p>In this page, users can check the accuracy of the converted model.</p>
<ul>
<li>Cavalry binary versions convert for different SDK version</li>
<li>Compare the EVK result and ADES result, ADES result is generated in "Convert Model" page.</li>
</ul>
<div class="image">
<img src="../../cnngui_accuracy_tools_page.jpg" alt=""/>
<div class="caption">
Accuracy Tools</div></div>
   <h2><a class="anchor" id="sub_sec_cnngui_config_page"></a>
4.9 Import Config and Export Config buttons</h2>
<p>Users can save all the settings and parameters of the conversion to a pkl file by "Export Config" button, and import them by "Import Config" button for easy configuration.</p>
<hr  />
<h1><a class="anchor" id="sec_quick_start"></a>
5 Quick Shell Script</h1>
<p>Locate a quick start script to convert a network in <em>cvflow_cnngen_samples_&lt;version&gt;/quick_start/quick_start&lt;_cvb&gt;.sh</em>. The examples below use pnet as an example. (The bold information below refers to the individual network’s information.)</p>
<h2><a class="anchor" id="sec_quick_start_parser"></a>
5.1 Normal Method</h2>
<pre class="fragment">build $ source path/to/env/cv22.env
build $ export PYTHONPATH=&lt;Full Path&gt;/caffe-install-cpu/usr/local/lib/python3.6/dist-packages:$PYTHONPATH (If needed)
build $ export LD_LIBRARY_PATH=&lt;Full Path&gt;/caffe-install-cpu/usr/local/lib:$LD_LIBRARY_PATH (If needed)
build $ ./quick_start.sh -f caffe -d ../caffe/demo_networks/mtcnn/pnet/dra_img/ -m ../caffe/demo_networks/mtcnn/pnet/models/det1.caffemodel --prototxt ../caffe/demo_networks/mtcnn/pnet/models/det1.prototxt
Network Name:pnet
Please enter the network parameters:
Initial input[0] parameters for '../caffe/demo_networks/mtcnn/pnet/dra_img/'
Input Node Name:data
DRA Images Extension(jpg/bin/...): jpg
Input Channel Number (1/2/3/...): 3
Input Height:22
Input Width:22
Color Format:
----------------------------------
(1) BGR
(*) RGB
----------------------------------
1
Meaning or No Meaning:
----------------------------------
(1) meaning
(*) no meaning
----------------------------------
1
Input Data Meaning (e.g. 100 or 100,100,100):127.5,127.5,127.5
Scale or No Scale:
----------------------------------
(1) Scale
(*) no Scale
----------------------------------
1
Input Data Scale (e.g. 0.0078125 or 0.017124,0.017507,0.017429):0.0078125
Transpose or No Transpose:
----------------------------------
(1) Transpose
(*) no Transpose
----------------------------------
1
Input Transpose (0,1,3,2):0,1,3,2
gen_image_list.py -f ../caffe/demo_networks/mtcnn/pnet/dra_img/ -o out_pnet/dra_image_bin0/img_dra_list0.txt -ns -e jpg -c 1 -d 0,0 -r 22,22 -bf out_pnet/dra_image_bin0 -bo out_pnet/dra_image_bin0/dra_bin_list0.txt
There are [1] inputs for this network.
DRA Strategy:
----------------------------------
(1) Fix8 (Best Performance)
(2) Fix16 (Best Accuracy)
(*) Default (Balance between Performance and Accuracy)
----------------------------------
1
Output Data Format(FP32 is not supported in CV7x):
----------------------------------
(0) FP32
(1) FP16
(2) Manually for Each
(*) Auto (Fixed 8 or 16)
----------------------------------
1
Output Number:2
Output Name:conv4-2
Transpose or No Transpose:
----------------------------------
(1) Transpose
(*) no Transpose
----------------------------------
z
Reshape or No Reshape:
----------------------------------
(1) Reshape
(*) no Reshape
----------------------------------
z
Output Name:prob1
Transpose or No Transpose:
----------------------------------
(1) Transpose
(*) no Transpose
----------------------------------
z
Reshape or No Reshape:
----------------------------------
(1) Reshape
(*) no Reshape
----------------------------------
z
caffeparser.py -p ../caffe/demo_networks/mtcnn/pnet/models/det1.prototxt -m ../caffe/demo_networks/mtcnn/pnet/models/det1.caffemodel -o pnet -of out_pnet/out_pnet_parser -isrc "i:data=out_pnet/dra_image_bin0/dra_bin_list0.txt|is:1,3,22,22|idf:0,0,0,0|iq|im:127.5,127.5,127.5|ic:128.000000|it:0,1,3,2" -c act-force-fx8,coeff-force-fx8 -odst "o:conv4-2|odf:fp16" -odst "o:prob1|odf:fp16" -dinf cerr
vas -auto -show-progress pnet.vas
ades_autogen.py -v pnet -p out_pnet/out_pnet_parser -l out_pnet/ades_pnet/ -ib data=$(cat out_pnet/dra_image_bin0/dra_bin_list0.txt | head -1)
Run Ades
----------------------------------
(1) yes
(*) no
----------------------------------
1
python3 $(tv2 -which run_ades_lite.py) -v pnet -p out_pnet/out_pnet_parser -ib data=$(cat out_pnet/dra_image_bin0/dra_bin_list0.txt | head -1)
Run layer_compare.py:
----------------------------------
(1) yes
(*) no
----------------------------------
1
Enable debugging option for layer_compare.py:
----------------------------------
(1) yes
(*) no
----------------------------------
z
layer_compare.py caffe -p ../caffe/demo_networks/mtcnn/pnet/models/det1.prototxt -m ../caffe/demo_networks/mtcnn/pnet/models/det1.caffemodel -isrc "i:data=out_pnet/dra_image_bin0/dra_bin_list0.txt|is:1,3,22,22|idf:0,0,0,0|iq|im:127.5,127.5,127.5|ic:128.000000|it:0,1,3,2" -v out_pnet/out_pnet_parser -o out_pnet/layer_compare_pnet/layer_compare_pnet -d 0
Run cavalry_gen
----------------------------------
(1) yes
(*) no
----------------------------------
1
cavalry_gen -d out_pnet/out_pnet_parser/vas_output/ -f out_pnet/cavalry_pnet/cavalry_pnet.bin -p out_pnet/ -v &gt; out_pnet/cavalry_pnet/cavalry_info.txt
Run cvflow_perf_eval
----------------------------------
(1) yes
(*) no
----------------------------------
1
[run_pnet_tools/cvflow_perf_eval.py] has been updated from [../tools/cvflow_perf_eval/cvflow_perf_eval.py]!
python3 run_pnet_tools/cvflow_perf_eval.py -f out_pnet/out_pnet_parser/vas_output
Run Or Quit to check 'run_pnet.sh'
----------------------------------
(1) Run
(*) Quit
----------------------------------
1
</pre><p>Then, it generates a script called <em>run_pnet.sh</em> and a tool folder <em>run_pnet_tools</em>, which should be shown as follows: </p><pre class="fragment">run_pnet.sh
run_pnet_tools
|-- cvflow_perf_eval.py (Script to run cvflow performance evaluation, will be updated if "--perf_eval_py" is specified)
</pre><p>Modify <em>run_pnet.sh</em> if there are any bugs. After running it, the folder below will be generated: </p><pre class="fragment">out_pnet/
|-- ades_pnet (Ades Files)
|-- cavalry_pnet (Cavalry Files)
|-- dra_image_bin0 (DRA binary)
|-- layer_compare_pnet (Layer Compare Result)
|-- out_pnet_parser (Parser output and Vas output)
|-- parse (Temp files)
</pre><h2><a class="anchor" id="sec_quick_start_cvb"></a>
5.2 CVFlowBackend</h2>
<pre class="fragment">build $ source path/to/env/cv22.env
build $ export PYTHONPATH=&lt;Full Path&gt;/caffe-install-cpu/usr/local/lib/python3.6/dist-packages:$PYTHONPATH (If needed)
build $ export LD_LIBRARY_PATH=&lt;Full Path&gt;/caffe-install-cpu/usr/local/lib:$LD_LIBRARY_PATH (If needed)
build $ ./quick_start_cvb.sh -f caffe -d ../caffe/test_networks/mobilenet_v1/dra_img/ -m ../caffe/test_networks/mobilenet_v1/models/mobilenet.caffemodel --prototxt ../caffe/test_networks/mobilenet_v1/models/mobilenet_deploy.prototxt --gen_graph_py ../build/bin/gen_graph_spec.py --perf_eval_py ../tools/cvflow_perf_eval/cvflow_perf_eval.py
FrameWork is [caffe]!
DRA image folder is [../caffe/test_networks/mobilenet_v1/dra_img/]!
Model File is [../caffe/test_networks/mobilenet_v1/models/mobilenet.caffemodel]!
Caffe Prototxt File is [../caffe/test_networks/mobilenet_v1/models/mobilenet_deploy.prototxt]!
Path to [gen_graph_spec.py] is [../build/bin/gen_graph_spec.py]!
Path to [cvflow_perf_eval.py] is [../tools/cvflow_perf_eval/cvflow_perf_eval.py]!
Network Name:mobilenet_v1
[run_mobilenet_v1_tools/gen_graph_spec.py] has been updated from [../build/bin/gen_graph_spec.py]!
Please enter the network parameters:
Initial input[0] parameters for '../caffe/test_networks/mobilenet_v1/dra_img/'
Input Node Name:data
DRA Images Extension(jpg/bin/...): jpg
Input Channel Number (1/2/3/...): 3
Input Height:224
Input Width:224
Color Format:
----------------------------------
(1) BGR
(*) RGB
----------------------------------
1
Meaning or No Meaning:
----------------------------------
(1) meaning
(*) no meaning
----------------------------------
1
Input Data Meaning (e.g. 100 or 100,100,100):103.94,116.78,123.68
Scale or No Scale:
----------------------------------
(1) Scale
(*) no Scale
----------------------------------
1
Input Data Scale (e.g. 0.0078125 or 0.017124,0.017507,0.017429):0.017
Transpose or No Transpose:
----------------------------------
(1) Transpose
(*) no Transpose
----------------------------------
z
gen_image_list.py -f ../caffe/test_networks/mobilenet_v1/dra_img/ -o out_mobilenet_v1/dra_image_bin0/img_dra_list0.txt -ns -e jpg -c 1 -d 0,0 -r 224,224 -bf out_mobilenet_v1/dra_image_bin0 -bo out_mobilenet_v1/dra_image_bin0/dra_bin_list0.txt
There are [1] inputs for this network.
DRA Strategy:
----------------------------------
(1) Fix8 (Best Performance)
(2) Fix16 (Best Accuracy)
(*) Default (Balance between Performance and Accuracy)
----------------------------------
1
Output Data Format(FP32 is not supported in CV7x):
----------------------------------
(0) FP32
(1) FP16
(2) Manually for Each
(*) Auto (Fixed 8 or 16)
----------------------------------
1
Output Number:1
Output Name:fc7
Transpose or No Transpose:
----------------------------------
(1) Transpose
(*) no Transpose
----------------------------------
z
Reshape or No Reshape:
----------------------------------
(1) Reshape
(*) no Reshape
----------------------------------
z

[mobilenet_v1_graph_spec.json] for CVFlowBackend is generated in directory [run_mobilenet_v1_tools]!

python3 $(tv2 -binpath cvflowbackend)/prepare.py ${OPSET_VERSION} -fw caffe -m ../caffe/test_networks/mobilenet_v1/models/mobilenet_deploy.prototxt -w ../caffe/test_networks/mobilenet_v1/models/mobilenet.caffemodel -g run_mobilenet_v1_tools/mobilenet_v1_graph_spec.json -type checkpoint -o mobilenet_v1 -of out_mobilenet_v1/out_mobilenet_v1_prepare --log_dir out_mobilenet_v1/out_mobilenet_v1_prepare/logs
Run Ades
----------------------------------
(1) yes
(*) no
----------------------------------
1
python3 $(tv2 -binpath cvflowbackend)/evaluate.py -m out_mobilenet_v1/out_mobilenet_v1_prepare/mobilenet_v1.ambapb.ckpt.onnx -em ades -i data=$(cat out_mobilenet_v1/dra_image_bin0/dra_bin_list0.txt | head -1) -of out_mobilenet_v1/ades_mobilenet_v1 -da
Run layer_compare.py:
----------------------------------
(1) yes
(*) no
----------------------------------
1
Enable debugging option for layer_compare.py:
----------------------------------
(1) yes
(*) no
----------------------------------
z
layer_compare.py caffe -p ../caffe/test_networks/mobilenet_v1/models/mobilenet_deploy.prototxt -m ../caffe/test_networks/mobilenet_v1/models/mobilenet.caffemodel -isrc "i:data=/lhome/wliu/daily/workspace/packages/cnngen_ambsh/cnngen/quick_start/out_mobilenet_v1/dra_image_bin0/dra_bin_list0.txt" -v out_mobilenet_v1/out_mobilenet_v1_prepare/mobilenet_v1.ambapb.ckpt.onnx -o out_mobilenet_v1/layer_compare_mobilenet_v1/layer_compare_mobilenet_v1 -d 0
Run cavalry_gen
----------------------------------
(1) yes
(*) no
----------------------------------
1
python3 $(tv2 -binpath cvflowbackend)/deconstruct.py -m out_mobilenet_v1/out_mobilenet_v1_prepare/mobilenet_v1.ambapb.ckpt.onnx -of out_mobilenet_v1/parse -sv ${SCHEMA_VERSION}
cavalry_gen -j out_mobilenet_v1/parse/manifest.json -f out_mobilenet_v1/cavalry_mobilenet_v1/cavalry_mobilenet_v1.bin -v &gt; out_mobilenet_v1/cavalry_mobilenet_v1/cavalry_info.txt
Run cvflow_perf_eval
----------------------------------
(1) yes
(*) no
----------------------------------
1
[run_mobilenet_v1_tools/cvflow_perf_eval.py] has been updated from [../tools/cvflow_perf_eval/cvflow_perf_eval.py]!
python3 run_mobilenet_v1_tools/cvflow_perf_eval.py -f out_mobilenet_v1/out_mobilenet_v1_prepare/cnngen_out/mobilenet_v1/${PARSER_OUTPUT_PATH}/vas_output
Run Or Quit to check 'run_mobilenet_v1.sh'
----------------------------------
(1) Run
(*) Quit
----------------------------------
1
</pre><p>Then, it generates a script called <em>run_mobilenet_v1.sh</em> and a tool folder <em>run_mobilenet_v1_tools</em>, which should be shown as follows: </p><pre class="fragment">run_mobilenet_v1.sh
run_mobilenet_v1_tools
|-- cvflow_perf_eval.py (Script to run cvflow performance evaluation, will be updated if "--perf_eval_py" is specified)
|-- mobilenet_v1_graph_spec.json (Generated MetaGraph Descriptor Json file, users can modify it directly if needed)
</pre><p>MetaGraph Descriptor Json file <em>mobilenet_v1_graph_spec.json</em> in <em>run_mobilenet_v1_tools</em> will be used as a config file in the script <em>run_mobilenet_v1.sh</em>. Modify the script and the Json file if needed (Bugs or Different Params). After running the script, the folder below will be generated: </p><pre class="fragment">out_mobilenet_v1/
|-- ades_mobilenet_v1 (Ades Files)
|-- cavalry_mobilenet_v1 (Cavalry Files)
|-- dra_image_bin0 (DRA binary Files)
|-- layer_compare_mobilenet_v1 (Layer Compare Result)
|-- out_mobilenet_v1_prepare (Output of Prepare Stage of CVFlowBackend)
|-- parse (Temp files)
</pre><dl class="section note"><dt>Note</dt><dd>FP32 is not supported for input and output in CV72, please use fp16 instead. When to deploy it with EazyAI, the inference API will convert FP16 to FP32 in default with ARM CPU.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_simple_tools"></a>
6 Simple Tools</h1>
<p>Some C tools are located in the folder <code>cvflow_cnngen_samples_&lt;version&gt;/tools/</code>.</p>
<p>Also there are some pytools in CVflow Python Library. For these pytools, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_simple">2.8 Simple PyTools</a>.</p>
<h2><a class="anchor" id="sub_sec_sample_tool_imx2bin"></a>
6.1 im2bin</h2>
<p>This tool is used to process the input for Cavalry and ADES and is located in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/im2bin</em>.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This tool converts the image to a 3 RBG channels binary or one data channel, such as LeNet.</li>
<li>For the LeNet model training method, users must normalize the scale. For example, (0~255) to (-1~1).</li>
<li>For Cavalry that will run on a board, users must align the board with 32 btypes. This is not necessary for ADES.</li>
<li>Users must ensure that the data format is consistent with the definitions listed in "-iq -idf".</li>
<li>This tool uses the <b>OpenCV</b> interface. If this tool is required, install the package using the following command: <pre class="fragment">  build $ sudo apt-get install libopencv-dev
</pre></li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_sample_tool_bin2val"></a>
6.2 bin2val</h2>
<p>This tool uses the opposite process as the <em>im2bin</em> tool. As <em>im2bin</em> converts the image to the network input, this tool converts the input or output back to the image. It can be found in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/bin2val</em>.</p>
<h2><a class="anchor" id="sub_sec_sample_tool_bincmp"></a>
6.3 bincmp</h2>
<p>This tool can be used to compare the outputs in ADES, Cavalry, Caffe, and TensorFlow. It can be found in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/bincmp</em>.</p>
<h2><a class="anchor" id="sub_sec_sample_tool_get_mean"></a>
6.4 get_mean</h2>
<p>This is the demonstration code used to obtain the mean value from Caffe files. It can be found in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/get_mean</em>.</p>
<h2><a class="anchor" id="sub_sec_sample_tool_fp32_to_fp16"></a>
6.5 fp32_2_fp16_convert</h2>
<p>This tool converts the submean file from FP32 to FP16 and can be found in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/fp32_2_fp16_convert</em>.</p>
<h2><a class="anchor" id="sub_sec_sample_tool_gen_fp16_mean"></a>
6.6 gen_fp16_mean</h2>
<p>This tool is used to generate the FP16 file. It can be found in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/gen_fp16_mean</em>.</p>
<h2><a class="anchor" id="sub_sec_cvflow_perf_eval"></a>
6.7 cvflow_perf_eval.py</h2>
<p>This tool is used to evaluate the performance based on generated VAS output.</p>
<p>The tool is placed in: <em>cvflow_cnngen_samples_&lt;version&gt;/tools/cvflow_perf_eval</em>.</p>
<p>The following table lists the detailed usage of the cvflow_perf_eval.py.</p>
<a class="anchor" id="Detailed Usage of cvflow_perf_eval"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Options </th><th>Description </th></tr>
<tr align="middle">
<td>-h, "--help" </td><td>Shows the help message. </td></tr>
<tr align="middle">
<td>-f, "--vasoutput" </td><td>Indispensible. Specify the path of VAS output folder </td></tr>
<tr align="middle">
<td>-c, "--clock" </td><td>Optional. Specify the VP clock (MHz) if needed. </td></tr>
</table>
<p>Example: </p><pre class="fragment">    build $ python3 cvflow_perf_eval.py -f out_parser/vas_output -c 1008
</pre><dl class="section note"><dt>Note</dt><dd>The estimated performance is based on ICE cycle count, which does not included some system status and does not include all the operators in the network. For some special networks, it will not so accurate. </dd></dl>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaigen-layercompare_html_ga4d5bb0c360b13429f65cd327c8d0aa12"><div class="ttname"><a href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a></div><div class="ttdeci">model_path</div></div>
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga81f22c9cd9a33cc05e5a1657974438bd"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a></div><div class="ttdeci">work_dir</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga8179f95715172cfcd3a44cd038a81a9f"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a></div><div class="ttdeci">transforms</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
