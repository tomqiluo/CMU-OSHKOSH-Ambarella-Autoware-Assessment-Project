<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: CNNGen Toolkits</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d2/d67/fs_cnngen.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">CNNGen Toolkits </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the environment settings and tools required for running CNNGen.</p>
<hr  />
<h1><a class="anchor" id="sec_cnngen_interface"></a>
1 Interface</h1>
<p>The following illustrates the CNNGen interface.</p>
<div class="image">
<img src="../../cnngen_interface.jpg" alt=""/>
<div class="caption">
CNNGen Interface</div></div>
   <hr  />
<h1><a class="anchor" id="sec_cnngen_install"></a>
2 Installation</h1>
<p>For installation information, refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_install">1.3 Installation</a>, or the <em>readme.txt</em> file in CNNGen toolchain package.</p>
<p>Also user can install it with the Docker file in CNNGen toolchain package.</p>
<hr  />
<h1><a class="anchor" id="sec_cnngen_env_set"></a>
3 Environment Setting</h1>
<p>Before using the CNNGen toolchain, please source the CNNGen toolchain EVN file to set up the system environment.</p>
<div class="fragment"><div class="line">build $ source /usr/local/amba-cv-tools-&lt;version&gt;.ubuntu-20.04/env/cv72.env</div>
</div><!-- fragment --><p>For lots of Caffe networks, official Caffe can not support lots of special layers, such as permite in SSD, reorg for Yolov2, and so on. Then users need to using a special Caffe, also they can use AmbaCaffe which provided in CNNGen 2.x.x toolchain.</p>
<div class="fragment"><div class="line">build $ source /usr/local/amba-cv-tools-&lt;version&gt;.ubuntu-20.04/env/cv*.env</div>
<div class="line">build $ export PYTHONPATH=&lt;Own Folder&gt;/caffe-install/usr/local/lib/python3.8/dist-packages:$PYTHONPATH  (Please correct the &lt;Own Folder&gt;)</div>
<div class="line">build $ export LD_LIBRARY_PATH=&lt;Own Folder&gt;/caffe-install/usr/local/lib:$LD_LIBRARY_PATH  (Please correct the &lt;Own Folder&gt;)</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>As Caffe is rarely used for current new networks, Since Ubuntu2004, CNNGen toolchain will not provide AmbaCaffe, users can find it in old CNNGen toolchain package.</li>
<li>To avoid redirecting the tool to the location of Caffe multiple times, build it, install it to <em>/usr/local/</em>, or add the export command to <em>.bashrc</em>. Please contact the Ambarella support team for more details.</li>
</ol>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_data_format"></a>
4 Data Format</h1>
<p>All elements in a vector share the same data format, no matter for input, coefficients, and output. The supported data types are signed or unsigned, 8, 16 or 32 bits&#160;wide, and fixed-point or floating-point.</p>
<p>The data_format parameter defines the signedness, element size, and precision for each vector which is as follows.</p>
<ul>
<li>1st argument: Specifies signed/unsigned, 0 is unsigned, 1 is signed</li>
<li>2nd argument: Specifies the width of the data elements (0 = 8-bit, 1 = 16-bit, and 2 = 32-bit)</li>
<li>3rd argument: Specifies the exponent offset<ul>
<li>Fixed-point, shifts the binary point to the right by expoffset (i.e, multiplies value by 2-expoffset), maximum range is [0, 15].</li>
<li>Float-point, applies a signed 2’s complement offset to the exponent, maximum range is [-8, 7].</li>
</ul>
</li>
<li>4th argument: Specifies the value to determine the number of bits reserved in a data element for the exponent field, maximum value is 7.<ul>
<li>A value of 0 indicates fixed-point numbers.</li>
<li>If greater than 0, the number of exponent bits is (expbits + 1), 4 means FP16, 7 means fp32.</li>
<li>Integer format is supported as a special case of fixed-point numbers where the expoffset and expbits fields are both 0.</li>
</ul>
</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>As fix point is always used for coefficients, so the 4th argument is always 0.</li>
<li>Output is always set to FP32, it should be [1, 2, 0, 7]. Of course, users can use FP16, it should be (1, 1, 0 ,4), which is equal with FP16 which is defined in IEEE-754.</li>
<li>Input data can be any format, but even users used FP16 or FP32, the parser will convert it to fix point first.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_in_data_format"></a>
4.1 Input Data Format</h2>
<p>It is important to pre-process the following data input while using CNN:</p>
<ul>
<li>Data type, including YUV, RGB, BGR, or others</li>
<li>Input dimension channel, width, and height</li>
<li>Meaning</li>
<li>Scale</li>
</ul>
<p>By default, the input data type and dimension conversion are not included in network model. The meaning and scale are performed in the network, so the input range is always Fixed-8 RGB or BGR.</p>
<p>Although Ambarella recommends using this method, users can opt to perform the meaning and scale before the network, but must use the correct data range for the parser’s input.</p>
<p>The following section provides information on choosing the data format for input used by the DRA and deployment (using <b>Mobilenet</b> as an example).</p>
<ol type="1">
<li>Identify the pre-process input for the network while training. <pre class="fragment"> Input_Data_Range: [0, 256)
 IN_MEAN: [103.94,116.78,123.68]
 Data_Range_After_MEAN: [-123.68, 151.06)
 IN_SCALE=0.017
 Data_Range_After_Scale: [-2.10256, 2.56802)
</pre></li>
<li>Ensure that the input data format fully covers the <b>Data_Range_After_Scale</b> above. <pre class="fragment"> a. ufix8_8 (0,0,8,0)  =&gt;  [0,2^(8-0)/2^8 )=[0,1) =&gt; Smaller than the Data_Range_After_Scale
 =&gt;  Can’t fully cover the Data_Range_After_Scale, result accuracy reduced
 b. sfix8_8 (1,0,8,0)  =&gt;  [-2^(8-1)/2^8 ,2^(8-1)/2^8 )=[-0.5,0.5)
 =&gt; Smaller than the Data_Range_After_Scale Scale  =&gt;  Can’t fully cover the Data_Range_After_Scale, result accuracy reduced
 c. sfix8_5 (1,0,5,0)  =&gt;  [-2^(8-1)/2^5 ,2^(8-1)/2^5 )=[-4,4)
 =&gt; Equal or larger than the Data_Range_After_Scale, fully cover the Data_Range_After_Scale
 d. sfix16_5 (1,1,5,0)  =&gt;  [-2^(16-1)/2^5 ,2^(16-1)/2^5 )=[-1024,1024)
 =&gt; Equal or larger than the Data_Range_After_Scale, fully cover the Data_Range_After_Scale
</pre></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>For the above example, <em>ufix8_8</em> and <em>sfix8_8</em> are not adequate; Ambarella recommends using <em>sfix8_5</em> or <em>sfix16_5</em> instead. Additionally, Ambarella recommends using <em>sfix16_5</em> instead of <em>sfix8_5</em>, as it can retain more data range. For details, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_data_format">4 Data Format</a>.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_dra_list_gen"></a>
5 DRA List Generation</h1>
<p>The tool gen_image_list.py performs default quantization list generation. For details, refer to the <em>Ambarella CV UG Generate Image List</em> in CNNGen toolchain package.</p>
<p>The following two examples are provided as a guide for generating the DRA list.</p>
<h4><a class="anchor" id="autotoc_md50"></a>
Example 1: Convert images to binaries and generate the DRA list.</h4>
<pre class="fragment">build $ gen_image_list.py -f test_image/ -o img_dra_list.txt -ns -e jpg -c 0 -d 0,0 -r 608,608 -bf dra_bin/ -bo dra_list.txt
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li><code>gen_image_list.py</code> can perform the resize and YUV / RGB / BGR conversion, and also can perform meaning and scale which is rarely used. As a result, it generates the DRA binary in fix8 (0,0,0,0) (as fix16 and float16 are rarely used). For the example shown above, it converts the images in the <code>test_image</code> folder to RGB and resizes them to 3x608x608. Then, it saves the binaries in <b>dra_bin</b> folder with the result list file <b>dra_list.txt</b>.</li>
</ul>
</dd></dl>
<h4><a class="anchor" id="autotoc_md51"></a>
Examples 2: Generate the DRA list with binary.</h4>
<pre class="fragment">build $ gen_image_list.py -f test-images/bin8_mean128/ -o dra.txt -e bin -N 1000
</pre><p>Mean value reduction and normalization are not supported in <em>gen_image_list.py</em>. If the user does not want to include meaning and scale in the parser, use <em>im2bin</em> to generate the DRA with resize and RGB / BGR conversion. Meaning and scale must be processed using a different network request, which is in the CNNGen Sample Package. For details, refer to Section <a class="el" href="../../da/dc5/fs_quick_start.html#sub_sec_sample_tool_imx2bin">6.1 im2bin</a>.</p>
<p>Users must choose a suitable input data range for the parser that can cover all the input data range with fix8 or fix16. For details, refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sub_sec_in_data_format">4.1 Input Data Format</a>.</p>
<p>The DRA is used by the parser to determine the data format by date distribution, which is calculated by the sample images provided by the designer.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The files should be in the binary with the YUV, RGB, or BGR format.</li>
<li>One image is in one binary.</li>
<li>The binaries’ data format should follow the definition of the parser after converting it via <em>gen_image_list.py</em> or <em>im2bin</em>.</li>
<li>The parser calculates the layer range of each image to help convert FP32 to the FX16 or FX8 format.</li>
<li>The final format should match the definition in option "-iq -idf" of parser.</li>
<li>This tool is related with third-party libraries, such as "python_opencv", "scikit-image" and "numpy". Note that if these libraries' versions are different with user's training environment, it will bring some accuracy gap.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_parser"></a>
6 Parsers</h1>
<p>Currently, the Ambarella toolchain supports the* caffeparser.py*, <em>tfparser.py</em>, and <em>onnxparser.p*y parsers. For details, refer to *Ambarella CV UG Parser Interface</em> in the CNNGen toolchain package.</p>
<p>Suggested parser usage is as follows:</p>
<ol type="1">
<li>Best performance: -c act-force-fx8,coeff-force-fx8</li>
<li>Best Accuracy:<ul>
<li>CVflowv2 of CV2x and CV5x: -c act-force-fx16,coeff-force-fx16</li>
<li>CVflowv3 of CV7x and CV3x: -c act-force-fp16,coeff-force-fp16</li>
</ul>
</li>
<li>Balance A and B: (null)</li>
<li>Only force act: -c act-force-fx8</li>
<li>Only force coeff: -c coeff-force-fx8</li>
<li>Mixed act and coeff: -c act-force-fx8,coeff-force-fx16</li>
</ol>
<p>For JSON pre-processing, users can refer to the <em>Ambarella-CV</em>-UG-Pre_and_Post-Processing_JSON* in the CNNGen toolchain package. There are also some examples shown in Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_mobilenetv1">Mobilenet Runtime Rotation</a>, Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_cgpp_deployment_for_yolov3">CGPP Deployment for Yolov3</a> and Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_resnet50_with_json_preprocessing">ResNet50 with Json Pre-Processing</a>.</p>
<p><b>DRAv1</b> is only for 8-bit quantization loss control.</p>
<p><b>DRAv2</b> advantages:</p>
<ul>
<li>Achieves a better network accuracy with minimal impact on the performance</li>
<li>Provides pro-accuracy and pro-performance choices</li>
<li>If the majority of the activation data spans beyond the 8-bit dynamic range, DRAv2 uses 16 bits to quantize the data</li>
<li>DRAv2 determines coverage of the 8-bit data format by computing the maximum area of the histogram that is covered by 8 bins on a log-2 histogram</li>
</ul>
<p><b>DRAv3</b> advantages (enable by "-dra mode=3 -gpu"):</p>
<ul>
<li>The main idea behind the Modes 1 and 2 is to limit data overflow / underflow introduced by quantization at each quantization boundary. This is achieved by observing the distribution of float-point data at the quantization boundaries. However, this may introduce unnecessary precision up-conversions, or incorrectly allocate precision in the earlier part of the network, because it does not take the effect of the cumulative quantization error into account.</li>
<li>Compared to the Modes 1 and 2, Mode 3 is based on cumulative quantization. For each frame of the sample input, it tests several quantization strategies for each quantization boundary, and then records the numerical accuracy of each strategy and applies the best one. Then, the quantized tensor is used for the input in the latter part of the network. After processing all the frames, the quantization strategy that achieves the best overall numerical accuracy is picked out at each quantization boundary.</li>
<li>As DRAv3 is more complex and slow with CPU, it can be supplemented with the GPU support.</li>
<li>For a detailed example, refer to Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_resnet50_with_drav3">ResNet50 with DRAv3</a>.</li>
<li>Ambarella suggests using FP32 for the output to easier parsing.</li>
<li>Ambarella suggests converting the width x height as 1x10 to 10x1, because the CV chip DRAM will align with 32 bytes. If the output is 1x10 and format is fix 8, the total output size is 32x10 bytes; however, if 10x1 is used and the format is fix 8, the output size will only be 32 bytes.</li>
<li>If users encounter a problem, Ambarella suggests using "-dinf ceff" for the first debug. This generates a log called <em>frame_000000_cumulative_err.txt</em>. Then, users can check the <em>psnr</em> value to see if there is a big gap in the continuous DAGs.</li>
<li>For the sideband file, refer to the <em>Ambarella CV UG Sideband Usage</em> and the A*mbarella CV UG Layer Level Sideband* in the CNNGen toolchain package.</li>
</ul>
<h2><a class="anchor" id="sub_sec_dra_ver_history"></a>
6.1 DRA Version History</h2>
<p>In general, DRA V1 is the simplest algorithm. In the simplest setting (<em>type=minmax</em>), it chooses a fixed-point quantization that can hold the minimum and maximum values of all the activation values it sees. In the advanced mode (<em>type=histo</em>), it collects a linear-scale histogram on the activation data, and then estimates a quantization loss for three 8-bit quantization settings. Finally, it selects the setting which will minimize quantization loss.</p>
<ul>
<li>The loss function could be selected from MSE, ABSERR and RELERR. The default setting is MSE because MSE yields the best accuracy from the testing experience.</li>
<li>The configuration key <em>histo-extra</em> is the coverage threshold feature retrofitted from DRA V2.</li>
</ul>
<p>DRA V2 improves on DRA V1 by introducing the dual histogram to automatically determine if an activation tensor within the 8-bit value range should be upgraded to use 16 bit. This is because 8-bit quantization could introduce an excessive underflow. If a certain portion of the activation data can be represented in 8-bit without overflowing or underflowing, the parser will choose 8-bit quantization. Otherwise, it will switch to 16 bits.</p>
<ul>
<li>The configuration key <em>coverage_th</em> controls the threshold of the coverage portion. The default value is 0.9.</li>
<li>The configuration key <em>coverage_recover_th</em> works like a safety net by evaluating the range that can be covered by the 8-bit data format to minimize MSE. By default, this value is 0, but users are recommended to set it to 0.5 if the DRA V2 accuracy is insufficient.</li>
</ul>
<p>DRA V2 focuses on the hardware-related quantization details, such as special data format constraints required by ICE hardware. DRA V2 also introduces scaling, which automatically scales large or small values out of the 8-bit quantization range to fit within the 8-bit range for activation tensors.</p>
<ul>
<li>This is controlled by the <em>allow_scaling</em> flag. This feature allows more computation to use 8 bits, providing the performance enhancements. However, sometimes users find that allowing more 8-bit computation, the accuracy drops. Users can turn off this feature to keep the scaled data in 16-bit format.</li>
</ul>
<p>The following options are also provided:</p>
<ul>
<li>The <em>no-opt</em> flag is mostly for debugging purposes. <em>no-opt</em> ensures that each layer's output is preserved, and users can dump all the intermediate results in the simulator. However, <em>no-opt</em> may also result in sub-optimal accuracy and deployment performance.</li>
</ul>
<p>DRA V1 and V2 could also benefits from GPU, as the underlying math operations are shared by all DRA methods. Because DRA V1 is not as robust as V2, and could generate underflow and overflow in many cases when quantization is performed. For this reason, it is not recommended that users focus primarily on DRA V1.</p>
<p>DRA V3 focuses on hardware-accurate simulation. It scans through many quantization plans for each operation, selecting the plan that uses less bits, but still ensures the final target accuracy (described by either the peak signal to noise ratio or Pearson correlation coefficient). As DRAv3 is more complex which has large amount of computation, users need to enable GPU to speed up the calculation process, such as Yolov3_Tiny example below with CNNGen Samples Package.</p>
<ul>
<li>Use GPU which only supports <b>Cuda 9.2</b> and <b>Cuda 10.2</b> for CV2x and CV5x, <b>Cuda 11.4</b> for CV72. <pre class="fragment">  build $ source build/env/cv22.env
  build $ make tf_tiny_yolov3 dra="-dra mode=3 -gpu -gpu_id 1"
</pre></li>
<li>Use CPU <pre class="fragment">  build $ source build/env/cv22.env
  build $ make tf_tiny_yolov3 dra="-dra mode=3"
</pre></li>
</ul>
<p>For more details, refer to the documents in <em>CNNGen Toolchain package</em> in the future.</p>
<hr  />
<h1><a class="anchor" id="sec_cnngen_caffe_parser"></a>
7 Caffe Parser</h1>
<p>The <em>caffeparser.py</em> converts a Caffe framework model to Ambarella’s format. For details, refer to the <em>Ambarella CV UG Caffe Parser</em> in CNNGen toolchain package.</p>
<dl class="section note"><dt>Note</dt><dd>Before using a layer, ensure that the layer type or format in <em>prototxt</em> is in the official Caffe format, and modify the number of data augmentation to 1 in <em>input_shape</em>.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_tf_parser"></a>
8 TensorFlow Parser</h1>
<p>The <em>tfparser.py</em> converts a TensorFlow’s frameworks model to Ambarella’s format. For details, please refer to <em>Ambarella CV UG TensorFlow Parser</em> in the CNNGen toolchain package.</p>
<p>Typically, the output of the TensorFlow training phase is a set of checkpoints [<em>.ckpt.data-, .ckpt.index, .ckpt.meta, checkpoint</em>] which must be converted to a protobuf, and further frozen.</p>
<p>For further details, refer to the document <em>Ambarella CV DDG TensorFlow Parser</em> and the following sections.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>A model is frozen to convert variables into constants.</li>
<li>To deploy it in production, it is better to optimize away batch normalization or other training-only features.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_known_error"></a>
8.1 Known Error in checkpoint_to_pb.py</h2>
<p>When using <em>checkpoint_to_pb.py</em>, the following TensorFlow error message can appear: </p><pre class="fragment">KeyError: 'InfeedEnqueueTuple'
</pre><p>This error occurs because the <em>InfeedEnqueueTuple</em> operation can only be used on a specific tensor processing unit (TPU). It should not be loaded or supported if the TPUs are not available. To avoid this error, perform the following steps:</p>
<ul>
<li>Download the trained model.<ul>
<li>Download the trained <b>mobilenet_v2</b> model from the following link: <a href="https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz">https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz</a></li>
<li>Copy the model to the working space and release it. This compressed package contains all the parameters of mobilenet_v2 and will be used in the following steps. <pre class="fragment">    build $ cp mobilenet_v2_1.4_224.tgz /&lt;your working space&gt;/
    build $ cd /&lt;your working space&gt;/
    build $ tar zxvf mobilenet_v2_1.4_224.tgz
</pre></li>
</ul>
</li>
<li>Source the Ambarella CV toolchain. <pre class="fragment">  build $ source /usr/local/amba-cv-tools-****.ubuntu-18.04/env/cv*.env
</pre></li>
<li><p class="startli">Download the GraphDef file.</p>
<p class="startli">The network trained by the TensorFlow framework is saved in a graph which contains all operations. Typically, this graph is saved as a <em>*.pd</em> file. To convert the mobilenet_v2 as an example, the user needs the parameters and the network descriptions file <em>*.pb</em>. The following steps show how to download the file. </p><pre class="fragment">build $ export GIT_SSL_NO_VERIFY=1
build $ git clone https://github.com/tensorflow/models
</pre></li>
<li>After the file has been downloaded, modify the Python script <em>/models/research/slim/datasets/imagenet.py</em>. <pre class="fragment">  (line 36) from six.moves import urllib
  (line 37) import tensorflow as tf
  (+++)      import ssl
  (+++)      ssl._create_default_https_context = ssl._create_unverified_context
</pre></li>
<li>Execute the following command to download the <em>*pb</em> file. <pre class="fragment">  build $ python3 models/research/slim/export_inference_graph.py \
            --alsologtostderr \
            --model_name=mobilenet_v2_140 \
            --output_file=./mobilenet_224.pb
</pre></li>
<li><p class="startli">Freeze the model.</p>
<p class="startli">The network descriptions file <em>mobilenet_224.pb</em> contains only the network structure; all the parameters are stored in a different file. However, TensorFlow includes a tool that enables users to conveniently store both the network structure and parameters in a single <em>*pb</em> file. Typically, this procedure is called freeze.</p>
<p class="startli">The command for freeze is as follows: </p><pre class="fragment">build $ freeze_graph.py \
          --input_graph=mobilenet_224.pb \
          --input_checkpoint=mobilenet_v2_1.4_224.ckpt \
          --input_binary=True \
          --output_graph=frozen_mobilenet_224.pb \
          --output_node_names=MobilenetV2/Predictions/Reshape_1
</pre></li>
<li><p class="startli">(Optional step) Optimize the frozen model.</p>
<p class="startli">After generating the frozen model, the next optional step is to apply graph transformations to the model. Ambarella strongly recommends users perform this procedure because not all graph transformations are currently performed by CNNGen. </p><pre class="fragment">build $ graph_surgery.py tf \
          -p frozen_mobilenet_224.pb \
          -isrc "i:input|is:1,224,224,3" \
          -on MobilenetV2/Predictions/Reshape_1 \
          -o frozen_mobilenet_224_v2_opt.pb
</pre></li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_in_tranpose"></a>
8.2 Input Transpose</h2>
<p>For more efficient calculation, CNNGen always treats the input as if it were <b>NCHW</b> format even though TensorFlow is typically in the <b>NHWC</b> format. It will switch in the default mode but if users need to use the NHWC format input in final deployment, they must transpose the data as follows. </p><pre class="fragment">build $ tfparser.py -p test_model.pb -i dra_list.txt -iq -idf 1,1,0,0 -is 1,7,7,512 -o ff -of cnngen_out –it 0,3,1,2
</pre><p>This command allows users to use the special input shape which is not in the NHCW format.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>This function also works for Caffe and ONNX, use the TensorFlow as a reference.</li>
<li>The whole pre-processing order is explained below:<ul>
<li>YUV420-YUV444 / YUV420-RGB</li>
<li>Transpose</li>
<li>Resample to the real network shape if it is different</li>
<li>Swap channels</li>
<li>Mean subtraction</li>
<li>Input scale</li>
</ul>
</li>
<li>“is” will always override the prototxt shape. If users provide a shape, the parser will take it and apply the pre-processing options.</li>
</ol>
</dd></dl>
<p>Below used TensorFlow <b>MobileNet v2</b> as an example which is provided to help users understand this function. </p><pre class="fragment">build $ tf_print_graph_summary.py -p frozen_mobilenet_224_v2_opt.pb
Graph summary:
Total number of ops: 314
Number of non constant ops: 156
Inputs (1):
  input (1, 224, 224, 3)
Outputs (1):
  MobilenetV2/Predictions/Softmax [1, 1001]
</pre><ol type="1">
<li><p class="startli">Input with default [1,3,224,224] </p><pre class="fragment"> build $ tfparser.py -p frozen_mobilenet_224_v2_opt.pb -isrc "is:1,3,224,224|iq|idf:0,0,0,0|im:127.5,127.5,127.5|ic:128|i:input=dra_image_bin/dra_bin_list.txt" -o tf_mobilenet_v2 -of out_tf_mobilenet_v2_parser -c act-force-fx8,coeff-force-fx8 -odst "o:MobilenetV2/Predictions/Softmax|ot:0,3,2,1|odf:fp32"
</pre><p class="startli">Users can find the generated VAS below. </p><pre class="fragment">VP_input(input, data_format(0, 0, 0, 0), vector(1, 3, 224, 224), // primitive 1, original input id = input
         dram_rotate = 0,
         rt_config = 0,
         VP_cnngen_demangled_id("input"),
         __cnngen_tracker = { 2 });
</pre><p class="startli">The parser will check the original input shape of NHWC [1,224,224,3] and switch it to NCHW [1,3,224,224], then NCHW is the final input shape when it is deployed on the chip. The parser will always treat the model in the NCHW shape, as the Ambarella chip is designed to calculate using NCHW.</p>
</li>
<li><p class="startli">Input with special [1,224,224,3] </p><pre class="fragment"> build $ tfparser.py -p frozen_mobilenet_224_v2_opt.pb -isrc "is:1,224,224,3|iq|idf:0,0,0,0|im:127.5,127.5,127.5|ic:128|it:0,3,1,2|i:input=dra_image_bin/dra_bin_list.txt" -o tf_mobilenet_v2 -of out_tf_mobilenet_v2_parser -c act-force-fx8,coeff-force-fx8 -odst "o:MobilenetV2/Predictions/Softmax|ot:0,3,2,1|odf:fp32"
</pre><p class="startli">Users can find the generated VAS below. </p><pre class="fragment">VP_input(input, data_format(0, 0, 0, 0), vector(1, 224, 224, 3),
         dram_rotate = 0,
         rt_config = 0,
         VP_cnngen_demangled_id("input"),
         __cnngen_tracker = { 2 });

VP_transpose(input,  // primitive 2
             VP_tensor(input_transpose, data_format(0, 0, 0, 0), vector(1, 3, 224, 224)),
             to_w = 2,
             to_h = 1,
             to_d = 0,
             to_p = 3,
             __cnngen_tracker = { 3 }
            );
</pre><p class="startli">For the same model, the example above can also use “is:1,224,224,3”. If users need to do this, “it:0,3,1,2” should be added. This is because NCHW is needed for all models. If not, the parser will report an error. With the commands above, users can feed an input with (NHWC) in their application.</p>
<p class="startli">For example, for two networks, network 2 needs to directly use network 1’s output whose output is “1,224,224,3”, then user can use “-it” to connect network 2 to network 1.</p>
<p class="startli">As in the default mode, network 2 is converted by input [1,3,224,224], if user do not use “-it”, the network 1’s output needs to be switched.</p>
<p class="startli">This option is only used when a special input shape is needed, such as shown in the example case 3.</p>
</li>
<li><p class="startli">Input with special [1,224,3,224] </p><pre class="fragment"> build $ tfparser.py -p frozen_mobilenet_224_v2_opt.pb -isrc "is:1,224,3,224|iq|idf:0,0,0,0|im:127.5,127.5,127.5|ic:128|it:0,2,1,3|i:input=dra_image_bin/dra_bin_list.txt" -o tf_mobilenet_v2 -of out_tf_mobilenet_v2_parser -c act-force-fx8,coeff-force-fx8 -odst "o:MobilenetV2/Predictions/Softmax|ot:0,3,2,1|odf:fp32"
</pre><p class="startli">Users can find the generated VAS below. </p><pre class="fragment">VP_input(input, data_format(0, 0, 0, 0), vector(1, 224, 3, 224),
         dram_rotate = 0,
         rt_config = 0,
         VP_cnngen_demangled_id("input"),
         __cnngen_tracker = { 2 });

VP_transpose(input, // primitive 2
             VP_tensor(input_transpose, data_format(0, 0, 0, 0), vector(1, 3, 224, 224)),
             to_w = 0,
             to_h = 2,
             to_d = 1,
             to_p = 3,
             __cnngen_tracker = { 3 }
            );
</pre><p class="startli">The parser always needs NCHW, but user can use “-it” to make “-is” variable for different requirements.</p>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_tflite"></a>
8.3 TFlite</h2>
<p>For TFLite, users can directly feed the TFLite model to <em>tfparser.py</em>, then the parser will convert it to a PB file, and eventually convert the PB to an Ambarella format.</p>
<p>Before conversion, it is recommended that users check <em>tf_print_graph_summary.py</em> to see if it is a VP friendly model. If there are unsupported operations or problems, please contact the Ambarella support team for assistance.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>When this model is retrained via quantization fix8, it mostly has a good accuracy with DRA fix8 after conversion.</li>
<li>But users cannot ensure that TFLite Uint 8 models have better deploy accuracy than TF models because TFLite has a little difference that is trained with a specific idea of running it on the <b>TPU</b>'s.<ul>
<li>In TFLite models, there are FQ ops for certain patterns only that TensorFlow optimizes. Rest of them, the data format needs to be determined by CNNGen DRA. The biases in TFLite model are also 32 bits, but it is not the case that CNNGen can pick 8 / 16 bits.</li>
<li>In general, TFLite models are more like partial data formats given to CNNGen and letting CNNGen use DRA images to figure out the remaining data formats. So it is not guaranteed that users always get higher deploy accuracy.</li>
</ul>
</li>
<li>The TFLite model is different with the PB file. For the most part, PB files do not include image preprocessing operations, but TFLite includes the input preprocessing as it is used for inference deployment. When converting it, users should pay attention to this scenario in case that it feeds the preprocessing operations again which will result in an accuracy problem.</li>
<li>For others, it is the same process when converting a PB file.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_nchw"></a>
8.4 NCHW Model</h2>
<p><b>Tensorflow</b> can support NCHW by setting a parameter in related operators or layers, but it does not mean that only supported input is NCHW. It means that all calculations are based on NCHW. </p><pre class="fragment">data_format=’NCHW’       # in Tensorflow layers
data_format=’channel_first’  # in Keras
</pre><p>An example as below. </p><pre class="fragment">model = models.Sequential()  # in Keras
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, data_format= 'channels_first'))
model.add(layers.MaxPooling2D((2, 2), data_format= 'channels_first'))
</pre><p>Also there are some operators like MaxPooling2D which can be handled only on GPU with NCHW format.</p>
<p>But <em>tfparser.py</em> in CNNGen toolchain cannot handle the NCHW format, such as logs below.</p>
<h4><a class="anchor" id="autotoc_md55"></a>
Example 1: load the MaxPooling2D in CPU.</h4>
<pre class="fragment">Executor failed to create kernel. Invalid argument: Default MaxPoolingOp only supports NHWC on device type CPU
     [[{{node max_pooling2d/MaxPool}}]]
</pre><h4><a class="anchor" id="autotoc_md56"></a>
Example 2: tf_print_graph_summay.py will show the recommendation below for NCHW model.</h4>
<pre class="fragment">====================================================================================│
Issue                          │ Recommendation                                     │
====================================================================================│
Model not in NHWC data format  │ Export inference graph in NHWC format              │
</pre><h4><a class="anchor" id="autotoc_md57"></a>
Example 3: error in tfparser.py.</h4>
<pre class="fragment">tfparser.py will print error message like this:
Error in create_conv_node
Unsupported data format: b'NCHW'
</pre><h2><a class="anchor" id="sub_sec_tf_2x"></a>
8.5 Tensorflow 2.x</h2>
<p>Tensorflow <b>2.x</b> is totally different with TensorFlow <b>1.x</b>, so there is an alpha method to convert it. Ambarella cannot guarantee whether this conversion is successful or not.</p>
<p>Note that, the problem is for TF2.x exporting. Right now the story is to "export from TF2.x to TFLite" and TF parser can handle that. During exporting to TFLite, customers may have to split; the split is similar to how the split did in Arm part as before.</p>
<p>This feature is only supported after <b>2.2.1.3.829</b>.</p>
<p>For how to convert TF2.x to Tflite, please refer to Tensorflow public web as follows.</p><ul>
<li>For basic networks, please refer to <a href="https://tensorflow.google.cn/lite/api_docs/python/tf/lite/TFLiteConverter">https://tensorflow.google.cn/lite/api_docs/python/tf/lite/TFLiteConverter</a> with the following three methods.<ul>
<li>Converting a SavedModel to a TensorFlow Lite model.</li>
<li>Converting a tf.Keras model to a TensorFlow Lite model.</li>
<li>Converting ConcreteFunctions to a TensorFlow Lite model.</li>
</ul>
</li>
<li>For detection networks, please refer to <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md</a> for details.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>When exporting TF 2.x models to Tflite, sometimes its operator's name string will be very strange, such as "concat;post/mul", "serving_default_input:0" and so on. For such special symbols, they will have some conflicts with some processing in the toolchain, please be careful of such symbols, it is better to remove and refine them. If they cannot be removed or refined, please ask Ambarella for help to check if they can be handled in the toolchain.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_onnx_parser"></a>
9 Onnx Parser</h1>
<p>The <em>onnxparser.py</em> converts the ONNX frameworks model to the Ambarella format. For details, refer to the Ambarella CV UG ONNX Parser in the CNNGen toolchain package.</p>
<p>For most of frameworks, it can be converted to ONNX, such as Tflite, Pytorch, and so on, it is always suggested to use ONNX for network convert.</p>
<dl class="section note"><dt>Note</dt><dd>ONNX is not a training framework, and most of the models are converted from other frameworks. So, ONNX will use <b>opset</b> version to control the compatibility, and the current parser will focus on <b>Opset 16</b>.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_x86_simulator"></a>
10 x86 Simulator</h1>
<p>There are two tools which is named <b>Acinference</b> and <b>Ades</b>.</p>
<p>In the CNNGen conversion, there are three steps to convert the original model to the final deploy model in Ambarella.</p>
<ol type="1">
<li>Convert public model to Ambarella node</li>
<li>DRA to decide the data format</li>
<li>VAS compile to compile it to machine code</li>
</ol>
<p>In every step, there will be some accuracy lost.</p>
<ul>
<li>For step 1, users can ignore the accuracy lost as it is very small.</li>
<li>For step 2, most accuracy lost is here.</li>
<li>For step 3, there will be minor accuracy lost.</li>
</ul>
<h2><a class="anchor" id="sub_sec_cnngen_eazyai"></a>
10.1 EazayAI</h2>
<p>EazyAI library has packaged below Acinference and Ades library to unify the APIs, which are the same with EVK deployment APIs. For detail, please refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_simulator">6 EazyAI Simulator Unit Test</a>. For eazyai fast compile, please refer to sub_sec_eazyai_simulator_qs.</p>
<p>Also it is provided as prebuild version which is inclcuded in tool <code>eazyai_inf.py</code>, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> for detail.</p>
<h2><a class="anchor" id="sub_sec_cnngen_acinf"></a>
10.2 Acinference</h2>
<p><b>Acinference</b> can be used to check the accuracy lost in step 1 and step 2.</p>
<p>Acinference runs much faster than Ades as Ades needs to simulate chips' behavior, also Acinference can benefit from GPU, and its result is very close to Ades, for Cosine Distance, the gap is about ~1%.</p>
<h3><a class="anchor" id="ssub_sec_acinf_binary"></a>
10.2.1 Acinference Binary</h3>
<p>Users need to use parser to generate the parser output, Acinference is based on the parser's output, such as the Json file in the following command.</p>
<div class="fragment"><div class="line">build $ acinference network.json -i ExpandDims=dra_bin_list.txt -o output</div>
</div><!-- fragment --><h3><a class="anchor" id="ssub_sec_acinf_library"></a>
10.2.2 Acinference Library</h3>
<p>Users can use this library to instantiate an Acinference instance in their PC test application.</p>
<p>The library and the header file are as follows.</p>
<div class="fragment"><div class="line">ACINF_LIB_PATH=`tv2 -libpath AmbaCnn`</div>
<div class="line">ACINF_HEADER_PATH=`tv2 -incpath AmbaCnn`</div>
</div><!-- fragment --><p>For detail, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_sample_quant">10.2.4 Quantization Inference Library Sample</a> for the detail example in CNNGen samples package.</p>
<h3><a class="anchor" id="ssub_sec_acinf_gpu"></a>
10.2.3 Enable GPU</h3>
<p>If users need to use GPU, the following modifications need to be done in CNNGen toolchain, below used CV22 as an examples. Please make sure the build server has support for GPU and CUDA first.</p>
<ul>
<li>Modify <code>LD_LIBRARY_PATH</code> in <code>&lt;toolchain path&gt;/env</code> to switch it to the specific version. <div class="fragment"><div class="line">Line 18 -&gt; # Setting up library search path</div>
<div class="line">Line 19 -&gt; LD_LIBRARY_PATH=...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/AmbaCnn/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:...</div>
</div><!-- fragment --></li>
<li>Modify the <code>project.tv2</code> in <code>&lt;toolchain path&gt;/cv22/tv2/cv22</code> to switch it to the specific version. <div class="fragment"><div class="line">AmbaCnn          cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;</div>
</div><!-- fragment --></li>
<li>Source the env file. <div class="fragment"><div class="line">build $ source &lt;toolchain path&gt;/env/cv22.env</div>
</div><!-- fragment --></li>
<li>Run as below. <div class="fragment"><div class="line">build $ acinference network.json -i ExpandDims=dra_bin_list.txt -o output -gpu 0</div>
</div><!-- fragment --></li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Since CNNGen toolchain 3.5.5.0, CUDA 11.4, CUDA 11.8 and CUDA 12.1 are supported for CVflowv3 of CV7x and CV3x, CUDA 11.4 is the default version which passes the test. Also it will be added to CNNGen toolchain 2.x for CVflowv2 of CV2x and CV5x in future which only supports CUDA 11.4 in current version.</dd></dl>
<h3><a class="anchor" id="sec_sample_quant"></a>
10.2.4 Quantization Inference Library Sample</h3>
<p>In the parser, there are three steps to convert the original model to the final deploy model in Ambarella.</p>
<ol type="1">
<li>Convert public model to Ambarella node</li>
<li>DRA to decide the data format</li>
<li>VAS compile to compile it to machine code</li>
</ol>
<p>In every step, there will be some accuracy lost.</p>
<ul>
<li>For step 1, users can ignore the accuracy lost as it is very small.</li>
<li>For step 2, most accuracy lost is here.</li>
<li>For step 3, there will be minor accuracy lost.</li>
</ul>
<p>Additionally this Quantization library (C++ interface) can be used to check the accuracy lost in step 1 and step 2, ADES library which is a bit perfect result with EVK (refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_sample_ades">10.3.4 Ades Library Sample</a>, C++ interface) can be used to check all the accuracy lost in step 1, step 2, and step 3.</p>
<p>Users can use this tool as below. </p><pre class="fragment">build $ make sync_build_mkcfg
build $ make cvflow_&lt;v2 or v3&gt;_config
build $ make mobilefacenets
build $ cd quant_api_example
build $ ./run_quantlib_example.sh or run_cnngen_quant_cpu.sh
</pre><h2><a class="anchor" id="sub_sec_cnngen_ades"></a>
10.3 Ades</h2>
<p><b>Ades</b> can be used to check all the accuracy lost in step 1, step 2, and step 3 which has bit perfect result with EVK board, users can call it executable binary or library APIs.</p>
<h3><a class="anchor" id="ssub_sec_ades_binary"></a>
10.3.1 ADES Binary</h3>
<p>Since CNNGen toolchain 2.5.4.09 and 3.5.4.0, <code>ades</code> command will not be supported anymore. Users can using EazyAI inference tool <code>eazyai_inf.py</code> in <a class="el" href="../../d2/d67/fs_cnngen.html#sub_sec_cnngen_eazyai">10.1 EazayAI</a> instead, also they call <code>run_ades_lite.py</code> to run Ades.</p>
<p>For details, refer to the <em>Ambarella CV UG ADES, Ambarella CV UG ADES Regression, the Ambarella CV UG ADES Script Autogen</em>, and <em>Ambarella CV UG Running ADES</em> in the CNNGen toolchain package.</p>
<h3><a class="anchor" id="ssub_sec_ades_library"></a>
10.3.2 ADES Library</h3>
<p>Users can use this library to instantiate an ADES instance in their PC test application, and the current version only supports one instance in one OS process.</p>
<p>The library and the header file are as follows.</p>
<div class="fragment"><div class="line">ADES_LIB_PATH=`tv2 -libpath VpRef`</div>
<div class="line">ADES_HEADER_PATH=`tv2 -incpath VpRef`</div>
</div><!-- fragment --><p>The library is <em>/libades.so</em>, and the header file is <em>/ades_api/ades_api.h</em>, the functions users required are declared in the header file.</p>
<p>Because this library needs <b>libvdg</b> to support, users should have the path of the libvdg link below accordingly.</p>
<div class="fragment"><div class="line">VDG_LIB_PATH=`tv2 -libpath libvdg`</div>
</div><!-- fragment --><p>Following messages are the gcc link options used to build an application:</p>
<div class="fragment"><div class="line">-L$(LIB_DIR)  -L/usr/lib64  -L$(ADES_LIB_PATH) -lades  -pthread -L$(VDG_LIB_DIR) -Wl,-rpath,$(ADES_LIB_PATH),-rpath,$(VDG_LIB_DIR)</div>
</div><!-- fragment --><p>For detail, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_sample_ades">10.3.4 Ades Library Sample</a> for the detail example in CNNGen samples package.</p>
<h3><a class="anchor" id="ssub_sec_ades_gpu"></a>
10.3.3 Enable GPU</h3>
<p>If users need to use GPU, the following modifications need to be done in CNNGen toolchain, below used CV22 as an examples. Please make sure the build server has support for GPU and CUDA first.</p>
<ul>
<li>Modify <code>LD_LIBRARY_PATH</code> in <code>&lt;toolchain path&gt;/env</code> to switch it to the specific version. <div class="fragment"><div class="line">Line 18 -&gt; # Setting up library search path</div>
<div class="line">Line 19 -&gt; LD_LIBRARY_PATH=...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/VpRef/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:</div>
<div class="line">                           ...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/AdesRuntime/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:...</div>
</div><!-- fragment --></li>
<li>Modify the <code>project.tv2</code> in <code>&lt;toolchain path&gt;/cv22/tv2/cv22</code> to switch it to the specific version. <div class="fragment"><div class="line">VpRef                cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;</div>
<div class="line">AdesRuntime          cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;</div>
</div><!-- fragment --></li>
<li>Source the env file. <div class="fragment"><div class="line">build $ source &lt;toolchain path&gt;/env/cv22.env</div>
</div><!-- fragment --></li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Since CNNGen 3.5.5.0, CUDA 11.4, CUDA 11.8 and CUDA 12.1 are supported for CVflowv3 of CV7x and CV3x, CUDA 11.4 is the default version which passes the test. And there is no plan to add these support in CNNGen toolchain 2.x for CVflowv2 of CV2x and CV5x.</dd></dl>
<h3><a class="anchor" id="sec_sample_ades"></a>
10.3.4 Ades Library Sample</h3>
<p>The following steps are the usage of the library:</p>
<ol type="1">
<li><p class="startli">Create an ADES instance</p>
<p class="startli">For example: </p><pre class="fragment">AdesAPI::AdesInstance* ades_ins = AdesAPI::CreateAdesInstance(ades_script_path, output_error_msg);
</pre><p class="startli">This function returns a pointer to an ADES instance. The first input parameter is the ADES script file path. If this operation fails, it returns a NULL pointer, and the second parameter (a std::string) includes the failure reason.</p>
</li>
<li><p class="startli">Set input vector data</p>
<p class="startli">For each input descriptor, users need to copy in the data. Additionally, only the binary data is taken. Please refer to the following example. </p><pre class="fragment">err_msg = ades_ins-&gt;SetInput(input_vector_name, input_buffer_addr, input_buffer_size, 0, false);
</pre><p class="startli">Detailed parameters are provided below.</p><ul>
<li>The first parameter is the input descriptor name;</li>
<li>The second parameter is the starting address of the input data;</li>
<li>The third parameter is the size of the total input data buffer size;</li>
<li>The fourth parameter is the pitch size of the input buffer (similar to -dp in "lbin" command);</li>
<li>The fifth parameter is to specify that if the input data stored in a DRAM format specified by the VAS descriptor (similar to -dfmt in lbin command).</li>
</ul>
<p class="startli">If something goes wrong, this function returns a non-empty string which provides the failure reason.</p>
</li>
<li><p class="startli">Run the script</p>
<p class="startli">Here is an example: </p><pre class="fragment">err_msg = ades_ins-&gt;Execute();
</pre><p class="startli">If something goes wrong, this function returns a non-empty string which provides the failure reason.</p>
</li>
<li><p class="startli">Get the outputs</p>
<p class="startli">For each output descriptor, users can copy out the output data after the execution</p>
<p class="startli">Here is an example: </p><pre class="fragment">err_msg = ades_ins-&gt;GetOutput(output_vector_name, output_buffer_addr, output_buffer_size, output_data_pitch, true);
</pre><p class="startli">Detailed parameters are provided as follows.</p><ul>
<li>The first parameter is the output descriptor name;</li>
<li>The second parameter is the starting address of the output data buffer;</li>
<li>The third parameter is the size of the total output data buffer size;</li>
<li>The fourth parameter is the pitch size of the output buffer (similar to "-dp" in "sbin" command, the default value is 0);</li>
<li>The fifth parameter is to specify that if the output data stored in a dram format specified by the vas descriptor (similar to "-dfmt" in sbin command, the default value is true).</li>
</ul>
<p class="startli">If something goes wrong, this function returns a non-empty string which provides the failure reason.</p>
</li>
<li><p class="startli">Process next frames</p>
<p class="startli">For the next input frame, repeat step <b>2</b> to <b>4</b>.</p>
</li>
<li><p class="startli">Delete the ADES instance after processing all frames</p>
<p class="startli">Here is an example: </p><pre class="fragment">delete ades_ins;
</pre><p class="startli">For detailed example, users can refer to the message below in CNNGen samples package. </p><pre class="fragment">Build $ ls -l cvflow_cnngen_samples_&lt;version&gt;/ades_api_example/run_adeslib_example.sh
</pre></li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_cnngen_cavalry_gen"></a>
11 Cavalry_gen</h1>
<p>This tool generates the final DAGs for the Cavalry input. For details, refer to the <em>Ambarella CV UG Cavalry Autogen</em> in the CNNGen toolchain package.</p>
<h2><a class="anchor" id="sub_sec_cavalry_compatibility"></a>
11.1 Compatibility</h2>
<p>The *cavalry_gen *tool is not fully backwards compatible. Users can generate a different Cavalry binary depending on their SDK:</p>
<p>Using <em>cavalry_gen</em> version 5: </p><pre class="fragment">build $ cavalry_gen -V 0.0.5 -d vas_output/ -f onnx_mobilenet_v2_cavalry.bin
</pre><p>Using <em>cavalry_gen</em> version 6: </p><pre class="fragment">build $ cavalry_gen -V 2.1.6 -d vas_output/ -f onnx_mobilenet_v2_cavalry.bin
</pre><p>Using the latest <em>cavalry_gen</em>: </p><pre class="fragment">build $ cavalry_gen -d vas_output/ -f onnx_mobilenet_v2_cavalry.bin
</pre><p>In SDK 3.0, the Cavalry framework will include some improvement for backwards compatibility.</p>
<ol type="1">
<li><p class="startli">It is fully backwards compatible in the same big version.</p>
<p class="startli">For example in 2.2.8, the current version is 2.2.8.1. In the future, it will have version as 2.2.8.2, 2.2.8.3 and so on, then to assume the latest cavalry_gen is 2.2.8.9 and the latest SDK version is 3.0.9.</p><ul>
<li>The latest SDK 3.0.9 version which is released together with 2.2.8.9 can run all the binaries converted by 2.2.8.* which is lower than 2.2.8.9.</li>
<li>The old SDK 3.0 version which is released together with 2.2.8.1 cannot run the binaries converted by 2.2.8.* which is higher than 2.2.8.1.</li>
<li>If SDK version upgrades 2.2.8 to 2.2.9, that means it will totally break the compatibility, users need to use “cavalry_gen” to convert 2.2.8 to 2.2.9 for the latest SDK.</li>
</ul>
</li>
<li>SDK 3.0 version will include <em>cavalry_gen</em> in EVK boards, then users can call this application when Cavalry version conversion is needed.</li>
</ol>
<h2><a class="anchor" id="sub_cnngen_cavalry_versions_convert"></a>
11.2 Convert Between Different Versions</h2>
<p>The <em>cavalry_gen</em> tool supports conversion between different SDK versions. </p><pre class="fragment">build $ cavalry_gen -f v5.bin -t v7.bin -V 2.1.7
</pre><h2><a class="anchor" id="sub_cnngen_cavalry_versions_mismatch"></a>
11.3 Mismatch Between SDK And Cavalry_gen</h2>
<p>If SDK version and <em>cavalry_gen</em> Version are mismatched, it is suggested to refer to below steps to convert the cavalry binary version.</p>
<ol type="1">
<li>Check SDK cavalry library version <div class="fragment"><div class="line">board # modprobe cavalry;cavalry_load -f /lib/firmware/cavalry.bin -r</div>
<div class="line">board # test_nnctrl --version</div>
<div class="line">Cavalry Parser Version: 2.1.7</div>
</div><!-- fragment --> So 2.1.7 cavalry network binary is needed.</li>
<li>Then please check <em>cavalry_gen</em> tool to see which version it can support, as shown below. <div class="fragment"><div class="line">build $ cavalry_gen -h</div>
<div class="line">Cavalry_gen Entry:</div>
<div class="line">-V, --vers [0.0.5, 2.1.6, 2.1.7, 2.2.8.x (x range 1~2), latest]</div>
<div class="line">  Cavalry_gen version <span class="keywordflow">default</span> is <span class="stringliteral">&quot;latest&quot;</span></div>
</div><!-- fragment --></li>
<li>At last, convert it as below. <div class="fragment"><div class="line">build $ cavalry_gen -V 2.1.7 -d vas_output/ -f onnx_mobilenet_v2_cavalry.bin</div>
</div><!-- fragment --></li>
<li>Also if users already have a model which is converted in <em>cavalry_gen</em>, users can easily convert it to the latest version as below. <div class="fragment"><div class="line">build $ cavalry_gen -f v6.bin -t v7.bin -V 2.1.7</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Users can find tool <code>cavalry_gen</code> in two places.<ol type="1">
<li>CNNGen Toolchain Package, this version can only be used in build server.</li>
<li>"ambarella/prebuild/ambarella/tools/cavalry_gen" in SDK package, this version can only be used in EVK board.</li>
<li>Users can check current cavalry binary's version in EVK as below. <div class="fragment"><div class="line">board # test_nnctrl -b yolov3_mnetv2_lite_cavalry.bin  --bin-version</div>
<div class="line">Cavalry Bin: [0][yolov3_mnetv2_lite_cavalry.bin]</div>
<div class="line">[yolov3_mnetv2_lite_cavalry.bin] Net Binary Cavalry Gen Version: 2.2.8.2</div>
<div class="line">VDG Chip Arch: cv22</div>
</div><!-- fragment --></li>
</ol>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_layer_comapre"></a>
12 layer_compare.py</h1>
<p>The <em>layer_compare.py</em> is a tool that compares the forward pass results of Caffe and TensorFlow against those of CNNGen and ADES for each layer. It is useful for debugging precise differences between the reference framework (such as <b>Caffe</b> / <b>TensorFlow</b>) and CNNGen / ADES. For details, refer to the Ambarella CV UG Layer Compare in the CNNGen toolchain package.</p>
<dl class="section note"><dt>Note</dt><dd>Since 2.4.2, there are some useless options removed in <em>layer_compare.py</em>, it will use the configuration in "parser_summary.json" file instead which is generated by Parser, such as below. <pre class="fragment">    --netname, --odest, --config, --draflags, --customnodedll, --customnodeparser, --customtflibpath
</pre></dd></dl>
<p>The following example shows how to use the <em>layer_compare.py</em> tool to compare the CNNGen ADES results with the <b>Caffe</b> model results after finding something with “-dinf cerr”.</p>
<ol type="1">
<li>Source the Ambarella toolchain.<ul>
<li>Before using layer_compare.py, source the Ambarella CV toolchain. <pre class="fragment">   build $ cd cvflow_cnngen_samples_&lt;version&gt;/
   build $ source build/env/cv*.env
</pre></li>
</ul>
</li>
<li><p class="startli">Run.</p>
<p class="startli">In this example, Ambarella uses AlexNet. To use layer_compare.py, users must first convert AlexNet, and then compile it. </p><pre class="fragment">build $ make sync_build_mkcfg
build $ make cvflow_&lt;v2 or v3&gt;_config
build $ make bvlc_alexnet run_mode=layer_compare
</pre><p class="startli">Additionally, refer to the Excel file, <em>error_list.xlsx</em>.</p>
</li>
<li><p class="startli">Check the comparison results.</p>
<p class="startli">The <em>error_list.xlsx</em> will reveal the following and the table format will be as follows:</p><ul>
<li>If an error occurred in the table “Caffe f32 vs CNNGen f32” the operator realization in CNN is incorrect.</li>
<li>If an error occurred in the table “Caffe f32 vs CNNGen Quant” the quantization is incorrect.</li>
<li><p class="startli">If an error occurred in the table “Caffe f32 vs ADES” ADES is incorrect.</p>
<a class="anchor" id="Example of error_list.xlsx"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Layer Name </th><th>Max Absolute Error </th><th>Max Absolute Error </th><th>Cosine Distance </th><th>Pearson Correlation </th></tr>
<tr align="middle">
<td>xxx </td><td></td><td></td><td></td><td></td></tr>
<tr align="middle">
<td>xxx </td><td></td><td></td><td></td><td></td></tr>
</table>
</li>
</ul>
</li>
</ol>
<p>There are three coefficients that can describe the errors between the results of CNNGen output and the results of the Caffe model:</p><ul>
<li>Max absolute error</li>
<li>Cosine distance</li>
<li>Pearson correlation</li>
</ul>
<p>If the max absolute error and cosine distance of each layer is close to 0, the CNNGen result is more accurate. Similarly, if the Pearson correlation is close to 1, the CNNGen result is more accurate.</p>
<dl class="section note"><dt>Note</dt><dd>CNNGen does some optimizations, so the results are not reflected layer by layer. If a specific layer’s accuracy seems strange, users should refer to the methods below to check if accuracy was lost during conversion.<ol type="1">
<li>Use the option “no-opt” to disable internal optimization. This makes sure every layer in the <b>Caffe</b> model can be compared, but is not recommended for production code. Use it only for debugging purposes.</li>
<li>The function "allow_focused_range" is set by default to “ON”. It applies the range of a clip primitive to its source. If the clip is the only user, it is possible to see the conv2iepbs difference is more significant, and the relu6 difference is smaller. The unnecessary range has been clamped earlier, which maintains a better smaller side precision. In the final report, it will show a big accuracy loss in some layers like <b>relu6</b>, but back in the final output layer, it is normal. This is the result of "allow_focused_range". Use “-c allow_focused_range=false” to disable it.</li>
<li>By default, <em>layer_compare</em> will generate CNN quantization result in folder “lc_cnn_output”, if the user needs ADES and original model result, please use “-d” option which is used to set debug level, if users set it 3, it will generate all these results for the comparison.</li>
</ol>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_preprocess_tools"></a>
13 Pre-Process Model Tools</h1>
<p>Users can use <em>graph_surgery.py</em> to convert the unfriendly VP model to a friendly VP model.</p>
<p>The <em>graph_surgery.py</em> enables users to modify a trained model file which can still run in the original framework, and works for <b>Caffe</b>, <b>TensorFlow</b>, and <b>ONNX</b>. For details, refer to the <em>Ambarella CV UG Graph Surgery</em> in the CNNGen toolchain package.</p>
<p>Before using this tool, users should use <em>onnx_print_graph_summary.py</em>, <em>tf_print_graph_summary.py,</em> and <em>caffe_print_graph_summary.py</em> to check the network model. If there are any problems, or it reports some issues, users do not need to try to convert as there will be problems. To resolve it, the following tools share suggestions on how <em>graph_surgery.py</em> can handle it.</p>
<dl class="section note"><dt>Note</dt><dd>Before converting a network, the user should do below things, or else, it will make convert more complex.<ul>
<li>Please use PC original framework to run this model to check if everything goes well. For example, the user converts a Pytorch model to ONNX, before using CNNGen tool, the user should check if there are some problems in this convert between Pytorch and ONNX.</li>
<li>Please use “*_print_graph_summary.py” to check if there will be some problems.</li>
</ul>
</dd></dl>
<p>Make above two checks go well, then try to convert.</p>
<h2><a class="anchor" id="sub_sec_print_summary"></a>
13.1 Print Summary</h2>
<p>Users can use <em>tf_print_graph_summary.py</em> to check the model’s status, and to see if there will be some problems. If there are some problems, please do not try to convert, for yolov3 example below with <b>ONNX</b> framework, <b>Tensorflow</b> and <b>Caffe</b> has same tools. </p><pre class="fragment">build $ onnx_print_graph_summary.py -p yolov3-spp.onnx
Creating non-existent log directory: ./logs
Application logs will be written to the log directory: ./logs
[Framework Information]
     Onnx Version: 1.7.0
     Onnx Path: /lhome/hqian/.local/lib/python3.6/site-packages/onnx
     Model Opset: [11]
     Model IR Version: 4
     Model Producer: pytorch-1.3
[Model Summary]
     Total Operators: 306
     Total Non-Constant Operators: 306
     Primary Inputs (1):
     -&gt; 'input.1' [1, 3, 416, 416]
     Primary Outputs (2):
     -&gt; '459' [10647, 80]
     -&gt; '462' [10647, 4]
     Tensors with rank &gt; 4 (2):
     -&gt; '300' [1, 3, 85, 13, 13]
     -&gt; '301' [1, 3, 13, 13, 85]
     Onnxparser Natively Unsupported Operators (1):
     -&gt; Shape: 2 instances
     Onnxparser Ignore Operators (1):
     -&gt; Shape: 2 instances
[Graph Surgery Recommendations]
     Found unnamed nodes, use ModNodeNames
     Found nodes supported by graph surgery, use ConstantifyShapes
     -&gt; Shape: 2 instances
     Found foldable nodes, use FoldConstants
</pre><p>As shown in the figure above, users can find the input / output name and dimensions, as well as some basic information for this model. Then it reports some errors and shared some suggestions below.</p>
<ol type="1">
<li>Dimension of tensors is larger than 4 which is not supported, CutGraph should be used to delete this operator</li>
<li>Unsupported operators of shape, use ConstantifyShapes to solve</li>
<li>Some unnamed nodes and foldable nodes, use ModNodeNames and FoldConstants to solve</li>
</ol>
<h2><a class="anchor" id="sub_sec_graph_surgery"></a>
13.2 Graph Surgery</h2>
<p>Once the suggestions have been provided by the above tool, users can then use <em>graph_surgery.py</em> to process the model. </p><pre class="fragment">build $ graph_surgery.py onnx -m yolov3-spp.onnx -o yolov3-spp-surgery.onnx -isrc "i:input.1|is:1,3,416,416" -on 298,360,422 -t ConstantifyShapes,FoldConstants,CutGraph,ModNodeNames
</pre><p>Then, it will generate a new model named <b>yolov3-spp-surgery.onnx</b>. Users can use <em>tf_print_graph_summary.py</em> to check it again. </p><pre class="fragment">build $ onnx_print_graph_summary.py -p yolov3-spp-surgery.onnx
Application logs will be written to the log directory: ./logs
[Framework Information]
     Onnx Version: 1.7.0
     Onnx Path: /lhome/hqian/.local/lib/python3.6/site-packages/onnx
     Model Opset: [11]
     Model IR Version: 7
     Model Producer: -
[Model Summary]
     Total Operators: 180
     Total Non-Constant Operators: 180
     Primary Inputs (1):
     -&gt; 'input.1' [1, 3, 416, 416]
     Primary Outputs (3):
     -&gt; '298' [1, 255, 13, 13]
     -&gt; '360' [1, 255, 26, 26]
     -&gt; '422' [1, 255, 52, 52]
</pre><p>After checking, users can see if the new model is a VP friendly model, then they can try to convert.</p>
<dl class="section note"><dt>Note</dt><dd>The <em>graph_surgery.py</em> is a power tool but not a universal tool, it can cover most of the cases, but not all, so users need to pre-process the model by themselves when <em>graph_surgery.py</em> cannot solve problems, or they can contact the Ambarella support team for more details.</dd></dl>
<h2><a class="anchor" id="sub_sec_replace_subgraph"></a>
13.3 Replace Subgraph</h2>
<p><em>ReplaceSubgraph</em> is a transform flag of <em>graph_surgery.py</em>, which provides a framework to replace a part of graph (a.k.a. sub graph) with that defined by the user. It's useful when part of the graph is unsupported on VP, for example if reshape or stack operation results in tensors larger than 4D. When possible, they can be replaced with ops that work with VP friendly operators (in this case, ops that result in tensors that are 4D or less).</p>
<p>To use <em>ReplaceSubgraph</em> transform, the user needs to provide two files. The first one is the new sub graph model and the second one is a <b>json</b> file containing information about the sub graph to be replaced.</p>
<p><b>Model file</b></p>
<p>Model file is a <b>Python</b> file that should contain the implementation of subgraph() method. For <b>TensorFlow</b> models, it should have the following interface. For other frameworks, the model file is also similar to this sample. </p><pre class="fragment">def subgraph(input_tensor, begin_nname, end_nname, attr=None):
    st_tensor = tf.op(input_tensor, ..., name=begin_nname)
    ...
    ...
    en_tensor = tf.op(..., name=end_nname)
    tensor_list = [st_tensor, ..., en_tensor]

    return tensor_list
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li>begin_nname and end_nname are the start and end op names of the sub graph to be replaced. Naming the first op in the new sub graph as begin_nname and last op as end_nname is user's responsibility.</li>
<li>The sub graph is expected to be continuous. Currently, single input and single output is supported.</li>
<li>The method is expected to return a list of tensors. These will be converted to NodeDef objects by the tool.</li>
<li><em>attr</em> argument can be used to communicate between the <b>json</b> file and the model file. This is useful when the same pattern occurs many times in the graph but with different attributes.</li>
</ul>
</dd></dl>
<p><b>JSON file</b></p>
<p>The <b>json</b> file should describe the sub graph to be replaced. It is a list of dictionaries. </p><pre class="fragment">{
    "subgraphs":
    [
        {
            "begin": "&lt;begin_nname&gt;",
            "end": "&lt;end_nname&gt;",
            "model": "&lt;model_file&gt;",
            "attr":
            {
                "attr": "&lt;attr_value&gt;"
            }
        }
    ]
}
</pre><p>The keys of <b>json</b> file and descriptions are as follows.</p>
<a class="anchor" id="replace_subgraph_tensorflow_json"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Key </th><th align="left">Description </th></tr>
<tr>
<td>begin </td><td>Sub graph starting op name in the existing graph </td></tr>
<tr>
<td>end </td><td>Sub graph ending op name in the existing graph </td></tr>
<tr>
<td>model </td><td>Path to python model file </td></tr>
<tr>
<td>attr </td><td>Attributes dictionary </td></tr>
</table>
<p><b>Sample Command Line</b></p>
<p>Implement <em>ReplaceSubgraph</em> using following commands. </p><pre class="fragment">graph_surgery.py tf -v \
    -p &lt;model_file.pb&gt; \
    -o &lt;model_modified.pb&gt; \
    -isrc 'i:&lt;input_node&gt;|is:&lt;input_shape&gt;' \
    -on &lt;output_nodes&gt; \
    -t ReplaceSubgraph=&lt;json_file.json&gt;
</pre><h2><a class="anchor" id="sub_sec_register_custom_operator"></a>
13.4 Register Custom Operator</h2>
<p>When users need to export an ONNX model from PyTorch, there is a known compatibility issue where the operator in PyTorch cannot be found in the target ONNX opset version. For example, the operator 'GridSample' is not supported until ONNX opset version 16, but the user can export a specific lower opset version of the ONNX model by registering a custom operator. The following Python script shows an example of registering the custom operator 'GridSample'. </p><pre class="fragment">import torch
from torch.onnx import register_custom_op_symbolic
import torch.onnx.symbolic_helper as sym_help

def grid_sampler(g, input, grid, mode, padding_mode, align_corners):
    # mode
    #   'bilinear'      : onnx::Constant[value={0}]
    #   'nearest'       : onnx::Constant[value={1}]
    #   'bicubic'       : onnx::Constant[value={2}]
    # padding_mode
    #   'zeros'         : onnx::Constant[value={0}]
    #   'border'        : onnx::Constant[value={1}]
    #   'reflection'    : onnx::Constant[value={2}]
    mode = sym_help._maybe_get_const(mode, "i")
    padding_mode = sym_help._maybe_get_const(padding_mode, "i")
    mode_str = ['bilinear', 'nearest', 'bicubic'][mode]
    padding_mode_str = ['zeros', 'border', 'reflection'][padding_mode]
    align_corners = int(sym_help._maybe_get_const(align_corners, "b"))

    return g.op("com.microsoft::GridSample", input, grid,
                mode_s=mode_str,
                padding_mode_s=padding_mode_str,
                align_corners_i=align_corners)

register_custom_op_symbolic('::grid_sampler', grid_sampler, 1)

import torch
import torch.nn.functional as F

# sample input and grid
x = torch.randn(1, 3, 224, 224)
grid = 2*torch.rand(1, 224, 224, 2) - 1 # scale as (-1, 1)

# Toy model including blinear_grid_sampler
class Sampler(torch.nn.Module):
def __init__(self):
    super().__init__()
    self.grid = grid
def forward(self, x, y):
    x = x + 0.5
    y = y + 0.5
    return F.grid_sample(x, y, mode='bilinear', align_corners=True) + 0.5

import numpy as np

model = Sampler()
torch.onnx.export(model, (x, grid), 'model.onnx')
</pre><hr  />
<h1><a class="anchor" id="sec_cnngen_eval_surgey"></a>
14 eval_surgery.py</h1>
<p>The <em>eval_graph.py</em> enables users to split, build, and run <b>TensorFlow</b> models on heterogeneous platforms such as TensorFlow (host), Vector Processor (ADES / BUB), and Arm (BUB). It can be used for quick evaluation as well as production. For quick evaluation, non VP parts can be run in TensorFlow. The tool handles DRA files at intermediate points as well as special situations such as in batch mode (for example, running classifier network on all ROIs). For details, refer to the Ambarella CV UG Eval Graph in the CNNGen toolchain package.</p>
<p><b>This tool will be deprecated soon as it is useless, Ambarella will not maintain it anymore.</b></p>
<hr  />
<h1><a class="anchor" id="sec_cnngen_supplementary_docs"></a>
15 Supplementary Documents</h1>
<p>The following sections overview the documents located in the VP_Toolkit folder in the SDK package. If these files are missing or additional information is required, contact the Ambarella support team for assistance.</p>
<h2><a class="anchor" id="sub_sec_cnngen_ug_cnn"></a>
15.1 UG CNNGen User Guide</h2>
<p>This document provides details on <b>CNNGen</b>, which is a code generation tool for the CV CVflow Vector Processor (VP) hardware unit, the key vision processing module for executing neural networks efficiently in Ambarella’s family of Computer Vision (CV) chips.</p>
<h2><a class="anchor" id="sub_sec_cnngen_python_api"></a>
15.2 API CNNGen Python Application Programming Interface</h2>
<p>This document provides information on the Python application programming interface to Ambarella’s CNNGen library, including the interface classes and their respective methods.</p>
<h2><a class="anchor" id="sub_sec_cnngen_ug_tools"></a>
15.3 UG Tools</h2>
<p>Documents in the CNNGen toolchain package that provide users guide information for each tool.</p>
<h2><a class="anchor" id="sub_sec_cnngen_ug_dra"></a>
15.4 UG Dynamic Range Analysis</h2>
<p>This document provides information on how to use different DRA strategies when converting the network.</p>
<h2><a class="anchor" id="sub_sec_cnngen_sdk_faq"></a>
15.5 Linux SDK CVflow FAQ</h2>
<p>This document provides the answers of some common questions for system, software, and network optimization.</p>
<hr  />
<h1><a class="anchor" id="sec_cnngen_ecternal_tools"></a>
16 External Tools</h1>
<h2><a class="anchor" id="sub_sec_cnngen_cv_f2vp_convert"></a>
16.1 cv_f2vp_convert</h2>
<pre class="fragment">build $ cv_f2vp_convert --help
CV Data Converter: data conversion between single precision floating point format  and vp data format.
Usage: cv_f2vp_convert [OPTIONS] -fmt &lt;sign,datasize,expoffset,expbits&gt; -i &lt;input_file_name&gt; -o &lt;output_file_name&gt;

Options:
    -h, --help                  prints this help message
    -in, --inverse              sets conversion direction as vp data to float, default is float to vp data
    -it, --input-txt            sets input file type as text file, default is binary file
    -ot, --output-txt           sets output file type as text file, default is binary file
    -nd, --no-dim-info          indicates there is no vector dimension information at the beginning of a text input file, default is dimension info is present
    -r, --reserved              input data contain reserved numbers, assuming none by default
    -j, --jagged                input data are jagged vectors, assuming non-jagged vectors by default

build $ cv_f2vp_convert -i prob.bin -fmt 0,1,-8,4 -o prob_fp32.bin -in
</pre><p>For a detailed example, refer to Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_lstm_warpctc">LSTM Warp-CTC</a>.</p>
<h2><a class="anchor" id="sub_sec_cnngen_keras_converter"></a>
16.2 keras_converter</h2>
<p>This tool is used to convert Keras model to TensroFlow model as CNNGen cannot support Keras directly which is in <em>cv2/tv2/release/frameworklibs/cv2.x.x.x.x.xxxx.ubu1804/lib/python3.x/site-packages/frameworklibs/tensorflow/keras/keras_converter.py</em>.</p>
<h2><a class="anchor" id="sub_sec_cnngen_torch_converter"></a>
16.3 torch2onnx</h2>
<p>To convert the PyTorch model to an ONNX model, original torch framework itself has export function as "torch.onnx.export".</p>
<div class="fragment"><div class="line">torch.onnx.export(model,args,f,export_params,verbose=False,training=False)</div>
</div><!-- fragment --><p>However, sometimes the export operator coverage may be insufficient for user's models, so CNNGen tools included a package wrapper that provides custom symbolic function libraries to supplement those in the user's native torch.onnx installation.</p>
<p>This package was originally built to support the exporting of quantization torch models which is not available as of <b>PyTorch-1.3.1</b>. As such, the package's interface follows version 1.3.1. When exporting a torch model to ONNX, users can essentially use the torch2onnx package where original torch.onnx is used, as this package also calls the user's local torch.onnx package's functions. Except when it encounters a symbolic function not supported by the local torch.onnx, it will use the corresponding custom symbolic function in torch2onnx's library if one is present.</p>
<p>This tool is in <em>cv2/tv2/release/CnnUtils/cv2.qa.xxxx/packages/torch_utils/torch2onnx/</em>, taking the example as shown below. </p><pre class="fragment"># Append CnnUtils/packages path to your PYTHONPATH

import os

cnnutils_path = os.popen('tv2 -basepath CnnUtils').read().split('\n')[0]

pkgs_path = os.path.join(cnnutils_path, 'packages')
sys.path.append(pkgs_path)
​
# Export the torch model
import torch
import torch_utils
import torch_utils.torch2onnx as torch2onnx​​

dummy_input = torch.randn(1, 3, 224, 224, device='cpu')
model = UserModel
torch2onnx.export(model=model, args=dummy_input, f='output/model/path', opset_version=10, verbose=True, do_constant_folding=True)
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li>Quantized torch operators are not completely supported in torch2onnx, please ask the Ambarella Support Team for assistance if problems occur.</li>
<li>Currently, torch2onnx's custom symbolic function library only supplements exporting to ONNX opset 10.</li>
<li>However, even if the user exports a model that does not require any of the custom symbolic function libraries, torch2onnx should still be backward and forward compatible with the user's local torch.onnx installation.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cnngen_pruning_tool"></a>
17 Pruning Test Tools</h1>
<p>CVflow will get huge benefit from pruning, so it is always suggested for user to do the pruning. Sometimes users may ask that how much pruning ratio can get their target performance, then below tools can performs simple pruning which is uniform pruning tools, as each layer has the same pruning ratio for the performance test. </p><pre class="fragment">build $ $ onnx_prune_model.py -h
usage: onnx_prune_model.py [-h] -m ONNX model [-th Sparsity threshold 0-1]
                           [-sm JSON sparsity map] [-cad] [-cld] [-macd]
                           [-o Output model name]

Script to prune onnx model to given sparsity

optional arguments:
  -h, --help            show this help message and exit
  -m ONNX model, --model ONNX model
                        ONNX model with weights of neural network
  -th Sparsity threshold (0-1), --threshold Sparsity threshold (0-1)
                        fraction of non-zero coefficients in each kernel that
                        will be set to 0. Default = 0.85.
  -sm JSON sparsity map, --sparsity_map JSON sparsity map
                        Path to JSON sparsity map file, which contains a
                        mapping of the 0-th output tensor name of prunable
                        layers to their corresponding sparsities.
  -cad, --check_average_density
                        Check the average density of the model. If True,
                        returns the average density on the model. Use only -p
                        option, no need to use -op, -th or -m options.
  -cld, --check_layer_density
                        Per layer average density. If True, returns the per
                        layer density on the model. Use only -p option, no
                        need to use -op, -th or -m options.
  -macd, --mac_density  Weight the density of the model with MAC. If True,
                        returns the MAC weighted density of the model. Use
                        only -p option, no need to use -op, -th or -m options.
  -o Output model name, --output Output model name
                        defaults to pruned_&lt;th&gt;_&lt;input_model&gt;.onnx

build $ caffe_prune_model.py -h
usage: caffe_prune_model.py [-h] -p Prototxt -m CaffeModel [-th Threshold 0-1]
                            [-cad] [-cld] [-macd] [-o Output model name]

Script to prune caffe model

optional arguments:
  -h, --help            show this help message and exit
  -p Prototxt, --prototxt Prototxt
                        deploy.prototxt
  -m CaffeModel, --model CaffeModel
                        caffemodel file
  -th Threshold (0-1), --threshold Threshold (0-1)
                        fraction of non-zero coefficients in each kernel that
                        will be set to 0. Default = 0.85
  -cad, --check_average_density
                        Check the average density of the model. If True,
                        returns the average density on the model. Use only -p
                        option, no need to use -op, -th or -m options.
  -cld, --check_layer_density
                        Per layer average density. If True, returns the per
                        layer density on the model. Use only -p option, no
                        need to use -op, -th or -m options.
  -macd, --mac_density  Weight the density of the model with MAC. If True,
                        returns the MAC weighted density of the model. Works
                        only with check_average_density option
  -o Output model name, --output Output model name
                        defaults to &lt;input_model&gt;_pruned_&lt;th&gt;.pb

build $ tf_prune_model.py -h
usage: tf_prune_model.py [-h] -p Input protobuf [-o Output protobuf name]
                         [-th Sparsity threshold 0-1]
                         [-m Json file containing weight sparsity map] [-cas]
                         [-cls]

Script to prune tensorflow coefficients to th sparsity

optional arguments:
  -h, --help            show this help message and exit
  -p Input protobuf, --protobuf Input protobuf
                        frozen model (protobuf)
  -o Output protobuf name, --output Output protobuf name
                        defaults to &lt;input_protobuf&gt;_pruned_&lt;th&gt;.pb
  -th Sparsity threshold (0-1), --threshold Sparsity threshold (0-1)
                        fraction of non-zero coefficients in each kernel that
                        will be set to 0. Default = 0.85
  -m Json file containing weight sparsity map, --map Json file containing weight sparsity map
                        sparsity map for individual layers (layers not in the
                        map are set to --threshold)
  -cas, --check_average_sparsity
                        Check the average sparsity of the model. If True,
                        returns the average sparsity on the model. Use only -p
                        option, no need to use -op, -th or -m options.
  -cls, --check_layer_sparsity
                        Per layer average sparsity. If True, returns the per
                        layer sparsity without on the model. Use only -p
                        option, no need to use -op, -th or -m options.
</pre><hr  />
<h1><a class="anchor" id="sec_cnngen_cvflow_layer_profiler"></a>
18 CVflow Layer Profiler</h1>
<ul>
<li><p class="startli">Why do users need CVflow Layer Profiler?</p>
<p class="startli">The profiler will generate a spreadsheet which contains detailed performance information about the neural network running on Ambarella's chip. With this tool, users can have a clearer understanding about the different bottlenecks in the current neural network structure as well as what optimizations they can do to improve the performance.</p>
</li>
<li>How do users use the CVflow Layer Profiler?<ol type="1">
<li><p class="startli">Parser.</p>
<p class="startli">Use parser (TensorFlow parser, Caffe parser, ONNX parser) to generate a CNNGen output folder for the network.</p>
</li>
<li>Vas Compilation. <pre class="fragment"> build $ vas -auto $(vas_basename).vas (e.g. vas -auto lenet.vas)
</pre></li>
<li>Generate Profiling Files for Cavalry. <pre class="fragment"> build $ cd $(path_to_cnngen_output_dir)/vas_output
 build(cv2x &amp; cv5x) $ v2app_prof -i $(vas_basename) --input-data $(name_of_input_node)=$(path_to_input_bin_img) --profile --no-driver --cavalry
     (e.g. v2app_prof -i bvlc_googlenet --input-data data=../../cavalry_bvlc_googlenet/13.bin --profile --no-driver --cavalry)
 build(cv7x) $ v2app_prof -i $(vas_basename) --input-data $(name_of_input_node)=$(path_to_input_bin_img) --profile --version 2 --cavalry
     (e.g. v2app_prof -i bvlc_googlenet --input-data data=../../cavalry_bvlc_googlenet/13.bin --profile --version 2 --cavalry)
</pre> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can find the input node’s name in .json. Oport's <code>id</code> of the primitives that have <code>type</code>:<code>input</code> are the input nodes name.</li>
<li>These profiler tools are supported since CNNGen Tool 2.3.0.</li>
</ul>
</dd></dl>
</li>
<li><p class="startli">Run on EVK on Cavalry.</p>
<p class="startli">a. Compile the CVflow profile tool. Select <code>test_vp_profile</code> tool when building the SDK. </p><pre class="fragment">    build $ make menuconfig
        [*] Ambarella Unit Test Configuration  ---&gt;
            [*] Ambarella Private Linux Unit test configs ---&gt;
                [*] Build CV unit tests  ---&gt;
                    [*] Build VP Profile unit tests
    build $ make -j8
</pre><p class="startli">b. Copy <code>v2app_exe.tar</code>, <code>network.bin</code> and <code>input_file.bin</code> to the EVK via NFS or SDcard, untar the <code>v2app_exe.tar</code> to <code>v2app_exe</code> folder. </p><pre class="fragment">    board # tar -xvf v2app_exe.tar
    board # ls –l v2app_exe
        v2app_exe/split_0/vp_prof.cfg
        v2app_exe/split_0/vp_prof_config.bin
        ...
</pre><p class="startli">c. Specify <code>-p</code> option to <code>v2app_exe</code> folder when running <code>test_vp_profile</code>. </p><pre class="fragment">    board # modprobe cavalry; cavalry_load –f /lib/firmware/cavalry.bin
    board # test_vp_profile –b &lt;network.bin&gt; --in &lt;input_name&gt;=&lt;input_file&gt; -p v2app_exe
    board # ls –1 v2app_exe
        clock.txt
        performance.csv
        split_0
        ...
    board # ls v2app_exe/split_0
        vp_prof.bin  vp_prof.cfg  vp_prof_config.bin
</pre><p class="startli">d. After success, <code>clock.txt</code>, <code>performance.csv</code>, and <code>vp_prof.bin</code> will be generated for each DAG. Copy back <code>v2app_exe</code> folder to host PC.</p>
</li>
<li>Run Profiler. <pre class="fragment"> build $ cd $(path_to_cnngen_output_dir)
 build $ profiler.py -n $(vas_base_name) -od $(path_to_cnngen_output_dir) -vo $(path_to_vas_output_dir) -b $(path_to_bub_output) –o &lt;result.xlsx&gt;
     (e.g.  profiler.py -n mobilenetv1_ssd -od . -vo vas_output/ -b vas_output/v2app_exe -o mobilenetv1_ssd.xlsx)
</pre></li>
</ol>
</li>
<li>Demo (CV5_TIMN [Toolchain:2.5.2.1] CVflow Profiler, take a example for yolov5s)<ol type="1">
<li><p class="startli">Parser.</p>
<p class="startli">Use parser <code>yolov5s.onnx</code> to generate a CNNGen output folder for the network.</p>
</li>
<li>Vas Compilation. <pre class="fragment"> build $ vas -auto onnx_yolov5s.vas
</pre> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Steps 1 and 2 can be generated separately, or can be generated in one step when quantifying the model using the cnngen tool chain.</li>
</ul>
</dd></dl>
</li>
<li>Generate Profiling Files for Cavalry. <pre class="fragment"> build $ cd onnx_yolov5s/out_onnx_yolov5s_parser/vas_output/
 build $ v2app_prof -i onnx_yolov5s --input-data images=cavalry_onnx_yolov5s/bus.bin --profile --no-driver --cavalry
</pre></li>
<li>Copy <code>v2app_exe.tar</code>, <code>network.bin</code> and <code>input_file.bin</code> to the EVK via NFS or SDcard, untar the <code>v2app_exe.tar</code> to <code>v2app_exe</code> folder. <pre class="fragment"> board # tar -xvf v2app_exe.tar
 board # ls –l v2app_exe
     v2app_exe/split_0/vp_prof.cfg
     v2app_exe/split_0/vp_prof_config.bin
     ...
</pre></li>
<li>Specify <code>-p</code> option to <code>v2app_exe</code> folder when running <code>test_vp_profile</code>. <pre class="fragment"> board # modprobe cavalry ; cavalry_load -f /lib/firmware/cavalry.bin
 board # test_vp_profile -b onnx_yolov5s_cavalry.bin --in images=bus.bin -p v2app_exe
         (It's finished after show "VP Profile Routine Done" message).
 &lt; Copy back v2app_exe folder to host via NFS or SDcard &gt;
</pre></li>
<li>When running layer_profiler on the host, set <code>-b</code> to the copied-back <code>v2app_exe</code>. It will generate a spreadsheet after the operation is done. <pre class="fragment">build $ cd onnx_yolov5s/out_onnx_yolov5s_parser/
build $ profiler.py -n onnx_yolov5s -od ./ -vo vas_output/ -b vas_output/v2app_exe -o onnx_yolov5s.xlsx
build $ ls -l onnx_yolov5s.xlsx
</pre></li>
</ol>
</li>
<li>Explanation and Q&amp;A of Terms<ol type="1">
<li>Spreadsheet Explanation<ol type="a">
<li><p class="startli">Spreadsheet Header</p>
<a class="anchor" id="Spreadsheet Header Parameters of CVflow Layer Profiler"></a>
<table class="doxtable">
<caption></caption>
<tr align="middle">
<th>Header </th><th>Explanation </th></tr>
<tr align="left">
<td>Chip </td><td>Chip version that generates the profiling data. </td></tr>
<tr align="left">
<td>VP Clock </td><td>VP clock of the bub that generates the profiling data. Users can modify the VP clock to estimate performances at different VP clocks. </td></tr>
<tr align="left">
<td>DRAM Clock </td><td>Dynamic random access memory (DRAM) clock of the bub that generates the profiling data. Users can modify the DRAM clock to estimate performances with different DRAM clocks. </td></tr>
<tr align="left">
<td>Total Execution Time (ms)</td><td>Total execution time of the network running on the board. </td></tr>
<tr align="left">
<td>Total Load Time (ms)</td><td>Total load time of loading the constant files (.dvi) of the network from DRAM to vision memory (VMEM). </td></tr>
<tr align="left">
<td>Total HMB Bandwidth (bytes) </td><td>Sum of the activation input and output size through DRAM of the whole network. </td></tr>
<tr align="left">
<td>Overall coeff density (%) </td><td>Overall coefficient density of the whole network. </td></tr>
</table>
</li>
<li><p class="startli">Split Summary</p>
<a class="anchor" id="Split Summary Parameters of CVflow Layer Profiler"></a>
<table class="doxtable">
<caption></caption>
<tr align="middle">
<th>Field </th><th>Explanation </th></tr>
<tr align="left">
<td>Cycles </td><td>Execution cycles of the split. </td></tr>
<tr align="left">
<td>Cycles% </td><td>How much the percentage of execution cycles the split contributes to the whole network. </td></tr>
<tr align="left">
<td>Execution time (ms) </td><td>Execution time of the split. </td></tr>
<tr align="left">
<td>Load time (ms) </td><td>Time of loading the constant files of the split DAG. </td></tr>
<tr align="left">
<td>Constant data </td><td>Constant data (such as weights files) size and related cycles. </td></tr>
<tr align="left">
<td>Activation DRAM input </td><td>Activation input through DRAM size and related cycles. </td></tr>
<tr align="left">
<td>Activation DRAM output </td><td>Activation output through DRAM size and related cycles. </td></tr>
<tr align="left">
<td>Hardware managed buffer (HMB) / DAG (%) </td><td><code>(Sum of activation DRAM cycles)</code> / <code>(split execution cycles)</code> (%). CVflow® manages these transfers so that the data is available in time before processing. HMB transfers occur in parallel with processing. This demonstrates total <code>input</code> / <code>output</code> (I/O) cycles against the split DAG cycles. In CVflow architecture, DRAM loads and stores to the internal memory in parallel with the execution of data engines. A ratio of <code>HMB</code> / <code>DAG</code> below 100% indicates that DRAM I/O is not currently a bottleneck when running standalone. Note, however, that the closer the ratio is to 100%, the respective split DAG duration may be more susceptible to more load in the system. At 100%, the duration will certainly be affected by more load in the system. </td></tr>
<tr align="left">
<td>Sum (critical cycles) / DAG (%) </td><td><code>(Sum of critical engine cycles)</code> / <code>(split execution cycles)</code> (%). This displays the efficiency of the split DAG. Typically, the total critical cycles should be very close to the DAG cycles. However, due to pipelines overhead, this may not be the case. </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Lower <code>hmb/dag (%)</code> or higher <code>sum(critical cycles)/dag (%)</code> indicates higher efficiency of the split DAG.</li>
</ul>
</dd></dl>
</li>
<li><p class="startli">Columns</p>
<a class="anchor" id="Columns Parameters of CVflow Layer Profiler"></a>
<table class="doxtable">
<caption></caption>
<tr align="middle">
<th>Column </th><th>Explanation </th></tr>
<tr align="left">
<td>Origin layer </td><td>Name of the layer in the original model. Blank means generated by CNNGen. Note that CVTools may have optimization where we fuse multiple layers into one. (Users can find the current and target density of the convolutional layers in the comment) </td></tr>
<tr align="left">
<td>Primitive ID </td><td>Number ID to identify different primitives. Users can find data format and the current and target density of the convolutional layers in the comments. </td></tr>
<tr align="left">
<td>Primitive type </td><td>Type of the primitive. Primitives are specified in CVflow specification. Users can find the I/O and kernel dimension in the comments. </td></tr>
<tr align="left">
<td>Critical engine cycles </td><td>Sum of the bottleneck engine’s execution cycles of the primitive. CVflow includes multiple data engines for processing. The primitives are mapped into those data engines. Total split DAG execution cycles represent data engines that use the most cycles. This is called the critical data engine. In most of the NN, convolution processing is a critical engine. If this column is 0, it indicates that the primitive is not mapped to the critical engine. Further, Ambarella placed the combined cycles of all data engines that the primitive is mapped to in the comments, called <b>hardware (HW) raw cycles</b>. </td></tr>
<tr align="left">
<td>Critical engine cycles (split%) </td><td><code>(Sum of bottleneck engine’s operator cycles in the primitive)</code> / <code>(Sum of bottleneck engine’s operator cycles in the split)</code> (%). With the percentage in this column, the user can understand which layers use most of the cycles in the split. Optimizing (pruning or quantizing) those layers may result in potential performance gain. </td></tr>
<tr align="left">
<td>Critical engine cycles (model%) </td><td><code>(Sum of bottleneck engine’s operator cycles in the primitive)</code> / <code>(Sum of bottleneck engine’s operator cycles in the whole model)</code> (%). With the percentage in this column, the user can know which layers use most of the cycles in the whole network. Optimizing (pruning or quantizing) those layers may result in potential performance gain. </td></tr>
<tr align="left">
<td>Macs </td><td>The number of Mac counts found in the primitive. Users can find the critical engine related Mac count in the comments. </td></tr>
<tr align="left">
<td>Critical engine efficiency (%) </td><td>Efficiency of the primitive that goes into critical engine. Note that the number could be higher than 100% due to the sparsity in activation data. </td></tr>
</table>
</li>
</ol>
</li>
<li>Extra Information<ol type="a">
<li>Users can find each primitive's input, constant, and output data format in the comments of column B.</li>
<li>Users can find the current density and the target density of the primitive in the comments of column B (only for those related to convolutional layers). If the target density is 0, it means that users should prune the kernel as sparsely as possible. If the target density is higher than the current density, it means that pruning will not improve performance.</li>
<li>Users can find each primitive's input, constant, and output dimension in the comments of column C.</li>
<li>If the primitive id is blue, it means either the input or weight of this primitive is larger than 8 bits. Can be further optimized by quantization. <div class="image">
<img src="../../Primitive_that_can_be_further_optimized_by_quantization.png" alt=""/>
<div class="caption">
Primitive_that_can_be_further_optimized_by_quantization.</div></div>
</li>
<li>If the primitive id background color is pink, it means this primitive is related to DRAM I/O. <div class="image">
<img src="../../Primitive_that_is_related_to_DRAM_IO.png" alt=""/>
<div class="caption">
Primitive_that_is_related_to_DRAM_IO.</div></div>
</li>
<li>If the primitive has no information, it means that it has been folded with other primitives during compilation.</li>
<li>Primitives in red color indicate that they are in the bottleneck data engine of this split (meaning those primitives are executed by the engine that has the longest cycles in the split). Ambarella prefers that the bottleneck to be convolution. In this case, users can benefit from the power of pruning, which will improve the performance of the NN.</li>
<li>If the primitive possesses <b>macs</b> but columns D through F are empty, it indicates that the cycles for this primitive are under the sampling resolution of the tool. Ambarella does not have the profiling data for it. Because the cycles are too small to detect, it will not be important to the total performance.</li>
</ol>
</li>
<li>FAQ<ol type="a">
<li><p class="startli"><b>Total load time</b> is the time for loading DAG binary to VMEM but it’s not included the time of loading input from DRAM to internal memory. Is there a way to know the time from DRAM to internal memory?</p>
<p class="startli">The initial load depends on how much the buffer is allocated in partial buffer and prefetched by CVflow. Typically this number is very small. Users can get some idea on how small it is by looking at <code>critical_cycles / DAG</code> (%). This includes all the overheads, including prefetching of partial buffers.</p>
</li>
<li><p class="startli">Why are there any <b>Critical engine efficiency</b> over 100%?</p>
<p class="startli">Ambarella's fast convolution engine does <code>A[i]*W[i] + B</code> operation where A stands for activation data which is a variable based on input image, <code>W</code> stands for pretrained coefficient of neural network, <code>B</code> stands for pretrained bias value, and <code>i</code> stands for each channel. Each of Ambarella's CV chipsets has a maximum number of such operations that they can do. Suppose Ambarella can perform hundreds of such operations per cycle. The efficiency number shows how many operations were actually executed out of the ones that were scheduled. As users know, CVflow smartly skips multiplication of zero. So if <code>A</code> is zero or <code>W</code> is zero; that multiplication is skipped. Because the weights are constants and are the results of training; Ambarella recommends pruning of the neural network. By this way, it will produce lots zeros in weights. Note that these savings for zeros are known already, it is not included in efficiency. However, any zeros in activation data depends on input image and hence, cannot be predetermined. So for example if 10 samples of the activation data are zeros, then Ambarella can execute 110 operations on the fast convolution data engine even though its capacity is 100. Hence, users can see 110% efficiency. Here is another example. Say there are 1000 weights and there are no other inefficiencies in execution of the data engine. If there are any other inefficiencies, it will show efficiency less than 100%.</p><ul>
<li>For full dense (no zero coefficients) and activation having no zeros, it will take 10 cycles and it will show 100% efficiency.</li>
<li>If the network is trained and there are only 20% (200) non-zero weights and no zeros in activation, then it will take 2 cycles and it will show 100% efficiency.</li>
<li>If the network pruned as above with only 20% non-zero weights and there 10 zeros in activations that matter (where corresponding weight is not zero), then it will take 1.9 cycles and it will show 105% efficiency.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Note above example is highly simplified. Typically other architectures have high inefficiency due to data not being ready for execution. In our architecture, we avoid that with HMB; however we could still have some inefficiencies due to neural network graph structure. It is very rare to see benefits due to activation zeros as one cannot count of input image. However customer can definitely take advantage of pruning. We do graph analysis to show what target sparsity will give you good usage of data engine.</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
<li>Improvement that Users can Make<ol type="1">
<li><p class="startli">Pruning</p>
<p class="startli">In the comment of <b>origin layer</b>, users can find the layer's <b>current density</b> and <b>target density</b> for convolutional layers. If <b>current density</b> is higher than <b>target density</b>, pruning the respective layer towards its target density should improve the performance. However, if the convolution engine is not the bottleneck of a respective DAG split, then pruning will not be likely to improve performance. If <b>current density</b> is lower than <b>target density</b>, it means pruning will not improve the performance of this layer.<br  />
 To estimate how much improvement can be provided by pruning, users can perform <b>dumb pruning</b> to the model and compare the performance with the fully dense model on Ambarella's silicon. Dumb pruning means not using any pruning algorithm, but just pruning each layer to a certain sparsity. </p><dl class="section note"><dt>Note</dt><dd>Dump pruning should only be used for performance estimation; it should not be used in real case since it normally comes with bad accuracy.</dd></dl>
</li>
<li><p class="startli">Quantization</p>
<p class="startli">If quite a few of the data size (in the comment of <b>primitive id</b>) are fp16 or even higher, doing quantization to the model will have potential performance gain.</p>
</li>
<li><p class="startli">Network Restructure</p>
<p class="startli">Some layer types are more efficient than others when mapped to hardware. Looking at the execution cycles of the primitives marked red and activation size going through DRAM, to and from the splits, are good indicators of places where customers can modify their network in order to optimize the performance.</p>
</li>
<li><p class="startli">Network Output</p>
<p class="startli">The end of a network may have data reordering operations, like reshape or transpose, which might be more beneficial to be handled on Arm® instead.</p>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_cnngen_cvflowbackend"></a>
19 CVFlowBackend</h1>
<p>CVFlowBackend is an integrated library for deploying models onto&#160;Ambarella's&#160;CV-series SoCs. The primary user interface of the library is through its Python APIs, in sync with most modern frameworks for deep learning algorithm development; usage of the library’s core features as a command-line tool is also available for simplified use.</p>
<ol type="1">
<li><p class="startli">Prepare Stage</p>
<p class="startli">The "Prepare Stage" refers to a collection of workflows and methods for constructing a MetaGraph. There are three distinct workflows currently supported by CVFlowBackend, which are summarized as following:</p><ul>
<li><p class="startli">Workflows</p>
<table class="doxtable">
<tr>
<th style="text-align: center;">Workflow </th><th style="text-align: center;">Required Inputs </th><th style="text-align: center;">Description </th></tr>
<tr>
<td style="text-align: center;">Prepare </td><td>1.Original model<br  />
 2.JSON MetaGraph descriptor<br  />
 3.DRA files </td><td>The canonical workflow for compiling neural networks defined in popular machine<br  />
learning frameworks.<br  />
 The resulting MetaGraph should be homeomorphic to the original computationgraph,<br  />
with the exception of pre/post-processing operations that were introduced at<br  />
compile time. </td></tr>
<tr>
<td style="text-align: center;">Compose </td><td>1.Pre-built artifacts<br  />
 2.JSON MetaGraph descriptor<br  />
 3.DRA files (optional) </td><td>Construct MetaGraphs with arbitrary topologies from pre-generated artifacts,<br  />
e.g. Pre-compiled CNNGen outputs. </td></tr>
<tr>
<td style="text-align: center;">Convert </td><td>1. MetaGraph (AmbaPB) </td><td>Convert MetaGraph (AmbaPB) Conversion of a MetaGraph to another type.<br  />
1. Fast-checkpoint &lt;-&gt; Checkpoint<br  />
2. Fast-checkpoint -&gt; Frozen<br  />
3. Checkpoint -&gt; Frozen<br  />
 </td></tr>
</table>
</li>
<li>Command-line tool for preparing a MetaGraph <pre class="fragment">  USAGE: prepare.py [flags]
      Try --helpfull to get a list of all flags.

  Examples:

  build $ prepare.py \
      -fw onnx \
      -m path/to/model.onnx \
      -g path/to/graph_desc.json

  build $ prepare.py \
      -fw tensorflow \
      -m path/to/frozen_graph.pb \
      -g path/to/graph_desc.json

  build $ prepare.py \
      -fw caffe \
      -m path/to/net.prototxt \
      -w path/to/model.caffemodel \
      -g path/to/graph_desc.json
</pre></li>
</ul>
</li>
<li><p class="startli">Evaluation Stage</p>
<p class="startli">The cvflowbackend library also provides inference support for the MetaGraphs constructed during <code>Prepare Stage</code>. Various modes are supported for precision or performance evaluation, all of which can be used from the user’s host machine environment.</p><ul>
<li><p class="startli">Evaluation Modes</p>
<table class="doxtable">
<tr>
<th style="text-align: center;">Mode </th><th style="text-align: center;">MetaGraph Type(s) </th><th style="text-align: center;">Description </th></tr>
<tr>
<td style="text-align: center;">ades </td><td style="text-align: center;">Checkpoint </td><td>Bit-accurate VP/NVP/GVP emulator on host machine. </td></tr>
<tr>
<td style="text-align: center;">acinference </td><td style="text-align: center;">Fast-Checkpoint / Checkpoint </td><td>AmbaCnn inference(GPU/CPU) on host machine. </td></tr>
</table>
</li>
<li>Command-line tool for running MetaGraph inference <pre class="fragment">  USAGE: evaluate.py [flags]
      Try --helpfull to get a list of all flags.

  Example:

  build $ evaluate.py \
              -m path/to/ambapb \
              -em ades \
              -i "input=path/to/input.bin" \
              -of path/to/ades \
              -da

  build $ evaluate.py \
              -m path/to/ambapb \
              -em acinf \
              -i "input=path/to/input.bin" \
              -of path/to/ac_inf_uq \
              -da

  build $ evaluate.py \
              -m path/to/ambapb \
              -em acinf \
              -fq \
              -i "input=path/to/input.bin" \
              -of path/to/ac_inf \
              -da
</pre></li>
</ul>
</li>
<li><p class="startli">Deploy Stage</p>
<p class="startli">The CVFlowBackend library implements support for converting MetaGraphs into deployment-specific artifacts. Users can choose from several deployment frameworks/modes which are designed to facilitate the optimized execution of neural network computations on Ambarella’s CVflow engines. When deploying for EVK, a cavalry binary is required, and can be generated from a MetaGraph in the following manner:</p><ul>
<li>Command-line tool for deconstructing a MetaGraph. <pre class="fragment">  USAGE: deconstruct.py [flags]
      Try --helpfull to get a list of all flags.

  Examples:

  build $ deconstruct.py \
              -m path/to/your/composed_ambapb.ambapb.ckpt.onnx \
              -of path/to/deploy/output_dir \
              -sv 0

  build $ cavalry_gen \
              -j path/to/deploy/output_dir/manifest.json \
              -f path/destination/cavalry/bin
</pre></li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Quick start with CVFlowBackend, please refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sec_quick_start_cvb">5.2 CVFlowBackend</a>.</li>
<li>For details, please refer to <em>Ambarella CV TOOLS User Guide cvflowbackend.pdf</em>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
<h1><a class="anchor" id="fs_custom_node"></a>
20 Custom Node</h1>
<p>This chapter provides information on the custom node in the Ambarella CNNGen. For more details, refer to the <em>Ambarella CV UG Custom Nodes</em> in the CNNGen toolchain package.</p>
<hr  />
<h2><a class="anchor" id="sec_custom_introduction"></a>
20.1 Introduction</h2>
<p>As deep learning technology continues to develop rapidly, new algorithms are released each year. If <b>Caffe</b> (a deep learning framework developed by BAIR in 2013) does not meet the user’s requirements, users can modify its source code or create unique layer files to accomplish a specific function or algorithm.</p>
<dl class="section note"><dt>Note</dt><dd>Custom node is always used in <b>Caffe</b>, as Caffe's layer unit is bigger than TensorFlow and ONNX. There are so many different layers which are not included in official Caffe, then Ambarella supplies this method to support such layers. For <b>TenforFlow</b> and <b>ONNX</b>, if there is an operator which is not supported. It is suggested not to write custom node, please ask the Ambarella support team for assistance or replace the operator with other operators.</dd></dl>
<h3><a class="anchor" id="sub_sec_custom_define"></a>
20.1.1 Defining the Custom Node</h3>
<p>Generally, Ambarella CNNGen supports the standard version of Caffe (BAIR). However, it is important to note that creating custom layers and converting them through CNNGen can lead to Caffe compilation errors, as the standard version of Caffe will not recognize these layers. To solve this issue, CNNGen provides a serial of specific APIs that enable users to implement their own layers. The layers defined or created by customers are treated as the custom node.</p>
<h3><a class="anchor" id="sub_sec_custom_workflow"></a>
20.1.2 Workflow</h3>
<p>To implement a custom node with the same functionality of permute, users should use the APIs mentioned above to write source codes. After implementing the source codes for the custom node, compile them to get a dynamic linked library <b>example_code.so</b>.</p>
<p>In addition, users must create a Python script such as <em>custom_nodes.py</em> to act as an interface between CNNGen and the dynamic linked library. The custom node can be only used to generate the VAS code. In other words, when <em>caffeparser.py</em> generates custom VAS codes, the Python script and dynamic linked library must be passed to <em>caffeparser.py</em>.</p>
<p>When CNNGen converts a network that contains normal layers, it converts the standard Caffe layers directly. However, for a custom layer, CNNGen calls the <em>custom_nodes.py</em> to check if it has been implemented in the system. If it has been implemented in the system, CNNGen utilizes the corresponding dynamic linked library. If it has not been implemented, a compiling error is returned. For further details about the dynamic linked library <b>example_code.so</b> and the Python script <em>custom_nodes.py</em>, refer to Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_custom_node_example">Custom Node Example</a>.</p>
<h3><a class="anchor" id="sub_sec_custom_usage"></a>
20.1.3 Usage</h3>
<p>To use a custom node, pass <em>custom_nodes.py</em> and example_code.so to the tool <em>caffeparser.py</em>. Refer to Chapter <a class="el" href="../../d2/d67/fs_cnngen.html">CNNGen Toolkits</a> for further details.</p>
<div class="image">
<img src="../../cus_node_flow.jpg" alt=""/>
<div class="caption">
Custom Node Flow</div></div>
   <hr  />
<h2><a class="anchor" id="sec_custom_spec_apis"></a>
20.2 Special APIs</h2>
<p>To enable the Ambarella CNNGen to support a custom layer, users must create a dynamic-link / shared library that implements the following functions for each custom node.</p>
<ul>
<li>init(): creates a node instance on the DLL side and returns node ID and output dimensions</li>
<li>expand(): calls primitive factory functions to construct a primitive sub-graph</li>
<li>release(): custom node release function that is called after expansion</li>
<li>query(): provides CNNGen with an array containing all the information about the custom node it supports</li>
</ul>
<h3><a class="anchor" id="sub_sec_custom_api_init"></a>
20.2.1 init() Function</h3>
<p>The <b>init()</b> function creates an instance of the specific custom node on the DLL side. It returns the DLL node ID and the output dimensions. The DLL node ID is used by CNNGen when it calls <b>expand()</b> and <b>release()</b> for this custom node instance. The output dimensions are used for allocating the output data space during simulation and configuring the subsequent nodes in the graph. The function prototype is provided details below.</p>
<h4><a class="anchor" id="autotoc_md68"></a>
Function Prototype:</h4>
<pre class="fragment">typedef int (*amba_cnn_ext_c_node_init_t)(int* cid, amba_cnn_c_vcoord_t* osz, int num_src, const amba_cnn_c_vcoord_t* isz, const char* attr, const amba_cnn_c_data_format_t* df)
</pre><h4><a class="anchor" id="autotoc_md69"></a>
Parameters:</h4>
<a class="anchor" id="Parameters of init() Function"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Parameter </th><th>Description </th></tr>
<tr align="middle">
<td>cid </td><td>Pointer to DLL node ID (returned by init() function) </td></tr>
<tr align="middle">
<td>osz </td><td>Pointer to array of output vector dimensions (returned by init()) </td></tr>
<tr align="middle">
<td>num_src </td><td>Number of inputs </td></tr>
<tr align="middle">
<td>isz </td><td>Pointer to array of input vector dimensions </td></tr>
<tr align="middle">
<td>attr </td><td>Custom node attributes, passed as a string </td></tr>
<tr align="middle">
<td>df </td><td>Pointer to array of output data formats </td></tr>
</table>
<h4><a class="anchor" id="autotoc_md70"></a>
Returns:</h4>
<pre class="fragment">init() function returns the user-defined error ID (set to 0 if there is no error)
</pre><h4><a class="anchor" id="autotoc_md71"></a>
Example:</h4>
<pre class="fragment">int example_node_init(int* cid, amba_cnn_c_vcoord_t* osz, int num, const amba_cnn_c_vcoord_t* isz, const char* attr, const amba_cnn_c_data_format_t* df)
{…...}
</pre><h3><a class="anchor" id="sub_sec_custom_api_expand"></a>
20.2.2 expand() Function</h3>
<p>The <b>expand()</b> function must construct a primitive sub-graph corresponding to the functionality of the custom node. It is constructed by calling the CNNGen’s primitive factory functions.</p>
<h4><a class="anchor" id="autotoc_md72"></a>
Function Prototype:</h4>
<pre class="fragment">typedef int (*amba_cnn_ext_c_node_expand_t)(int cid, const amba_cnn_ext_c_node_prim_factories_t* funcs)
</pre><h4><a class="anchor" id="autotoc_md73"></a>
Parameters:</h4>
<a class="anchor" id="Parameters of expand() Function"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Parameter </th><th>Description </th></tr>
<tr align="middle">
<td>cid </td><td>DLL node ID returned by init() function </td></tr>
<tr align="middle">
<td>funcs </td><td>Primitive factory functions </td></tr>
</table>
<h4><a class="anchor" id="autotoc_md74"></a>
Returns:</h4>
<pre class="fragment">expand() function returns the user-defined error ID (set to 0 if there is no error)
</pre><h4><a class="anchor" id="autotoc_md75"></a>
Example:</h4>
<pre class="fragment">int example_node_expand(int cid, const amba_cnn_ext_c_prim_factories_t* funcs) {......}
</pre><h3><a class="anchor" id="sub_sec_custom_api_release"></a>
20.2.3 release() Function</h3>
<p>The <b>release()</b> function frees any memory allocated by the custom node instance. It is called after the expansion.</p>
<h4><a class="anchor" id="autotoc_md76"></a>
Function Prototype:</h4>
<pre class="fragment">typedef int (*amba_cnn_ext_c_node_release_t) (int cid);
</pre><h4><a class="anchor" id="autotoc_md77"></a>
Parameters:</h4>
<a class="anchor" id="Parameters of release() Function"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Parameter </th><th>Description </th></tr>
<tr align="middle">
<td>cid </td><td>DLL node ID returned by init() function </td></tr>
</table>
<h4><a class="anchor" id="autotoc_md78"></a>
Returns:</h4>
<pre class="fragment">release() function returns the user-defined error ID (set to 0 if no error)
</pre><h4><a class="anchor" id="autotoc_md79"></a>
Example:</h4>
<pre class="fragment">int example_node_release(int cid){......}
</pre><h3><a class="anchor" id="sub_sec_custom_api_query"></a>
20.2.4 query() Function</h3>
<p>Because a single DLL / SO can contain more than one custom node, every DLL / SO has to provide a <b>query()</b> function for the nodes it contains. The <b>query()</b> function provides CNNGen with an array containing the information of the custom node it supports.</p>
<h4><a class="anchor" id="autotoc_md80"></a>
Function Prototype</h4>
<pre class="fragment">typedef int (*amba_cnn_ext_c_dll_query_t)
(int* num_types, const amba_cnn_ext_c_node_type_t** types);
</pre><h4><a class="anchor" id="autotoc_md81"></a>
Parameters:</h4>
<a class="anchor" id="Parameters of query() Function"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Parameter </th><th>Description </th></tr>
<tr align="middle">
<td>num_types </td><td>Number of entries available (returned by the query() function) </td></tr>
<tr align="middle">
<td>types </td><td>Pointer to the entry (returned by the query() function) </td></tr>
</table>
<h4><a class="anchor" id="autotoc_md82"></a>
Returns:</h4>
<pre class="fragment">The query() function returns the user-defined error ID (set to 0 if there is no error).
</pre><hr  />
<h2><a class="anchor" id="sec_custom_call_seq"></a>
20.3 API Calling Sequence</h2>
<p>After users compile the source codes of custom nodes, they receive a dynamic linked library (<b>example_code.so</b>) that contains the APIs as shown below.</p>
<p>The following diagram displays the calling sequence of custom node APIs.</p>
<div class="image">
<img src="../../cus_node_api_flow.jpg" alt=""/>
<div class="caption">
Custom Node APIs Calling Sequence</div></div>
   <p>When CNNGen uses the custom node, it first calls the <b>query()</b> function to register <b>init()</b>, <b>expand()</b>, and <b>release()</b>. Then, it calls the <b>init()</b> function to initialize the custom node. Next, it calls <b>expand()</b> to bring about the functionality of custom node. Finally, it uses <b>release()</b> to free any memory allocated by the custom node. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
