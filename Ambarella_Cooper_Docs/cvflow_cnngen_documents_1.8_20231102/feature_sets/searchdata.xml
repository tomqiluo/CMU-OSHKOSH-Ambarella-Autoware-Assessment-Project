<?xml version="1.0" encoding="UTF-8"?>
<add>
  <doc>
    <field name="type">file</field>
    <field name="name">feature_sets.doxy</field>
    <field name="url">da/dca/feature__sets_8doxy.html</field>
    <field name="keywords">feature_sets.doxy</field>
    <field name="text">This is a dummy header file for doxygen 0.1 2 0 2 0 1 0 2 0 Copyright c 2 0 2 1 Ambarella International LP This file and its contents are protected by intellectual property rights including without limitation U.S and/or foreign copyrights This Software is also the confidential and proprietary information of Ambarella International LP and its licensors You may not use reproduce disclose distribute modify or otherwise prepare derivative works of this Software or any portion thereof except pursuant to a signed license agreement or nondisclosure agreement with Ambarella International LP or its authorized affiliates In the absence of such an agreement you agree to promptly notify and return this Software to Ambarella International LP THIS SOFTWARE IS PROVIDED AND ANY EXPRESS OR IMPLIED WARRANTIES INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF NON-INFRINGEMENT MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL AMBARELLA INTERNATIONAL LP OR ITS AFFILIATES BE LIABLE FOR ANY DIRECT INDIRECT INCIDENTAL SPECIAL EXEMPLARY OR CONSEQUENTIAL DAMAGES INCLUDING BUT NOT LIMITED TO PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES LOSS OF USE DATA OR PROFITS COMPUTER FAILURE OR MALFUNCTION OR BUSINESS INTERRUPTION HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY WHETHER IN CONTRACT STRICT LIABILITY OR TORT INCLUDING NEGLIGENCE OR OTHERWISE ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE</field>
  </doc>
  <doc>
    <field name="type">file</field>
    <field name="name">basic_feature.doxy</field>
    <field name="url">d9/d12/basic__feature_8doxy.html</field>
    <field name="keywords">basic_feature.doxy</field>
    <field name="text"></field>
  </doc>
  <doc>
    <field name="type">file</field>
    <field name="name">cnngen_samples_package.doxy</field>
    <field name="url">dc/df0/cnngen__samples__package_8doxy.html</field>
    <field name="keywords">cnngen_samples_package.doxy</field>
    <field name="text"></field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_ov</field>
    <field name="url">d9/d7e/fs_ov.html</field>
    <field name="keywords"></field>
    <field name="text">Overview Overview CVflow® Superior AI Performance Per Watt which can accelerate complex and huge calculations no matter for neural networks and traditional algorithm The engine will split the huge neural networks to different DAGs for one DAG this engine will use internal memeory to do the inference instead of slow DRAM To simplify DAG creations for the deep learning network CNNGen toolchain converts neural network to Ambarella format by using a rich set of optimizations and APIs to perform the conversion CNNGen toolchin currently supports Caffe TensorFlow® and ONNX for CV2x CV5x CV7x and CV3x And the converted neural networks can be deployed with Ambarella Cooper SDK on the chip This documents will includes all the process of converting and deployment and it adds lots of examples to help user to understand how to use CNNGen toolchain and Cooper SDK which is organized as shown below Before reading this document the user should have a basic knowledge of deep learning and some experience working with Caffe TensorFlow or ONNX Overview overview and key updates Work Flow specifies the whole work flow on how to convert a public network EazyAI Framework specifies the full EazyAI framework which includes Ptyhon tools Python libraries and C libraries to help users easily port networks Quick Start specifies how users start with CNNGen toolchain to convert networks and deploy with cooper SDK Deployment specifies lots of detailed features for how to deploy a converted models in high efficiency CNNGen Toolkits specifies the tools which are in the CNNGen Toolchain Package and shows some basic usage ONNX Demos specifies lots of reference demos with ONNX Tensorflow Demos specifies lots of reference demos with Tensorflow Caffe Demos specifies lots of reference demos with Caffe Historical Updates old key updates For quick starting with network convert and deployment please refer to Quick Start 3 EazyAI CFlite Python Tools quick guide for EazyAI CFlite python tools which introduces how to port networks with EazyAI Python Tools 4 CNNGUI GUI tools which include quick conversion and quick deployment 5 Quick Shell Script the quick shell script which help users to create the network conversion script easily will be not supported in future SDK Regardless of CVflowv2 and CVflowv3 non-structural pruning or Wegiths pruning will benefit from the CVflow architecture while lots of other chips will not see 2.1 Pruning for details CVflow generation v2 can support lots of different NN networks such as Yolo SSD Centernet Tracking LSTM Transformer and so on For more detail please refer to Ambarella CV DG_Flexible Linux SDK CVflow FAQ Since CNNGen Samples Package 1.6.0 CV72 will be supported in this package which is CVflow generation v3 For CV72 there are lots of more strong features Add support for FP16 and FIX4 for fast convolution data engine Add support for hardware acceleration of transformer networks such as MatrixMultiply operator Efficient element-wise processing arithmetic and binary ops to support complex activation functions Add hardware support for FFT Add new operators support such as 3 D convolution Transpose convolution Deformable convolution Correlation and so on CNNGen toolchain only supports Ubuntu 2 0 0 4 since 2.5.4 and 3.5.4 CVflowv3 for CV72 NVP only can support FP16 output while CVflowv2 for CV2x and CV5x can support FP32 CVflowv3 for CV72 and CV3 can use FP16 for the best accuracy as its ICE can support FP16 CVflowv2 for CV2x and CV5x can only use FX16</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_workflow</field>
    <field name="url">d1/d82/fs_workflow.html</field>
    <field name="keywords"></field>
    <field name="text">Work Flow Work Flow The Ambarella CNNGen converts networks on a PC and then runs them on the CV board In Ubuntu 2 0 0 4 the user provides the model files to CNNGen which then converts the files and generates DAGs that can run on the CV board The basic workflow is as follows Select suitable network with suitable framework Model Optization with quantization and pruning if needed Convert the network to Ambarella format Evalute the accuracy to see if it can meet with the requirement Deploy with Cooper SDK Workflow Workflow Workflow Workflow User should follow below points for differe CVflow chips Floow the chip computing power abality to select the lighter or higher networks Select the networks which will benefit more from CVflow fro detail please refer to Ambarella_CV*_DG_Flexible_Linux_SDK* Choose the networks which will benefit more from weights pruning as CVflow chip will benefit from weights pruning Better not to use public quantization networks differnt quantization will result big accuracy drop Currently three mainstream frameworks are supported as follows Caffe Will not be maintained anymore since CNNGen 2.5.4 and 3.5.4 with Ubuntu2004 Tensorflow 1.x ONNX For other frameworks users should handle as follows For Tensorflow 2.x models users needs to convert them to tflite then to use CNNGen toolchain to convert them to Tensorlfow 1.x then to Ambarella format For Tflite models users need to use CNNGen toolchain to convert them to Tensorlfow 1.x then to Ambarella format For Keras models users need to convert them to Tensorflow 1.x then to use CNNGen toolchain for Ambarella format For Pytorch and others users needs to convert it to ONNX then to use CNNGen toolchain for Ambarella format There are some examples in fs_cnngen_samples_package Darknet to Caffe caffe/demo_networks/yolo_v3/script/darknet2caffe Darknet to Tensorflow tensorflow/demo_networks/tiny_yolo_v3/models/readme.txt Tflite to Tensorflow tensorflow/test_networks/yolo_v4/scripts/readme.txt MXNET to ONNX onnx/demo_networks/retinaface/script/export_retinaface_onnx_model.py Pytorch to ONNX onnx/demo_networks/yolov5/models/readme.txt CNNGen will convert Float 3 2 original model to fix point 8 or 1 6 and DRA will do dynamic analysis for every operator and choose the right format to reduce the accuracy lost If users are using full fix16 there will be less accuracy lost even to be ignored If users are using mixed fix16 and fix8 there will be a little accuracy lost but the performance will be better it is suggested to use this one If users are using full fix 8 the performance will be the best but there will be more accuracy lost Of course it is decided by different networks if users cannot accept this loss with this setting quantization fix8 retrain needs to be done CNNGen has no limitation for which framework users should use there are some suggestions below If users do not needs quantization fix8 retrain they can use whichever framework they prefer and CNNGen can handle well for every framework If users need to use the best performance with fix8 the tool status is as follows Caffe there are some quantization fix8 retrain experience and special tools which can help users to easily retrain but as everyone knows the Caffe framework is so old that it cannot cover some types of networks and it will not be maintain anymore For some basic networks such as SSD or Yolov3 Caffe is good Tensorflow it is the most flexible framework but it is so flexible that there will be a little difficulty handling its model file Also Ambarella has limited experience with doing quantization fix8 retrain with this framework so users should cover it by themselves ONNX is currently the most popular framework as more and more users are using PyTorch and converting the model to ONNX Ambarella will pay more resource on this for conversion and quantization retrain So the framework to be used should be decided by which network users would use and CNNGen tool will not limit the framework users need to use The following diagram shows the recommended optimization flow of the CNN model before deployment CNN Model Optimization Workflow CNN Model Optimization Workflow CNN Model Optimization Workflow CNN Model Optimization Workflow Users can apply Ambarella retrain tools from Ambarella support team to perform pruning and quantization To achieve optimal performance on CV Ambarella recommends pruning the CNN model CVflow chip includes a new faster convolution engine called the inception convolution engine ICE However because ICE supports a fixed-point data format exclusively the data format must be in either FX16 or FX8 CNNGen converts the model’s parameters to either the FX16 or FX8 data format If the original model is based on FX32 accuracy can be lost during conversion less accuracy is lost with 1 6 bit Additionally accuracy can be lost while achieving the best performance with 8 bit quantization Therefore users should first prune perform quantization and then retrain the program before using CNNGen Pruning methods include connection pruning coefficient pruning and more such as below Sketch of Pruning Sketch of Pruning Sketch of Pruning Sketch of Pruning The CVflow benefits exclusively from coefficient pruning sparsification as it includes a high density of zero-valued coefficients Because the CVflow execution engine can bypass known results without performing the MAC there is a direct acceleration when the system is MAC-limited Users can achieve a low non-zero coefficient density while retaining accuracy and receiving a 3 x―4x speed boost on many networks Therefore Ambarella recommends that users prune their CNN models before they are mapped into the CV chip Additionally to prevent a loss of accuracy in the final network mAP Ambarella recommends users retrain the network after pruning For further information contact the Ambarella support team lots of acceleration engines might not benefit from coefficient pruning Connection pruning applies a different pruning technique By reducing the number of convolution kernels processed connection pruning helps both CVflow and other engines but it will result big accuracy lost The following describes the steps for pruning the VGG-SSD network Prepare the original DNN at 1 0 0 density Use a modified training tool to prune the NN for the lower density After pruning retrain the DNN to match the accuracy of the original DNN The following table displays the DNN accuracy after both pruning and retraining VGG16+SSD Data mAP Accuracy Lost 1 0 0 Density FP32 76.50 0 1 8 Density FP32 76.59 0.09 1 3 Density FP32 76.05 8 Density FP32 75.45 5 Density FP32 74.26 The table above shows how the accuracy can match the original 1 0 0 0 4 5 by pruning and retraining The image input size must be 2 4 4 x244 Use VOC0712 full datasets with 2 0 categories for re-training Retrain with 2 2 1 3 5 images Use VOC0712 image for mAP report Use 4 9 5 2 images The mAP testing images are 4 9 5 2 images found in the Pascal VOC2007 datasets Although floating point 3 2 bit is used for training and for inference fixed 8 and fixed 1 6 multiples are faster and more power-efficient As a result all acceleration engines use fixed 8 bit and fixed 1 6 bit for inference at the edge The performance improvement for an 8 bit network is approximately twice that of the 1 6 bit version In Ambarella solution there are two quantization tools to avoid accuracy loss Dynamic Range Analysis DRA which is a post quantization method without retrain Please refer to 6.1 DRA Version History for detail Pre-Quantization Retrain tool which is a post quantization method without retrain This tool is not in default release please ask help from Ambarella support team Only DRA in 1 cannot cover the accuracy then Pre-Quantization Retrain is needed And there are also two method such as below Fake Quantization may have minor accuracy lost for the final deployment in CVflow chip Ambarella Quantization the same with the final accuracy in CVflow chip Sketch of Quantization Sketch of Quantization Sketch of Quantization Sketch of Quantization It is not suggest to use quantization midel which is based on public quantization special for some asymmetric quantization models The following describes the steps for performing quantization for the VGG-SSD network Prepare the sparse NN Use CNNGen DRA tools to quantize from FP to FX8 FX16 Retrain with a modified training tool if accuracy loss is too high The following table shows the DNN accuracy after quantization with CNNGen DRA tool VGG16+SSD Data mAP Accuracy Lost 1 8 Density FP32 76.59 0.09 1 8 Density FX8 76.05 This shows that by using the CNNGen DRA tool to quantize the network the accuracy nearly matches the original level The image input size must be 2 4 4 x244 Use VOC0712 full datasets with 2 0 categories for retraining Retrain with 2 2 1 3 5 images Use VOC0712 image for the mAP report Use 4 9 5 2 images The mAP testing images are the 4 9 5 2 images that come from the Pascal VOC2007 datasets The figure below shows the flow of the porting procedure Porting Flow Porting Flow Porting Flow Porting Flow The porting flow shown above is as follows The CNNGen parser expands the node graph to primitive graph and performs quantization and computation reduction with DRA and custom node DRA determines the data format by data distribution calculated by the sample images given by the designer Custom node provides a serial of specific APIs that enables users to implement their own layers Run Acinference in build server to verify results after DRA VAS expands the primitive graph to operator graph DAG and performs the low-level optimization and DAG splitting Users run the VAS generated DAGs on ADES in build server to verify results after Vas compiler Users use cavalry_gen script to generate final executive binary and use Ambarella’s interface to run it on CV board Users can use eazyai_cvt xxx which inlcudes all above steps with below models For details please refer to 2.3 EazyAI Convert Tool Preprocess Mdels in 1 3 Pre-Process Model Tools Pruning test toools in 1 7 Pruning Test Tools Parser in 6 Parsers Vas Cavarly_gen in 1 1 Cavalry_gen Layer_compare in 1 2 layer_compare.py CVflowbackend in CVFlowBackend Layer Profiler in CVflow Layer Profiler It is suggested that users need to save the parser output for their networks for details please refer to 1 0 Compatibility The CNNGen uses dynamic range analysis DRA to determine the data format using data distribution The CNNGen calculates the sample images provided by the designer Ambarella recommends using between 1 0 0 and 2 0 0 images for the DRA to maintain accuracy between the following two cases Case 1 Customer uses the side-band information input of CNNGen to perform custom quantization on the required layers to maintain NN constraints such as preservation of accuracy Case 2 Customer uses the Ambarella CNNGen tool with DRA to perform quantization on all layers DRA Flow DRA Flow DRA Flow DRA Flow After converting the model using CNNGen users must confirm the accuracy of the model Use layer compare with eazyai_cvt xxx lc in 2.3 EazyAI Convert Tool to check the details Actually this tool will call layer_compare.py in 1 2 layer_compare.py to run network infrence with PC Acinference and Ades then to compare every layer s result to see if there is accuracy lost Note that this tool s result is only based on one image Of course user also can draw the network result on images to check if it is correct for this user can use eazyai_inf xxx in 2.4 EazyAI Inference Tool to do this inference which will call the tools in 1 0 x86 Simulator Due to the architectural differences with the x86 platforms CVflow performs the CNN in the fixed-data type such as Fix-8 Fix-16 So besides above basice accuracy test user need to perform accuracy testing with own dataset for cosine topn and mAP Therefore there is a little difference between the CVflow accuracy result and the original FP32 accuracy result short data range may bring accuracy loss For quantization problems please refer to 2.2 Quantization for detail Accuracy evaluation is very important to measure if the convert session is successful Based on a big test datasets good accuracy means this network can cover different scenes with good accuracy And close accuracy between original framework and CVflow Ades Acinf means the convert session are successful Different with layer_compare or simple compare with several images between accuracy is the final measurement indicators for the network Then EazyAI inference tools eazyai_inf in 2.4 EazyAI Inference Tool is able to generate four types of network results for this evaluation Original Framework FP32 results Acinference Simulator results Ades Simulator results CVflow results For the difference between Acinference and Ades please refer to 1 0 x86 Simulator for detail And it is more suggested that users can use Original Framework and CVflow to get the accuracy loss as CVflow chip is much faster than Acinference and Ades Below chapter includes guidelines for setting up the accuracy evaluation flow There are four major parts in the accuracy workflow data preparation inference post-processing and evaluation Workflow of the Accuracy Regression Test Workflow of the Accuracy Regression Test Workflow of the Accuracy Regression Test Workflow of the Accuracy Regression Test Below is the simpe introduction for these modules Data Prepare The data preparation module generates the data for network forward inference It includes converting data from its original format to the inference format For example it is able to convert images from jpg format to binary format it is also able to convert wav format data to binary format 2 Inference The inference module does the neuron network forward inference there are four available network inference modes which is metioned in previous Original inference The forward inference is done by the network original platform(x86) which means Caffe/TensorFlow/ONNX Acinference The forward inference is done by the AmbaCNN simulator(x86) which is the converted network after the DRA stage The result after this stage can help the users to know if there is any accuracy lost after network converting and DRA quantization The result of this mode is very close to the result of the mode cvflow Ades The forward inference is done by the Ades simulator(x86) which is the converted network after the VAS stage The result after this stage can help the users to know if there is accuracy lost after network converting DRA quantization and VAS compilation The result of this mode should be bit-perfect with the mode cvflow CVflow The forward inference is done by the vector processor(CVflow) of the EVK board This mode reflects the real inference speed and the real inference result in the deployment Post Process The post-process module gets the forward inference result and do the post-processing on the network result For example draw segmentation map on the original pictures return bounding-boxes to the application side etc Evaluation The evaluation module does the network accuracy evaluation There are four valid modes in evaluation configuration TopN Cosine mAP and mIOU However users are responsible for determining whether the new test mode fits the network type Typically TopN evaluates classification networks the Cosine evaluates feature extraction networks mAP evaluates detection networks and mIOU evaluate segmentation networks For more information about CFLite modules please refer to 2 CFlite Modules Please to 1 Prepare the Binary foe detail Please refer to 3 Environment Setting for detail Please refer to 1.4 Download Models to download data folder from Ambarella web share and put is in CNNGen samples ROOT_DIR folder such as below For ILSVRC_2012 please download ILSVRC2012_img_val.tar manually and check it md5sum as 2 9 b22e2961454d5413ddabcf34fc5622 For this dataset it requires special register to have the permisson to download it The csv file is the ground truth file for TOPn and mAP seg_ground_truth is the ground truth file for MIOU These are all open source dataset for study and test purpose so please pay attention to their license statement if not only for study and test Of course users should have their own dataset for their own network To use the dataset prepare the label file in the same format as _label.csv located in the same folder as the download script Users can use below command to convert the network After the compiling the result is stored under There is a convert summary YAML file which is named as _cvt_summary.yaml generated under the same folder This summary file is an input configuration for the inference stage in the next step For the detail of this convert tool please refer to 2.3 EazyAI Convert Tool After compilation start the accuracy test execution using the following commands for original framework and CVflow Please ensuring that the network connection between the PC and EVK is valid when using CVflow Using platform ades and platform acinf to do simulator inference if needed In normal case only _cvt_summary.yaml is needed for network forward inference However in the accuracy case a ea_inf_&lt;network_name&gt;.yaml is necessary since it provides post-processing configuration and evaluation configuration IP address is needed for platform cvflow Whether to run post-processing or evaluation depends on whether the ea_inf_&lt;network_name&gt;.yaml has the root key or For TopN cosine mIOU these three mode do not forcedly require post-processing For mAP post-processing is necessary For the detail of this convert tool please refer to 2.4 EazyAI Inference Tool Please use _no_postfix.csv as the ground truth file For cosine evaluation mode the evaluation will be done during the second inference while using the option compare_folder Besides the accuracy evaluation result the network result is alse saved in below locations The inference result files can be found in The post-processing result files can be found in The evaluation report file can be found in Please refer to Regression for VGG16-SSD for detailed exmaples To test the accuracy for original and CVflow chip in the same baseline above full accuracy test flow has done below things Using the same test datasets Using the same method for data prepare Using the same method for post process Using the same method for accuracy evaluation But users may need to use their own data prepare method and post process then they can do as below based on current accuracy test flow Users can develop their own data prepare module and calls the inference module The return value of the data prepare module is a string of a list file path in which contains the absolute path of the converted binary files Therefore users can starts from changing the calling of this part to use their own method Users can also change from the inference side The inference module takes a portname-filepath pair to set the file path for each network input port Users can add their own post-processing by adapting to the output of the inference module The output of the inference module is a dictionary using network output port name as key and the inference result file as value Then users can load these result files and do their own post-processing Answers for some common questions are as follows Question How does data prepare module convert images to binaries Answer The default tool to convert images to binary files is OpenCV Question How to narrow the accuracy gap between different inference modes Answer For different inference modes ensure that the same tool and environment are used to generate the RGB data Using different tools or environments to convert the images to binaries may import data difference and cause issues in the binaries data and ultimately skew the final result Question Why did cvflow inference mode fail Answer Users need to ensure that the network(telnet) connection between the EVK and the PC is valid And as the CVflow forward inference is done by the test_eazyai application user also can check if it is available from the EVK side Question How to debug when some failure happens during inference Answer Users can set the option loglevel as 4 to check the debug log and see if the user can get any clue If not please contact Ambarella support Users can use eazyai_inf to do the network inference which will call CVflow chip with enternet port and generate an app ication command which can run on EVK for user s reference As this application will call the EazyAI C library in chip user can refer to this comand and to be familar with how to write own application wioth EazyAI library For more information refer to 4 EazyAI Inference C Library For more information of the deployment with Cooper SDK please refer ro Deployment The following table lists the network performances used in the CNNGen samples package The values below are only for reference and are subject to changes with different toolchain versions For an accurate value test it with the CNNGen samples package As the samples package does not provide all of the model files listed in the table below users should find and download the relevant versions to CNNGen As shown in the table below CV2 4 x CV22 1 6 x CV28 CV5 0.6 0.8 x CV2 CV72 1.2 1.5 x CV2 CV25 is the half of CV22 performance as CV22 Vision clock is 1 0 0 8 MHz but CV25 is 5 0 4 MHz they share the same VP hardware For CV28 pruning no rested performance yet but users can refer to other chips pruning performance improvement as the ratio should be the same CVflow will benefit a lot from pruning there will be large performance improvement Some networks will benefit little from pruning such as efficientnet and mobilenet as they have less weights Although CVflow for CV5 is a little weaker than CV2 CV5 has much more strong Arm DSP and sufficient DRAM bandwidth That mean Arm will help more with SVE support and high resolution encoding will not affect CVflow performance as there are sufficient DRAM bandwidth As CV72 adds new hardware support for some networks such transformer networks non-linear activation and so on it will have huge improvement even 4 x 5 x or 1 0 x CNN Models CV28 Performance 1 x ms CV22 Performance ms CV2 Performance ms CV5 Performance 8 x ms CV72 Performance ms Full Model 2 0 Density Full Model 2 0 Density Full Model 2 0 Density Full Model 2 0 Density Full Model 2 0 Density mobilefacenets_fix8_112_96 8.3448 6.87466 2.0846 N/A 0.5955 N/A 0.4779 N/A 0.306 N/A mobilenetv1_ssd_fix8_300_300 27.8482 13.9781 6.9175 3.1537 1.9493 0.9593 2.3929 1.2895 N/A N/A resnet50_fix8_224_224 73.4317 24.5195 18.7301 6.0244 5.6007 2.1567 6.5914 2.5625 N/A N/A vgg19_fix8_224_224 336.095 90.159 88.0988 22.3899 23.5030 7.0427 28.971 8.5444 N/A N/A vgg16_ssd_fix8_300_300 506.294 168.002 122.4476 35.4200 23.7717 7.1844 34.761 11.727 N/A N/A yolov3_fastest_fix8_320_320 3.8925 3.68744 1.1967 N/A 0.5232 N/A 0.6751 N/A N/A N/A yolov3_mnetv2_nano_fix8_320_320 6.423 4.39249 1.7658 N/A 0.6142 N/A 0.8235 N/A N/A N/A yolov3_fastest_xl_fix8_320_320 9.0313 7.22087 2.5402 2.1876 0.9035 N/A 1.1873 N/A N/A N/A yolov3_mnetv2_lite_fix8_320_320 20.5904 12.6861 5.0584 3.1196 1.4410 N/A 1.9417 N/A N/A N/A tf_yolo_v3_mixed_416_416 717.187 216.842 180.2604 47.9553 32.6579 9.9896 47.395 14.358 N/A N/A tf_deeplab_v3_fix8_513_513 74.9494 53.3049 17.2360 12.0236 6.5434 5.6261 7.3219 6.5522 N/A N/A bodypix_fix8_481_641 106.684 51.1286 25.4898 12.3964 6.3852 3.4461 8.1349 4.9407 3.78 1.954 tf_inceptionv1_fix8_224_224 26.9522 7.74111 6.8038 1.9494 1.7514 0.7449 2.2538 0.8414 N/A N/A tf_inception_v3_fix8_299_299 114.902 38.673 28.9170 8.7074 6.9894 2.6025 9.3727 3.3619 N/A N/A tf_mobilenetv1_fix8_224_224 12.6974 6.33895 3.2801 1.6908 1.2207 0.6947 1.4082 0.9183 0.785 0.421 tf_mobilenetv2_fix8_224_224 8.6233 5.54697 2.4037 1.6884 1.0785 0.7508 1.3304 0.9978 0.679 0.466 tf_mobilenetv2_ssdlite_fix8_300_300 23.1544 15.4431 6.0922 4.0318 2.3392 1.7149 3.0763 2.2548 1.331 0.961 tf_efficientdet_d0_fix8_512_512 255.603 281.293 91.6419 N/A 52.7723 N/A 63.263 N/A 7.961 N/A onnx_efficientnet_b0_fix8_224_224 41.6688 39.4835 11.6885 11.4160 9.3859 8.9508 9.5599 9.2246 1.515 1.181 onnx_centernet_fix8_384_384 267.942 83.3991 65.2046 17.0723 11.9234 4.4619 18.471 5.7173 N/A N/A onnx_fgfd_fix8_240_320 2.6721 1.79946 0.7421 N/A 0.3235 N/A 0.4339 N/A N/A N/A onnx_retinaface_fix8_640_640 18.5969 8.13562 4.9085 2.4567 1.1852 N/A 1.6213 N/A N/A N/A onnx_yolov3_fix8_416_416 660.701 172.863 166.9117 39.9476 30.4711 9.0585 45.180 12.521 N/A N/A onnx_yolov3_spp_fix8_416_416 663.897 175.592 167.5850 40.2857 31.0350 9.2179 45.184 12.831 N/A N/A onnx_yolov3_tiny_fix8_416_416 52.8347 14.2004 13.2943 3.3210 2.7986 1.0299 3.8731 1.3145 N/A N/A onnx_yolov5s_fix8_416_416 86.6313 32.111 21.9484 7.4219 5.4054 3.4200 7.1014 4.3617 N/A N/A onnx_yolov5m_fix8_416_416 241.999 78.3978 60.7822 17.7189 12.7043 6.6894 17.424 8.891 N/A N/A onnx_yolov5l_fix8_416_416 522.304 166.883 130.7734 37.0621 26.1736 12.1659 36.560 16.274 N/A N/A onnx_yolov5x_fix8_416_416 967.496 297.953 239.9791 66.6298 49.4302 20.6322 67.718 26.99 N/A N/A onnx_fairmot_fix8_160_288 107.292 32.8526 26.8830 7.0717 6.1175 2.0780 8.489 2.54379 4.995 1.607 swin_tiny_mixed_224_224 N/A N/A N/A N/A 59.4 N/A N/A N/A 14.315 N/A Update CV28 performance as it has big improvement for VAS refinement which will not affect accuracy Above performance is based on specific CNNGen toolchain versions just for reference please check the real performance with the CVflow CNNGen Samples pacakges as the performacne maybe improved in new CNNGen Toolchain More networks performance will be added in future versions The following table provides network accuracy results based on the CNNGen samples package The values below are only for reference as the accuracy from different toolchain versions may vary The tests provided below only use a small number of images For an accurate value users should test with the samples package As it does not provide all of the files listed in the table below it is recommended to find and download the relevant files to CNNGen Framework CNN Models Method EVK DRAv2 Accuracy EVK DRAv3 Accuracy PC Accuracy Input Size DRA Stategy Quant Prune Image Numbers Caffe AlexNet TOP1 0.5537 0.5491 0.5530 2 2 7 x227 Default No No ILSVRC_2012 1 2 8 4 TOP5 0.7484 0.7874 0.7889 GoogLeNet TOP1 0.6838 0.6815 0.6807 2 2 4 x224 Default No No TOP5 0.8497 0.8754 0.8793 InsightFace Cosine 0.9892 0.9720 0.9883 1 1 2 x112 Default No No MobileNet V1 TOP1 0.6332 0.6752 0.6768 2 2 4 x224 Default No No TOP5 0.8115 0.8512 0.8800 ResNet50 TOP1 0.6900 0.6900 0.6908 2 2 4 x224 Default No No TOP5 0.8653 0.8964 0.8956 SqueezeNet V1 TOP1 0.5685 0.5646 0.5748 2 2 7 x227 Default No No TOP5 0.7578 0.7616 0.7967 VGG16 SSD mAP 0.7764 0.7764 0.7797 3 0 0 x300 Default No No VOC_07 4 9 5 2 Yolov3 mAP 0.5815 0.5729 0.5873 4 1 6 x416 Fix16 No No COCO_2014 2 5 0 0 Yolov3 mAP 0.5235 0.5244 0.5283 4 1 6 x416 Default Fix8 Quant Sp50 ONNX Yolov5s mAP 0.4625 0.4759 0.5084 4 1 6 x416 Default No No COCO_2014 2 5 0 0 Mobilenet V2 TOP1 0.6277 0.6643 0.6680 2 2 4 x224 Default No No ILSVRC_2012 1 2 8 4 TOP5 0.8287 0.8668 0.8660 Swin_tiny TOP1 0.7617 N/A 0.7889 2 2 4 x224 Default No No ILSVRC_2012 1 2 8 4 TOP5 0.9167 N/A 0.9439 Tensorflow Deeplab V3 mIOU 0.6832 0.7009 0.7011 5 1 3 x513 Default No No VOC 0 7 Inception V1 TOP1 0.6636 0.6729 0.6706 2 2 4 x224 Default No No ILSVRC_2012 1 2 8 4 TOP5 0.8419 0.8614 0.8645</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_quick_start</field>
    <field name="url">da/dc5/fs_quick_start.html</field>
    <field name="keywords"></field>
    <field name="text">Quick Start Quick Start Before reading the following documentation and quickly getting started using the CNNGen toolchain an important note is that users are advised to use pruning test tools to test performance improvements in CVflow For detail please refer to 2.1 Pruning For how to do the test please refer to 3.1 Quick Performance Evaluation Before starting please refer to below sections to see what are needed User should have two SDK packges and two toolchain pacakges as below CVflow CNNGen SDK cvflow_cnngen_samples_ version _ date tar.xz cvflow_cnngen_samples_ version _ date tar Easy tools for convert accuracy debug and inference test with x86 Simulator or real chip on EVK Users needs an EVK if CVflow inference is needed Please refer to 1.2 HW for detail Lots of network examples cvflow_cnngen_documents_ VERSION _ date tar.bz2 which is the prebuild doxygen documents for converting and deployment also users can generate it from above SDK packqge directly Ambarella_Toolchain_CNNGen_ version _ date tar.xz which is the CNNGen toolchain and EazyAI CFlite Python tools for network convert and compile It also provides quick shell installation script and docker file installation both for CPU and GPU only for Ubuntu2004 And there should be some simple tool guides in this package Cooper SDK cooper_linux_sdk_ version _ date tar.xz cooper_linux_ version _ date tar which provids the MP level APIs for the final deployment Cavalry Driver API official low level APIs Cavalry_Mem Library API official low-level APIs NNCtrl Library API official low-level APIs VPROC Library API official low-level APIs EazyAI Library API easy-to-use high-level APIs for running neural networks on CV platforms of Ambarella this library is just for demos not a official APIs of course users can use it if they need It encapsulates low-level modules in SDK including IAV driver Cavalry_Mem lib VPROC lib NNCtrl lib and SmartFB lib then arrange functions in high-level with general conceptions in CV ground like preprocess forward and post process cooper_linux_sdk_ version _ arch _doxygen_ date tar.xz which is the prebuild doxygen documents also users can generate it from above SDK package directly Ambarella_CortexA76_Toolchain_ version _ date tar.xz which is the linaro toolchain for cross compile for ARM It also provides quick shell installation script CV2x CV5x are using Cavalry v2 for Cavalry Driver NNCtrl and Vproc but CV72 is using Cavalry v3 for these libraries EazyAI library is compatible for both CV2x CV5x and CV7x CV5x and CV7x will use the unify Linux SDK pacakge which is named cooper_linux_sdk_&lt;version&gt; _&lt;date&gt;.tar.xz Even in future CV2x will be included in it EazyAI source code are both included in above two pacakges User can refer to the follow hardware list which are needed for network inference It is suggested that users can use IMX274_MIPI which can support lots of different VIN resolutions It is suggested that users can use 1 5 HDMI YUV Input to feed videos to EVK for algorithm evaluation It is suggested to use below EVK for CVflow inference with eazyai_inf CV2_Chestnut CV22_Walnut CV25_Hazenut CV28M_Cashewnut CV5_Timn CV72_Gage CV3_DK_Mini One TV Screen with HDMI One computer which has installed VLC to receive RTSP streaming It is suggested to prepare one SDCARD as NAND is too small may not be enough to save the big models There are two CNNGen packages for CVflow_v2 and CVflow_v3 both of which use Ubuntu 2 0 0 4 system Ambarella_Toolchain_CNNGen_2 tar.xz used is for CVflow_v2 CV2x and CV5x conversion Ambarella_Toolchain_CNNGen_cv7x.3 tar.xz used for CVflow_v3 CV7x conversion Since Ubuntu2004 AmbaCaffe will not be provided in above toolchain package user can find it in the old CNNGen toolchain package Before running the installation script for the toolchain run the following command Users should regularly upgrade the operating system as shown below as the toolchain is developed using the latest version OS Although the command above is included in the installation script Ambarella recommends noting it and assigning it to an independent machine to avoid a possible build conflict Install the toolchain as shown below For Ubuntu Linux the commands are as follows installToolChainONLY-qa.2023 which will install toolchain only installWithNormalUser-qa.2023 which will install python libraries and toolchain with normal user Also there is docker file to help users to install the docker environment As models and images are too big to be included in CNNGen Samples package since CVflow CNNGen Samples 1.8 users can download these models DRA images and dataset in https://amba.egnyte.com/fl/pnAckpTfWI with password Amba_2023 to get the access permission Then users can get all the files from below page users can download all the folders or download what they need Figure 1-4 Download Web After downloading user can get below folder structure and copy them to CNNGen samples folders as below The folder structure between CNNGen Samples Package and this web are totally the same Also user can only copy the network they want as below To reproduce the accuracy test examples first users should prepare the test dataset with the script in data folder For how to use it please refer to 4.2.3 Download the Test Dataset Users should make sure the build server and EVK are in the same LAN then users can do belows on server Use FTP to get files from EVK and upload files to EVK Use Telnet to connect to the EVK Use SSH to connect to the EVK Use eazyai_inf eazyai_inf_simple_dummy and eazyai_inf_simple_live to do the inference on EVK directly There are some training videos and documents which are not included in doxygen documents CNNGen Training Video EN CN and CNNGen Training PDF if users need please ask Ambarella Support for assistance SDK PDF Documents there are lots of documents for CVfLow related please refer to the followings Ambarella_Cooper_Linux_SDK_Code_Building which help users to install the CNNGen Toolchain and setup build environment Ambarella_Cooper_Linux_SDK_Getting_Started_Guide which help users to run some simple demos in Ambarella EVK and instruct them how to generated doxygen documents Ambarella_CV*_DG_Flexible_Linux_SDK* which lists the answers of some common questions Users can refer to 1.4 Download Models to download the prebuild cavalry binaries and refer to 1.3 Installation to install CFlite also prepare below EVKs CV2_Chestnut CV22_Walnut CV25_Hazenut CV28M_Cashewnut CV5_Timn CV72_Gage Then to run demos as below caffe mobilenetv1_ssd onnx_ddrnet caffe yolov3_fastest_xl tf_hfnet RetinaFace Please refer to the following development flow for CVflow development before that please refer to 1.3 Installation to install CFlite And refer to 2.1 EazyAI Helper Tool for quick workflow and usage User can use eazyai_cvt dummy mode for quick performance roughly evaluation for different networks Below commands will print the theory performacne at the end Such as below example Please make sure the network binary if CVflow friendly Users can use below tools to check it tf_print_graph_summary.py onnx_print_graph_summary.py caffe_print_graph_summary.py If there is some warning message please fix it with graph_surgery.py User can use eazyai_inf_simple_dummy for quick network performance evaluation on real chip based on the model which is generated with eazyai_cvt Please make sure build server can conect EVK board modify the ip for EVK IP is needed Use eazyai_cfg to generate a correct network yaml configuration file For detail please refer to 2.2 EazyAI Configuration Tool Use eazyai_cvt to convert the nerwotk to Ambarella format and run layer_compare For CVflow v3 such as CV7x please use FP16 for input and output data format users can modify the input and output data format in ea_cvt_cfg.yaml manually or use below adapt_chip ac to adapt unsupported parameters For CVflow v2 user do not need to use it The command will generate cflite_cvt_summary.yaml for below inference For detail please refer to 2.3 EazyAI Convert Tool If the accuracy is not so good in the excel file which is generated in above command Users can modify the DRA parameters to do some fine tuning or to use a custom sideband file to define the layer format For detailed meaning of the parameters please refer to 2.2.2 Full Convert Parameters Use eazyai_inf to run inference to check the real result with JPG PNG file mode input Then user can compare its network result between simulator and original framework For detail please refer to 2.4 EazyAI Inference Tool For the differece between Ades and Acinference please refer to 1 0 x86 Simulator Use platform ades acinf cvflow orig to switch the platform and compare its result For ea_inf.yaml pelase refer to the exmaples in the config folder of every network such as tensorflow/demo_networks/deeplab_v3/config/ea_inf_tf_deeplab_v3.yaml in CNNGen Samples Package For platform cvflow please run below command first Use eazyai_inf_simple_live to run live demo with EVK please refer to 2.5 CVflow Simple Inference Tool for detail For detail please refer to fs_accuracy_tool If the accuracy is not OK please go step 3 again to do DRA fine tuning again if DRA cannot help anymore please apply Ambarella quantization tool Also if accuracy is OK but performance is not OK please apply Ambarella Pruning tools For detail please refer to 2 Network Optimization For ea_acc_inf.yaml pelase refer to the exmaples in the config folder of every network such as tensorflow/demo_networks/deeplab_v3/config/ea_inf_acc_tf_deeplab_v3.yaml in CNNGen Samples Package There are lots of configuration files for users reference please find them in cvflow_cnngen_samples/ framework xxx_networks network_name Also please refer to the examples in Caffe Demos Tensorflow Demos and ONNX Demos Profiler tool in 2.7 CVflow Layer Profiler Tool can help user to find the operators which has poor efficiency with CVflow Above are all for debug stage after everything is OK in above steps users need write real application to deploy the converted network How to transfer debug stage to real development stage Get the test_eazyai full command from eazyai_inf or eazyai_inf_simple_live Follow the full command and sourece code of test_eazyai to write users own application Refer to in 2 CFlite Modules for how eazyai_inf calls test_eazyai Refer to below sections for EazyAI APIs and Unit test 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application Above debug tools are calling the real C deployed APIs to do the test so if debug stage does not have problems there will be no risk when to write users own application CNNGUI is a user-friendly interface UI to convert and evaluate CNN models It is located in the folder cvflow_cnngen_samples_&lt;version&gt; In current only platforms below can be supported more and more platforms will be added in future cv2_chestnut cv22_walnut cv25_hazelnut cv25m_pinenut cv28m_cashewnut cv5_timn cv52_crco cv72_gage In old CV72 SDK CV72 board is named please modify it to in below two files If firmware will be rebuilt for the EVK please modify ambarella/packages/eazyai/unit_test/cnngui/cnngui.json If only need to modify directly in EVK please modify json CNNGUI depends on Ambarella CNNGen Toolchain and PyQt5 also Ubuntu 18.04 is recommended Install Ambarella CNNGen Toolchain users should install it before using CNNGUI for installation information please refer to 2 Installation Install PyQt5 as below Run PyQt5 with debug mode As the PyQt5 may run fail for the lack of some other libraries users can run PyQt5 with debug mode to debug this problem The missing libraries will be listed then use apt-get to install it For example when there is a lack of libxcb-xinerama.so users can debug the problem with the steps below a Run CNNGUI with debug mode and it shows that is missing b Search the missing library by apt-cache c Install the missing library Below lists some libraries for Ubuntu 1 8 0 4 server every user s environment is different please follow the steps above if there are some problems Run on a remote host Qt UI When CNNGUI is running on a remote host users must need to set the remote display enviroment with the command below Users should close the debug model after it works well or else there will be some debug messages There are six pages in CNNGUI Choose Model Select CNNGen toolchain version CNN framework Model and so on Port Information The Network Information for convert most parameters are generated automatically Convert Model Convert Process Debug and so on Simulation Tools Simulate on PC by ADES and Acinference Deploy Model Deploy converted model to EVK easily Accuracy Tools Check the accurcy of the converted model Steps to convert and evaluate a NN model by CNNGUI are as below Specify model framework model file model name and env file in page Set parameters of input and output nodes in page Convert model in page Evaluate model on PC in page Evaluate model on EVK in page This section provides the mainly usage of each pages For more details users can refer to the help information in each component displayed in CNNGUI with moving the mouse to the component In this page users can specify the NN framework env file NN model file and model name Choose Model Choose Model Choose Model Choose Model For NN framework when Caffe is chosen button will display which could be used to specify the environment values for user own built Caffe Before entering the second page users can click button to check the model to see if it is a VP friendly model In addition users can use two pre-process model tools graph_surgery and prune_model by clicking the list Graph Surgery In this page users can surgery the model to make it VP friendly Graph Surgery Graph Surgery Graph Surgery Graph Surgery Users need to specify the input option output nodes and the output file path To get the input and output nodes information click the button which will dispaly the graph summary It is recommended to use button to check the output model to see if it is surgeried correctly CNNGUI will use the default transform method to surgery the model users can click button to specify other methods Prune Model In this page users can prune the model with the specified sparsity threshold which means the fraction of non-zero coefficients in each kernel that will be set to 0 For example “0 8 ” means 2 0 density just left 2 0 valid weights Prune Model Prune Model Prune Model Prune Model For more detailed information about these tools please refer to 13.2 Graph Surgery and sec_deep_learning_dg_prune In this page users can set all needed parameters for convert most parameters are generated automatically Port Information Port Information Port Information Port Information It is recommended that clicking button to check the parameters is correct or not The output folder path is specified in tab as well which will save all of the output files In this page users can convert and evaluate the NN model by CV toolchain with specified env file and cavalry version Convert Model Convert Model Convert Model Convert Model There are three levels for the displayed logs error notice and verbose When is chosen it will display the stages of conversion When is chosen it will display the stages and commands of conversion When is chosen it will display the stages commands and logs of conversion In this page users can simulate the converted model and evaluate the performance on PC ADES Acinference Estimated Performance Simulation Tools Simulation Tools Simulation Tools Simulation Tools To simulate on PC users should install the 3 rd party libs first by below commands Install json-c build cd CNNGen Sample Package sh Install lua and eigen build cd CNNGen Sample Package sh In this page users can deploy the converted model to EVK File Mode Live Mode Dummy Mode Check the EVK and IAV status in the Status window Run on Board Run on Board Run on Board Run on Board To evaluate on EVK users should ensure that the EVK and the host machine are in the same local network for Telnet and FTP connection then specify the IP adress and port number of EVK and click button to build the connection between CNNGUI and the EVK shows the work folder on the EVK which can be specified by users SD card will be selected by default The window is as below In this window users can check the information and status of the EVK IAV and VP to evaluate the demo s performance including IAV driver NNCtrl version CMA DRAM top dmesg and so on IAV VP Status IAV VP Status IAV VP Status IAV VP Status In this page users can check the accuracy of the converted model Cavalry binary versions convert for different SDK version Compare the EVK result and ADES result ADES result is generated in page Accuracy Tools Accuracy Tools Accuracy Tools Accuracy Tools Users can save all the settings and parameters of the conversion to a pkl file by button and import them by button for easy configuration Locate a quick start script to convert a network in cvflow_cnngen_samples_&lt;version&gt; sh The examples below use pnet as an example The bold information below refers to the individual network’s information Then it generates a script called run_pnet.sh and a tool folder run_pnet_tools which should be shown as follows Modify run_pnet.sh if there are any bugs After running it the folder below will be generated Then it generates a script called run_mobilenet_v1.sh and a tool folder run_mobilenet_v1_tools which should be shown as follows MetaGraph Descriptor Json file mobilenet_v1_graph_spec.json in run_mobilenet_v1_tools will be used as a config file in the script run_mobilenet_v1.sh Modify the script and the Json file if needed Bugs or Different Params After running the script the folder below will be generated FP32 is not supported for input and output in CV72 please use fp16 instead When to deploy it with EazyAI the inference API will convert FP16 to FP32 in default with ARM CPU Some C tools are located in the folder cvflow_cnngen_samples_ version Also there are some pytools in CVflow Python Library For these pytools please refer to 2.8 Simple PyTools This tool is used to process the input for Cavalry and ADES and is located in cvflow_cnngen_samples_&lt;version&gt; This tool converts the image to a 3 RBG channels binary or one data channel such as LeNet For the LeNet model training method users must normalize the scale For example 0 to 1 For Cavalry that will run on a board users must align the board with 3 2 btypes This is not necessary for ADES Users must ensure that the data format is consistent with the definitions listed in This tool uses the OpenCV interface If this tool is required install the package using the following command This tool uses the opposite process as the im2bin tool As im2bin converts the image to the network input this tool converts the input or output back to the image It can be found in cvflow_cnngen_samples_&lt;version&gt; This tool can be used to compare the outputs in ADES Cavalry Caffe and TensorFlow It can be found in cvflow_cnngen_samples_&lt;version&gt; This is the demonstration code used to obtain the mean value from Caffe files It can be found in cvflow_cnngen_samples_&lt;version&gt; This tool converts the submean file from FP32 to FP16 and can be found in cvflow_cnngen_samples_&lt;version&gt; This tool is used to generate the FP16 file It can be found in cvflow_cnngen_samples_&lt;version&gt; This tool is used to evaluate the performance based on generated VAS output The tool is placed in cvflow_cnngen_samples_&lt;version&gt; The following table lists the detailed usage of the cvflow_perf_eval.py Options Description h Shows the help message f Indispensible Specify the path of VAS output folder c Optional Specify the VP clock MHz if needed Example The estimated performance is based on ICE cycle count which does not included some system status and does not include all the operators in the network For some special networks it will not so accurate</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_eazyai</field>
    <field name="url">dd/de5/fs_eazyai.html</field>
    <field name="keywords"></field>
    <field name="text">EazyAI Framework EazyAI Framework EazyAI which means Easy AI is a complete framework for the debugging phase and the final product development phase Figure 0 EazyAI Framework Features in dashed boxes have not been supported yet may be added in future SDK This framework has included lots of useful modules the only purpose is to make users easier EazyAI CVflow Lite Python Library CFlite it provides a series of python APIs to help users to convert and deploy a network on Ambarella chips easily It will call CNNGen toolchain CNNGen Toolkits for network convert It will call below Module 3 7 for network inference Please find its source code in cvflow_cnngen_samples/library/cflite Please refer to 1 EazyAI CVflow Lite Python Library and CVflow Lite API for detail EazyAI Python Tools it provides a series of tools which are based on the APIs in to help users to convert and deploy a network on Ambarella chips easily Please find its source code in cvflow_cnngen_samples/library/cflite/eazyaitools/ Please refer to 2 EazyAI Python Tools for detail EazyAI Video Shell Script it can help users to setup DSP video pipeline in different chips easily it is only used in debug stage Users should use IAV to their own video application in final product developing stage Please find its source code in cooper_linux_sdk/ambarella/packages/eazyai/unit_test/eazyai_video Please refer to 3 Eazyai Video for detail EazyAI MP Level Inference C Library it provided the C APIs that helps users to write their final CVflow inference application or Simulator inference application Please find its source code in cvflow_cnngen_samples/library/eazyai Please refer to 4 EazyAI Inference C Library and EazyAI Library API for detail EazyAI NN Postprocess C Library it provides lots of network post process sample code such as NMS bounding boxes decode and so on Please find its source code in cvflow_cnngen_samples/library/eazyai/unit_test/nn_arm_task Please refer to 5 EazyAI Postprocess C Library and 4 EazyAI Arm Postprocess for detail EazyAI Unit Test it provides an unify application which can run network on PC Simulator and CVflow chip Please find its source code in cvflow_cnngen_samples/library/unit_test/test_eazyai.c Please refer to 6 EazyAI Unit Test 5 EazyAI Unit Test and 6 EazyAI Simulator Unit Test for detail EazyAI Live Applications it provided lots of live network demos which only can work on CVflow chips Please find its source code in cooper_linux_sdk/ambarella/packages/eazyai/apps Please refer to 7 EazyAI Live Application Caffe Demos Tensorflow Demos and ONNX Demos for detail For debug stage users may need to convert a network quickly and do some quick test then the tools in Module 2 can cover all what they need For specific convert flow user also can write their own convert application with the APIs in Module 1 For final product developing stage user can use the APIs in Module 4 to write their CVflow applications and they can refer to the sample code in Module 5 7 As in debug stage it also uses Module 4 7 to do the network inference it will be very easy for users to switch between debug stage and final product developing stage With previous solution of 5 Quick Shell Script and sec_cnngen_sample_network users needs to convert the network in build server and copy related files to EVK board then to run the inference The two old convert flow methods will be deleted in future CNNGen Samples Package which will be replaced by CFlite With CFlite everything is different users can use python to convert and run CNN on build server directly users do not need to know how to operate the EVK board as EVK which will always be an external device of PC it is little similar with developing on a chip which support native compilation That means users can do everything in build server also users can share most of the parameters between convert stage and inference stage For detail please refer to CVflow Lite API as below 0 Revision History 1 Introduction 2 CFlite Modules 3 CFlite Python Tools 4 CFlite Python API This section provides information on the EazyAI Python Tools below is the full flow to use these tools Figure 2 EazyAI Python Tools And below is the detailed tool list eazyai_helper which shows supported tools and utils in CFLite eazyai_cfg which helps users easily generate the network convert configuration files eazyai_cvt which is used for users to convert the network with the network convert configucation files generated by eazyai_cfg.py eazyai_inf which is used for network inference CVflow Chip Ades or Acinference Simulator Original Framework eazyai_inf_simple_dummy which is for quick performance inference test on CVflow chip eazyai_inf_simple_live which can enable lots of live network demos on CVflow chip eazyai_video which is used to setup DSP video pipeline for different chips eazyai_profiler which is used to generate a spreadsheet to describe CVflow hardware efficiency for current network These tools will be install when installing CNNGen toolchain in 1.3 Installation users can use it as eazyai_cvt eazyai_inf and so on directly Also they can find them in cvflow_cnngen_samples for these source code level python applications users can use it as python3 eazyai_cvt.py python3 eazyai_inf.py and so on Actually they are the same eazyai_helper will help users understand the tools and utils supported by cflite For detail please refer to help information in the tool such as below The following is the terminal information generated by executing the default command to show workflow The following is the terminal information generated by executing the command to show the tool list eazyai_cfg will help users to generate a right network configuration quickly and correctly For detail please refer to help information in the tool such as below Guide Mode The applition will guide users to fill network parameters one by one users can get two full Yaml configuration files easily which will be feed to eazyai_cvt.py for convert directly a which includes the basic convert parameters b which will more special parameters for further tuning that are rarely used for most of the case Fast Mode With this users get two reference Yaml configuration files quickly the same files with above boot mode but in fast mode the configuration file cannot be used directly as it lacks the right network information so users need to modify manually a As above files is generated by fast mode so it lack lots of necessary information that users need to fill them before using it to convert the network b data_prepare dra_data_1 means this network input will use to generate the DRA image binaries and list c transforms is the method to do data precoess please run below command to check the support list As above there are lots of help comments in these configuration files that users can refer to if users do not need them they can disable it as below eazyai_cvt will help users to call CNNGen toochain to convert the network there are two modes For detail please refer to help information in the tool such as below Dummy Mode With dummy mode users only need to feed the model to it nothing else needed also the tool will share the theory performation without running on EVK This performance is only for reference please run it on EVK for more accurate performance And this dummy mode is only for performance test Standard Mode Users can use eazyai_cfg.py to create ea_cvt_cfg.yaml With this users can get a Ambarella model easily and run it with Simulator or Chip Additional Mode The parameters in ea_cvt_ext_cfg.yaml are finally used when converting It is used when users do not want to modify ea_cvt_cfg just to change some parameters through ea_cvt_ext_cfg.yaml to finish some special converting For CVflow v3 such as CV7x please use FP16 for input and output data format users can modify the input and output data format in ea_cvt_cfg.yaml manually or use below adapt_chip to adapt unsupported parameters eazyai_inf can run the network on CVflow chip Simulator and Network Original Framework For detail please refer to help information in the tool such as below For the inference with cvflow please run below command in 2.6 Video Control Tool to initialize the chips first Inference Without PostProcess ea_cvt_summary.yaml is generated by eazyai_cvt.py Inference With PostProcess Users needs to wrtie ea_inf.yaml manually such as the example in below There are four key nodes in above inference yaml file data_prepare and input_nodes are similar with the definition in eazyai_cfg.py popt_process is used to set the postprocess parameters eval_info is used to set the accuracy test parameters To enable GPU for Simulator inference users need to modify the LD_LIBRARY_PATH in CNNGen toolchain ENV file the difference is only to use different libraries as they share the same execute binary Ades Modify LD_LIBRARY_PATH in toolchain path to switch it to the specific version Source the env file Ades with GPU is only supported in CNNGen 3.x for CVflowv3 of CV7x and CV3x Acinference Modify LD_LIBRARY_PATH in toolchain path to switch it to the specific version Source the env file eazyai_inf_simple_dummy and eazyai_inf_simple_live can only work in CVflow chip For detail please refer to help information in the tool such as below eazyai_video is used to set up the DSP video pipeline actually it wil call eazyai_video.sh of 3 Eazyai Video on EVK For detail please refer to help information in the tool such as below eazyai_profiler is used to generate a spreadsheet to describe CVflow hardware efficiency for current network this spreadsheet will help users understand different bottlenecks in the current network structure For detail please refer to help information in the tool such as below Please refer to 1 8 CVflow Layer Profiler for how to read the spreadsheet this tool is used to simplify the whole flow in that section There are lots of tools are provided in CVflow Python Library For detailed list please refer to 3 CFlite Python Tools in 1 EazyAI CVflow Lite Python Library binrotate Command Result The rotate_cat.bin can be check by and should be rotated with 9 0 degree simple_bincmp Command Result Print the check result convert_jpg2nv12 Command Results Y and UV files are generated in out_y and out_uv folders compare_classification_network_result Command Results Draw the result figure im_reader Command Result Generate the read_cat.jpg im_quant Command Result Generate the bin and info txt filese in im_quant/output padding_append Command Result Generate the padding_append/padding_cat.bin tf_remove_node Command Result Check the modified_mobilenet_224.pb by tf_print_graph_summary the output should be mtcnn_get_pnet_scales Command Result It should print below message Before running the CV demo users must set up some environments modify Lua according to requirements setup DSP and more Command settings are different for different chips Therefore this setup process can be challenging To simplify this process the shell script named can be used Users must pass a few simple parameters to eazyai_video.sh according to their requirements then eazyai_video.sh automatically generates the corresponding Lua and sets up the DSP Eazyai_video.sh can simplify the preparation process to setup DSP function and reduce the probability of human error Eazyai_video.sh cannot fully support the features in test_encode it only supports some common cases for easy DSP setup Find this script in Linux SDK Package sh Use the following command to show the detailed usage of Currently eazyai_video.sh only supports the following boards CV2_Chestnut CV22_Walnut CV25_Hazenut CV25M_Pinenut CV28M_Cashewnut CV5_Timn CV52_Crco CV72_Gage If an unsupported board is used users can use to generate a reference script for a similar board then modify this script to be suitable for the real used board Use as an example this board is not supported in As is similar to users can use the command below to generate a reference script for After executing the command above the script and Lua below will be generated sh lua Users can then modify the above script and Lua to meet the requirements of Ambarella does not recommend using the same canvas buffer for algorithm analysis and encoding as overlay will be added in encoding buffers EazyAI means Easy AI interface this library provides easy-to-use APIs for running neural networks on x86 simulator and CVflow chip For more details refer to EazyAI Library API as below 0 Revision History 1 Introduction 2 EazyAI Deployment 3 EazyAI Simulator 4 EazyAI Arm Postprocess 5 EazyAI Unit Test 6 EazyAI Simulator Unit Test 7 EazyAI API 8 EazyAI Core API Test 1 0 License It encapsulates lots of low-level modules in the SDK and some basic functions listed below It also uses the unified data format “Tensor” to connect them in one pipeline IAV driver Cavalry_Mem library VPROC library NNCtrl library SmartFB library PostProcess Library x86 Ades Library x86 Acinference library The arrange functions in high-level with general conceptions in CV ground such as preprocess forward and post process Unify unit test for different networks Figure 4-1 EazyAI Flow Example 1 Figure 4-2 EazyAI Flow Example 2 Most demos in the following chapters have been modified to use this library If users want to use the same canvas buffer for the two algorithms Ambarella suggests to query the canvas buffer in main thread and share the frame to the two algorithm threads Users cannot query the same canvas buffer in the same multiple threads or multiple processes This library includes lots of network postprocess users can easily use current post process interface also it is easy for them to register their own postprocess in to this library For detail please refer to 4 EazyAI Arm Postprocess Currently it has supported below postprocess Also there is a simple library for SSD and Yolo in Data Process Library API as below 0 Revision History 1 Overview 2 Dproc Library Process 3 Unit Test 4 Dproc API 5 License This unit test allow users to run different networks on different platform in one application but only limit to single network For detail please refer to 5 EazyAI Unit Test and 6 EazyAI Simulator Unit Test For x86 Simulator Run Ambarella Directed Acyclic Graph DAG Emulator System ADES mode i The raw.bin is used as an input without the preprocess or postprocess ii The image is used as an input with the correct preprocess and postprocess Run Acinference mode i The raw.bin is used as an input without the preprocess or postprocess ii The image is used as an input with the correct preprocess and postprocess For more details of the x86 simulator executable file refer to 3 EazyAI Simulator and 6 EazyAI Simulator Unit Test to build the x86 binary Then the executable file test_eazyai can be found in cvflow_cnngen_samples/library/eazyai/unit_test/build/ Refer to Caffe Demos Tensorflow Demos ONNX Demos for information on how to generate and run the model.json and model_ades.cmd of each model Different models may require different parameters Refer to 3 EazyAI Simulator for specific usage For the CVflow chip i Dummy mode only for CVflow performance test ii The real image is used as an input with the correct preprocess and postprocess iii The raw.bin is used as an input without the preprocess or postprocess For more details refer to EazyAI Library API Content related to Cavalry and neural network control NNCtrl can be found in the Linux SDK Doxygen document EazyAI has provided lots of different live demos for users to refer to some can co-work with DSO video features For detailed usage please refer to Caffe Demos Tensorflow Demos and ONNX Demos</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_deployment</field>
    <field name="url">d7/d53/fs_deployment.html</field>
    <field name="keywords"></field>
    <field name="text">Deployment Deployment The following illustrates the deployment process Deployment Flow Deployment Flow Deployment Flow Deployment Flow Users must prepare a binary that can run Cavalry This section describes the basic steps to prepare the binary Compile the binary as follows Use the binary in the SDK package For more details refer to the Ambarella CV UG Flexible Linux EVK Getting Started Guide document Different types of demos are divided into the following four folders Users can find the required demo code in the corresponding folder For EazyAI-related information refer to EazyAI Library API Images must be processed for different networks The processing methods correspond with their network training inputs with the following considerations Input type channel number RGB BGR YUV and more Meaning normalization or non-normalization Use gen_image_list.py to prepare the test input binary For more details refer to 5 DRA List Generation which provides an example similar to the following which uses You only look once YOLO real-time object detection system Use the VProc library to prepare the test input with the sensor b buffer select canvas buffer id Y yuv capture YUV data from source buffer f filename yuv filename to store output yuv F format 0 YUV420 data format for encode buffer 0 IYUV(I420) 1 YV12 2 NV12 3 YU16 4 YV16 5 NV16 6 YUV444 Default is IYUV format r frames 0 frame counts to capture 0 countless frames Convert 4 K NV12 to 4 K RGB Resize the 4 K RGB to 1 0 8 0 p RGB For more details refer to the VProc Doxygen document in the Linux SDK package At present there are two basic unit tests test_nnctrl and test_eazyai test_eazyai is a further encapsulation of test_nnctrl which is more simple and convenient to use This test can support the x86 simulator Do not run the test in serial port with lots of print messages which will result unstable system and bad performance An example using the command is shown below Flash the binary to the evaluation kit EVK board and run as follows Load Cavalry only for the EVK board the x86 simulator does not require this step Run Inference with test_nnctrl Dummy mode only for CVflow performance test Show each Arm and CVflow run time Show average Arm and CVflow run time The image is used as an input with the correct preprocess and postprocess b cavalry_gen_generated.bin this bin is generated by cavalry_gen in the CNNGen tools For in and out users should know the first and last output names This information can be found in when generating the The input binary is the image and the image output is saved to the analysis The network output will also be generated with 3 2 byte or 1 2 8 byte alignment in dynamic random access memory DRAM which is decided by different chips When saving to the binary test_nnctrl will remove the padding For more details refer to 5.3 Alignment v prints the debug messages dump saves all the outputs in It cannot change the current patch If required users can modify the code manually e prints the inference time the vp_time and arm_time arm_time will include the vp_time and API call time The arm_time is found by adding vp_time and application programming interface API call time The total time is arm_time and the pure vector processor VP time is vp_time c mmap cache memory The default is noncache t iter Iteration of run network loop forever 0 xffffffff if set 0 Default is 1 print-interval shows average Arm and CVflow run time print-interval option is depends on option t a print-interval should smaller than total iteration times e.g b print-interval option support MODE_SERIAL mode run_mode 0 only e.g Run Inference with test_eazyai Please refer to 6 EazyAI Unit Test for the detailed information The following diagram illustrates the calling sequence for the custom node APIs Figure 2-1 Custom Node API Calling Sequence VProc is a processing library based on the Cavalry driver which utilizes the CVflow VP to perform the following functions YUV image to RGB image Image resizing Mean value subtraction Data convert Region of interest ROI crop Some users may find that the VProc resize results are different from those resulting from open source libraries There are different resize methods in this API For the same method in OpenCV users can use “RESIZE_SIN_STEP” to match the result between Vproc and OpenCV “RESIZE_MUL_STEPS” has minor differences from “RESIZE_SIN_STEP” If users do not care about this difference they can ignore this flag and use the default Both methods use a bilinear interpolation method but with different strategies to approach the targeted resize ratio For example assuming that there is a request to resize an image from 1 9 2 0 x1080 to 6 4 0 x480 “RESIZE_SIN_STEP” uses a one-shot way directly to handle the resize as OpenCV does “RESIZE_MUL_STEPS” proceeds through a few steps such as 1 9 2 0 x1080 9 6 0 x540 6 4 0 x480 in this case They are designed to meet different requirements for different stages Generally the difference between these two methods are minor and users can choose one of them For additional information on VProc refer to the VProc Library in SDK Doxygen document NNCtrl is used to load the library with the Cavalry driver For more details about NNCtrl refer to NNCtrl library in SDK Doxygen document Cavalry_mem is used to configure CV memory For more details refer to Cavalry Memory Library in SDK Doxygen document The Cavalry driver for CV’s VP includes a set of Linux device drivers that encapsulate the lower-level complexities of the CV’s VP functionalities The drivers conform to the standard Linux driver model exposing a series of APIs that can be invoked by input output control IOCTL() system calls These APIs are used by applications to configure and control the CV VP in detail For more details refer to Cavalry Driver in SDK Doxygen document The image audio video IAV driver includes a set of Linux device drivers that encapsulate the lower-level complexities of the CV digital signal processor DSP core functionalities The drivers conform to the standard Linux driver model exposing a series of APIs that can be invoked by IOCTL() system calls These APIs are used by applications to configure and control the CV system on chip SoC in detail including controls of image audio and video audio control is new in this version For more details refer to IAV Driver in SDK Doxygen document For information on the continuous memory allocator** CMA memory** refer to Ambarella CV DG Flexible Linux SDK CMA Driver When deploying a network if the network output memory will be accessed by the application the cache enable will include 1 0 x its normal speed When DSP boots up it competes for bandwidth with VP As the DSP has a higher priority than VP the VP performance will decrease To unify the interface in user applications between CV5x and CV2x since CV5x SDK 0.5 and CV2x SDK 3.0.9 IAV_INFO_CANVAS is used to query the YUV buffer base This application is used for all common networks with only VP tasks It pre-processes the input sending the final results to an on-screen display OSD server Flash the binary to the EVK board and run as follows The live mode data flow is described as below Certain steps may be unnecessary if the user already has the submean and scale in the Caffe Model Convert the output data format of the resize to the neural network s input data format using no option “-m” and “-r” For a detailed example refer to Section 4 Deploy with Live Mode The common applications are described as follows Buffer_YUV enables the user to specify the input data from canvas pyramid buffers with “-s” and sets the input buffer ID or pyramid layer ID with “-i” YUV2RGB supports RGB and BGR color space output The default is BGR for Caffe models users can change the default using the “-t” option “-v” prints a debug message Resize data input size is the same as the YUV2RGB output data output size information is derived from cavalry.bin Sub_mean Optional data input is FP16 The data source comes from the data format conversion and after resizing vproc_scale_ext() converts FIX8 to FP16 Users can remove the data format conversion if there is no data overflow after the submean and scale The data format of the mean file should be FP16 and in the same dimension as the NN s input dimension If the input of the NN is the BGR color space the mean file should also be the BGR color space Scale Optional data input is FP16 from the submean output data is for the NN s input If the output data format is different than the input data format input data is converted to the output data format The scale factor can be 1 if users do not want to change the input data range Note that there can be a data overflow if the NN s input data format represents a lower range compared to the input data format Neural_network the input is from scale and the network is from cavalry.bin Execute on the VP users can specify the cavalry.bin generated by the CNNgen tool named cavalry_gen Then users can set the input and output name which should be the same as Caffe model’s layer name Because the output is sent to a socket server for a parser in the osd_server if there is more than one the order of the output will be the same as the order of the parameters Socket_send NN output is sent to osd_server with a specified socket port option can specify the socket port which should be the same port in osd_server OSD_server receives data from the NN and parses it Draws an overlay on a screen connected by a high-definition multimedia interface HDMI® cable This enables the users to write their own OSD server for the network osd_server_yolov2.cpp for Yolov2 original mode in the CNNGen Samples Package for the model trained by the user This is different from the original model and some code should be modified osd_server_yolov3.c for Yolov3 Tiny_Yolov3 and Yolov5 original mode in the CNNGen Samples Package for the model trained by the user As this is also different from the original model some code should be modified osd_server_imagenet.cpp for any classification networks This application is a unify application to run different networks in live mode For more detail refer to Section 6 EazyAI Unit Test The test_pvanet_live.cpp is more complex as it requires coordination between Arm and VP tasks For more details refer to PVANET Demo The EazyAI unit test enables users to add support for file and live mode of a new network easily without modifying the unit test code For more detail refer to 5 EazyAI Postprocess C Library This chapter mainly introduces some libraries required for running the model mainly including VProc NNCtrl and EazyAI VProc stands for CVflow VP processing which is desinged to manage some operations or mathematical calculations whose performance could be boosted by utilizing Ambarella CVflow chips Most heavy calculations are carried out on the CVflow VP The VProc library is based on the Cavalry driver Users should first enable the Cavalry driver and then enable VProc library Follow the commands below to build the library If a user wants to verify some pre-defined algorithms for example CVfilter in the VProc library follow the commands below to build the unit tests For more details refer to VProc-related content in the Linux SDK Doxygen documents NNCtrl is used for users to control the neural network For more details refer to NNCtrl-related content in the Linux SDK Doxygen documents Please refer to 4 EazyAI Inference C Library for the detailed information During the training and deployment stages users must perform preprocessing which includes decoding images to RGB or BGR data converting YUV to RGB or BGR data resizing to the resolution required by networks and more There are many different preprocessing methods which may affect the final network result Two examples in the Ambarella SDK are as follows VProc which is for final deployment VProc supports different preprocessing features The most frequently used is converting YUV to RGB BGR and resizing For resizing VProc supports two different methods RESIZE_MUL_STEPS and RESIZE_SIN_STEP Both methods use Bilinear-Interpolation for resizing Generally speaking their differences are minor If users do not care about minor effects on the final result they can choose either method or use the default RESIZE_MUL_STEPS performs resizing with multiply steps as follows This method is smoother It leaves more surrounding details but is relatively blurry RESIZE_SIN_STEP uses the one-shot method to manage resizing which is similar to the method in OpenCV This method retains more edges and increased sharpness so the final result is clearer gen_image_list.py prepares image dynamic range analysis DRA gen_image_list.py uses OpenCV to process the image as follows Decode the image to RAW BGR data Convert the image to Float32 Perform the resizing operations Convert to a different data format if it is set in the command line If users use the image binary which is generated by gen_image_list.py for the network inference test the result may be different from the VProc preprocessing results Even in VProc the two methods below may also cause different results Convert YUV to RGB then to perform resizing Resize YUV first then perform RGB conversion Ambarella suggests that the preprocessing method in the deployment stage should be the same as the method used in the training stage VProc includes one common rotate API vproc_rotate() which can perform some basic rotations For rotation of any degree users can use the API in VProc seen below Users can get the affine Matrix with an OpenCV API such as getRotationMatrix2D() and feed it to this API Then VProc can perform rotations of any degree For a detailed example refer to the function hand_landmark_run() of ambarella/unit_test/private/cv_test/hand_landmark/src/hand_landmark.cpp in the demo Hand Landmark CNNGen toolchain enables users to add network preprocess for color conversion from YUV to RGB Similar steps are used for other preprocesses such as mean scale resize warp and more The basic workflow is as follows CNNGen toolchain supports conversions of both NV12 and IYUV to RGB NV12 format U and V are interleaved buffers IYUV format U and V are planar buffers Add color conversion preprocess using a JavaScript Object Notation JSON file Two inputs are required one luma image of the shape 1 1 height width and a chroma image of the shape 1 2 height/2 width/2 with uint8 Use gen_image_list.py to generate dra_y_list.txt and dra_uv_list.txt which contain the lists of Y and UV image paths for DRA The CSC node performs a conversion from YUV to RGB The name must match the original network s input layer as seen in the following example For NV12 and IYUV CNNGen toolchain manages the inputs using different channels Set the input_UV shape as 1 2 height/2 width/2 in the JSON file For a detailed example of preparing a JSON file refer to 2 Prepare the JSON File For information about JSON files refer to the CNNGen tool guide Ambarella CV UG JSON Preprocessing Format As configured in the JSON file CNNGen toolchain converts the input of the original network to input_Y and input_UV in the following shapes Input_Y shape is 1 1 height width Input_UV shape is 1 2 height 2 width 2 By default input_UV s dram_format is set to 1 and VP reads UV input with interleave mode Interleaved UV input** is required which means that UV is grouped together in dynamic random-access memory DRAM which is for NV12 format The interleave mode reading is shown below Figure 4-1 VP Reading Interleave Mode Further users can set dram_format to 0 This tells VP to read in normal mode When dram_format is set to 1 as mentioned in step 4 the UV input is interleaved and has shape of 1 1 height/2 width CNNGen toolchain automatically sets UV input pitch to be the same as the Y input which corresponds to the shape of interleaved UV input For example assuming the original network s input is 1 3 2 2 4 2 0 0 for RGB the input s information in cavalry_info.txt is as below Input_Y 1 1 2 2 4 2 0 0 pitch 2 2 4 Input_UV 1 2 1 1 2 1 0 0 pitch 2 2 4 EazyAI test_nnctrl Gets the input pitch from the Cavalry binary Checks dram_format If 1 test_nnctrl EazyAI fuse the depth dim to width dim in order to read the interleaved UV input correctly If users want CVflow to read UV input with normal mode which is for IYUV they must set the dram_format to 0 in vas and generate the Cavalry binary Then VP will read UV input in planar format The steps are shown below The output Cavalry binary is cavalry_ network name network name bin Assume the original network s input is and its shape is 1 3 2 2 4 2 0 0 which corresponds to images with the size of 2 2 4 x200 in RGB format The following are two examples on CV22 of preprocessing for color conversion of NV12 and IYUV to RGB separately Example NV12 to RGB on CV22 Prepare the JSON file as shown below Convert the model to cavalry binary using the following commands Check the port information of UV input in cavalry_info.txt The shape of input_UV is 1 2 1 1 2 1 0 0 dram_format is 1 So the pitch is 2 2 4 on CV22 which corresponds to the shape of interleaved UV input 1 1 1 1 2 2 0 0 Use images in NV12 format as input files for test_nnctrl EazyAI The UV input should be interleaved which has shape of 1 1 1 1 2 2 0 0 For test_nnctrl EazyAI it checks the dram_format is 1 fuse the depth dim to width dim as 1 1 1 1 2 2 0 0 and get the pitch 2 2 4 from the cavalry Then it can read the UV input file correctly For VP as the dram_format is 1 it reads the UV input with interleave mode which converts the UV input s shape from 1 1 1 1 2 2 0 0 to 1 2 1 1 2 1 0 0 Lastly VP converts the YUV input to RGB and feed it to the network In this example the pitch is 2 2 4 as CV22 will align with 3 2 bytes For detailed information about the alignment refer to 5.3 Alignment Example IYUV to RGB on CV22 Prepare the JSON file as shown below Convert the model to the Cavalry binary as follows Set dram_format to 0 in vas Convert the model to cavalry binary using the following commands Then check the port information of UV input in cavalry_info.txt The shape of input_UV is 1 2 1 1 2 1 0 0 pitch is 1 2 8 and dram_format is 0 Use images in IYUV format as input files for test_nnctrl EazyAI The UV input should be planar which has shape of 1 2 1 1 2 1 0 0 For test_nnctrl EazyAI it checks the dram_format is 0 then read the input file with shape of 1 2 1 1 2 1 0 0 and get the pitch 1 2 8 from the cavalry For VP as the dram_format is 0 it reads the UV input with normal mode which reads the UV input with shape of 1 2 1 1 2 1 0 0 Lastly VP converts the YUV input to RGB and feed it to the network In this example the pitch is 1 2 8 as CV22 aligns with 3 2 bytes For detailed information about the alignment refer to 5.3 Alignment For a detailed example refer to the demo CGPP Deployment for Yolov3 Occasionally users will be required to perform some postprocessing for the VP result so it can be easily processed by Arm The parser sets the output format using DRA and the established network configuration Check the format in the VAS code with the string data_format using the layer “VP_output” For data format details refer to 4 Data Format The examples below demonstrate possible fix-points data_format(0, 0, 0, 0) int8 0 data_format(1, 0, 7, 0) 1 data_format(0, 0, 8, 0) 0 data_format(1, 0, 0, 0) if the DRAM value is 2 5 5 the real data is 1 with signed char then the data is data_format(1, 0, 2, 0) if DRAM value is 2 5 5 the real data is 1 with signed char then the data is 1 1.0 2 data_format(1, 0, 9, 0) if DRAM value is 2 5 5 the real data is 1 with signed char then the data is 1 1.0 2 If FP16 is 1 1 0 4 it is equal with FP16 which is defined in IEEE-754 as shown below Sign bit 1 bit Exponent width 5 bits Significant precision ： 1 1 bits The Ambarella side is different from the standard side above One more parameter is added “exponent offset” tunes its expoffset field for underflow and overflow Users must parse FP16 in two steps Parse FP16 as 1 1 0 4 users can use the header file “half.hpp” which is in the Arm “ComputeLibrary” library to convert Users can also follow IEEE-754 and write code to parse it If the expoffset is not 0 such as 6 use FP16 value in memory which means the value parsed by 1 1 0 4 2 6 1 For easier analysis Ambarella suggests using FP16 or FP32 preferred for mandatory output Set the standard FP16 mandatory output using the data_format(1, 1, 0, 4) real value FP16 value in memory Set the standard FP32 mandatory output using the data_format(1, 2, 0, 7) real value FP 3 2 value in memory Transpose and Reshape This can be set using the following commands in “-odst” option with ONet More postprocessing methods can be found in the parser such as “ot” and “os” The “ot” example seen below has similar usage to “os” All VP output widths align with special bytes for different chips such as 3 2 bytes in the example below For alignment details refer to the next section Output 1 1 2 2 8 the output format is fix8 1 byte the real output is as follows The last 4 bytes are padding Output 1 1 2 1 5 the output format is float32 4 bytes as 1 1 2 6 0 the real output is as follows The last 4 bytes are padding First use Tensorflow mobilenetv2 as an example For the case above the output 1 1 0 0 1 1 1 total size is 1 0 0 1 3 2 bytes as it aligns with 3 2 bytes Arm cannot effectively process it as users must get the next class’s confidence with 3 2 bytes of padding More postprocess functions are in the parser The command below switches the channel and width The output should be 1 1 1 1 0 0 1 meaning that the width is 1 0 0 1 Users can visit all class’s confidence one by one in the output DRAM In the CV chip family the CVflow® engine can accelerate NNs For the best performance and special hardware design network data must align with different pitches which includes both input data and output data The pitch alignment rule varies based on the type of chip due to differences in the chips hardware designs CV25 CV22 and CV2 align with 3 2 bytes CV28 aligns with 6 4 bytes CV5 aligns with 1 2 8 bytes CV72 aligns with 1 2 8 bytes Future chips may align with larger pitches Users must pay attention to this pitch alignment rule For most networks CVflow results cannot be directly used Arm must perform some post-process based on pitch alignment rules For liveview NN applications Ambarella suggests that the user must use the pitch from network output structure The structure is defined in nnctrl.h as shown below For test_nnctrl the network output will also be generated with 3 2 6 4 1 2 8 byte alignment in DRAM which is decided by different chips When saving to binary test_nnctrl will remove the padding DSP also has alignment mismatch issues between different chips Refer to the basic Linux SDK document for more details Cavalry utilizes the following two methods to manage Cavalry memory CMA the same method used for DSP this method is very flexible The advantage of this method is that Arm can use the free memory in both DSP and VP However for some low cost-products with small DRAM as CMA enables Arm to use Cavalry s memory when Cavalry requires it it must to get the memory back Sometimes users cannot get the memory back from Arm This case is mentioned in 6.3 CMA Allocate Fail AMA different fom CMA s flexibility using this method the VP s free memory cannot be used by Arm For more details refer to 6.1 AMA Since SDK 3.0.2 AMA is the default method The following uses CV25M as an example Its total size is 1 GB and it is shared by DSP Arm and VP Users can configure it as seen below if necessary The configuration above indicates the points below CV 0 x30000000 0 x40000000 2 5 6 MB DSP 0 x10000000 0 x30000000 5 1 2 MB Arm 0 x00000000 0 x10000000 2 5 6 MB For the CV25M EVK users can check the information seen below CV 0 x30000000 0 x40000000 2 5 6 MB DSP 0 x10000000 0 x30000000 5 1 2 MB Arm 0 x00000000 0 x10000000 2 5 6 MB The leftover Arm memory is 2 8 0 3 5 6 1 0 2 4 2 7 3 MB but it is not equal to 2 5 6 MB This is because the CMA buffer is used meaning that Arm can use the free CMA memory in DSP For VP if AMA is enabled Arm cannot use the VP memory If disabled Arm can use the memory Since SDK 3.0.2 AMA is the default it can only use the free CMA buffer in DSP As mentioned in the log above DSP free memory is 5 1 2 3 9 2 1 2 0 MB The total leftover memory is 2 5 6 1 2 0 3 7 6 MB but because the Linux kernel costs more than 1 0 0 MB the total free is 2 7 3 MB For more details refer to DRAM_Optimization in Linux SDK Doxygen document Users can run test_nnctrl to estimate the Cavalry DRAM cost A DRAM cost estimation includes the following Total Memory Size the total memory which will cost 4 8 7 9 6 3 5 2 1 0 2 4 1 0 2 4 46.5 MB DVI the original network model size is 4 0 8 3 6 9 8 8 1 0 2 4 1 0 2 4 38.9 MB It is close to original model size Quantitative level Float 3 2 fix 8 or fix 1 6 Sparse percentage for example if fix 8 and sp 5 0 is used the size is approximately the original model size 3 2 8 2 If using fix 1 6 it is approximately the original model size 3 2 1 6 2 If it is mixed it is approximately original model size 2 2 If users want a rough estimate they can use this size as it is much greater than the blob size Blob the temp data from running the network Temp input and output for every DAG is 7 9 5 8 9 1 2 1 0 2 4 1 0 2 4 7.6 MB Total Memory Size DVI Blob Users can run all networks in CNNGen samples to see each network’s final memory cost for reference or perform a rough estimate of memory cost If users need to get a rough cost DVI size Cavalry Binary Size can be used for DRAM cost as BLOB memory is much lower than DVI in most of the times user can just evaluate a rough BLOB memory size and add it with the DVI size If users need to get a rough DRAM cost for different resolution based on current tested resolution they can refer to follows As above example total cost of 9 6 0 x960 is about 9 8 MB 1 0 3 1 4 2 1 4 4 1 0 2 4 1 0 2 4 and 4 1 6 x416 is 5 0 MB 5 1 6 7 9 6 1 6 1 0 2 4 1 0 2 4 And DVI size will not be affected by the input resolution only blob size will be affected So the estimated calculation formula is follows Then DRAM cost for 9 6 0 x960 is about 4 0 8 4 3 4 0 4 1 0 7 2 7 8 0 8 9 6 0 9 6 0 4 1 6 4 1 6 1 0 2 4 1 0 2 4 93.4 MB which is close to real DRAM cost 9 8 MB In Arm s high workload Arm Linux may allocate memory from free CMA memory but when the CMA memory is required it will get it back from Arm Linux Sometimes the Cavalry memory interface alloc_cache_recycle() will report “EBUSY” meaning that CMA requires time to take the memory back it does not indicate a failure or lack of memory With this error the user’s application must wait and try again Cache is a high-speed buffer between the CPU and DRAM to address the problem of data read/write speed mismatch in the system In a multi-core system the following two factors may cause the cache inconsistency issue that leads the CPU or I/O devices to read the wrong data As the CPU writes data to the cache the contents change However as the DRAM is not written immediately the data of DRAM is not changed The I/O processor or I/O device writes data to the main memory the contents of DRAM are modified but the contents of cache are not To solve the problem of incorrect data reading caused by the cache inconsistency cavalry provides the API cavalry_mem_sync_cache() with below flags Clean flag check the dirty bit of the memory cache line If the dirty bit is 1 write the contents of the cache line back to the next-level storage and set the dirty bit to 0 Invalid flag check the valid bit of the memory cache line If the valid bit is 1 set it to 0 to discard the cached contents The combination of clean cache and invalidate cache operations above is called the flush cache process Users can decide which operation should be executed according to the direction of data access The detailed usages are listed below If the CPU writes data to the DRAM first and then DMA or other devices access the DRAM users must execute clean cache first to ensure that the DMA or other devices access DRAM accurately If the CPU reads the DRAM first DMA or other devices do not need to execute clean cache before accessing it Once DMA or other devices re-write the DRAM users need to execute invalidate cache So that the CPU will read the data from the DRAM directly instead of the data in the cache However if both CPU and DMA access a block of DRAM concurrently Ambarella suggests users execute flush cache before read/write such block of DRAM to avoid the cache inconsistency issue Then for NNCtrl and Vproc users needs to decide if to enable cache use NNCtrl as an example First NNCTRL can only report the size that network need without allocation as below no_mem 0 means to return the size with input and output no_mem 1 means to return the size without input and output Then user should use this size to allocate memory with cavalry_mem api by themselves so is used to control if to apply memory with cache disable or enable The best suggestions are as follows Use no_mem 1 to exclude input and output then apply network memory with cache disable Apply input and output with cache enable or disable for different cases refer to the example below For output Ambarella suggests to enable cache as it always required for Arm to read and process Input depends on individual use cases Input from DSP or VProc that is sent directly to NNCtrl means that Arm will not visit this memory then it is better to disable cache If input should go through Arm the flow will be DSP VProc Arm NNCtrl In this case it is better to enable cache Users must remember that cache enable is only an advantage for Arm read Libnnctrl can support mprotect function for the DVI memory of NET.bin With library user applications cannot write data on NET.bin DVI memory after calling the API nnctrl_load_net() It will trigger a segmentation fault if the user application writes data on DVI memory which can protect VP from a VP hang as broken DVI memory will result in a VP hang Debugging memory trash is performed easily when segmentation fault is reported By default this feature is enabled Users can disable it by setting “struct net_cfg no_dvi_mprot” when calling the API nnctrl_init_net() This feature only can prevent the destructive behavior of Arm but cannot prevent the destructive behavior of CVflow and DSP For CV2 there are 3 2 address lines enabling users to access 0 4 GB address space However due to the fact that some addresses are reserved for the chip s registers Arm can only access 0 3.5 GB of DRAM space 0.5 GB memory is wasted when hardware uses a 4 GB DRAM size DSP and VP can visit this wasted 0.5 GB For how to use this in DSP cases refer to of the Feature Sets Document in SDK Doxygen documents For how to use this in VP cases refer to the following There are five different types of VP DRAM cost as follows DVI memory which stores the real network Blob memory which is used when the network is running Network input and output Commands between VP and Cavalry driver VP firmware Users can only transfer the memory in 1 and 2 to the 0.5 GB For 3 4 and 5 users should place it in CMA which is visible for Arm As described above VP and DSP use this DRAM Currently there are no protection for combined usage of DSP and VP so users should be cautious if they must use this memory Ensure that VP and DSP will not visit the same address Ambarella suggests that Users run networks one by one to get the “actual size” returned by the NNCtrl library and record the size for each network Users pack all networks together and group them as “VP high memory” Then calculate the total size VP_total_size for all networks Users split the high 5 1 2 MB into two parts DSP and VP where “DSP_total_size VP_total_size 5 1 2 MB” Users can use the command below to check the Cavalry CMA cost Since SDK 2.5.7 auto recycle is enabled by default in the API cavalry_mem_alloc() When the application process exits Cavalry will recycle the unused memory automatically However in some situations that memory cannot be recycled For example users create a main process Then in this main process users fork a new sub-process When there are issues in the application such as which makes the main process stop but does not stop the sub-process the memory cannot be recycled as this memory is locked by the sub-process The only method is to kill this sub-process then the memory will be recycled Users can use the command below to free all unused Cavalry memory If users want to recycle the memory by themselves they must use the API cavalry_mem_alloc_persist() For CV5x can support a maximum of 3 2 GB of DRAM As Arm is 6 4 bits it can access the full DRAM Becuse CVflow and DSP are 3 2 bits they can only visit 4 GB of DRAM There are some limitations for cases when data is interactive between CVflow and DSP or when DRAM is greater than 4 GB For more details refer to Ambarella CV5x DG Flexible Linux SDK* DRAM Optimization Also the result can match the calculated result from test_nnctrl we do both to make sure everything is right There are two methods to test the network bandwidth MB/s test_nnctrl DDR statistics driver for detail please refer to 1 9 DRAM Bandwidth Statistics Here test_nnctrl is used to check the network bandwidth cost Users can get Bandwidth size 7 9 0 0 8 0 0 then to use this value and inference time to get the bandwidth such as follows For roughly estimating users can the bandwidth size to calculate the whole cost For example NET 1 run 3 0 fps Net 2 run 1 5 fps then its total bandwidth size in 1 s is as below There are two methods for memory copy GDMA copy and VP copy in the Vproc library The GDMA copy can save the CPU cycle and is managed by the IAV driver Although GDMA copy can also be used for Cavalry memory copy Cavalry memory is not managed by the IAV driver Therefore users should use the direct physical address as shown below When using VP copy ensure that the buffer size is greater than 2 0 0 KB as the VP loads DAGs to run the copy If the buffer size is smaller loading DAGs can be slower than using memcopy in Linux For more details refer to the API vproc_copy() in the VPRoc Library in SDK Doxygen Documents The EazyAI library supports VP copy in ea_tensor_memcpy() The following table shows the time taken by different copy mode Table 7 1 Time taken by Different Copy Mode Mode 1 0 KB 1 0 2 4 0 B 0 x2800B 1 0 0 KB 1 0 2 4 0 0 B 0 x19000B 2 0 0 KB 2 0 4 8 0 0 B 0 x32000B 1 MB 1 0 4 8 5 7 6 B 0 x100000B 2 MB 2 0 9 7 1 5 2 B 0 x200000B 5 MB 5 2 4 2 8 8 0 B 0 x500000B 1 0 MB 1 0 4 8 5 7 6 0 B 0 xA00000B pure copy with 4 K encoding pure copy with 4 K encoding pure copy with 4 K encoding pure copy with 4 K encoding pure copy with 4 K encoding pure copy with 4 K encoding pure copy with 4 K encoding vproc_memcpy 0.129 ms 0.189 ms 0.159 ms 0.241 ms 0.187 ms 0.264 ms 0.405 ms 0.511 ms 0.716 ms 1.064 ms 1.543 ms 1.994 ms 2.982 ms 4.017 ms GDMA copy 0.056 ms 0.066 ms 0.208 ms 0.244 ms 0.392 ms 0.438 ms 1.805 ms 3.203 ms 3.612 ms 4.285 ms 8.963 ms 10.932 ms 17.901 ms 25.624 ms memory copy 0.104 ms 0.148 ms 0.893 ms 1.111 ms 1.777 ms 3.731 ms 9.094 ms 11.857 ms 18.178 ms 25.728 ms 45.493 ms 67.777 ms 90.825 ms 135.632 ms The data above is tested on CV22_WALNUT The configuration file used by is cv22_ipcam_config and no other app is running in the background The configuration file used by is cv22s66_ipcam_config and DSP 4 K encoding is running in the background Users can refer to the following steps to get the test results The vproc_resize_vect_batch() in the VProc library can perform the image resizing in batches and net_loop_cnt in the NNCtrl library can run network in batches saving DAG load time in the VP This results in optimized performance There are two variables which must be configured for the batch mode such as shown below The first is used for network initialization to allocate the resource for the worst case The second is used to define the real batch number when running the network For example users can set the first one to 1 0 when running the network they can set the second one to any value which is less than 1 0 Users can use the commands below to check the performance between single run and batch Single run Batch with 1 0 There is another loop_cnt in the following structure which is only for long short-term memory LSTM In LSTM the network has internal loops If users set it to 4 and net_loop_cnt to 1 0 it can perform looping with 4 x 1 0 In most of the cases users are not required to use it For this batch mode example users can refer to ea_crop_resize_vproc() in the EazyAI library which uses batch mode Users can perform crop and resize in DSP or VP using the methods below For more details refer to Basic Pyramid and Basic Canvas in SDK doxygen feature_sets Documents For single buffers users can set the canvas buffer s resolution close to network s input resolution in Lua In addition the smallest canvas buffer resolution is 3 2 0 x240 For multiple resolution buffers users can use a pyramid buffer which has six layers for different resolutions Users can also use two canvas buffers but this may waste DRAM bandwidth It is assumed that the main buffer is 4 KP in DSP Two Canvas buffers 4 8 0 p and 7 2 0 p converts 4 KP to 4 8 0 p or converts 4 KP to 7 2 0 p Pyramid buffers convert 4 KP to 7 2 0 p then convert 7 2 0 p to 4 8 0 p DSP buffers can support manual feed to lock buffers that will not be deleted when in use Users are not required to perform memory copy which increases DRAM bandwidth CVflow APIs are more flexible than those for DSP DSP can only generate the frames in defined frames per second fps VP can generate the frames when required meaning that DSP may waste some DRAM bandwidth Users can use one canvas buffer then use VP to perform resizing and cropping at any time If users do not use this process it saves bandwidth If users use VP to perform resizing and cropping it will increase VP loading For some networks postprocessing there are many calculations that run in serial mode but have no dependencies Then users can enable OpenMP Open Multi-Processing to accelerate it OpenMP** performs these calculations in parallel through multiple threads For detailed usage refer to sub_sec_caffe_mobilenetv1_ssd_run_with_live_mode CV chips include three interfaces or methods showing the final results Frame buffer in VOUT test_smartfb.c which cannot perform a frame synchronization Overlay in stream test_overlay.c which cannot perform a frame synchronization Encode the final result to H.264 H.265 metadata and MJPEG user data test_custom_sei.c which can insert the result in the correct frame with the frame synchronization on presentation time stamp PTS Perform blur on detected objects test_blur.c by DSP EazyAI library supports some of the methods above in ea_display.c There are some differences between different chips CV25 CV22 and CV2 have two VOUT devices fb0 and fb1 fb0 is for digital VOUT and fb1 is for HDMI composite video broadcast signal CVBS VOUT CV28 has one VOUT device fb0 to show frame buffer on VOUT CV5 has three VOUT devices fb0 fb1 and fb2 fb2 is used as default for HDMI VOUT The SmartFB library will switch automatically for different chips CV2 CV22 CV25 has two VOUT controllers VOUTA LCD digital and VOUTB HDMI CVBS CV28 has one VOUT controller VOUTA for CVBS MIPI DSI® CV2 CV22 CV25 chips have two VOUT mixers and CV28 has one VOUT mixer which is shared by VOUT and blur The table below lists some use cases for the VOUT mixer Table 8 1 VOUT Mixer Configuration Chip Mixer Default VOUT Default Blur CV2/CV22/CV25 VOUT mixer a LCD digital Blur VOUT mixer b HDMI CVBS HDMI CVBS CV28 VOUT mixer a CVBS MIPI_DSI Blur By default owns and owns whether or not they are functioning Blur cannot own the VOUT mixer when are in the working state Users can specify the or to blur or VOUT manually If the blur is enabled it will use by default It is suggested to close the VOUT device whose mixer is used by blur The following cases show how to use blur Draw on stream For CV2 CV22 CV25 CV28 As HDMI VOUT is not required users can delete the canvas buffer whose type is in Lua is used to enable stream blur insertion is used to specify for blur which is the default setting If the VP is hanging but the reason cannot be identified there are three possible reasons Core voltage is correct for chip specification but VP will not be stable if core voltage is too low There are some issues in the DAGs which are generated by the VAS compiler The user application has some issues which destroy DAGs in DRAM cavalry_log can be used when VP is hanging which can help catch some useful information For detailed usage refer to Ambarella CV* UG Flexible Linux SDK* Code Building and Debug Environment Follow the example below when a VP hang issue is encountered Save the printed error message of NNCtrl related interfaces to app_log.txt and share the app_log.txt to Ambarella Run the command below and share the full log dmesg_log.txt to Ambarella Do not use dmesg c to clean the log the full message is required Run the command below to check the interrupt Run the command below to trigger VP The command below sends a message to VP If it is alive the interrupt number will increase Run the command below to check the interrupt again to see if the VP is alive If the following message is received VP is alive Run the command below again and save the printed information to cavalry_log.txt and then share cavalry_log.txt file to Ambarella Send the app_log.txt dmesg_log.txt and cavalry_log.txt to Ambarella support team Please do not use d 4 in step 6 as cavalry_log with d 4 will return immediately without dumping any log Users can perform the steps below to check if there are issues with VAS When VP hangs users can utilize the tool below to dump all necessary information into a single binary on their own board Then users can use the binary to be replayed on the Ambarella EVK to determine if the problem is due to the VP In addition after running the following command dmesg c can also get more information If there are some problems in replayed dump the binary below then report this issue and send vpstatus_dump.bin below to the Ambarella support team For DRAM problems users can use shmoo tool to perform DRAM tuning then use the cases below to check DRAM stability DSP stream diff If DRAM is not stable an example of a diff message shows Users can also add “ memtester-arm 5 0 ” to give DRAM more stress on the Arm side The example above is based on CV22 the commands of different boards are different For example CV28 does not have direct HDMI output it can only use HDMI with device VP CRC output check together with DSP case This indicates that users run three SSDs in multi-thread Ambarella recommends that users use their own network Using the mobilnetv1_ssd as an example after running the last command this application will be blocked If no print message appears that means it is functioning If DRAM is not stable CRC error report occurs that means with the same input the network output is different Also if DRAM is not stable VP will hang For details on DRAM tuning refer to Ambarella CV S6LM UG Flexible Linux SDK2.5 DRAM Tuning Sometimes user applications can destroy the DVI binary in DRAM due to incorrect operation DVI includes weights and bias If it is destroyed a VP hang may occur Users can refer to the option in test_nnctrl for how to dump the DVI binary into files then compare the dumped DVI files with the original DVI file in VAS output to find if this DVI binary has been broken The primitive code generated by the parser is similar to the C code which is compiled by GCC For CV2x chips as CNNGen will use the same parser for different chips users are not required to run a DRA for different CV2x chips They can use the same analysis for chips in the same architecture and the same usage for CV5x chips But for different architectures such as CV2x and CV5x the primitive code is not compatible The means the primitive code which is generated by CV2x parser cannot be directly used on CV5x chips Of course the difference is very small the performance is almost the same but minor accuracy may be lost If users are not concerned about this they can also use CV2x s primitive code on CV5x For CV72 it is totally different with different Parser and VAS compiler Ambarella suggests that users save the parser output for their networks It will be easier for users to use the same network in different Ambarella chips in future USers must recompile the parser output with the correct VAS compiler Saving the final Cavalry binary is not enough currently only CV22 and CV25 share the same Cavalry binary The difference starts in the VAS stage For CV2x the detailed difference is as follows CV25 and CV22 use the same VAS compiler CV2 CV22 and CV28 use different VAS compilers All CV5x chips use the same VAS compiler different from CV2x CV72 chip uses different VAS compiler with CV2x and CV5x Cavalry_gen is a packaging tool for VAS built binaries And CV2x and CV5x are using cavalry_gen v2 and CV72 is using cavalry_gen v3 NNCtrl checks the following For chips that use different VAS compilers In one CNNGen toolchain if users use different VAS to compile the same primitive code which is generated by the same parser the results of CV chips should be bit-perfect matches for most cases except for those that require convolution or operator splitting Even in those cases the gap would be in the range of rounding difference In one situation for example users used old CNNGen to convert a network on CV25 then they were required to use a new chip which used different VAS compiler but this tool is only supported in the latest CNNGen If users use the latest VAS to compile old primitive code Ambarella cannot currently promise this compatibility There may be speed accuracy improvement in the latest CnnGen that will be missed if using old primitive graphs Tracking target compatibility is tedious for different VAS and primitive versions Newer versions of primitive definition may be compatible to earlier versions in syntax but not in semantics Mixing earlier primitive and later VAS are not a flow that will be QA’ed in Ambarella side All CV chips share the same APIs At the application code level they can use the same code with a different Cavalry deployment binary between CV2x and CV5x with cavalry v2 The NNCtrl VProc and cavalry_mem libraries which are used for the network deployment are all binary level compatible The application which is compiled with the old SDK can be used directly on the new SDK For CV72 it has similar APIs for code level compatible but not binary level compatible with CV2x and CV5x as it is based on cavalry v3 Ambarella does not recommend using the application developed with SDK 2.5 directly on SDK 3.0 as Cavalry from SDK 3.0 and SDK 2.5 are very different SDK 3.0 has multiple advantageous features that SDK 2.5 does not have IAV APIs used for basic video features are currently only code level compatible The application developed with the old SDK can be used with re-compiling on the new SDK For CV72 users must load cavalry firmware when DSP is in INIT status And it only can generate FP16 output with CVflow engine but EazyAI library will convert it to FP32 automatically with ARM resource when doing inference In a normal deployment the user must save the model to NAND The application will load it to DRAM then to VP internal memory then execute it Saving to NAND is dangerous There are many methods to analyse NAND data Saving to DRAM is not as dangerous but is not considered safe as it can be visited by Arm applications VP internal memory which can be visited only by the VP processor is very safe because it is a special hardware flow only for Ambarella For old SDK SDK 2.0.3 and SDK 2.5 No encryption save the original model to NAND load to DRAM and VP This is very dangerous Users perform encryption and save to NAND When loading customers decrypt the encrypted model to DRAM then load to VP memory This method is less dangerous Both old methods above have risk as NAND can be copied easily and DRAM can be visited by a hacker’s Arm applications Since SDK 3.0.6 NN model protection methods are introducted in VP Some points to keep in mind The NN model which is used for final production is encrypted by users own private key With secure flow users can share the private key to VP and VP will use its own private key to encrypt this key and save it to NAND The private key used by the VP is different in every chip and on the same chip if used by different users When required VP will decrypt to get users private key and use it to decrypt the ciphertext NN model then load it to DRAM In DRAM it is ciphertext Only the VP processor can see the real plaintext model All important private keys are in VP memory which Arm cannot visit As VP must perform decryption both in loading and running some performance lost will be lost but the amount lost is negligable For the running time performance lost is at the microseconds level For example yolov3 is about 3 0 3 us For the loading time performance lost is at the millisecond level loading occurrs during network initialization For example yolov3 is about 4 0 0 ms Users should load Cavalry before DSP bootup for encryption mode For more details on API refer to the Cavalry document in Doxygen which is included in the SDK package and Ambarella UG Flexible Linux SDK NN Model Protection Unit license protection ULP is a mechanism of restricting the software so that it can only run on licensed devices With this mechanism software vendors are able to release and track their software more precisely To get the per unit license PUL there are two core security bases and means that the device owns a unique ID has two possible approaches License checking by connecting with an internet server License checking via the device hardware The first approach requires that the device has Internet access when checking licenses As some embedded devices do not have Internet access it is not ideal for all devices License checking using the device hardware is more appropriate for devices without Internet access Before SDK 3.0.6 this feature uses the DSP for key verifification with the following conditions DSP should be in IDLE mode If the PUF-level 2 5 6 bit unique ID in the Ambarelle chip is used it is easy to fake this ID Since SDK 3.0.6 this feature will use the VP for key verification VP does not have as many limitation as DSP With VP the unique ID is not the the PUF-level 2 5 6 bit unique ID VP uses public key and private key mechanisms The private key is stored in VP and users use the public key to verify the certificate which signed the private key to confirm the chip s legitimacy This cannot be faked For more details refer to Ambarella UG Flexible Linux SDK Unit License Protection by CVflow In multi-network deployments there may be some issues which users must manage for high-efficiency usage of VP resources For example suppose there are two networks one is very big and the other is small If users allow the system to decide by itself the sample network may be called in far less frequently which will result in low-efficiency VP usage An old SDK version provides early quit and resume APIs to control the VP task priority in user applications but it is difficult to use this for the purpose of controlling all logics in the application level In the latest SDK 3.0 there is an easy method to set the API for VP task priority which means that the VP will control scheduling In the user application level only the network priority must be set For example there are two networks as follows Net 0 large net which is around 4 1 ms the priority is set to 0 lowest Net 1 small net which is around 2.5 ms the priority is set to 3 1 highest If net 0 is running and net 1 is activated net 1 will take the hardware resources as there should be only one network that can run at the same time Net 0 will stop and wait for net 1 to finish This result is reasonable as net 1 has higher priority But users may find that the running time of net 0 is prolonged to 4 4 ms and the running time of net 1 jumps to ms As shown in the the following log note that the vp_time is from in the application This is because indicates that the time that the network spends in the VP note that it is not a pure computing time has included the waiting time For “Got result Net_id 0 Frame_id 0 vp_time 43.90284 ms” as Net 0 is waiting for Net 1 the time is the running time of and For “Got result Net_id 1 Frame_id 0 vp_time 16.50830 ms” when Net 1 comes Net 0 does not stop immediately as it must finish the current DAG which is running This time should include the running time of and Users can use cavalry_top or 1 6 Cavalry Profiler to check VP usage efficiency For more details on the API refer to the Cavalry document in Doxygen which is included in the SDK package For usage examples refer to Section Car License Plate Detection and Recognition Most networks require pre-processing Users must call VProc for pre-processing such as YUV to RGB convert resize and so on Ambarella suggests that users set these pre-processing tasks to use the same priority as the networks However an issue can occur as follows For example A1 is the YUV to RGB for network A A2 is the RGB resize for network A A3 is network A and B is network B If users run them in two threads the flow will be as follows The VProc APIs will take more time as it includes the time of the DAG in B If users want to run A1 A2 and A3 together without being interrupted by B they can use the flag “no_auto_resume” for network B meaning that B will not be resumed automatically Users must resume it manually Users can then control the flow as shown below Refer to the image below for how to use the manual resume APIs Figure 14-1 Manual Resume Flow For detailed examples refer to Car License Plate Detection and Recognition Halting DAGs when running from the hardware perspective is not supported Some large DAGs will block the small nets which have higher priority If a single DAG is too computationally intensive users can split the large DAG into smaller DAGs First users can check if there are some large DAGs as shown below There are two methods to perform this force splitting Split with specific primitives To configure a specific DAG split users can use the option “-prim-split-point” with the output tensor s name to tell the VAS compiler how to split as shown below Single split Multiple split Since version 2.4.0 the option splits the input VAS program into multiple VAS programs so that each VAS program includes primitives as shown in the following command Please refer to 3 Eazyai Video for the detailed information Users may be required to check CNN algorithm results for specific videos LT6911 is suggested in this case This module s full name is With this module users can connect computer HDMI output to the CV EVK sensor interface The computer HDMI output resource will be used as a sensor input This is not limited to computer HDMI output any device which has HDMI output can function Then users can play video files on their devices and CV EVK board can receive these video frames as sensor inputs as seen in the images below Figure 15-1 Flow Users can use the commands below to enable this module in EVK using Yolov3 below as an example If LT6911 is unstable delete the LT6911 default microcontroller unit MCU code using the command If the YUV input is selected set the mode to for the “vsrc_id” option in the Lua script For the deployment of multiplication algorithms it is important for users to write applications that use the VP resource effectively Poor logic may result in the waste of VP resources Users can use cavalry_top to check the idle rate of VP processor cavalry_top is similar with top in Linux This command is to sample with interval 1 0 0 us and doing 1 0 0 0 0 times so it means during the duration 1 0 0 0 0 x 1 0 0 us 1 s do sampling 1 0 0 0 0 with interval 1 0 0 us As it has limited information for users to debug the Cavalry Profiler provides more detailed information with Web UI Use the test_fdfr_v1 demo below on CV22_Walnut as an example For details on this demo refer to Face Recognition Generate cavalry_profiler_fdfrv1.bin After test_fdfr_v1 is running execute the following command Open the Web UI Users should open the in SDK/ambarella/app/utility/cavalry_profiler with the browser Any browser application can be used as it is implemented with HTML5 however old IE browser versions should not be used Ambarella suggests using the latset version of the Chrome browser Users can see the following interface Figure 17-1 Cavalry Profiler Show the VP information Drag to the prompted button on the web then the VP profile data will be analyzed and shown Users can move the mouse and hover over an element to see related information Figure 16-2 Cavalry Profiler Histogram Note By analyzing the above figure and the application s source code users can get the following information Part 1 VProc uses the VP resource for converting YUV to RGB and resizing RGB Part 2 VProc uses the VP resource to prepare the input data with different resolutions for the eight pnets Part 3 pnet is using the VP resource to run eight pnet models Part 4 the post-processing of pnet and pre-processing of rnet is running on Arm Part 5 rnet is using the VP resource Part 6 onet is using the VP resource Part 7 mobilefacenet is using the VP resource for face recognition Part 8 the application is waiting for the next frame as it used the block API in IAV 1 8 is a full loop for this application to analyze one frame For how to use the VP resource effectively the most important principle is to run VP without stopping Users can run Arm tasks and CV tasks at the same time as they are different and independent cores Through the above analysis for test_fdfr_v1 users can optimize the logic so that part 4 of the first frame and part 7 of the second frame can run at the same time which can improve the efficiency of the VP resource Show the VP information separately by PID Users can click which is marked by the red box in the figure above Then users can see the information as follows PID 6 5 2 is for face detection which includes the pre-process with VProc Pnet rnet and onet PID 6 5 1 is for face recognition with mobilefacenet Figure 17-3 Cavalry Profiler Histogram by PID Check the utilization rate When scrolling down users can see the utilization rate of VP for different tasks currently it supports CVflow feature extrating FEX feature match FMA Figure 17-4 Cavalry Profiler Chart Only CV2 contains FEX FMA and FEX FMA is for stereo Different processes will be displayed in different colors User can use below command to restart VP as below when VP is hang In CV72 user only can do it when DSP is in stage Users may be required to decrease the VP clock to save power and can perform the following procedures Users cannot use the commands above when the VP is running certain tasks There are many common image process and traditional algorithm examples in VProc which will benefit from the CVflow engine For detailed performance refer to d1/d66/page_lib_vproc_doc.html in the Doxygen documents of the Linux SDK package For network performance CV2 is approximately 4 x CV22 1 6 x CV28M or more The traditional algorithm in VProc is decided by different cases the reason being some traditional operators that VProc used have similar performances among CV28M CV22 and CV2 Arm DSP and CVflow will use the same double data rate DDR controller Priority DSP Arm CVflow If DSP runs out of bandwidth CVflow and Arm will be too slow for DRAM I/O operation And even ARM have higher priority than CVflow CVflow will get higher bandwidth than ARM as it is too faster Users can use the statistics tool to check the bandwidth usage provided below For more details refer to feature_sets/d5/dd4/fs_others_dram_statistics.html in the Doxygen documents of the Linux SDK package</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_cnngen</field>
    <field name="url">d2/d67/fs_cnngen.html</field>
    <field name="keywords"></field>
    <field name="text">CNNGen Toolkits CNNGen Toolkits This chapter describes the environment settings and tools required for running CNNGen The following illustrates the CNNGen interface CNNGen Interface CNNGen Interface CNNGen Interface CNNGen Interface For installation information refer to 1.3 Installation or the readme.txt file in CNNGen toolchain package Also user can install it with the Docker file in CNNGen toolchain package Before using the CNNGen toolchain please source the CNNGen toolchain EVN file to set up the system environment For lots of Caffe networks official Caffe can not support lots of special layers such as permite in SSD reorg for Yolov2 and so on Then users need to using a special Caffe also they can use AmbaCaffe which provided in CNNGen 2.x.x toolchain As Caffe is rarely used for current new networks Since Ubuntu2004 CNNGen toolchain will not provide AmbaCaffe users can find it in old CNNGen toolchain package To avoid redirecting the tool to the location of Caffe multiple times build it install it to or add the export command to bashrc Please contact the Ambarella support team for more details All elements in a vector share the same data format no matter for input coefficients and output The supported data types are signed or unsigned 8 1 6 or 3 2 bits wide and fixed-point or floating-point The data_format parameter defines the signedness element size and precision for each vector which is as follows 1 st argument Specifies signed/unsigned 0 is unsigned 1 is signed 2 nd argument Specifies the width of the data elements 0 8 bit 1 1 6 bit and 2 3 2 bit 3 rd argument Specifies the exponent offset Fixed-point shifts the binary point to the right by expoffset i.e multiplies value by 2 expoffset maximum range is 0 1 5 Float-point applies a signed 2 ’s complement offset to the exponent maximum range is 8 7 4 th argument Specifies the value to determine the number of bits reserved in a data element for the exponent field maximum value is 7 A value of 0 indicates fixed-point numbers If greater than 0 the number of exponent bits is expbits 1 4 means FP16 7 means fp32 Integer format is supported as a special case of fixed-point numbers where the expoffset and expbits fields are both 0 As fix point is always used for coefficients so the 4 th argument is always 0 Output is always set to FP32 it should be 1 2 0 7 Of course users can use FP16 it should be 1 1 0 4 which is equal with FP16 which is defined in IEEE-754 Input data can be any format but even users used FP16 or FP32 the parser will convert it to fix point first It is important to pre-process the following data input while using CNN Data type including YUV RGB BGR or others Input dimension channel width and height Meaning Scale By default the input data type and dimension conversion are not included in network model The meaning and scale are performed in the network so the input range is always Fixed-8 RGB or BGR Although Ambarella recommends using this method users can opt to perform the meaning and scale before the network but must use the correct data range for the parser’s input The following section provides information on choosing the data format for input used by the DRA and deployment using Mobilenet as an example Identify the pre-process input for the network while training Ensure that the input data format fully covers the Data_Range_After_Scale above For the above example ufix8_8 and sfix8_8 are not adequate Ambarella recommends using sfix8_5 or sfix16_5 instead Additionally Ambarella recommends using sfix16_5 instead of sfix8_5 as it can retain more data range For details please refer to 4 Data Format The tool gen_image_list.py performs default quantization list generation For details refer to the Ambarella CV UG Generate Image List in CNNGen toolchain package The following two examples are provided as a guide for generating the DRA list gen_image_list.py can perform the resize and YUV RGB BGR conversion and also can perform meaning and scale which is rarely used As a result it generates the DRA binary in fix8 0 0 0 0 as fix16 and float16 are rarely used For the example shown above it converts the images in the test_image folder to RGB and resizes them to 3 x608x608 Then it saves the binaries in dra_bin folder with the result list file dra_list.txt Mean value reduction and normalization are not supported in gen_image_list.py If the user does not want to include meaning and scale in the parser use im2bin to generate the DRA with resize and RGB BGR conversion Meaning and scale must be processed using a different network request which is in the CNNGen Sample Package For details refer to Section 6.1 im2bin Users must choose a suitable input data range for the parser that can cover all the input data range with fix8 or fix16 For details refer to 4.1 Input Data Format The DRA is used by the parser to determine the data format by date distribution which is calculated by the sample images provided by the designer The files should be in the binary with the YUV RGB or BGR format One image is in one binary The binaries’ data format should follow the definition of the parser after converting it via gen_image_list.py or im2bin The parser calculates the layer range of each image to help convert FP32 to the FX16 or FX8 format The final format should match the definition in option of parser This tool is related with third-party libraries such as and Note that if these libraries versions are different with user s training environment it will bring some accuracy gap Currently the Ambarella toolchain supports the* caffeparser.py tfparser.py and onnxparser.p parsers For details refer to CV UG Parser Interface in the CNNGen toolchain package Suggested parser usage is as follows Best performance c act-force-fx8 coeff-force-fx8 Best Accuracy CVflowv2 of CV2x and CV5x c act-force-fx16 coeff-force-fx16 CVflowv3 of CV7x and CV3x c act-force-fp16 coeff-force-fp16 Balance A and B null Only force act c act-force-fx8 Only force coeff c coeff-force-fx8 Mixed act and coeff c act-force-fx8 coeff-force-fx16 For JSON pre-processing users can refer to the Ambarella-CV UG-Pre_and_Post-Processing_JSON* in the CNNGen toolchain package There are also some examples shown in Section Mobilenet Runtime Rotation Section CGPP Deployment for Yolov3 and Section ResNet50 with Json Pre-Processing DRAv1 is only for 8 bit quantization loss control DRAv2 advantages Achieves a better network accuracy with minimal impact on the performance Provides pro-accuracy and pro-performance choices If the majority of the activation data spans beyond the 8 bit dynamic range DRAv2 uses 1 6 bits to quantize the data DRAv2 determines coverage of the 8 bit data format by computing the maximum area of the histogram that is covered by 8 bins on a log-2 histogram DRAv3 advantages enable by The main idea behind the Modes 1 and 2 is to limit data overflow underflow introduced by quantization at each quantization boundary This is achieved by observing the distribution of float-point data at the quantization boundaries However this may introduce unnecessary precision up-conversions or incorrectly allocate precision in the earlier part of the network because it does not take the effect of the cumulative quantization error into account Compared to the Modes 1 and 2 Mode 3 is based on cumulative quantization For each frame of the sample input it tests several quantization strategies for each quantization boundary and then records the numerical accuracy of each strategy and applies the best one Then the quantized tensor is used for the input in the latter part of the network After processing all the frames the quantization strategy that achieves the best overall numerical accuracy is picked out at each quantization boundary As DRAv3 is more complex and slow with CPU it can be supplemented with the GPU support For a detailed example refer to Section ResNet50 with DRAv3 Ambarella suggests using FP32 for the output to easier parsing Ambarella suggests converting the width x height as 1 x10 to 1 0 x1 because the CV chip DRAM will align with 3 2 bytes If the output is 1 x10 and format is fix 8 the total output size is 3 2 x10 bytes however if 1 0 x1 is used and the format is fix 8 the output size will only be 3 2 bytes If users encounter a problem Ambarella suggests using for the first debug This generates a log called frame_000000_cumulative_err.txt Then users can check the psnr value to see if there is a big gap in the continuous DAGs For the sideband file refer to the Ambarella CV UG Sideband Usage and the A*mbarella CV UG Layer Level Sideband* in the CNNGen toolchain package In general DRA V1 is the simplest algorithm In the simplest setting type minmax it chooses a fixed-point quantization that can hold the minimum and maximum values of all the activation values it sees In the advanced mode type histo it collects a linear-scale histogram on the activation data and then estimates a quantization loss for three 8 bit quantization settings Finally it selects the setting which will minimize quantization loss The loss function could be selected from MSE ABSERR and RELERR The default setting is MSE because MSE yields the best accuracy from the testing experience The configuration key histo-extra is the coverage threshold feature retrofitted from DRA V2 DRA V2 improves on DRA V1 by introducing the dual histogram to automatically determine if an activation tensor within the 8 bit value range should be upgraded to use 1 6 bit This is because 8 bit quantization could introduce an excessive underflow If a certain portion of the activation data can be represented in 8 bit without overflowing or underflowing the parser will choose 8 bit quantization Otherwise it will switch to 1 6 bits The configuration key coverage_th controls the threshold of the coverage portion The default value is 0.9 The configuration key coverage_recover_th works like a safety net by evaluating the range that can be covered by the 8 bit data format to minimize MSE By default this value is 0 but users are recommended to set it to 0.5 if the DRA V2 accuracy is insufficient DRA V2 focuses on the hardware-related quantization details such as special data format constraints required by ICE hardware DRA V2 also introduces scaling which automatically scales large or small values out of the 8 bit quantization range to fit within the 8 bit range for activation tensors This is controlled by the allow_scaling flag This feature allows more computation to use 8 bits providing the performance enhancements However sometimes users find that allowing more 8 bit computation the accuracy drops Users can turn off this feature to keep the scaled data in 1 6 bit format The following options are also provided The no-opt flag is mostly for debugging purposes no-opt ensures that each layer s output is preserved and users can dump all the intermediate results in the simulator However no-opt may also result in sub-optimal accuracy and deployment performance DRA V1 and V2 could also benefits from GPU as the underlying math operations are shared by all DRA methods Because DRA V1 is not as robust as V2 and could generate underflow and overflow in many cases when quantization is performed For this reason it is not recommended that users focus primarily on DRA V1 DRA V3 focuses on hardware-accurate simulation It scans through many quantization plans for each operation selecting the plan that uses less bits but still ensures the final target accuracy described by either the peak signal to noise ratio or Pearson correlation coefficient As DRAv3 is more complex which has large amount of computation users need to enable GPU to speed up the calculation process such as Yolov3_Tiny example below with CNNGen Samples Package Use GPU which only supports Cuda 9.2 and Cuda 10.2 for CV2x and CV5x Cuda 11.4 for CV72 Use CPU For more details refer to the documents in CNNGen Toolchain package in the future The caffeparser.py converts a Caffe framework model to Ambarella’s format For details refer to the Ambarella CV UG Caffe Parser in CNNGen toolchain package Before using a layer ensure that the layer type or format in prototxt is in the official Caffe format and modify the number of data augmentation to 1 in input_shape The tfparser.py converts a TensorFlow’s frameworks model to Ambarella’s format For details please refer to Ambarella CV UG TensorFlow Parser in the CNNGen toolchain package Typically the output of the TensorFlow training phase is a set of checkpoints ckpt.data ckpt.index ckpt.meta checkpoint which must be converted to a protobuf and further frozen For further details refer to the document Ambarella CV DDG TensorFlow Parser and the following sections A model is frozen to convert variables into constants To deploy it in production it is better to optimize away batch normalization or other training-only features When using checkpoint_to_pb.py the following TensorFlow error message can appear This error occurs because the InfeedEnqueueTuple operation can only be used on a specific tensor processing unit TPU It should not be loaded or supported if the TPUs are not available To avoid this error perform the following steps Download the trained model Download the trained mobilenet_v2 model from the following link https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz Copy the model to the working space and release it This compressed package contains all the parameters of mobilenet_v2 and will be used in the following steps Source the Ambarella CV toolchain Download the GraphDef file The network trained by the TensorFlow framework is saved in a graph which contains all operations Typically this graph is saved as a pd file To convert the mobilenet_v2 as an example the user needs the parameters and the network descriptions file pb The following steps show how to download the file After the file has been downloaded modify the Python script py Execute the following command to download the file Freeze the model The network descriptions file mobilenet_224.pb contains only the network structure all the parameters are stored in a different file However TensorFlow includes a tool that enables users to conveniently store both the network structure and parameters in a single file Typically this procedure is called freeze The command for freeze is as follows Optional step Optimize the frozen model After generating the frozen model the next optional step is to apply graph transformations to the model Ambarella strongly recommends users perform this procedure because not all graph transformations are currently performed by CNNGen For more efficient calculation CNNGen always treats the input as if it were NCHW format even though TensorFlow is typically in the NHWC format It will switch in the default mode but if users need to use the NHWC format input in final deployment they must transpose the data as follows This command allows users to use the special input shape which is not in the NHCW format This function also works for Caffe and ONNX use the TensorFlow as a reference The whole pre-processing order is explained below YUV420-YUV444 YUV420-RGB Transpose Resample to the real network shape if it is different Swap channels Mean subtraction Input scale “is” will always override the prototxt shape If users provide a shape the parser will take it and apply the pre-processing options Below used TensorFlow MobileNet v2 as an example which is provided to help users understand this function Input with default 1 3 2 2 4 2 2 4 Users can find the generated VAS below The parser will check the original input shape of NHWC 1 2 2 4 2 2 4 3 and switch it to NCHW 1 3 2 2 4 2 2 4 then NCHW is the final input shape when it is deployed on the chip The parser will always treat the model in the NCHW shape as the Ambarella chip is designed to calculate using NCHW Input with special 1 2 2 4 2 2 4 3 Users can find the generated VAS below For the same model the example above can also use “is 1 2 2 4 2 2 4 3 ” If users need to do this “it 0 3 1 2 ” should be added This is because NCHW is needed for all models If not the parser will report an error With the commands above users can feed an input with NHWC in their application For example for two networks network 2 needs to directly use network 1 ’s output whose output is “1 2 2 4 2 2 4 3 ” then user can use “-it” to connect network 2 to network 1 As in the default mode network 2 is converted by input 1 3 2 2 4 2 2 4 if user do not use “-it” the network 1 ’s output needs to be switched This option is only used when a special input shape is needed such as shown in the example case 3 Input with special 1 2 2 4 3 2 2 4 Users can find the generated VAS below The parser always needs NCHW but user can use “-it” to make “-is” variable for different requirements For TFLite users can directly feed the TFLite model to tfparser.py then the parser will convert it to a PB file and eventually convert the PB to an Ambarella format Before conversion it is recommended that users check tf_print_graph_summary.py to see if it is a VP friendly model If there are unsupported operations or problems please contact the Ambarella support team for assistance When this model is retrained via quantization fix8 it mostly has a good accuracy with DRA fix8 after conversion But users cannot ensure that TFLite Uint 8 models have better deploy accuracy than TF models because TFLite has a little difference that is trained with a specific idea of running it on the TPU s In TFLite models there are FQ ops for certain patterns only that TensorFlow optimizes Rest of them the data format needs to be determined by CNNGen DRA The biases in TFLite model are also 3 2 bits but it is not the case that CNNGen can pick 8 1 6 bits In general TFLite models are more like partial data formats given to CNNGen and letting CNNGen use DRA images to figure out the remaining data formats So it is not guaranteed that users always get higher deploy accuracy The TFLite model is different with the PB file For the most part PB files do not include image preprocessing operations but TFLite includes the input preprocessing as it is used for inference deployment When converting it users should pay attention to this scenario in case that it feeds the preprocessing operations again which will result in an accuracy problem For others it is the same process when converting a PB file Tensorflow can support NCHW by setting a parameter in related operators or layers but it does not mean that only supported input is NCHW It means that all calculations are based on NCHW An example as below Also there are some operators like MaxPooling2D which can be handled only on GPU with NCHW format But tfparser.py in CNNGen toolchain cannot handle the NCHW format such as logs below Tensorflow 2.x is totally different with TensorFlow 1.x so there is an alpha method to convert it Ambarella cannot guarantee whether this conversion is successful or not Note that the problem is for TF2.x exporting Right now the story is to and TF parser can handle that During exporting to TFLite customers may have to split the split is similar to how the split did in Arm part as before This feature is only supported after 2.2.1.3.829 For how to convert TF2.x to Tflite please refer to Tensorflow public web as follows For basic networks please refer to with the following three methods Converting a SavedModel to a TensorFlow Lite model Converting a tf.Keras model to a TensorFlow Lite model Converting ConcreteFunctions to a TensorFlow Lite model For detection networks please refer to for details When exporting TF 2.x models to Tflite sometimes its operator s name string will be very strange such as and so on For such special symbols they will have some conflicts with some processing in the toolchain please be careful of such symbols it is better to remove and refine them If they cannot be removed or refined please ask Ambarella for help to check if they can be handled in the toolchain The onnxparser.py converts the ONNX frameworks model to the Ambarella format For details refer to the Ambarella CV UG ONNX Parser in the CNNGen toolchain package For most of frameworks it can be converted to ONNX such as Tflite Pytorch and so on it is always suggested to use ONNX for network convert ONNX is not a training framework and most of the models are converted from other frameworks So ONNX will use opset version to control the compatibility and the current parser will focus on Opset 1 6 There are two tools which is named Acinference and Ades In the CNNGen conversion there are three steps to convert the original model to the final deploy model in Ambarella Convert public model to Ambarella node DRA to decide the data format VAS compile to compile it to machine code In every step there will be some accuracy lost For step 1 users can ignore the accuracy lost as it is very small For step 2 most accuracy lost is here For step 3 there will be minor accuracy lost EazyAI library has packaged below Acinference and Ades library to unify the APIs which are the same with EVK deployment APIs For detail please refer to 3 EazyAI Simulator and 6 EazyAI Simulator Unit Test For eazyai fast compile please refer to sub_sec_eazyai_simulator_qs Also it is provided as prebuild version which is inclcuded in tool eazyai_inf.py please refer to 2.4 EazyAI Inference Tool for detail Acinference can be used to check the accuracy lost in step 1 and step 2 Acinference runs much faster than Ades as Ades needs to simulate chips behavior also Acinference can benefit from GPU and its result is very close to Ades for Cosine Distance the gap is about Users need to use parser to generate the parser output Acinference is based on the parser s output such as the Json file in the following command Users can use this library to instantiate an Acinference instance in their PC test application The library and the header file are as follows For detail please refer to 10.2.4 Quantization Inference Library Sample for the detail example in CNNGen samples package If users need to use GPU the following modifications need to be done in CNNGen toolchain below used CV22 as an examples Please make sure the build server has support for GPU and CUDA first Modify LD_LIBRARY_PATH in toolchain path to switch it to the specific version Modify the project.tv2 in toolchain path to switch it to the specific version Source the env file Run as below Since CNNGen toolchain 3.5.5.0 CUDA 11.4 CUDA 11.8 and CUDA 12.1 are supported for CVflowv3 of CV7x and CV3x CUDA 11.4 is the default version which passes the test Also it will be added to CNNGen toolchain 2.x for CVflowv2 of CV2x and CV5x in future which only supports CUDA 11.4 in current version In the parser there are three steps to convert the original model to the final deploy model in Ambarella Convert public model to Ambarella node DRA to decide the data format VAS compile to compile it to machine code In every step there will be some accuracy lost For step 1 users can ignore the accuracy lost as it is very small For step 2 most accuracy lost is here For step 3 there will be minor accuracy lost Additionally this Quantization library C++ interface can be used to check the accuracy lost in step 1 and step 2 ADES library which is a bit perfect result with EVK refer to 10.3.4 Ades Library Sample C++ interface can be used to check all the accuracy lost in step 1 step 2 and step 3 Users can use this tool as below Ades can be used to check all the accuracy lost in step 1 step 2 and step 3 which has bit perfect result with EVK board users can call it executable binary or library APIs Since CNNGen toolchain 2.5.4.09 and 3.5.4.0 ades command will not be supported anymore Users can using EazyAI inference tool eazyai_inf.py in 10.1 EazayAI instead also they call run_ades_lite.py to run Ades For details refer to the Ambarella CV UG ADES Ambarella CV UG ADES Regression the Ambarella CV UG ADES Script Autogen and Ambarella CV UG Running ADES in the CNNGen toolchain package Users can use this library to instantiate an ADES instance in their PC test application and the current version only supports one instance in one OS process The library and the header file are as follows The library is so and the header file is h the functions users required are declared in the header file Because this library needs libvdg to support users should have the path of the libvdg link below accordingly Following messages are the gcc link options used to build an application For detail please refer to 10.3.4 Ades Library Sample for the detail example in CNNGen samples package If users need to use GPU the following modifications need to be done in CNNGen toolchain below used CV22 as an examples Please make sure the build server has support for GPU and CUDA first Modify LD_LIBRARY_PATH in toolchain path to switch it to the specific version Modify the project.tv2 in toolchain path to switch it to the specific version Source the env file Since CNNGen 3.5.5.0 CUDA 11.4 CUDA 11.8 and CUDA 12.1 are supported for CVflowv3 of CV7x and CV3x CUDA 11.4 is the default version which passes the test And there is no plan to add these support in CNNGen toolchain 2.x for CVflowv2 of CV2x and CV5x The following steps are the usage of the library Create an ADES instance For example This function returns a pointer to an ADES instance The first input parameter is the ADES script file path If this operation fails it returns a NULL pointer and the second parameter a std::string includes the failure reason Set input vector data For each input descriptor users need to copy in the data Additionally only the binary data is taken Please refer to the following example Detailed parameters are provided below The first parameter is the input descriptor name The second parameter is the starting address of the input data The third parameter is the size of the total input data buffer size The fourth parameter is the pitch size of the input buffer similar to dp in command The fifth parameter is to specify that if the input data stored in a DRAM format specified by the VAS descriptor similar to dfmt in lbin command If something goes wrong this function returns a non-empty string which provides the failure reason Run the script Here is an example If something goes wrong this function returns a non-empty string which provides the failure reason Get the outputs For each output descriptor users can copy out the output data after the execution Here is an example Detailed parameters are provided as follows The first parameter is the output descriptor name The second parameter is the starting address of the output data buffer The third parameter is the size of the total output data buffer size The fourth parameter is the pitch size of the output buffer similar to in command the default value is 0 The fifth parameter is to specify that if the output data stored in a dram format specified by the vas descriptor similar to in sbin command the default value is true If something goes wrong this function returns a non-empty string which provides the failure reason Process next frames For the next input frame repeat step 2 to 4 Delete the ADES instance after processing all frames Here is an example For detailed example users can refer to the message below in CNNGen samples package This tool generates the final DAGs for the Cavalry input For details refer to the Ambarella CV UG Cavalry Autogen in the CNNGen toolchain package The is not fully backwards compatible Users can generate a different Cavalry binary depending on their SDK Using cavalry_gen version 5 Using cavalry_gen version 6 Using the latest cavalry_gen In SDK 3.0 the Cavalry framework will include some improvement for backwards compatibility It is fully backwards compatible in the same big version For example in 2.2.8 the current version is 2.2.8.1 In the future it will have version as 2.2.8.2 2.2.8.3 and so on then to assume the latest cavalry_gen is 2.2.8.9 and the latest SDK version is 3.0.9 The latest SDK 3.0.9 version which is released together with 2.2.8.9 can run all the binaries converted by 2.2.8 which is lower than 2.2.8.9 The old SDK 3.0 version which is released together with 2.2.8.1 cannot run the binaries converted by 2.2.8 which is higher than 2.2.8.1 If SDK version upgrades 2.2.8 to 2.2.9 that means it will totally break the compatibility users need to use “cavalry_gen” to convert 2.2.8 to 2.2.9 for the latest SDK SDK 3.0 version will include cavalry_gen in EVK boards then users can call this application when Cavalry version conversion is needed The cavalry_gen tool supports conversion between different SDK versions If SDK version and cavalry_gen Version are mismatched it is suggested to refer to below steps to convert the cavalry binary version Check SDK cavalry library version So 2.1.7 cavalry network binary is needed Then please check cavalry_gen tool to see which version it can support as shown below At last convert it as below Also if users already have a model which is converted in cavalry_gen users can easily convert it to the latest version as below Users can find tool cavalry_gen in two places CNNGen Toolchain Package this version can only be used in build server in SDK package this version can only be used in EVK board Users can check current cavalry binary s version in EVK as below The layer_compare.py is a tool that compares the forward pass results of Caffe and TensorFlow against those of CNNGen and ADES for each layer It is useful for debugging precise differences between the reference framework such as Caffe TensorFlow and CNNGen ADES For details refer to the Ambarella CV UG Layer Compare in the CNNGen toolchain package Since 2.4.2 there are some useless options removed in layer_compare.py it will use the configuration in file instead which is generated by Parser such as below The following example shows how to use the layer_compare.py tool to compare the CNNGen ADES results with the Caffe model results after finding something with “-dinf cerr” Source the Ambarella toolchain Before using layer_compare.py source the Ambarella CV toolchain Run In this example Ambarella uses AlexNet To use layer_compare.py users must first convert AlexNet and then compile it Additionally refer to the Excel file error_list.xlsx Check the comparison results The error_list.xlsx will reveal the following and the table format will be as follows If an error occurred in the table “Caffe f32 vs CNNGen f32” the operator realization in CNN is incorrect If an error occurred in the table “Caffe f32 vs CNNGen Quant” the quantization is incorrect If an error occurred in the table “Caffe f32 vs ADES” ADES is incorrect Layer Name Max Absolute Error Max Absolute Error Cosine Distance Pearson Correlation xxx xxx There are three coefficients that can describe the errors between the results of CNNGen output and the results of the Caffe model Max absolute error Cosine distance Pearson correlation If the max absolute error and cosine distance of each layer is close to 0 the CNNGen result is more accurate Similarly if the Pearson correlation is close to 1 the CNNGen result is more accurate CNNGen does some optimizations so the results are not reflected layer by layer If a specific layer’s accuracy seems strange users should refer to the methods below to check if accuracy was lost during conversion Use the option “no-opt” to disable internal optimization This makes sure every layer in the Caffe model can be compared but is not recommended for production code Use it only for debugging purposes The function is set by default to “ON” It applies the range of a clip primitive to its source If the clip is the only user it is possible to see the conv2iepbs difference is more significant and the relu6 difference is smaller The unnecessary range has been clamped earlier which maintains a better smaller side precision In the final report it will show a big accuracy loss in some layers like relu6 but back in the final output layer it is normal This is the result of Use “-c allow_focused_range false” to disable it By default layer_compare will generate CNN quantization result in folder “lc_cnn_output” if the user needs ADES and original model result please use “-d” option which is used to set debug level if users set it 3 it will generate all these results for the comparison Users can use graph_surgery.py to convert the unfriendly VP model to a friendly VP model The graph_surgery.py enables users to modify a trained model file which can still run in the original framework and works for Caffe TensorFlow and ONNX For details refer to the Ambarella CV UG Graph Surgery in the CNNGen toolchain package Before using this tool users should use onnx_print_graph_summary.py tf_print_graph_summary.py and caffe_print_graph_summary.py to check the network model If there are any problems or it reports some issues users do not need to try to convert as there will be problems To resolve it the following tools share suggestions on how graph_surgery.py can handle it Before converting a network the user should do below things or else it will make convert more complex Please use PC original framework to run this model to check if everything goes well For example the user converts a Pytorch model to ONNX before using CNNGen tool the user should check if there are some problems in this convert between Pytorch and ONNX Please use “*_print_graph_summary py” to check if there will be some problems Make above two checks go well then try to convert Users can use tf_print_graph_summary.py to check the model’s status and to see if there will be some problems If there are some problems please do not try to convert for yolov3 example below with ONNX framework Tensorflow and Caffe has same tools As shown in the figure above users can find the input output name and dimensions as well as some basic information for this model Then it reports some errors and shared some suggestions below Dimension of tensors is larger than 4 which is not supported CutGraph should be used to delete this operator Unsupported operators of shape use ConstantifyShapes to solve Some unnamed nodes and foldable nodes use ModNodeNames and FoldConstants to solve Once the suggestions have been provided by the above tool users can then use graph_surgery.py to process the model Then it will generate a new model named yolov3-spp-surgery.onnx Users can use tf_print_graph_summary.py to check it again After checking users can see if the new model is a VP friendly model then they can try to convert The graph_surgery.py is a power tool but not a universal tool it can cover most of the cases but not all so users need to pre-process the model by themselves when graph_surgery.py cannot solve problems or they can contact the Ambarella support team for more details ReplaceSubgraph is a transform flag of graph_surgery.py which provides a framework to replace a part of graph a.k.a sub graph with that defined by the user It s useful when part of the graph is unsupported on VP for example if reshape or stack operation results in tensors larger than 4 D When possible they can be replaced with ops that work with VP friendly operators in this case ops that result in tensors that are 4 D or less To use ReplaceSubgraph transform the user needs to provide two files The first one is the new sub graph model and the second one is a json file containing information about the sub graph to be replaced Model file Model file is a Python file that should contain the implementation of subgraph() method For TensorFlow models it should have the following interface For other frameworks the model file is also similar to this sample begin_nname and end_nname are the start and end op names of the sub graph to be replaced Naming the first op in the new sub graph as begin_nname and last op as end_nname is user s responsibility The sub graph is expected to be continuous Currently single input and single output is supported The method is expected to return a list of tensors These will be converted to NodeDef objects by the tool attr argument can be used to communicate between the json file and the model file This is useful when the same pattern occurs many times in the graph but with different attributes JSON file The json file should describe the sub graph to be replaced It is a list of dictionaries The keys of json file and descriptions are as follows Key Description begin Sub graph starting op name in the existing graph end Sub graph ending op name in the existing graph model Path to python model file attr Attributes dictionary Sample Command Line Implement ReplaceSubgraph using following commands When users need to export an ONNX model from PyTorch there is a known compatibility issue where the operator in PyTorch cannot be found in the target ONNX opset version For example the operator GridSample is not supported until ONNX opset version 1 6 but the user can export a specific lower opset version of the ONNX model by registering a custom operator The following Python script shows an example of registering the custom operator GridSample The eval_graph.py enables users to split build and run TensorFlow models on heterogeneous platforms such as TensorFlow host Vector Processor ADES BUB and Arm BUB It can be used for quick evaluation as well as production For quick evaluation non VP parts can be run in TensorFlow The tool handles DRA files at intermediate points as well as special situations such as in batch mode for example running classifier network on all ROIs For details refer to the Ambarella CV UG Eval Graph in the CNNGen toolchain package This tool will be deprecated soon as it is useless Ambarella will not maintain it anymore The following sections overview the documents located in the VP_Toolkit folder in the SDK package If these files are missing or additional information is required contact the Ambarella support team for assistance This document provides details on CNNGen which is a code generation tool for the CV CVflow Vector Processor VP hardware unit the key vision processing module for executing neural networks efficiently in Ambarella’s family of Computer Vision CV chips This document provides information on the Python application programming interface to Ambarella’s CNNGen library including the interface classes and their respective methods Documents in the CNNGen toolchain package that provide users guide information for each tool This document provides information on how to use different DRA strategies when converting the network This document provides the answers of some common questions for system software and network optimization For a detailed example refer to Section LSTM Warp-CTC This tool is used to convert Keras model to TensroFlow model as CNNGen cannot support Keras directly which is in cv2/tv2/release/frameworklibs/cv2.x.x.x.x.xxxx.ubu1804 x/site-packages/frameworklibs/tensorflow/keras/keras_converter.py To convert the PyTorch model to an ONNX model original torch framework itself has export function as However sometimes the export operator coverage may be insufficient for user s models so CNNGen tools included a package wrapper that provides custom symbolic function libraries to supplement those in the user s native torch.onnx installation This package was originally built to support the exporting of quantization torch models which is not available as of PyTorch-1.3.1 As such the package s interface follows version 1.3.1 When exporting a torch model to ONNX users can essentially use the torch2onnx package where original torch.onnx is used as this package also calls the user s local torch.onnx package s functions Except when it encounters a symbolic function not supported by the local torch.onnx it will use the corresponding custom symbolic function in torch2onnx s library if one is present This tool is in cv2/tv2/release/CnnUtils/cv2.qa.xxxx taking the example as shown below Quantized torch operators are not completely supported in torch2onnx please ask the Ambarella Support Team for assistance if problems occur Currently torch2onnx s custom symbolic function library only supplements exporting to ONNX opset 1 0 However even if the user exports a model that does not require any of the custom symbolic function libraries torch2onnx should still be backward and forward compatible with the user s local torch.onnx installation CVflow will get huge benefit from pruning so it is always suggested for user to do the pruning Sometimes users may ask that how much pruning ratio can get their target performance then below tools can performs simple pruning which is uniform pruning tools as each layer has the same pruning ratio for the performance test Why do users need CVflow Layer Profiler The profiler will generate a spreadsheet which contains detailed performance information about the neural network running on Ambarella s chip With this tool users can have a clearer understanding about the different bottlenecks in the current neural network structure as well as what optimizations they can do to improve the performance How do users use the CVflow Layer Profiler Parser Use parser TensorFlow parser Caffe parser ONNX parser to generate a CNNGen output folder for the network Vas Compilation Generate Profiling Files for Cavalry Users can find the input node’s name in json Oport s id of the primitives that have type input are the input nodes name These profiler tools are supported since CNNGen Tool 2.3.0 Run on EVK on Cavalry a Compile the CVflow profile tool Select test_vp_profile tool when building the SDK b Copy v2app_exe.tar network.bin and input_file.bin to the EVK via NFS or SDcard untar the v2app_exe.tar to v2app_exe folder c Specify p option to v2app_exe folder when running test_vp_profile d After success clock.txt performance.csv and vp_prof.bin will be generated for each DAG Copy back v2app_exe folder to host PC Run Profiler Demo CV5_TIMN Toolchain 2.5.2.1 CVflow Profiler take a example for yolov5s Parser Use parser yolov5s.onnx to generate a CNNGen output folder for the network Vas Compilation Steps 1 and 2 can be generated separately or can be generated in one step when quantifying the model using the cnngen tool chain Generate Profiling Files for Cavalry Copy v2app_exe.tar network.bin and input_file.bin to the EVK via NFS or SDcard untar the v2app_exe.tar to v2app_exe folder Specify p option to v2app_exe folder when running test_vp_profile When running layer_profiler on the host set b to the copied-back v2app_exe It will generate a spreadsheet after the operation is done Explanation and Q A of Terms Spreadsheet Explanation Spreadsheet Header Header Explanation Chip Chip version that generates the profiling data VP Clock VP clock of the bub that generates the profiling data Users can modify the VP clock to estimate performances at different VP clocks DRAM Clock Dynamic random access memory DRAM clock of the bub that generates the profiling data Users can modify the DRAM clock to estimate performances with different DRAM clocks Total Execution Time ms Total execution time of the network running on the board Total Load Time ms Total load time of loading the constant files dvi of the network from DRAM to vision memory VMEM Total HMB Bandwidth bytes Sum of the activation input and output size through DRAM of the whole network Overall coeff density Overall coefficient density of the whole network Split Summary Field Explanation Cycles Execution cycles of the split Cycles How much the percentage of execution cycles the split contributes to the whole network Execution time ms Execution time of the split Load time ms Time of loading the constant files of the split DAG Constant data Constant data such as weights files size and related cycles Activation DRAM input Activation input through DRAM size and related cycles Activation DRAM output Activation output through DRAM size and related cycles Hardware managed buffer HMB DAG Sum of activation DRAM cycles split execution cycles CVflow® manages these transfers so that the data is available in time before processing HMB transfers occur in parallel with processing This demonstrates total input output I/O cycles against the split DAG cycles In CVflow architecture DRAM loads and stores to the internal memory in parallel with the execution of data engines A ratio of HMB DAG below 1 0 0 indicates that DRAM I/O is not currently a bottleneck when running standalone Note however that the closer the ratio is to 1 0 0 the respective split DAG duration may be more susceptible to more load in the system At 1 0 0 the duration will certainly be affected by more load in the system Sum critical cycles DAG Sum of critical engine cycles split execution cycles This displays the efficiency of the split DAG Typically the total critical cycles should be very close to the DAG cycles However due to pipelines overhead this may not be the case Lower hmb/dag or higher sum(critical cycles) indicates higher efficiency of the split DAG Columns Column Explanation Origin layer Name of the layer in the original model Blank means generated by CNNGen Note that CVTools may have optimization where we fuse multiple layers into one Users can find the current and target density of the convolutional layers in the comment Primitive ID Number ID to identify different primitives Users can find data format and the current and target density of the convolutional layers in the comments Primitive type Type of the primitive Primitives are specified in CVflow specification Users can find the I/O and kernel dimension in the comments Critical engine cycles Sum of the bottleneck engine’s execution cycles of the primitive CVflow includes multiple data engines for processing The primitives are mapped into those data engines Total split DAG execution cycles represent data engines that use the most cycles This is called the critical data engine In most of the NN convolution processing is a critical engine If this column is 0 it indicates that the primitive is not mapped to the critical engine Further Ambarella placed the combined cycles of all data engines that the primitive is mapped to in the comments called hardware HW raw cycles Critical engine cycles split Sum of bottleneck engine’s operator cycles in the primitive Sum of bottleneck engine’s operator cycles in the split With the percentage in this column the user can understand which layers use most of the cycles in the split Optimizing pruning or quantizing those layers may result in potential performance gain Critical engine cycles model Sum of bottleneck engine’s operator cycles in the primitive Sum of bottleneck engine’s operator cycles in the whole model With the percentage in this column the user can know which layers use most of the cycles in the whole network Optimizing pruning or quantizing those layers may result in potential performance gain Macs The number of Mac counts found in the primitive Users can find the critical engine related Mac count in the comments Critical engine efficiency Efficiency of the primitive that goes into critical engine Note that the number could be higher than 1 0 0 due to the sparsity in activation data Extra Information Users can find each primitive s input constant and output data format in the comments of column B Users can find the current density and the target density of the primitive in the comments of column B only for those related to convolutional layers If the target density is 0 it means that users should prune the kernel as sparsely as possible If the target density is higher than the current density it means that pruning will not improve performance Users can find each primitive s input constant and output dimension in the comments of column C If the primitive id is blue it means either the input or weight of this primitive is larger than 8 bits Can be further optimized by quantization Primitive_that_can_be_further_optimized_by_quantization If the primitive id background color is pink it means this primitive is related to DRAM I/O Primitive_that_is_related_to_DRAM_IO If the primitive has no information it means that it has been folded with other primitives during compilation Primitives in red color indicate that they are in the bottleneck data engine of this split meaning those primitives are executed by the engine that has the longest cycles in the split Ambarella prefers that the bottleneck to be convolution In this case users can benefit from the power of pruning which will improve the performance of the NN If the primitive possesses macs but columns D through F are empty it indicates that the cycles for this primitive are under the sampling resolution of the tool Ambarella does not have the profiling data for it Because the cycles are too small to detect it will not be important to the total performance FAQ Total load time is the time for loading DAG binary to VMEM but it’s not included the time of loading input from DRAM to internal memory Is there a way to know the time from DRAM to internal memory The initial load depends on how much the buffer is allocated in partial buffer and prefetched by CVflow Typically this number is very small Users can get some idea on how small it is by looking at critical_cycles DAG This includes all the overheads including prefetching of partial buffers Why are there any Critical engine efficiency over 1 0 0 Ambarella s fast convolution engine does A i i B operation where A stands for activation data which is a variable based on input image W stands for pretrained coefficient of neural network B stands for pretrained bias value and i stands for each channel Each of Ambarella s CV chipsets has a maximum number of such operations that they can do Suppose Ambarella can perform hundreds of such operations per cycle The efficiency number shows how many operations were actually executed out of the ones that were scheduled As users know CVflow smartly skips multiplication of zero So if A is zero or W is zero that multiplication is skipped Because the weights are constants and are the results of training Ambarella recommends pruning of the neural network By this way it will produce lots zeros in weights Note that these savings for zeros are known already it is not included in efficiency However any zeros in activation data depends on input image and hence cannot be predetermined So for example if 1 0 samples of the activation data are zeros then Ambarella can execute 1 1 0 operations on the fast convolution data engine even though its capacity is 1 0 0 Hence users can see 1 1 0 efficiency Here is another example Say there are 1 0 0 0 weights and there are no other inefficiencies in execution of the data engine If there are any other inefficiencies it will show efficiency less than 1 0 0 For full dense no zero coefficients and activation having no zeros it will take 1 0 cycles and it will show 1 0 0 efficiency If the network is trained and there are only 2 0 2 0 0 non-zero weights and no zeros in activation then it will take 2 cycles and it will show 1 0 0 efficiency If the network pruned as above with only 2 0 non-zero weights and there 1 0 zeros in activations that matter where corresponding weight is not zero then it will take 1.9 cycles and it will show 1 0 5 efficiency Note above example is highly simplified Typically other architectures have high inefficiency due to data not being ready for execution In our architecture we avoid that with HMB however we could still have some inefficiencies due to neural network graph structure It is very rare to see benefits due to activation zeros as one cannot count of input image However customer can definitely take advantage of pruning We do graph analysis to show what target sparsity will give you good usage of data engine Improvement that Users can Make Pruning In the comment of origin layer users can find the layer s current density and target density for convolutional layers If current density is higher than target density pruning the respective layer towards its target density should improve the performance However if the convolution engine is not the bottleneck of a respective DAG split then pruning will not be likely to improve performance If current density is lower than target density it means pruning will not improve the performance of this layer To estimate how much improvement can be provided by pruning users can perform dumb pruning to the model and compare the performance with the fully dense model on Ambarella s silicon Dumb pruning means not using any pruning algorithm but just pruning each layer to a certain sparsity Dump pruning should only be used for performance estimation it should not be used in real case since it normally comes with bad accuracy Quantization If quite a few of the data size in the comment of primitive id are fp16 or even higher doing quantization to the model will have potential performance gain Network Restructure Some layer types are more efficient than others when mapped to hardware Looking at the execution cycles of the primitives marked red and activation size going through DRAM to and from the splits are good indicators of places where customers can modify their network in order to optimize the performance Network Output The end of a network may have data reordering operations like reshape or transpose which might be more beneficial to be handled on Arm® instead CVFlowBackend is an integrated library for deploying models onto Ambarella s CV-series SoCs The primary user interface of the library is through its Python APIs in sync with most modern frameworks for deep learning algorithm development usage of the library’s core features as a command-line tool is also available for simplified use Prepare Stage The refers to a collection of workflows and methods for constructing a MetaGraph There are three distinct workflows currently supported by CVFlowBackend which are summarized as following Workflows Workflow Required Inputs Description Prepare 1.Original model 2.JSON MetaGraph descriptor 3.DRA files The canonical workflow for compiling neural networks defined in popular machine learning frameworks The resulting MetaGraph should be homeomorphic to the original computationgraph with the exception of pre/post-processing operations that were introduced at compile time Compose 1.Pre built artifacts 2.JSON MetaGraph descriptor 3.DRA files optional Construct MetaGraphs with arbitrary topologies from pre-generated artifacts e.g Pre-compiled CNNGen outputs Convert 1 MetaGraph AmbaPB Convert MetaGraph AmbaPB Conversion of a MetaGraph to another type 1 Fast-checkpoint Checkpoint 2 Fast-checkpoint Frozen 3 Checkpoint Frozen Command-line tool for preparing a MetaGraph Evaluation Stage The cvflowbackend library also provides inference support for the MetaGraphs constructed during Prepare Stage Various modes are supported for precision or performance evaluation all of which can be used from the user’s host machine environment Evaluation Modes Mode MetaGraph Type(s) Description ades Checkpoint Bit-accurate VP/NVP/GVP emulator on host machine acinference Fast-Checkpoint Checkpoint AmbaCnn inference GPU/CPU on host machine Command-line tool for running MetaGraph inference Deploy Stage The CVFlowBackend library implements support for converting MetaGraphs into deployment-specific artifacts Users can choose from several deployment frameworks/modes which are designed to facilitate the optimized execution of neural network computations on Ambarella’s CVflow engines When deploying for EVK a cavalry binary is required and can be generated from a MetaGraph in the following manner Command-line tool for deconstructing a MetaGraph Quick start with CVFlowBackend please refer to 5.2 CVFlowBackend For details please refer to Ambarella CV TOOLS User Guide cvflowbackend.pdf This chapter provides information on the custom node in the Ambarella CNNGen For more details refer to the Ambarella CV UG Custom Nodes in the CNNGen toolchain package As deep learning technology continues to develop rapidly new algorithms are released each year If Caffe a deep learning framework developed by BAIR in 2 0 1 3 does not meet the user’s requirements users can modify its source code or create unique layer files to accomplish a specific function or algorithm Custom node is always used in Caffe as Caffe s layer unit is bigger than TensorFlow and ONNX There are so many different layers which are not included in official Caffe then Ambarella supplies this method to support such layers For TenforFlow and ONNX if there is an operator which is not supported It is suggested not to write custom node please ask the Ambarella support team for assistance or replace the operator with other operators Generally Ambarella CNNGen supports the standard version of Caffe BAIR However it is important to note that creating custom layers and converting them through CNNGen can lead to Caffe compilation errors as the standard version of Caffe will not recognize these layers To solve this issue CNNGen provides a serial of specific APIs that enable users to implement their own layers The layers defined or created by customers are treated as the custom node To implement a custom node with the same functionality of permute users should use the APIs mentioned above to write source codes After implementing the source codes for the custom node compile them to get a dynamic linked library example_code.so In addition users must create a Python script such as custom_nodes.py to act as an interface between CNNGen and the dynamic linked library The custom node can be only used to generate the VAS code In other words when caffeparser.py generates custom VAS codes the Python script and dynamic linked library must be passed to caffeparser.py When CNNGen converts a network that contains normal layers it converts the standard Caffe layers directly However for a custom layer CNNGen calls the custom_nodes.py to check if it has been implemented in the system If it has been implemented in the system CNNGen utilizes the corresponding dynamic linked library If it has not been implemented a compiling error is returned For further details about the dynamic linked library example_code.so and the Python script custom_nodes.py refer to Section Custom Node Example To use a custom node pass custom_nodes.py and example_code.so to the tool caffeparser.py Refer to Chapter CNNGen Toolkits for further details Custom Node Flow Custom Node Flow Custom Node Flow Custom Node Flow To enable the Ambarella CNNGen to support a custom layer users must create a dynamic-link shared library that implements the following functions for each custom node init() creates a node instance on the DLL side and returns node ID and output dimensions expand() calls primitive factory functions to construct a primitive sub-graph release() custom node release function that is called after expansion query() provides CNNGen with an array containing all the information about the custom node it supports The init() function creates an instance of the specific custom node on the DLL side It returns the DLL node ID and the output dimensions The DLL node ID is used by CNNGen when it calls expand() and release() for this custom node instance The output dimensions are used for allocating the output data space during simulation and configuring the subsequent nodes in the graph The function prototype is provided details below Parameter Description cid Pointer to DLL node ID returned by init() function osz Pointer to array of output vector dimensions returned by init() num_src Number of inputs isz Pointer to array of input vector dimensions attr Custom node attributes passed as a string df Pointer to array of output data formats The expand() function must construct a primitive sub-graph corresponding to the functionality of the custom node It is constructed by calling the CNNGen’s primitive factory functions Parameter Description cid DLL node ID returned by init() function funcs Primitive factory functions The release() function frees any memory allocated by the custom node instance It is called after the expansion Parameter Description cid DLL node ID returned by init() function Because a single DLL SO can contain more than one custom node every DLL SO has to provide a query() function for the nodes it contains The query() function provides CNNGen with an array containing the information of the custom node it supports Parameter Description num_types Number of entries available returned by the query() function types Pointer to the entry returned by the query() function After users compile the source codes of custom nodes they receive a dynamic linked library example_code.so that contains the APIs as shown below The following diagram displays the calling sequence of custom node APIs Custom Node APIs Calling Sequence Custom Node APIs Calling Sequence Custom Node APIs Calling Sequence Custom Node APIs Calling Sequence When CNNGen uses the custom node it first calls the query() function to register init() expand() and release() Then it calls the init() function to initialize the custom node Next it calls expand() to bring about the functionality of custom node Finally it uses release() to free any memory allocated by the custom node</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_cnngen_caffe_demos</field>
    <field name="url">d6/d57/fs_cnngen_caffe_demos.html</field>
    <field name="keywords"></field>
    <field name="text">Caffe Demos Caffe Demos This chapter describes the Caffe CNNGen demos Since CNNGen toolchain with Ubuntu2004 Caffe networks will not be maintained in future release also there will be no maintenance for AmbaCaffe since Ubuntu2004 Toolchain allows users to combine a series of pre-processing operations before running on the neural networks with variable input This tool gains performance benefits by connecting to VP and DSP directly reducing DRAM bandwidth A few extra steps are required to enable this feature The example below uses JSON for pre-processing and the CGPP library for deployment For details refer to the CNNGen tool guide Ambarella CV UG Json Preprocessing Format With this function the network input is converted to a luma and chroma image NV12 separately instead of a fixed RGB image This is because color conversion can be run as an operation in the pre-processing phase and the luma/chroma images can be obtained from IDSP in deployment For resizing there is a limit that a resized node can only support up to 2 x down sampling ratio in each dimension If users run the pyramid buffer in IDSP it can generate an input resolution with a different scale size of six layers so the 2 x ratio is acceptable for close resolution in the pyramid buffer Users can always set this to the 2 x real network input resolution then it can accept any real input as long as the size is smaller than 2 x real network input resolution With YOLOv3 as an example the input shape is 1 3 4 1 6 4 1 6 for luma and 1 2 2 0 8 2 0 8 for chroma Users can set one input luma image of the shape 1 1 8 3 2 8 3 2 and one input chroma image of the shape 1 2 4 1 6 4 1 6 Once users have the right input data the next step is preparing a JSON file for the pre-processing operations resize color space conversion There are two inputs required one luma image of shape 1 1 8 3 2 8 3 2 and one chroma image of shape 1 1 4 1 6 4 1 6 with fix-point 8 Use gen_image_list.py to generate a text file that contains a list of the image path for DRA Then the two variable resample operators followed that is used for resizing the Luma and Chroma to a fixed size that neural network requires such as 1 1 4 1 6 4 1 6 and 1 1 2 0 8 2 0 8 for YOLO v3 The last operator is a CSC node that performs a conversion from YUV to RGB The name needs to match the name of the network s input layer such as the example below Warp color space conversion The warp affine transformation requires four inputs The luma image of shape 1 1 7 2 0 1 2 8 0 The chroma image of shape 1 1 3 6 0 6 4 0 The luma warp table of shape 2 1 2 2 The chroma warp table of shape 2 1 2 2 The warp data is used for a transformation to affine a region in a 7 2 0 p image to a fixed size that YOLOv3 requires Each input has to be configured with a file path that points to the directory of the prepared data The data format of these warp tables are always set to which represents a signed fixed 1 6 bit point number with 4 bit fraction Two warp operators follow which warp the luma and chroma image to a desired size according to the warp tables The numbers in the fields and are obtained by the formulations ceil log⁡2 out_w and ceil log2⁡ out_h The last operator is a CSC node that performs a conversion from YUV to RGB The name needs to match the name of network s input layer seen in the example below This section explains how users can deploy the CGPP pre-processing For easier reproduction on the customer side the CNNGen samples package includes the example below In these examples the pre-process JSON files are automatically generated by the script and the input image is an 8 3 2 x832 image named dog.jpg in cvflow_cnngen_samples_&lt;version&gt; which will be converted to NV12 by script automatically as well Convert the YOLOv3 resize with the JSON pre-process Then yolov3_cgpp_resize.bin is generated in cvflow_cnngen_samples_&lt;version&gt; Convert the YOLOv3 warp with the JSON pre-process Then yolov3_cgpp_warp.bin is generated in cvflow_cnngen_samples_&lt;version&gt; In this conversion the input images are prepared beforehand To use a specific input image replace dog.jpg in cvflow_cnngen_samples_&lt;version&gt; with a new image The size should be 8 3 2 x 8 3 2 or less for the 2 x ratio limitation In this convert pre-processing json file is generated automatically and used by pp option in caffeparser.py To use specified json file users can modify it based on the json file of this example Run ADES below with the CNNGen samples package Run ADES of YOLOv3 resize Run ADES of YOLOv3 warp For this example user should pay attention to the different inputs between parser and ADES For the parser it only needs two inputs as luma and chroma but ADES needs two more inputs luma_resize_params.bin and chroma_resize_params.bin in cvflow_cnngen_samples_&lt;version&gt; which is generated by the parser such as below To build the EVK binary Users can use test_cgpp to run these two examples resize and warp This application will do variable resample to crop out the ROI area and resize the area to required input resolution by network then do network inference Run resize example The test_cgpp application could be run at resize mode which does variable resample to crop out the ROI area and resize the area to the network required input resolution and then do the network inference Running in warp mode The test_cgpp application could be run at warpAffine mode which does an affine transform from a specified pyramid layer to the target that the network requires Users can use test_cgpp_live to take advantage of CGPP library to handle CNNGen Pre-processing nodes with live stream Customers can take it as a sample code to write their own application It will query NV12 frame data from IDSP pyramid buffer Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use Run resize with CGPP Setup IAV and enable preview with imx274_mipi Please use pyramid_manual_map 0 x7F for CV5x and CV7x as they support seve layers Run resize with CGPP The inputs have to follow the order luma chroma luma resize parameters chroma resize parameters Streams live mode draw on stream without frame sync machine rtsp 0.0.2 VOUT live mode draw on VOUT HDMI The option in osd_server_yolov3 and in test_cgpp_live sepcify the ROI range defined as The resolution of source buffer is 1 9 2 0 x1080 and the ROI area(480,180,960,720) of source buffer is scaled to 4 1 6 x416 as the input of the network The resolution of framebuffer is 7 2 0 x480 in default which can be modified by option in eazyai_video.sh the ROI of framebuffer is 1 8 0 8 0 3 6 0 3 2 0 which needs to be a proportional correspondence with the ROI of source buffer The calculation process for the ROI of framebuffer is and the ROI of framebuffer will be 1 8 0 8 0 3 6 0 3 2 0 in this case Run in warpAffine with CGPP Setup IAV and enable preview with imx274_mipi Please use pyramid_manual_map 0 x7F for CV5x and CV7x as they support seven layers Run in warpAffine with CGPP The inputs have to follow the order luma chroma luma warp field chroma warp field Streams live mode draw on stream without frame sync machine rtsp 0.0.2 VOUT live mode draw on VOUT HDMI The resolution of source buffer is 8 3 2 x832 which is transformed to 4 1 6 x416 by as the input of the network This live demo is for Chinese car license plate detection and recognition The following sections will explain how to convert the models and run the live demo This demo includes three Caffe models which need to be converted and run on CVflow lpr.caffemodel is used for license plate detection It is a MobileNet SSD based model which can be downloaded from HorizonalFinemapping.caffemodel is used for license plate cropping It can be downloaded from SegmenationFree-Inception.caffemodel is used for license plate character recognition It could be downloaded from The Cavalry binary files can be generated with the CNNGen sample package For mobilenetv1_ssd please refer to 3 CNNGen Conversion In this demo the model used is different from mobilenetv1_ssd 3 CNNGen Conversion and ea_cvt_mobilenetv1_ssd.yaml needs to be changed before conversion In this demo if using the mobilenetv1_ssd compilation method it is necessary to change the model_path caffe_prototxt_path and out_shape in ea_cvt_mobilenetv1_ssd.yaml as follows For segfree_inception generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/segfree_inception/ For LPHM generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/LPHM/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples LPHM For EVK the cavalry binary chip _cavalry version _segfree_inception(LPHM) bin is in the cnngen output folder out/caffe/demo_networks/segfree_inception LPHM chip chip _cavalry_segfree_inception(LPHM) For X86 simulator model desc json file segfree_inception(LPHM) json is in the cnngen output folder out/caffe/demo_networks/segfree_inception LPHM chip LPHM _parser/ ades command segfree_inception(LPHM) _ades.cmd is in the cnngen output folder out/caffe/demo_networks/segfree_inception LPHM chip chip _ades_segfree_inception(LPHM) Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Choose SSD_LPR unit test in the SDK package to build the EVK binary For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Without Postprocess Accuracy Mode Not supported Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig For live mode as this network has not been supported in CFlite Python Inference library please refer to 4 Run C inference Place the generated binary models mobilenetv1_ssd_cavalry.bin lpr_priorbox_fp32.bin segfree_inception(LPHM) _cavalry.bin LPHM_cavalry.bin to the SD card which can be under in the following example In the manual feed mode the image could be hold in the memory until the algorithm finishes the related calculation and the application side could set this memory part free The item_num should be equal to the application side necessary buffer number plus 3 since DSP also need 3 buffers by default In this application the state buffer by default needs at most 3 buffers LPR need at most 1 and SSD need at most 1 so the total buffer number should be set as 8 This example uses the LT6911 module and CV22 board The result will be shown on the overlay of stream A with the license plate bounding box the license plate picture and the license plate text The overlay buffer size should be enlarged before running the LPR demo so that enough memory could be allocated to draw the license plates on overlay In the previous command line the test_mempart sets the overlay buffer size to 6 7 MB This size can be applied to a case where there are two streams with the resolution 1 0 8 0 p+480p If users need to show car plate detection result on more stream overlay or the stream resolution is very high such as 4 K continuously enlarge the size of this buffer if necessary Note that large overall size will impact the DSP s encode performance Users can adjust the license plate detection threshold using the options and Users can adjust the license plate recognition threshold by using the option or option can be used to adjust the number of car license plates that will be drawn on the overlay Users need to adjust this number while overlay is being displayed on small resolution stream or there is not enough space for drawing too much licenses can set the overlay x axis offset the default is zero The overlay can be on the left by default can set the new license highlight duration frames The default value is 3 0 which can mean one second can set the license to show duration frames The default is 3 0 which means 5 minutes on 1 0 8 0 p can set the overlay text background length ratio the default value is 1.0 This parameter adjusts the text box background length This ratio multiplies the default length For other options refer to the the application information The performance optimization includes the following two parts This application uses EazyAI library and enabled VP priority which improves 2 5 performance From SDK 3.0.6 the manual resume mechanism is added to this application Context For SSD thread there is only one vproc quick one network slow about 2 0 ms on CV22 For LPR thread there are 2 N vprocs quick plus 2 N networks quick N is license number VP tasks in the LPR thread has been set to the highest priority In the auto resume mode after the SSD network is interrupted by the LPR VP tasks the SSD network tries to occupy the VP as soon as the VP is in idle state As there are small intervals between the LPR VP tasks then the SSD network usually succeeds in regain the VP However these intervals are very short the SSD network is then quickly be interrupted again by the next LPR VP task Although the SSD network needs to quit the VP again it still has to finish the current DAG before quitting the VP Because the SSD networks is split into multiple DAGs and the NN can only quit VP in the interval of the DAGs After the SSD DAG quits the VP then the LPR VP task could enter into the VP This is a waste of performance because loading DAG could be time consuming and waiting for the SSD DAG could be more time consuming Now in the manual resume mechanism when the SSD network is interrupted it needs to be manually before regaining the VP resource In this case the interrupted SSD network needs to wait for the program call so that it is only able to run after the complete LPR VP task group is finished Below is the performance test result for the auto resume mode and manual resume mode Manual Resume Auto Resume SSD ms/loop LPR ms/loop VP Idle percentage SSD ms/loop LPR ms/loop VP Idle percentage 1 license 23.5 20.2 33.12 22.9 24.8 31.35 5 licenses 25.0 15.6 30.35 23.1 16.5 30.65 In the 1 license case the LPR time has been saved 24.8 20.2 4.6ms while it only causes minor performance drop for SSD 0.6ms In the 5 license case the LPR time has been saved 16.5 15.6 5 4.5ms which matches the case This manual resume mode could be controlled via the test_ssd_lpr command line parameter Abort the network if interrupted The default value for this option is 1 which means to use manual resume In 2 0 1 5 Liu et al proposed a new method for detecting objects called the SSD They released a Caffe implementation that contained several new layers such as permute and priorbox This section provides an example of implementing a custom node permute Then it describes how to develop a simple Caffe model containing a permute layer Finally it provides the steps for converting the Caffe model through CNNGen To complete a custom node users must create four files example_node.cc example_node.h export.cc and export.h Additionally users must create a Python script The following provides examples of implementing the four files as well as the Python script As introduced in 20.2 Special APIs users must implement three functions init() expand() and release() These functions should be completed in the example_node.cc file Note that users must construct a primitive sub-graph that corresponds to the functionality of the custom node in expand() For example permute rearranges the dimensions of a tensor so that they are in the order specified by users Ambarella CNNGen provides the following function Because the function of the transpose_factory() is the same as permute users can call this function inside expand() to enable permute The following code provides an example nd_permute.cc This is the head file corresponding to the example_node.cc file shown in the previous example nd_permute.h As introduced in 20.2.4 query() Function users must implement a query() function This function can be implemented in the export.cc file see the following code export.cc This is the head file that corresponds to the export.cc file shown in the previous section export.h To compile the source code listed above Ambarella provides a Makefile for implementation Makefile As mentioned in sub_sec_example_node_h users are expected to create a Python script custom_nodes.py which helps CNNGen use the custom node see the following code custom_nodes.py To test the functionality of the custom node users need to prepare a Caffe model that contains at least one permute layer Then CNNGen can be used to convert the model and verify the custom node Because the standard Caffe BVLC does not contain a permute layer users can choose to either A change the currently used Caffe to a custom Caffe or B add the custom layers to the standard Caffe Both methods are described in this section Change to Custom Caffe If users would like to change their version of Caffe to a custom Caffe released by Wei Liu it can be downloaded from After downloading users can compile it and set it as the default Caffe Add Custom Layers If users do not want to change their current version of Caffe they can add the permute layers and then re-compile Caffe Download the Caffe released by Wei Liu from This version is named Caffe-SSD After downloading users must copy the permute_layer.hpp file under Caffe-SSD_root to Next users must copy the permute_layer.cpp file and permute_layer.cu if they use GPU under Caffe-SSD root to Finally the permute layer needs to be registered in caffe.proto under Add optional PermuteParameter permute_param 2 0 2 to message LayerParameter around lines 3 2 6 to 4 2 4 Also add at approximately line 9 1 8 After finishing the procedures above Caffe must be recompiled using the script in CV toolchain package Because the CNNGen uses deploy.prototxt and caffemodel as inputs and then converts the corresponding networks users must create a train.prototxt containing permute to obtain the Caffe model for testing For example the following train.prototxt contains only an input layer and a permute layer train.prototxt In the example above because the content of deploy.prototxt file is the same as the content in train.prototxt only train.prototxt is required To train a network under Caffe framework provide a solver.prototxt file as shown below solver.prototxt The following example provides a Python script to get the caffemodel file train.py The following command is used to get the caffemodel file After executing the commands above users receive a file called test_permute_iter_1.caffemodel At this point in the workflow users should have three folders custom_nodes test_img and models The contents of the folders are as follows custom_nodes Makefile custon_nodes.py export.cc and export.h nd_permute.cc and nd_permute.h dra_img test.jpg models train.prototxt test_permute_iter_1.caffemodel Compile as follows The following command shows an example of using one jpg file in the dra_img folder to generate a default DRA file dra_bin_list.txt The test_img folder is the same as that introduced in 1 gen_image_list.py The following commands show how to convert the Caffe model to primitive and VAS codes the dynamic linked library custom_node.so and Python script custom_nodes.py are used The following commands show how to compile the VAS code described in sub_sec_custom_example_get_caffemodel_file The following commands are used to generate the ADES command and can be run on a PC The following commands are used to run the ADES command described above If users have successfully run the custom node permute then the following information appears For instructions on how to use AmbaCaffe to retrain Yolov3 with pruning and quantization refer to the documents under Ambarella CV UG AmbaCaffe This demo will be only valid in old CNNGen toolchain in Ubuntu 1 6 0 4 and Ubuntu1804 since Ubuntu2005 Caffe is not maintained any more The section below introduces how to deploy a retrained model with CNNGen tools For easy reproduction on the customer side the CNNGen samples package includes the example below Refer to the code below for the most important step in this deployment and in this deployment enable both DRA and JSON With this setting the data formats are determined by the JSON sideband file and DRA together This is because the old sideband file cannot be guaranteed to work with the latest CNNGen tool unless the new version has same primitive IDs as old version Unfortunately primitive IDs always change due to optimization steps or orders So during conversion with old sideband file and new CNNGen tool some primitive IDs in the sideband file will match the compiled graph and some will not Then the sideband file will only be applied to the primitives that have matches and the rest will be determined by DRA With this method the final result is slightly different from the quantization retraining result in PC The user can ignore these differences in the accuracy If accuracy problems are important to the user it is possible to only enable the JSON file as shown below The right CNNGen tool version is needed to generate the retrained JSON file but this cannot be the latest CNNGen version build caffeparser.py p cvflow_cnngen_samples_&lt;version&gt; 5_amb_quant.prototxt m cvflow_cnngen_samples_&lt;version&gt; 5_13800_amb_quant.caffemodel isrc sb cvflow_cnngen_samples_&lt;version&gt; json o yolo_v3_sp50_amb_quant of cvflow_cnngen_samples_&lt;version&gt; c coeff-force-fx8 act-force-fx8 no-pre-dr-ccr no-post-dr-ccr no-cf no-c2d no-lpe no-rcdf dra allow_scaling False odst odst odst With step 2 ensure that the final deployment command is identical to the previous command used to generate the JSON files For example when generating the files before retraining do not use and as the value is 0 and 1 but when deployed use and Although 0 and 1 signify nothing they will affect the primitive IDs and result in problems matching the JSON file and the new primitive code some CNNGen optimization tricks do network surgery to reduce the quantization errors for the retraining case For quantization cases disable these tricks Run below with the CNNGen samples package As shown in the command above the caffeparser uses a different prototxt with layer_compare The difference between these two prototxts is provided below The prototxt for the parser is an original FP32 prototxt that can work with any version of Caffe The prototxt for layer_compare is a quantization prototxt which only runs with AmbaCaffe because it contains the quantization layers Furthermore the quantization prototxt uses in the folder that the parser generates With this method when the final report is generated by layer_compare users can find the ADES result is identical to the result run by AmbaCaffe** with the quantization prototxt so the user will know the convert process is accurate Another possible problem arises when the quantization prototxt does not use the old in which is generated before retraining This happens because the user enabled the JSON file without the DRA The new generated by the parser has only slight differences from using the old There will only be a minor gap between final ADES result and AmbaCaffe If users need to ensure that the ADES result is identical to the retraining result perform the following Use JSON with no DRA to convert the retrained model Replace all of the new with the old For accuracy run the test below with the CNNGen samples package The mAP takes 0.5075 with 2 5 0 0 images in the COCO_2014 dataset The original YOLOv3 model accuracy is approximately 0.5873 To mitigate accuracy loss users can do more retraining on the pruning and quantization model Face detection and face alignment work in a complementary manner A CNN network locates the sub-image of each face in the entire picture for face detection Another CNN model locates 6 8 landmark points on a face for face alignment This section explains how to deploy the face alignment demo In this demo the MTCNN network is used to detect the faces a custom Caffe network is used to locate 6 8 landmark points on a face The process steps in the demo can be summarized as shown below Detect bounding boxes of faces and 5 landmark points on each face by running MTCNN Generate input images for the custom Caffe network by adjusting the position of each bounding box Generate 6 8 landmark points on each face by using the custom Caffe model The custom network is trained by the bounding boxes detected by dlib The position of the bounding box around each face is slightly different from the detection result of MTCNN Therefore in this demo the position of each bounding box detected by MTCNN is adjusted by moving the nose more central The original model which detects 6 8 landmark points can be downloaded from here The pre-trained model files VanFace.caffemodel and landmark_deploy.prototxt are used to generate Cavalry binary Generate the Cavalry binary as below The output is in out/caffe/demo_networks/face_landmarks_68/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _face_landmarks_68.bin is in the cnngen output folder out/caffe/demo_networks/face_landmarks_68/ chip chip _face_landmarks_68 For X86 simulator model desc json file face_landmarks_68.json is in the cnngen output folder out/caffe/demo_networks/face_landmarks_68/out_face_landmarks_68_parser/ ades command face_landmarks_68_ades.cmd is in the cnngen output folder out/caffe/demo_networks/face_landmarks_68/ chip chip _ades_face_landmarks_68 Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as follows Select if the file mode will be run For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Without Postprocess Accuracy Mode Not Supported Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig For live mode as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Copy Cavalry binary models to SD card on the board The pnet model name must be pnet* bin the rnet model name must be rnet* bin the onet model name must be onet* bin and the face_landmarks_68 model name must be face_landmarks_68_cavalry.bin for read by the test_face_align demo For example place files on the SD card with the following structure Initialize the environment on the CV board Take CV22 Walnut and imx274_mipi for example If there is no display on the stream or the display is not fluency please use bigger value in enc_dummy_latency 4 such as 7 Then if the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x01400000 Run the demonstration Stream live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI File mode the input of JPEG image files are placed under result of JPEG image files are generated under Face detection and face recognition FDFR function works in a complementary manner A CNN model locates the sub-image of each face in the entire picture for face detection Another CNN model converts the image of each face into a feature vector for face recognition This section explains how to deploy the face recognition demo In this demo the MTCNN network or the RetinaFace network is used to detect the faces the MobileFaceNets network is used to generate feature vectors The steps performed in the demo is as below Detect five landmark points on each face by running MTCNN or RetinaFace Generate feature vectors of each face by running MobileFaceNets Recognize each detected face by calculating cosine similarity between the generated feature vectors and the recorded feature vectors with label in the database For MTCNN model please refer to MTCNN For RetinaFace model please refer to RetinaFace For MobileFaceNets the original model can be downloaded from https://github.com/zhanglaplace/MobileFaceNet The pre-trained model files face_snapshot/MobileFaceNet.caffemodel and MobileFaceNet_deploy.prototxt are used to generate the Cavalry binary For MTCNN refer to MTCNN For MobileFaceNets generate the Cavalry binary as below The output is in out/caffe/demo_networks/mobilefacenets/cavalry_mobilefacenets Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _mobilefacenets.bin is in the cnngen output folder out/caffe/demo_networks/mobilefacenets/ chip chip _mobilefacenets For X86 simulator model desc json file mobilefacenets.json is in the cnngen output folder out/caffe/demo_networks/mobilefacenets/out_mobilefacenets_parser/ ades command mobilefacenets_ades.cmd is in the cnngen output folder out/caffe/demo_networks/mobilefacenets/ chip chip _ades_mobilefacenets Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool There are two versions of FDFR one is MTCNN and MobileFaceNets and another is RetinaFace and MobileFaceNets Build face recognition v1 EVK binary Build face recognition v2 EVK binary Select if the file mode will be run For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Without Postprocess Accuracy Mode Not Supported Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig For live mode as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference With V1 and 6 Run C Inference With V2 Copy Cavalry bin models to board The PNet model name must be pnet* bin the RNet model name must be rnet* bin the ONet model name must be onet* bin the MobileFaceNets model name must be mobilefacenets_cavalry.bin for read by the test_fdfr_v1 demo For example place files on the SD card with the following structure Initialize the environment on the CV board Take CV22 Walnut and imx274_mipi for example If there is no display on the stream or the display is not fluency please use bigger value in enc_dummy_latency 4 such as 7 Then if the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x01400000 Run the demo Stream live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI File mode the input of JPEG image files are placed under result of JPEG image files are generated under Copy Cavalry bin models to board The RetinaFace model name must be onnx_retinaface_cavalry.bin the MobileFaceNets model name must be mobilefacenets_cavalry.bin for read by the test_fdfr_v2 demo For example place files on the SD card with the following structure Initialize environment on the CV board Take CV22 Walnut and imx274_mipi for example If there is no display on the stream or the display is not fluency please use bigger value in enc_dummy_latency 4 such as 7 Then if the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x01400000 Run the demo Stream live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI File mode the input of JPEG image files are placed under result of JPEG image files are generated under When the demo is in live mode press the f button on the keyboard to begin the procedure Follow the prompt on the terminal to add one item of face data into the database The database file is saved in the same folder with the Cavalry model files for example The primary steps to record an item of face data are as below Press f on the keyboard to start the procedure Enter 2 and then press Enter to choose Enter a name and then press Enter If the name already exists in the database reenter another name After entering the name the recording procedure begins On the HDMI VOUT the detected faces are drawn in boxes Ensure that only one face is detected during the recording procedure Record up to 9 views Take the straight direction first then move your face slightly from the straight direction at 8 different angles left right up down up-left up-right down-left down-right pressing r one by one to record each view After recording 9 views the recording procedure exits Users can press f to begin again or press f then choose to check the database On the HDMI VOUT or stream if a face is recognized the bounding box is drawn in green if a face is not recognized the bounding box is drawn in yellow The name and score of a recognized individual is shown on the terminal for example “ Someone 0.743951 ” The number of faces drawn on the screen is configured by the e max_faces option In this example face recognition demo does detection and recognition on pyramid buffers Run the following commands on the board If there is no display on the stream or the display is not fluency please use bigger value in enc_dummy_latency 4 such as 7 Then if the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x01400000 Run live mode draw on stream rtsp 0.0.2 The similarity transform in the source code assumes the shape of the face on the source image is normal There is no width height ratio correction based on 5 landmark points Therefore when performing face recording and recognition it s better to maintain the same width height ratio on the source images The similarity transform in the source code may generate noise when downscaling from a large size So in this demo the bigger face may not be better in the result The option k 0 1 0 specifies the IAV pyramid configuration in the following order channel ID of the VIN pyramid layer index for detection and pyramid layer index for recognition This section uses GoogLeNet as an example with cavalry_gen and ADES It uses FX8 for the data format Refer to 3 Environment Setting for more information on how to configure the environments Dynamic range analysis DRA is a quantization scheme in AmbaCNN which is used to generate the gen_image_list.py file in the default configuration The following section describes how to modify the quantization The following command shows an example of using four jpg files in the dra_img folder to generate a default DRA file dra_bin_list.txt In an actual scenario more than four images are required chooses random pictures that are in the defined folder If the folder has more pictures than every result will be different If the user removes it will use all of the images with the extension in this folder is used to disable the random selection setting The final DRA binaries are in the folder and needs to be resized to refers to BGR means unsigned fixed8 which also specifies that the option below should be defined as Use the caffeparser.py tool to convert it to primitive and VAS code is generated by gen_image_list.py is the output folder if the user used must be defined for the input data format Typically for fix8 BGR is acceptable if the user includes the meaning and scale in the network For data format detail please refer to 4 Data Format refers to the meaning value refers to the scale ratio indicates that the output is 8 bit indicates that the weight is 8 bit and if it is not used the system determines the value o prob fp32 indicates that the output should convert to float-32 By default the parser tool generates the sideband file which enables users to determine and manually modify the quantization of each stage Use the following code Compile the VAS code as shown below Use ades_autogen.py to generate the ADES command to run on the PC First Command refers to the input files mapped to the input layer names For example For this input the test binary can be found in dra_image_bin in 1 gen_image_list.py or gen_image_list.py to generate new images Ensure that the data format is correct with the definition in RGB or BGR and the resolution is the same as defined in the prototxt Second Command ades generates prob.bin in ades_bvlc_googlenet which can be compared to the output in 7 test_nnctrl cavalry_gen compiles the CV code and generates an output that can run on CV Save to file bvlc_googlenet_cavalry.bin with file size 7 0 7 6 3 3 6 refers to the directory path for VAS outputs Similar to out_bvlc_googlenet_parser/vas_output it is generated in 3 VAS Compile is for the output file used to deploy on CV for example bvlc_googlenet_cavalry.bin prints a debug message Use the test binary in the folder dra_image_bin in 1 gen_image_list.py or gen_image_list.py to generate new images Ensure that the data format is in alignment with the definition in RGB or BGR and the resolution is the same as defined in prototxt Flash the binary to the EVK board and then run the following commands For more information about and refer to 5 cavalry_gen The definition is the name of first layer input and last layer output Users can generate a new prob.bin and compare it to the prob.bin in 4 ades_autogen.py Using the same input the results will be identical except for minor differences that occur as a result of the 3 2 bytes alignment This chapter explains how to port and run the Warp-CTC LSTM net The network takes in two inputs one CAPTCHA image and one continuation indicator The indicator resets the LSTM state because running LSTM must include looping over a time factor Loops cannot run on VP so the given prototxt should be split into LSTM and non-LSTM parts This allows the LSTM prototxt to run through the CV tools and be converted into DAGs where the loop control is realized in the Arm part To download the Warp-CTC Caffe use the following commands Modify the file caffe/src/caffe/layers/ctc_loss_layer.cpp to avoid a build error accumulate is not a member of std User could use the build script in to build the warpctc Caffe This file is included in the CNNGen Toolchain package The following command is an example for building the Caffe in Ubuntu18.04 CPU environment There will be a caffe-install-cpu folder generated under the current path Next source the CNNGen toolchain environment file and then export the Caffe environment Refer to 3 Environment Setting Replace the custom layers in deploy.prototxt with layers in Stock Caffe Because the layer is not supported in the Stock Caffe it can be replaced by an layer This step generates a new prototxt called Split the deploy_modified.prototxt into different prototxts using graph_surgery This network is structured as A- LSTM1- LSTM2- B The user needs to read the prototxt and decide the split points Split points correspond to layer names in the graph Remove or comment the indicator layer in deploy_modified_split1.prototxt In deploy_modified_split2.prototxt and deploy_modified_split3.prototxt change the plane number of and from 8 0 to 1 Use the following command to convert and compile The LSTM layers require two inputs the output from the previous layer and the continuation indicator input LSTM parsing needs DRA files for the c0 and h0 inputs which correspond to the cell state c0 and hidden state h0 The make-workflow uses the parser option to generate the c0 and h0 middle results for the LSTM layer As a result the parser needs to run twice The first time will generate the c0 and h0 middle inputs for DRA and the second time will take the DRA from the first execution and parses the next The CNNGen forward pass results are generated as a binary file in the folder The h0 and c0 for the next steps will be generated as lstm-layername _000000_unquant.bin and lstm-layername _c_000000_unquant.bin The middle binaries are in FP32 format so a data format conversion may be necessary depending on the next layer s input data format If necessary it is possible to hack the custom Caffe code to dump the middle results from the PC Caffe results instead of using the parser option To quickly run LSTM on the board install the application before compiling the SDK The application helps run the LSTM in the file mode Options for test_lstm are provided below f option This option tells the app that the port data is not coming from the local binary file but from another port It can connect to different networks by memory if it is specified after each input port Example t option This option specifies the network type If t is specified the current network or the current cavalry.bin should be regarded as a LSTM layer l option This option is placed at the end of the current network to set the loop number Example When each port is connected by memory the LSTM layer input port name with is the cell state whereas is the hidden state The output port with is the cell state output which will feed the next cell state input The other output without is the output of the hidden state which will feed to next hidden state input As descriptions in LSTM Warp-CTC loops cannot run on VP while running LSTM net With old covert flow the user needs to split LSTM model into different parts and realize the loop control in the Arm part It s hard for user to prepare DRA files for every stage and deploy on chip so this chapter provides a new approach to compile LSTM model in one shot For chineseocr lite which is in onnx/test_networks/chineseocr_lite/ the conversion process is similar to the network in this section The only difference is that the warpctc_one_shot requires an indicator DRA file as an input of some stages and chineseocr lite does not need So users can refer to this section for Onnx demo chineseocr lite conversion To prepare the environment please refer to 1 Download Warp-CTC Caffe The original LSTM model should be compiled with the Ambarella CNNGen ToolKit and then the compiled files can be deployed on board The necessary files are listed as shown below Pretrained model should be frozen before being fed into the CNNGen ToolKit In this example a Caffe framework pretrained LSTM model is used Among them deploy_modified.prototxt is generated by the following command The CNNGen ToolKit needs one or more binary files to do the dynamic range analysis DRA for each input node There are two input nodes in this LSTM model “dra_image_bin” and “indicator” Take input node “dra_image_bin” for example there is a text file “dra_bin_list.txt” created to list all DRA files The file path in “dra_bin_list.txt” must be listed as an absolute filepath To generate “dra_image_bin” files using the following command “indicator.bin” is the DRA file for “indicator” node to generate it using the following command To compile the LSTM model in one shot the CNNGen ToolKit should read in one model graph description file to check the compilation options This description file is in JSON format It provides information on CNNGen DRA configuration and parser options for IO nodes Configuration information is like data shape data format using FX8 FX16 or FP16 The original LSTM model will be split into four blocks in one shot after CNNGen ToolKit compilation User can specify different DRA configurations and parser configurations for each split block in graph description file Adjacent split blocks which share the same CNNGen configurations can be merged together into this description file For LSTM model there are split1 split2 split3 and split4 blocks after CNNGen compilation split1’s CNNGen configurations are different from those of split2-split4 while split2-split4 which share the same CNNGen configurations can be configured together in one description block Therefore this model graph description file includes two parts and would be shown as below after the substitution of configuration options lstm_graph.json Make menuconfig for warpctc one shot For chineseocr lite the user can refer to the following steps Generate the Cavalry binary for EVK Then is generated in out/caffe/demo_networks/warpctc_one_shot/ cavalry_warpctc_one_shot Next the user should copy this binary file into the sdcard of the EVK board to generate the final result as the steps in 4 Run LSTM on Board Generate the simulator result and calculate the MD5 value Then is generated as the simulator result in out/caffe/demo_networks/ warpctc_one_shot/ades_warpctc_one_shot The MD5 value of this result should be exactly the same as the MD5 value of the EVK result The step2 and step3 above have integrated everything which is mentioned in 2.2 DRA Files and 2.3 Model Graph Description File However this example does not cover all the cases the user must modify the scripts if needed All output files are in cv2x_cnngen_samples/out/caffe/ demo_networks/warpctc_one_shot/ The main structure of the output files is as follows LSTM networks are not ready on CV72 please try the other chips first Run the Cavalry network binary on board by test_nnctrl Parse the result by warpctc_captcha_parse_result Calculate the MD5 value of the result by md5sum On CV22_Walnut the performance of Caffe LSTM is Onnx LSTM is with DRA strategy density 1 0 0 Refer to the following script to get PC Caffe result For this step the user can find the script gen_warpctc_caffe_result.py in cv2x_cnngen_samples/caffe/ demo_networks/warpctc/pc_caffe_script The file paths in and are the absolute paths of the test image and indicator.bin respectively Or the user can run the script run_pc.sh in cv2x_cnngen_samples/caffe/demo_networks/warpctc/warpctc_one_shot/pc_caffe_script/ directly a bin file will be generated as a result Compare the PC Caffe result and EVK result Then the result will be given as below Datasize 8 8 0 the cosine similarity is 0.999724 norm1 246.738113 norm2 245.874847 euclidean_distance/norm1 0.023730 euclidean_distance/norm2 0.023814 Compare the simulator result ADES and the EVK result The MD5 values of the ADES and EVK results should be exactly the same This version of YOLO is a public release from dog-qiuqiu on https://github.com/dog-qiuqiu as below https://github.com/dog-qiuqiu/Yolo-Fastest Yolo-Fastest is based on Tiny Yolov3 and using EfficientNet-lite as backbone there are two models https://github.com/dog-qiuqiu/MobileNet-Yolo Mobilenetv2-yolov3 is based on Tiny Yolov3 and using mobilenetv2 as backbone there are two models The detail mAP is as below table Network VOC mAP 0.5 COCO mAP 0.5 Resolution Weight size CV22 EVK Inference time with Default DRA ms MobileNetV2-YOLOv3-Lite 73.26 37.44 3 2 0 8.0MB 7.3 MobileNetV2-YOLOv3-Nano 65.27 30.13 3 2 0 3.0MB 1.9 Yolo-Fastest 61.02 23.65 3 2 0 1.3MB 1.5 Yolo-Fastest-XL 69.43 32.45 3 2 0 3.5MB 3.1 Below used VOC models as an example if to use COCO model the step are the same If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The Caffe model is exported from Darknet with the tool in caffe/demo_networks/light_yolo/script/darknet2caffe which is based on https://github.com/dog-qiuqiu/MobileNet-Yolo/tree/master/darknet2caffe with yolo license Please refer in above folder for how to convert The Cavalry binary files can be generated with the CNNGen sample package For yolov3_fastest generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolov3_fastest/ For yolov3_fastest_xl generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolov3_fastest_xl/ For yolov3_mnetv2_nano generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolov3_mnetv2_nano/ For yolov3_mnetv2_lite generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolov3_mnetv2_lite/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _yolov3_(*) bin is in the cnngen output folder out/caffe/demo_networks/yolov3_ chip chip _yolov3_(*) For X86 simulator model desc json file yolov3_(*) json is in the cnngen output folder out/caffe/demo_networks/yolov3_ _parser/ ades command yolov3_(*) _ades.cmd is in the cnngen output folder out/caffe/demo_networks/yolov3_ chip chip _ades_yolov3_(*) Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as shown below Build X86 simulator binary with make Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Accuracy Mode Not Supported Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK board Refer to 2 CNNGen Conversion for how to generate _cavalry.bin The yolov3_* lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate json and _ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure live_tiny_anchors.txt has the following content inside 2 6 4 8 6 7 8 4 7 2 1 7 5 1 8 9 1 2 6 1 3 7 2 3 6 2 6 5 2 5 9 Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users must keep the file path consistent during use File mode For X86 simulator Run ADES mode The raw.bin is used as an input without the preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano The image is used as an input with the correct preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano Run Acinference mode The raw.bin is used as input without the preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano The image is used as an input with the correct preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano For EVK board Load Cavalry For CV7x cavalry has been already boot up users do not need to run this command Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode to test CVflow performance yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano The real image is used as an input with the correct preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano The raw.bin is used as an input without the correct preprocess and postprocess yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano For the file mode with image as input place the test image such as cvflow_cnngen_samples/caffe/demo_networks/light_yolo/dra_img/dog.jpg in and create as the output directory For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Run the following Streams live mode draw on stream without frame sync machine rtsp 0.0.2 yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano For stream live mode option fsync_off disables frame sync If need to enable frame sync users should enable encode dummy in eazyai_video.sh Video output VOUT live mode draw on VOUT high definition multimedia interface HDMI® yolov3_fastest yolov3_fastest_xl yolov3_mnetv2_lite yolov3_mnetv2_nano The pre-trained MTCNN model files can be downloaded from here The pre-trained model files include the following files PNet det1.caffemodel det1.prototxt RNet det2.caffemodel det2.prototxt ONet det3.caffemodel det3.prototxt Because the default dimension order in Caffe N C Height Width differs from the dimension order of each input data layer in the downloaded model 0 3 width height transpose pre-processing for input is added in caffeparser.py Since the PNet model does not include a fully connected layer it can be used with different input resolutions However because the VP only supports a fixed input size users cannot use one Cavalry binary for various input resolutions Therefore users should generate several Cavalry binaries of PNet with different input sizes Use the following steps to calculate the input sizes in the image pyramid of PNet Determine the size of the original FoV and the minimum squared face size on the FoV For example an FoV with resolution 1 9 0 2 x1080 is used the minimum squared face size is 6 0 x60 Calculate sizes in the pyramid Use mtcnn_get_pnet_scales in 3 CFlite Python Tools to calculate sizes in the pyramid The factor option is used to configure the scale-down factor between two neighbor sizes in the pyramid Typically it is 0.709 The factor takes effect only from the second layer The first layer’s resolution is not determined by the factor option it is determined by the min option and the sliding windown size 1 2 of PNet For further details refer to the source code in mtcnn_get_pnet_scales.py Generate a PNet Cavalry binary for each size in the above calculated pyramid sizes Using 2 1 6 3 8 4 as an example modify the input shape in cnngen/caffe/demo_networks/mtcnn/pnet/models/det1.protxt Generate the Cavalry binary The output is in out/caffe/demo_networks/pnet/ The default configuration uses default parser option for the balance between accuracy and performance As users seeing the shapes are different between prototxt and the cfg_yaml file ea_cvt_pnet.yaml In prototxt it is 3 8 4 2 1 6 but in ea_cvt_pnet.yaml it should be 2 1 6 3 8 4 which will be used by parser The reason is that the network was trained in Matlab The input of the caffe model is an image which is transposed between width and height So for parser the input image is 2 1 6 3 8 4 which is defined in ea_cvt_pnet.yaml it will use tranpose to transpose to the real network input 3 8 4 2 1 6 which is defined in the prototxt file CNNGen sample has included a full flow to convert all the different input sizes of PNet The following Cavalry binary files are output under the following folder out/caffe/demo_networks/pnet_multi/cavalry Generate the RNet Cavalry binary The output is in out/caffe/demo_networks/rnet/ Generate the ONET Cavalry binary The output is in out/caffe/demo_networks/onet/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples pnet/onet For EVK the cavalry binary chip _cavalry version _rnet pnet/onet bin is in the cnngen output folder out/caffe/demo_networks/rnet pnet/onet chip chip _rnet pnet/onet For X86 simulator model desc json file rnet pnet/onet json is in the cnngen output folder out/caffe/demo_networks/rnet pnet/onet pnet/onet _parser/ ades command rnet pnet/onet _ades.cmd is in the cnngen output folder out/caffe/demo_networks/rnet pnet/onet chip chip _ades_rnet pnet/onet Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as below Select if the file mode will run For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Without Postprocess Accuracy Mode Not supported onet can be replaced with pnet or rnet Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig For live mode as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Copy files to SD card on the CV board The demo application reads the Cavalry binary files of PNet RNet and ONet from a single folder specified in the parameters The folder includes several PNet Cavalry binary files one RNet file and one ONet Cavalry file The demo application identifies the PNet binary files using the key words pnet and bin identifies the RNet binary file by the key words rnet and bin and identifies the ONet binary file by the key words onet and bin So the PNet model name must be pnet* bin the rnet model name must be rnet* bin and the ONet model name must be onet* bin for reading by test_mtcnn demo For example place files on the SD card with the following structure Because the demo application does not identify the input resolution from the names of the PNet Cavalry binaries users do not need to specify the size in the file name The demo application can load a maximum of 1 6 PNet binary files in the folder by default If more than 1 6 PNet Cavalry binary files are required change MAX_PNET_NUM as defined in SDK h If detection of faraway faces is not required users can remove the larger-sized PNet binary files from the folder For example if pnet_288x384_cavalry.bin and pnet_205x273_cavalry.bin are removed then some small faces will no longer be detected If detection of large faces is not required users can remove the smaller-sized PNet binary files from the folder For example if pnet_14x18_cavalry.bin and pnet_19x25_cavalry.bin are removed some large faces will no longer be detected Initialize the environment on the CV board Take CV22 Walnut and imx274_mipi for example If there is no display on the stream or the display is not fluency please use bigger value in enc_dummy_latency 4 such as 7 Then if the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x01400000 Run the demo Live mode draw on stream rtsp 0.0.2 Live mode draw on VOUT HDMI File mode the input of JPEG image files are placed under the result of JPEG image files are generated under Runtime rotation flip helps the camera adapt to variance deployment posture The following section provides an example of performing the mobilenet 1 8 0 degree rotation Create a json preprocessing file On one side the CNNGen network input name changes because of the definition of the value in the field On the other side the value in the last field should be the same as the original input layer name in the prototxt/pb file Use the json pre-processing file in the parser option Do not use the option option option option simultaneously with the option In future releases Ambarella will consider using the json file to unify the pre-processing workflow Use rotate-flip flag in The rotation-flip value follows bitmap order Plan_flip Depth_flip Vertical_flip Horizontal_flip Rotate_clockwise_90 A 1 8 0 degree rotation could be divided into a vertical flip 0 x04 and a horizontal flip 0 x02 resulting in a 1 8 0 degree rotation of 0 x06 Similarly a 2 7 0 degree rotation would result in 0 x07 Note that because rotate-flip is a port character in this input rotation case the parameter should follow the port instead of the port Check the result Use the tool binrotate.py to rotate an original RGB file to 1 8 0 degree Copy the origin and the rotated binary input file to board Run the following Use md5sum or other tools to check the output binaries The org_no_flag.bin is the same as the rot_with_flag.bin while rot_no_flag.bin is different Also note that if the input image is rotated clockwise 9 0 degrees then test_nnctrl should use 0 x07 to rotate the input anti-clockwise 9 0 degrees instead of using 0 x01 With the development of deep neural networks DNN some networks use a two-step structure to perform object detection which reduces computational costs and improves the detection accuracy for most practical usages Due to the efficiency networks such as the Fast-RCNN 1 and Faster-RCNN 2 are becoming popular in practical scenarios This section uses PVANet 3 as an example for deploying multiple-step structure networks on to the Ambarella CV platform using the toolchain provided by Ambarella The following section describes the PVANet network structure and the tools required for conversion explains procedures for performing individual network tasks and then provides the performance of the PVANet on the Ambarella CV platform Some references 1 2 3 4 and 5 are as below Ross Girshick Fast R-CNN Proceedings of the International Conference on Computer Vision ICCV 2 0 1 5 Shaoqing Ren Kaiming He Ross Girshick and Jian Sun Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks Advances in Neural Information Processing Systems 2 0 1 5 Sanghoon Hong Byungseok Roh Kye-Hyeon Kim Yeongjae Cheon and Minje Park PVANet Lightweight Deep Neural 4 Networks for Real-time Object Detection arXiv preprint arXiv 1611.08588 2 0 1 6 Sean Bell C.Lawrence Zitnick Kavita Bala and Ross Girshick Inside-outside net Detection objects in context with skip pooling and recurrent neural networks Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition CVPR 2 0 1 6 Tao Kong Anbang Yao Yurong Chen and Fuchun Sun HyperNet Towards accurate region proposal generation and joint object detection Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition CVPR 2 0 1 6 Download PVANet and its fork Caffe from the GitHub repository Download the PVANet detection model for VOC2012 Use the Google Drive link provided below to download the full Caffe model for VOC2012 https://drive.google.com/open?id=0Bw_6VpHzQoMVa3M0Zm5zNnEtQUE Use the Google Drive link provided below to download the compressed version https://drive.google.com/open?id=0Bw_6VpHzQoMVZU1BdEJDZG5MVXM Follow the instructions on the PVANet GitHub page to setup Caffe or use AmbaCaffe Basically the PVANet is a fork that comes from Faster-RCNN 2 which utilizes a two-step structure to perform object detection It follows the same method as Faster-RCNN 2 with some modifications specialized for object detection 3 The entire structure of PVANet detection network is shown in the figure below The Structure of PVANet Detection Network The Structure of PVANet Detection Network The Structure of PVANet Detection Network The Structure of PVANet Detection Network The structure shows how the PVANet uses a convolution neural network CNN to extract features Similar to the ION 4 and HyperNet 5 PVANet takes advantage of the structure of Hyper-feature concatenation that combines the features from the last layer and two intermediate layers Next the region proposal network RPN takes first 1 2 8 channels from ‘convf’ as an input to reduce the computational costs without decreasing accuracy The RPN has two predication layers for 8 4 scores 2 x 4 2 anchors and 1 6 8 bounding box regressions 4 x 4 2 anchors following a 3 8 4 x3x3 convolutional layer All 5 1 2 channels from ‘convf’ are used for the classification network Therefore a 5 1 2 x6x6 tensor is generated from each ROI pooling Afterward the tensors are fed into two fully-connected layers to generated 2 1 scores and 4 x 2 1 predicated values of 2 1 bounding box For 2 0 class object detection RCNN produces 2 1 predicated scores 2 0 classes 1 background Because of the constraints of the Ambarella CVflow Vector Processor some logistical operations such as ROI pooling and non-maximum suppression NMS are not performed in the current toolchain instead they must be performed on the Arm side As a result manual network splitting is required before feeding into tools for auto-conversion Based upon this analysis of the PVANet detection network structure Ambarella splits it into four parts 1 FEN feature extraction network and RPN 2 ROI pooling 3 fully-connected classifier and 4 post-processing This proposed splitting is shown in the figure below The Split Structure of PVANet Detection Network The Split Structure of PVANet Detection Network The Split Structure of PVANet Detection Network The Split Structure of PVANet Detection Network Run the FEN and RPN on the CVflow Vector Processor using the following steps Modify the original prototxt file such as faster_rcnn_train_test_21cls.pt Remove the layers that come after proposal layers such as the ROI pooling and FC layers Remove layers that can only be used in training phase such as SoftmaxWithLoss layer Replace the Dummy Python data layer with a simple data layer that only provides the data dimensions To utilize NEON technology to boost ROI pooling stage it requires a reshape layer and a permute layer to convert the shape of the output feature map from 5 1 2 xh/16xw/16 to 3 2 xh/16xw/16x16 For the prototxt file that was modified for FEN and RPN refer to the file name faster_rcnn_train_test_21cls_rpn_mod.pt The original PVANet utilizes OpenCV to take the input image as a BGR order For easier integration on the Arm side the output “rpn_bbox_pred” and “rpn_cls_prob_reshape” of the FEN and RPN model use the 1 6 bit data format in floating point while the output “concat_convf_permute” still takes the 1 6 bit data format in fixed point The ROI pooling block is performed on the Arm side which implements the “proposal” Python layer along with the “roi_pool_conv5” ROI pooling layer in the original Caffe model It takes the output of FEN and RPN as its input and then outputs the final results of the ROI pooling to the FC classifier This module leverages the multithreading technique in order to achieve a better performance The following figure depicts the ROI pooling workflow The green blocks represent the data buffers and the blue blocks represent the processing functions Flowchart for ROI Pooling Flowchart for ROI Pooling Flowchart for ROI Pooling Flowchart for ROI Pooling As illustrated in the flowchart above resize_anchor uses the original 4 2 anchors and generates new anchors at each feature map point bbox_trans_inv uses these generated anchors and the RPN-generated bbox deltas adjusts each bbox and filters out the bboxes that are not large enough pre_nms sorts the proposal bboxes according to the foreground probability supplied by RPN for each box and then keeps only a small portion for the later NMS nms calculates the IOU over the remaining bboxes and keeps only one bbox among several bboxes that have an IOU greater than the predefined threshold ROI pooling performs pooling using the ROI ouput of NMS and the feature map of FEN It generates pooled feature maps for the FC classifier to obtain more accurate bbox coordinates and scores over each object class Run the FC classifier on the CVflow Vector Processor by following these steps Modify the original prototxt file such as faster_rcnn_train_test_21cls.pt Keep only the layers that start from the fully-connected layers and remove the rest Add the data input layer with the data dimensions Add a permute layer and a reshape layer to revert the ROI tensors from 3 2 x6x6x16 to 5 1 2 x6x6 For the prototxt file modified for the FC classifier refer to the file faster_rcnn_train_test_21cls_fc_mod.pt Here the input data format should be set the same as that of the feature maps coming from the FEN and RPN in the fixed 1 6 bit data format For easier integration on the Arm side the outputs take the 1 6 bit data format in floating point format After obtaining the RCNN output rcnn_bbox_delta and rcnn_cls_prob the post-processing module adjusts the final bbox and performs another NMS for each object class Then it gets the final result which runs on the Arm side The following flowchart demonstrates the workflow of the post-processing module Flowchart of Post-Processing Flowchart of Post-Processing Flowchart of Post-Processing Flowchart of Post-Processing As illustrated in the flowchart bbox_trans_inv_prcnn uses the output bboxes of the previous NMS and the bbox delta of RCNN to perform a final adjustment on the bboxes nms_all_class fetches the classification probability for each bbox from RCNN Then for each class it sorts all of the remaining bboxes according to their probability performing NMS afterwards to prevent repeating a bbox for the same object FEN RPN and FC classifier run on the CVflow Vector Processor Alternatively the ROI pooling and post-processing operations require Arm To link these together Ambarella uses the Cavalry framework to chain these blocks together The following figure provides an overview of this structure Overview of PVANet Implementation on Ambarella CV Overview of PVANet Implementation on Ambarella CV Overview of PVANet Implementation on Ambarella CV Overview of PVANet Implementation on Ambarella CV For easier integration the data flow between the CVflow Vector Processor and Arm uses 1 6 bit floating point format Additionally by default the ROI pooling number N is set to 5 0 to improve the performance without damaging the final detection results For the illustration below assume the average time to process one frame is 1 5 6 ms 3 0 7 5 5 0 1 in serial Arm and VP are independent hardwares so they can run simultaneously if the pipeline technique is applied For example when VP processes the N frames Arm can process the N-1 frames The following figure shows the pipeline flow when it is fed five frames The more frames that are fed the sooner that the average time reaches 8 0 ms Pipeline Flow Pipeline Flow Pipeline Flow Pipeline Flow The following figure demonstrates stage dependency using all of the stages that require the semaphore to communicate ARM_1 and ARM_2 cannot run at the same time When the Arm loading is full the mutex lock arm_locker protects them Data dependency vp_stage_1_done arm_stage_1_done and vp_stage_2_done Hardware dependency vp_stage_1_read and arm_stage_1_ready Pipeline Dependency Pipeline Dependency Pipeline Dependency Pipeline Dependency In order to use the pipeline technique VP and Arm process different frames in a single flow For example if VP is processing the N frame Arm will simultaneously process the N-1 frame As a result of this process it is important to use the ring buffer to cache the data history The following explains the required amount of ring buffers For P3 users only need to cache one frame of the 2.VP_1 output after 1.VP_2 is done The VP_1 output and the ARM_1 input use the dual buffer For P3 users only need to cache one frame of the 2.ARM_1 output after 1.VP_2 is done The ARM_1 output uses the dual buffer For P3 P7 the VP_2 and ARM_2 is the final output This means that the VP_2 and ARM_2 output can be one buffer In order to keep the VP_2 capability for future changes use the dual buffer for the VP_2 output―it will still function if the VP_2 output gets changed to a single buffer The Cavalry binary files converted from the split Caffe models can be generated with the CNNGen sample package in one command The output is in out/caffe/demo_networks/pvanet/cavalry_pvanet/ Build the EVK binary as follows test_pvanet uses one binary file generated for the FEN and RPN phase and one binary file for the FC classifier Additionally it requires the I/O port name Assuming the two binary files are named pvanet_rpn_cavalry.bin and pvanet_fc_cavalry.bin which are generated by cavalry_gen with command cavalry_gen –d vas_output/ f xxx.bin use the following to run PVANet In this application input.bin was preprocessed by the submean and RGB to BGR since the RPN network excluded it It supports the draw bbox result on the original image file JPG/PNG/BMP with the “ out-img xxx.jpg” added as an option in test_pvanet It generates a new output file including the bbox result with the prefix “out_” test_pvanet_live takes the live stream into account and provides an end-to-end application for detection and classification purposes Use the following commands to enable the livestream and run the PVANet for content analysis Users can find in The following shows the performance of the PVANet detection network using the model for VOC2012 on the Ambarella CV when Arm VP pipeline and Arm NEON technology is enabled The following performance results are based on CV22 Model RPN ROI Pooling FC Classifier Post-Processing Average time per frame Full Model 33.5 ms 9.5 ms 258.9 ms 0.5 ms 292.5 ms Compressed Model 33.5 ms 9.5 ms 50.4 ms 0.5 ms 8 5 ms The above results are based on the two public models on public web with ROI number 5 0 one is original model the other is compress model which is similar to a pruned model This is the live demo for quick response QR code detection There are three parts in the original project QR code detection clarity enhancement and QR code decoding The detect network is used to detect the location of multiple QR codes The super-resolution network is used for clarity enhancement of the detected QR codes The QRCodeDetector() in OpenCV is used for decoding In this demo the super-resolution module is not included The address of this public project is as follows Users can directly use the following to download the complete opencv_3rdparty Then the detector Caffe model files detect.caffemodel and detect.prototxt can be found To download the SSD Caffe use the following commands Next compile and export the Caffe environment Refer to 3 Environment Setting Use buildcaffe to build Caffe the script is included in the toolchain package The SSD framework includes two distinctive layers and generates prior bounding boxes on the last few feature maps It uses the input image size and the feature map size as inputs and does not include back propagation calculation Because the input image size and the feature map size are fixed the results of remain unchanged takes the output of each and additional data as inputs Users can consider the output of as parameters such as the weights of convolution layer and hence dump the output of from the Caffe model file directly Ambarella provides the tool ssd_prior_box_handling.py which generates the offline binary of and removes and in the prototxt file Because the computer vision CV chip cannot accelerate the calculation speed of the DetectionOutputLayer as it is a large network management system NMS that includes judgment logic the code exists in ambarella/packages/data_process/ Therefore ssd_prior_box_handling.py removes these two layers and implements them on Arm® The following commands show users how to use ssd_prior_box_handling.py Source the toolchain following this command Change the Caffe version to Caffe-SSD Enter the qrcode folder Use to remove and and dump the results of The Cavalry binary files can be generated with the CNNGen sample package The output is in out/caffe/demo_networks/qrcode/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _qrcode.bin is in the cnngen output folder out/caffe/demo_networks/qrcode/ chip chip _qrcode For X86 simulator model desc json file qrcode.json is in the cnngen output folder out/caffe/demo_networks/qrcode/out_qrcode_parser/ ades command qrcode_ades.cmd is in the cnngen output folder out/caffe/demo_networks/qrcode/ chip chip _ades_qrcode Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build unit test for the EVK Build unit test for the X86 simulator Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the X86 unit test The executable file test_eazyai can be found in SDK For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Without Postprocess Accuracy Mode Not Supported Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For the EVK Refer to 3 CNNGen Conversion for information on how to generate qrcode_cavalry.bin The qrcode.lua is included in the path of the EVK If it does not exist users can find it in cvflow_cnngen_samples For X86 Refer to 3 CNNGen Conversion for information on how to generate qrcode.json and qrcode_ades.cmd Copy files to an SD card for the EVK test Place files on the SD card with the following example structure Users can find label_qrcode.txt in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ Users can find in cvflow_cnngen_samples/caffe/demo_networks/qrcode/models/ This file saving method is only an example The file can be placed freely according to the user s requirements Users must keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as an input without the preprocess and postprocess The image is used as an input with right preprocess and postprocess Run Acinference mode The raw.bin is used as an input without the preprocess and postprocess The image is used as an input with right preprocess and postprocess For EVK Board Load cavalry For CV7x cavalry has been already boot up users do not need to run this command Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode for CVflow® performance test The raw.bin is used as an input without preprocess or postprocess The image is used as an input with correct preprocess and postprocess For the file mode using an image as input place the test image such as cvflow_cnngen_samples jpg in then create as the output directory Default preprocess is based on OpenCV users can enable VProc if required with the option The default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Performance on CV22_Walnut This demo included two parts Detect the QR code with SSD which include VP part 1 ms for Default DRA ARM part 1 ms these time will not be effected by QR code number just the Arm part will wave a little QR code recognition with OepnCV detectAndDecode() based on SSD result it needs to do the second edge point positioning which will be much more accurate than the SSD result It will take 3 7 5 ms with 2 5 CPU for one QR code increasing linearly with the QR code number which means running in parallel may get 4 x performance Live mode For EVK Initialize the environment on the CV board CV22 Walnut and imx274_mipi are used as examples Run the following Stream live mode draw on stream rtsp 0.0.2 Video output VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluent check the following two points If the display is not fluent use a greater value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be increased by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to This demo runs inference and postprocess in parallel and needs the same image for postprocess which is fed in inference stage For live mode this demo adds option hold_img to convert and save image for postprocess every frame in case of later frames overwrite current frame buffer in postprocess stage In postprocess stage this demo crops an image into some images including QR code according to the detection results and then resizes the size of these images to 1 x1x244x244 to do QR code recognition It will affect the accuracy of QR code recognition To get a better accuracy please query canvas images with the big resolution as input images or get QR code close to the sensor This section provides the steps for evaluating the mAP accuracy for detection network including steps for resource preparation running the regression tool and obtaining results The VGG16-SSD is used as an example other CNN detection networks should use this example as a reference for accuracy regression The original model of VGG-SSD can be downloaded from the following address The original mAP accuracy is 77.2 according to the author’s statement with IOU 0.5 after training To reproduce this mAP value follow the instructions on the GitHub page to set up Caffe and download the VOC2007 and VOC2012 dataset The script can also be used to compile the Caffe source code in the CV toolchain package Users can utilize the sparse accelerator of the Ambarella CVflow engine to perform coefficient pruning This enables a consistent performance improvement and accuracy The deeper the network is pruned the higher the level of performance that can be achieved on the CVflow engine Because Ambarella is unable to support all training methods customers are encouraged to perform the sparse training independently Additionally to best utilize the fast convolution/inner-productor engine the original model should be quantized to the 8 or 1 6 bit fixed data type If this leads to quantization loss from the fractions value users may need to perform quantization training before deploying the Caffe/TensorFlow model to CVflow engine For further details refer to 2 Network Optimization The VGG-SSD is a combined CNN that uses VGG as a front-end feature-map extractor and SSD as a back-end object detector The accuracy regression test can be divided into two sub-tasks one to obtain an accuracy number from Caffe/TensorFlow which is treated as the baseline accuracy the second sub-task obtains the accuracy number from the CVflow deployment determining the accuracy gap by comparing it to the baseline accuracy after deployment Because CNNGen is limited in parsing certain special CNN layers the entire network is separated into two parts the parser-enabled network and the offline process layers In VGG-SSD because the output data from the prior_box layer requires only input dimensions it can be separated from VGG-SSD and the result can be generated offline by Caffe The detect_out layer is the back-end layer that implements box decoding and NMS processing It can also be separated from VGG-SSD and processed offline The VGG-SSD CVflow deployment includes two steps 1 run CVflow using the makefile work-flow to obtain the results of mbox_loc and mbox_conf 2 run it offline to obtain the results of prior_box and detect_out To increase the accuracy between Caffe and CVflow the Caffe deployment uses the two-step method as well VGG-SSD Workflow VGG-SSD Workflow VGG-SSD Workflow VGG-SSD Workflow VGG-SSD Network Deployment Structure VGG-SSD Network Deployment Structure VGG-SSD Network Deployment Structure VGG-SSD Network Deployment Structure The dedicated tool configures the box decoder and NMS processing enabling the same mAP calculation method used by the source version of Caffe Additionally the Python tool is provided for mAP evaluation The following diagram shows the workflow of the VGG-SSD accuracy regression test VGG-SSD Regression Workflow VGG-SSD Regression Workflow VGG-SSD Regression Workflow VGG-SSD Regression Workflow In this implementation test images are extracted from VOC2007 dataset as recommended by the user The ground truth annotations in xml file are reformatted to a csv file for maintenance purposes The detection result from tool is stored in the same manner The ground truth file format in the CSV file includes one line for each image with several ground truth object boxes Image Name Box Quantity Interval Object label X_min Y_min X_max Y_max Confidence Difficult Tag 009888.jpg 1 7 total of 7 elements from object label to difficult tag 2 0 2 0 2 1 1 3 3 1 6 2 7 1 1.0 always keep 1.0 in ground truth 0 0 easy to detect 1 difficult to detect always keep in detection result The following provides the steps for implementing the classification network evaluation and the detection network evaluation Download the SSD version of Caffe For more information about downloading Caffe SSD refer to 1 Download SSD Caffe Generate the prior box binary and prototxt file For detailed steps on generating these files refer to 2 Extract Parameters of PriorBox Layer The primary steps are the same only the file path must be changed Remember to export the PYTHONPATH and LD_LIBRARY_PATH per the previous section Get the test image and the remaining DRA images After executing the following command two folders are generated and If the folders are not generated users manually generate them by copying the corresponding number of images from the user dataset to the two folders Convert the network if necessary If users need to do inference with mode cvflow acinf ades use the previous command to convert original network to CVFlow format Otherwise for orig mode user could skip the this section and directly do the forward inference with the network original platform The following command starts the convert process The output is generated under There is a convert summary YAML file which is named as vgg16_ssd_cvt_summary.yaml generated under the same folder This summary file is an input configuration for the inference in the next step Do the forward inference The SSD post-processing and mAP evaluation is integrated in the inference workflow Valid platform choices include cvflow acinf ades and orig IP address is needed on platform cvflow Some inference results are as follows The inference result files can be found in The post-processing result files can be found in The evaluation report file can be found in For detailed introduction please refer to fs_accuracy_tool DRAv3 can work with either a CPU or GPU but it is suggested to use GPU as DRAv3 is a little complex—which can result in slower speeds with a CPU For DRAv3 GPU it only can support Cuda9.2 so if users need to use some other version in their GPU server they need install Cuda9.2 by themselves And it is more suggested to use Docker with special Cuda9.2 version which won’t break the user’s original environment Steps to install GPU docker image on the host Ubuntu1804-Cuda10.1 Then users need to run to check if the docker image works well and install CNNGen toolchain in this system Users can find right docker image in For Cuda in host and docker they will share the same Cuda driver so users should make sure this docker image’s Cuda library should be compatible with host’s Cuda driver for details please refer to For more details on how to set up the docker image and install the toolchain please refer to Section_3.5 of Ambarella CV* UG Flexible Linux SDK Code Building and Debug Environment The example below uses ResNet50 as an example Other programs are similar The accuracy output is in out/caffe/test_networks/resnet50/accuracy/ Users can run to get the final accuracy For information on how to use the accuracy tool in the CNNGen Samples refer to fs_accuracy_tool Then users will get the results as Top1 0.6908 and Top5 0.8941 for DRAv2 Top1 is 0.69 and Top5 is 0.86 is used to choose which GPU to be used current parser can only use one GPU if the user has not set it the parser will choose a free one is used to enable GPU but if user’s environment has no GPU support it will use CPU Users could tune strategy of Mode 3 by adding command line options after the flag If the user is not satisfied with the default Mode 3 accuracy or speed these steps could be followed to find a good operating point Find a tuning baseline by trying This setting will have the best performance if the baseline is fast enough but not accurate enough This setting will have the best accuracy Then modify “slope_pcc_end” values between 0.95 default and 1 max to find a good combination If the baseline is accurate but not fast enough adjust the parameters as follows CNNGen tool allows users to combine a series of pre-processing operations before running on the neural networks with variable input This can improve the performance by connecting to VP and DSP directly and reducing DRAM bandwidth It is an advanced feature compared to the normal input pre-process This section provides users with steps to convert and deploy ResNet50 network with using Json for pre-processing which subtracts mean values from a file** For details on Json refer to the CNNGen tool guide Ambarella CV UG Json Preprocessing Format.pdf In this sample the pre-processing performs mean subtraction operation of input data The mean values should be packaged into a binary file which can be converted from binaryproto file by get_mean tool Using Resnet50 as an example the shape of input data and mean binary file is 1 3 2 2 4 2 2 4 The get_mean tool and the binaryproto file resnet50.binaryproto are located in cvflow_cnngen_samples_&lt;version&gt; Steps to get the correct mean file with get_mean tool are shown below The output is cvflow_cnngen_samples_&lt;version&gt; bin Once users have the right input data and mean file use the following steps to prepare a Json file for the pre-processing operations In this case the Json file should include below arrays There is only one input for the shape 1 3 2 2 4 2 2 4 required in this case Users can use gen_image_list.py to generate the text file that contains a list of images path for DRA This array has the constant tensors needed for subtraction In this case the mean file is output_Resnet_mean.bin which is generated by get_mean tool and the shape is 1 3 2 2 4 2 2 4 The operator is a SUBTRACT node that performs a subtraction of the input data The name needs to match the name of the network s input layer An example Json file is shown below The section below introduces how to convert the Resnet50 with using Json for pre-processing For easier reproduction on the customer side the CNNGen samples package includes the example below In this sample the Json file is automatically generated by the script and the input images are in cvflow_cnngen_samples_&lt;version&gt; which will be converted to NV12 by script automatically as well Convert the Resnet50 with Json pre-processing by the following commands Then resnet50_json_preprocess_cavalry.bin will be generated in cvflow_cnngen_samples_&lt;version&gt; In this convert pre-processing json file is generated automatically and used by option in caffeparser.py To use specified json file users can modify it based on the json file of the example Json file which is located in cvflow_cnngen_samples_&lt;version&gt; Run ADES below with the CNNGen samples package Users should pay attention to the option of ades_autogen.py which should match the first input name in Json file as shown below Steps to run accuracy test are as below Compile and generate the command file by the following commands Run accuracy test by the following commands Locate the accuracy report file from the following folder path csv The result binary is stored in the following folder path The test result for the converted network based on ILSVRC_2012 dataset is For more information about this accuracy test tool please refer to fs_accuracy_tool Build the EVK binary as follows In this example the camera module imx274 and CV22 board are used Copy files to SD card for EVK test For example place files on the SD card with the following structure The label name file imagenet_1000.txt is located in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ of cvflow_cnngen_samples package This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use Run by the following commands The option specifies the input of network which should match the name of first input in Json file The option specifies the input data source 0 canvas_buffer 1 pyramid_buffer The option specifies the input data canvas/pyramid id The option specifies output data color space type 0 RGB 1 BGR To download the SSD Caffe use the following commands Next compile and export the Caffe environment Refer to 3 Environment Setting Use buildcaffe to build Caffe the script is included in the toolchain package The SSD framework includes two distinctive layers and generates prior bounding boxes on the last few feature maps It uses the input image size and the feature map size as inputs and does not include back propagation calculation Because the input image size and the feature map size are fixed the results of remain unchanged takes the output of each and additional data as inputs Users can consider the output of as parameters such as the weights of convolution layer and hence dump the output of from the Caffe model file directly Ambarella provides the tool which will generate the offline binary of and also remove and in the prototxt file Because the CV chip cannot accelerate the calculation speed of the DetectionOutputLayer as it is a large NMS which includes judgment logic the code exists in ambarella/packages/data_process/ Therefore removes these two layers and implements them on Arm® The following commands show users how to use Source the toolchain following this command Change the Caffe version to Caffe-SSD Enter the mobilenetv1_ssd folder Use to remove and and dump the results of Generate the SSD Mobilenet as follows The output is in out/caffe/demo_networks/mobilenetv1_ssd/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _mobilenetv1_ssd.bin is in the cnngen output folder out/caffe/demo_networks/mobilenetv1_ssd/ chip chip _mobilenetv1_ssd For X86 simulator model desc json file mobilenetv1_ssd.json is in the cnngen output folder out/caffe/demo_networks/mobilenetv1_ssd/out_mobilenetv1_ssd_parser/ ades command mobilenetv1_ssd_ades.cmd is in the cnngen output folder out/caffe/demo_networks/mobilenetv1_ssd/ chip chip _ades_mobilenetv1_ssd Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Accuracy Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 3 CNNGen Conversion for how to generate mobilenetv1_ssd_cavalry.bin The ssd_caffe.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Simulator Refer to 3 CNNGen Conversion for how to generate mobilenetv1_ssd.json and mobilenetv1_ssd_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resoutce/ It displays the segmentation color map of label_voc_with_bg.txt Users can find in cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/models/ This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use Before running model mobilenetv1_ssd_cavalry.bin users need to verify whether the parameters num_class output_score_name output_loc_name tf_scale_factors in the ssd_caffe.lua file are correct If not users need to modify them OpenMP is used in data process library to process the loops in parallel which will benefit from multiple cores for performance Please specify the number of parallel threads for Arm tasks through the default is the maximum core number of the chip File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry For CV7x cavalry has been already boot up users do not need to run this command Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/dra_img/dog.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 For this Caffe network it is BGR input not RGB which is the case for the Tensorflow SSD model The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents You only look once YOLO is a state-of-the-art real-time object detection system The following sections demonstrate how to convert the YOLO V2 using the Ambarella CNNGen Samples Package Using YOLO V2 as an example users create a layer called Reorg that the standard Caffe BVLC cannot support As a result users must add the custom layers to the standard Caffe using the same steps as the second method provided in the 2 Caffe model Add custom layers In cvflow_cnngen_samples_&lt;version&gt; locate the folder caffe_source which includes three files reorg_layer.hpp reorg_layer.cpp and caffe.proto Copy the reorg_layer.hpp file to Copy the reorg_layer.cpp file to Register the reorg layer in caffe.proto which is under Add optional ReorgParameter reorg_param 1 4 8 to message LayerParameter around lines 3 2 6 to 4 2 4 Also add codes below to approximately line 1 4 4 5 Re-compile Caffe Although the author of YOLO V2 created the network under a deep learning framework called Darknet Ambarella has converted it to a Caffe framework Users can find the caffemodel and deploy.prototxt files under Users are also encouraged to convert the Darknet framework to Caffe framework The following steps describe how to convert the YOLO_v2 The Cavalry binary files can be generated with the CNNGen sample package Please execute the following commands to generate DLL before running eazyai_cvt For yolo_v2_sp60 generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolo_v2_sp60/ For yolo_v2_full generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolo_v2_full/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _yolo_v2_(*) bin is in the cnngen output folder out/caffe/demo_networks/yolo_v2_ chip chip _yolo_v2_(*) For X86 simulator model desc json file yolo_v2_(*) json is in the cnngen output folder out/caffe/demo_networks/yolo_v2_ _parser/ ades command yolo_v2_(*) _ades.cmd is in the cnngen output folder out/caffe/demo_networks/yolo_v2_ chip chip _ades_yolo_v2_(*) Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool For details on the live mode refer to sec_deploy_live_mode Run the binary as follows YOLOv2 SP60 As shown above the input data format is 0 0 8 0 for YOLO As a result users need to add scale YOLOv2 Full Use the applications below to run YOLO in the Cavalry live mode receives the result from test_nnctrl_live and draws the OSD in VOUT The following steps describe how to convert the yolo_v3 The Cavalry binary files can be generated with the CNNGen sample package For yolo_v3_sp50 generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolo_v3_sp50/ For yolo_v3_full generate the Cavalry binary using the following commands The output is in out/caffe/demo_networks/yolo_v3_full/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _yolo_v3_(*) bin is in the cnngen output folder out/caffe/demo_networks/yolo_v3_ chip chip _yolo_v3_(*) For X86 simulator model desc json file yolo_v3_(*) json is in the cnngen output folder out/caffe/demo_networks/yolo_v3_ _parser/ ades command yolo_v3_(*) _ades.cmd is in the cnngen output folder out/caffe/demo_networks/yolo_v3_ chip chip _ades_yolo_v3_(*) Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as shown below Build X86 simulator binary with make Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2 EazyAI Python Tools Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Accuracy Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please use option p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For the EVK board Refer to sub_sec_caffe_yolo_v3_sp50_full_cnngen_conversion for information on how to generate yolo_v3_*_cavalry bin The yolov3_sp50_full.lua is included in the path of the EVK If it does not exist it can be found in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to sub_sec_caffe_yolo_v3_sp50_full_cnngen_conversion for information on how to generate yolo_v3_* json and yolo_v3_*_ades cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users must keep the file path consistent during use File mode For the X86 simulator Run ADES mode The raw.bin is used as an input without the preprocess or postprocess yolo_v3_sp50 yolo_v3_full The image is used as an input with the correct preprocess and postprocess yolo_v3_sp50 yolo_v3_full Run Acinference mode The raw.bin is used as an input without the preprocess or postprocess yolo_v3_sp50 yolo_v3_full The image is used as an input with the correct preprocess and postprocess yolo_v3_sp50 yolo_v3_full For the EVK board Load Cavalry For CV7x cavalry has been already boot up users do not need to run this command Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode to test CVflow performance yolo_v3_sp50 yolo_v3_full The image is used as an input with preprocess and postprocess yolo_v3_sp50 yolo_v3_full The raw.bin is used as an input without the corrcet preprocess or postprocess yolo_v3_sp50 yolo_v3_full For the file mode using image as input place the test image such as cvflow_cnngen_samples/caffe/demo_networks/yolo_v3/dra_img/dog.jpg in and create as the output directory For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board CV22 Walnut and imx274_mipi are used as examples Run the following Stream live mode draw on stream rtsp 0.0.2 yolo_v3_sp50 yolo_v3_full For stream live mode option fsync_off disables frame sync If need to enable frame sync users should enable encode dummy in eazyai_video.sh Video output VOUT live mode draw on VOUT high definition multimedia interface HDMI® yolo_v3_sp50 yolo_v3_full</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_cnngen_tf_demos</field>
    <field name="url">d7/d0a/fs_cnngen_tf_demos.html</field>
    <field name="keywords"></field>
    <field name="text">Tensorflow Demos Tensorflow Demos This chapter describes the Tensorflow CNNGen demos DS_CNN_KWS is a tensorflow implementation of keyword spotting model The published model recognizes 1 2 different classes with and The input should be a WAV binary as follows Length is one second 1 6 kHz 1 6 bit 1 channel The following sections demonstrate how to convert ds_cnn model with Ambarella CNNgen toolchain and how to run model in file mode with SDK package The original model is from a project in Github with Apache License v2.0 The preprocessing steps are as shown below Download source code project from and reset to below commit Model can be found at User_Path pb As the WAV decoder node and MFCC node cannot be supported please use script to delete these two nodes is in User_Path As the model needs audio MFCC features as the input please prepare it as below for DRA analysis Run script to extract MFCC features and to save features in bin file Input file must be 1 6 kHz 1 6 bit 1 channel and one second length audio file with WAV format This script will generate DRA binaries list file of source file and features binary file which is for parser is in User_Path Generate the Cavalry binary as shown below The output is in out/tensorflow/test_networks/tf_ds_cnn_kws/ The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _tf_ds_cnn_kws.bin is in the cnngen output folder out/tensorflow/test_networks/tf_ds_cnn_kws/ chip chip _cavalry_tf_ds_cnn_kws For X86 simulator model desc json file tf_ds_cnn_kws.json is in the cnngen output folder out/tensorflow/test_networks/tf_ds_cnn_kws/ chip build_target _parser/ ades command tf_ds_cnn_kws_ades.cmd is in the cnngen output folder out/tensorflow/test_networks/tf_ds_cnn_kws/ chip chip _ades_tf_ds_cnn_kws Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Current default output data format is float32 For CV7x please use od fp16 in command as it does not support FP32 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Copy the generated binary model _cavalry&lt;version&gt; _tf_ds_cnn_kws.bin and silence.bin to SD card and plug the SD card TO EVK Run with File mode on EVK board Load Cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Use script in PC to get the inference result from binary file which is generated at step 2 labels.txt is in out/tensorflow/test_networks/tf_ds_cnn_kws/cavalry_tf_ds_cnn_kws/ is in User_Path This live demo is for human body segmentation human body part segmentation and human pose detection The following sections explains how to convert the models and run the live demo Please note that the tfjs model of this demo is in a project which has Apache License 2.0 But please ask for detailed information from Tensorflow official public if customers need to use Ambarella assumes no responsibility for this Address of this public project is as follows Users could refer to cnngen_sample_package/tensorflow/demo_networks/bodypix/scripts/get_convert_model.sh for the tfjs models downloading and the pb file converting Tensorflow pb models will be generated under cnngen_samplepackge/tensorflow/demo_networks/bodypix/models By default the get_convert_model.sh script installs tensorflowjs However the installation of tensorflowjs upgrades tensorflow version to 2.1.X Please read this script and decide whether the command fits users environment After the execution of this script users may need to downgrade the tensorflow version back to the CNNGen toolchain default setting before using the CNNGen tool The default tensorflow version could be found in the toolchain installation script Ambarella_Toolchain_CNNGen_ ver _ date ver ver date ubuntu- ver search key word in that file The previous script will download mobilenet based bodypix tfjs model and resnet50 based bodypix tfjs model For mobilenet based bodypix it will also download three kinds of weight including sp25 sp50 full model But for resnet50 only full version is available Users could also choose stride 3 2 1 6 or 8 for these models As there are many choices the get_convert_model.sh only supports mobilenet full stride 1 6 version and mobilenet sp25 stride 1 6 version If the user has other demands please refer to the command in that script and generate your own model The original model of bodypix is stored here Followings are some information about the content in get_convert_model.sh if users are not interested in it please skip and continue to the next subsection The models stored in the previous link is in tfjs format so it is necessary to convert them to tensorflow frozen pb models The convert tool could refer to Users could use the following command to install this tfjs-graph-converter tool Or users could download the tfjs-to-tf source code and use the following command to generate tensorflow pb frozen model The tensorflowjs need to be installed before using this tfjs_graph_converter The original model is from a project in Github please refer to 1 Download and Export Model File for detailed information The preprocessing steps are as shown below For full model Graph Surgery for optimization Generate input files for the post-processing model Generate post-processing model Yaml info tensorflow/demo_networks/bodypix/config/ Bodypix Bodypix post_proc For Bodypix post_proc user should modify data_prepare the step2 Generate input files for the post-processing model input output of yaml For sp25 model Please refer to the first part For full model of chapter 2 Model preprocessing It only needs to change the model to sp25 bodypix_mobilenet_s16_sp25.pb and follow the steps in sequence Generate the Cavalry binary Generate the Cavalry binary as shown below The tf_bodypix_post_proc model uses bin input in this demo the cv7x/cv3 does not support the fp32 input/output format To facilitate conversion different data formats of input and yaml are provided for tf_bodypix_post_proc The output is in out/tensorflow/demo_networks/tf_bodypix/ The output is in out/tensorflow/demo_networks/tf_bodypix_post/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples and cvflow_cnngen_samples For EVK the tf_bodypix cavalry binary chip _cavalry version _tf_bodypix.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix/ chip chip _cavalry_tf_bodypix The tf_bodypix_post cavalry binary chip _cavalry version _tf_bodypix_post.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix/ chip chip _cavalry_tf_bodypix_post For X86 simulator the tf_bodypix model desc json file tf_bodypix.json is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix/ chip build_target _parser/ ades command tf_bodypix_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix/ chip chip _ades_tf_bodypix The tf_bodypix_post model desc json file tf_bodypix_post.json is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix_post/ chip build_target _parser/ ades command tf_bodypix_post_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_bodypix_post/ chip chip _ades_tf_bodypix_post Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool The relationship between input resolution and output size About changing the input resolution When the user need to change the input resolution it is also necessary to change the output size As this network has multiple outputs the flattened size could be calculated as output_h floor input_h stride 1 output_w floor input_w stride 1 pose_keypoint_num 1 7 pose_line_num 1 6 float_heatmaps output_h output_w pose_keypoint_num float_short_offsets output_h output_w pose_keypoint_num 2 MobilenetV1/displacement_fwd_2/BiasAdd output_h output_w pose_line_num 2 MobilenetV1/displacement_bwd_2/BiasAdd output_h output_w pose_line_num 2 For example A 4 8 1 x641 input resolution has a feature output resolution as 3 1 4 1 so float_heatmaps is 3 1 4 1 1 7 2 1 6 0 7 float_short_offsets is float_heatmaps 2 4 3 2 1 4 MobilenetV1/displacement_fwd_2/BiasAdd is 3 1 4 1 2 1 6 4 0 6 7 2 Here are some output sizes corresponding to other resolutions in yaml input 3 2 1 x481 input 4 8 1 x641 input 7 2 1 x1281 input 1 0 5 7 x1921 About output permuting OpenCV uses H W C as data dimension order while CNNGen uses P C H W by default.In order to facilitate the post-processing with OpenCV the CNNGen output is transposed to H W C order using 0 2 3 1 Ambarella recommends to use 4 8 1 6 4 1 resolution full mobilenet model to reach 1 5 fps on CV25M VP 5 0 4 MHz or 7 2 1 1 2 8 1 resolution sp25 mobilenet model to reach 1 0 fps on CV25M VP 5 0 4 MHz Users could try other sparse ratio or resolution for different requirement In the post-processing work-flow a tensorflow pb model is real-time generated and converted to cavalry binary This model mainly contains two tensorflow operators and These two operators could be fairly speeded up if they are calculated by VP part instead of Arm The DRA image of this model is generated by the backbone network with the help of run_tensorflow_model.py under cnngen_sample_package/tools/accuracy_tool/pc_module Choose the bodypix application in the menuconfig Download the binary to the board For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 6 Run C Inference Place the generated binary model _cavalry&lt;version&gt; _tf_bodypix.bin and _cavalry&lt;version&gt; _tf_bodypix_post.bin to the board Run This live demo is not ready on CV72 please try file mode first The result could be observed from the stream or VOUT depending on the show mode that the user chosen by the parameter draw_mode A Lua script is generated under lua Users could modify this Lua if needed and reuse this Lua script as If detection result is shown on the stream overlay then the users could adjust the enc_dummy_latency value 0 7 The setting of this value depends on the VP Arm processing total time For example if enc_dummy_latency is set as 7 and fps is set to 3 0 considering there are 2 frame latency kept for vDSP depth and 1 frame kept for assurance then the available calculation time window for VP ARM processing is 7 2 1 33.3ms which equals to 133.2ms Options part_enable enables body part detection The default is disabled seg_thres segmentation score threshold setting 1 0 1 0 the default value is 0 As this threshold is set before the sigmoid function the range is larger When it is set as 0 it equals to 0.5 after sigmoid pose_enable enables pose detection The default is disabled stride stride of the backbone network The default value is 1 6 refine_steps keypoint refinement iteration number The default value is 1 0 The larger this value is the larger the size of the detected human skeleton is and the more stretched the posture is pose_max_num detects maximum number of pose the default value is 5 pose_thres poses score threshold the default value is 0.4 nms_radius keypoint NMS filtering radius the default value is 20.0 circle_buf_num the number of circular_buffer the default value is 2 When the number is 1 it means serial processing draw_mode sets the result showing method 0 VOUT 1 Stream the default value is 1 If setting the draw mode to VOUT the user could use the parameter of the test_encode to roughly control the detection result s synchronization effect with the orginal image If setting draw mode to stream as the method is applied in this application please ensure this parameter is large enough to allow DSP to apply the detection result to the stream mask_trans segmentation mask transparency the default value is 2 1 0 The larger this value is the less transparent the mask is draw_pose_num the num of pose drawn on overlay the default value is 5 This number doesn t affect pose detection result It only draws this number of poses in score decrease order text_ratio sets the overlay text scale ratio the default value is 0.6 Users could enlarge this parameter while using higher output resolution so that the text is large enough to be seen on the screen x_offset sets the overlay text y offset the default value is 3 0 When the text_ratio is very large some part of the title may exceed the top edge users could use this parameter to adjust the title horizontal position y_offset sets the overlay text y offset the default value is 5 0 Use this parameter to adjust the title vertical position background_file gives a 8 bits per pixel three-channel image as the background Person segmentation is shown this background above There are some sample images under ambarella/unit_test/private/cv_test/bodypix/background_image_samples crop_enable while this parameter is enabled the boarder of the feature map is discarded The discarded boarder width equals to stride 2 The default is disabled roi_w roi_h while this parameter is set as non-zero value the detection is only focused on part of the display window The default value is 0 The Lua script also needs to be adjusted correspondingly while using this ROI feature For example set the previous Lua script as The previous command uses the first canvas as the input buffer of the network and the ROI is set to 1 2 8 0 9 6 0 This live demo is for palm detection and hand landmark detection It is a two-stage demo the first network is a palm detection network which outputs the bounding box and seven key-points for each detected palm the second network is a hand landmark detection network which outputs 2 1 3 D key-points for each input hand image Address of this public project is as follows Users could directly use to download the complete google mediapipe project Then the model files could be found under user_path tflite and user_path tflite or use the following commands to download the related model files only Generate the Cavalry binary as shown below The outputs are generated in out/tensorflow/demo_networks/tf_palm_detection/ and out/tensorflow/demo_networks/tf_hand_landmark/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen tf_palm_detection output folder is in cvflow_cnngen_samples the cnngen tf_hand_landmark output folder is in cvflow_cnngen_samples For EVK the tf_palm_detection cavalry binary chip _cavalry version _tf_palm_detection.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_palm_detection/ chip chip _cavalry_tf_palm_detection the tf_hand_landmark cavalry binary chip _cavalry version _tf_hand_landmark.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_hand_landmark/ chip chip _cavalry_tf_hand_landmark For X86 simulator model tf_palm_detection desc json file tf_palm_detection.json is in the cnngen output folder out/tensorflow/demo_networks/tf_palm_detection/ chip build_target _parser/ ades command tf_palm_detection_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_palm_detection/ chip chip _ades_tf_palm_detection The model tf_hand_landmark desc json file tf_hand_landmark.json is in the cnngen output folder out/tensorflow/demo_networks/tf_hand_landmark/ chip build_target _parser/ ades command tf_hand_landmark_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_hand_landmark/ chip chip _ades_tf_hand_landmark Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool About the coverage_th The default DRA coverage_th is 0.90 Enlarging this ratio could let the DRA quantization result tend to use more fx16 data format In this case the detection result would be more accurate but the calculation speed would be decreased In this example users could enlarge this ratio to 0.95 to get a better detection result Users are also encouraged to use other coverage_th ratios based on users different requirements for accuracy or performance It can be modified in ea_cvt_tf_hand_landmark.yaml Choose the hand landmark application in the menuconfig Download the binary to the board For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Place the generated binary model _cavalry&lt;version&gt; _tf_palm_detection.bin and _cavalry&lt;version&gt; _tf_hand_landmark.bin to the board Run The result could be observed from the stream or VOUT depending on the show mode chosen by the parameter draw_mode As different CV platforms could have different CV performance if the detection result is shown on the stream overlay then users may need to adjust the enc_dummy_latency value 0 7 to let the detection result be applied to the correct image frame For how to use eazyai_video.sh please refer to 1 4 Eazyai Video For how to adjust this value and other overlay related parameters please refer to sub_sec_tf_hand_landmark_run_live_demo For how to adjust the pyramid buffer related parameters please refer to subsec_hyperlpr_run Some useful command line options det_thres Users could use this option to set the palm detection score threshold The larger this ratio is the more accurate the detection result will be but the less palms are detected The default value is 0.6 nms_thres Users could use this option to set NMS IOU filtering threshold The smaller this ratio is the less neighbor bounding boxes will be accepted by the algorithm The default value is 0.2 pose_thres Users could use this option to set the hand landmark detection threshold The larger this ratio is the more accurate the detection result will be but the less landmarks are detected The default value is 0.9 HF-Net is a monolithic deep neural network DNN for descriptor extraction It was introduced in the CVPR 2 0 1 9 paper From Coarse to Fine: Robust Hierarchical Localization at Large Scale The following sections demonstrate how to export an HF-Net TensorFlow model from the public source project implemented with TensorFlow as well as how to convert the model in the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The TensorFlow model is exported from a source project in GitHub with a license from Massachusetts Institute of Technology MIT The steps are shown as below Install the Python packages required by HF-Net in here Only tensorflow is necessary for freezing the pre-trained model Download the package for the trained model from here and extract it Create a Python file freeze_model.py with the following source code to freeze the whole model Then run the following command The file hfnet_frozen_graph.pb will be generated View the model in tensorboard and identify the input and output nodes for local descriptor detection For local descriptor detection only the input node is pred/strided_slice_3 0 the output nodes are pred/local_head/detector/DepthToSpace and pred/local_head/descriptor/l2_normalize The tool import_pb_to_tensorboard.py in CNNGen toolchain requires to function on the HF-Net model Use the following source code and run as a Python file import_pb_to_tensorboard.py Create a Python file freeze_model_part.py with the following source code to freeze the required part of the model Then run the following command The file hfnet_frozen_graph_part.pb will be generated Create a Python file optimize_graph.py to optimize the model in order to avoid errors in running the OpenCV DNN Then run the following command The file hfnet_frozen_graph_part_optimized.pb will be generated Perform a graph surgery The file hfnet_320x240_surgery.pb will be generated This file is used to generate the Cavalry binary model Run the following command to verify that there are no unsupported operators Generate the Cavalry binary as shown below The output is in out/tensorflow/demo_networks/tf_hfnet/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _tf_hfnet.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_hfnet/ chip chip _cavalry_tf_hfnet For X86 simulator model desc json file tf_hfnet.json is in the cnngen output folder out/tensorflow/demo_networks/tf_hfnet/ chip build_target _parser/ ades command tf_hfnet_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_hfnet/ chip chip _ades_tf_hfnet Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build a unit test for the EVK Build a unit test for the X86 simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For the EVK board Refer to 2 CNNGen Conversion for how to generate chip _cavalry version _tf_hfnet.bin The hfnet.lua is included in the path of the EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 simulator Refer to 2 CNNGen Conversion for how to generate tf_hfnet.json and tf_hfnet_ades.cmd Copy files to the SD card for the EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s requirements Users must keep the file path consistent during use File mode For the X86 simulator Run ADES mode The raw.bin is used as input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess Run acinference mode The raw.bin is used as an input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess For the EVK board Load Cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode only for CVflow® performance test The image is used as an input with preprocess and postprocess The raw.bin is used as an input without the correct preprocess or postprocess For the file mode with the image as an input place the test image such as cvflow_cnngen_samples/tensorflow/demo_networks/hfnet/dra_img/1341847980.722988.jpg in and create as the output directory For the option isrc the default preprocess is based on OpenCV users can enable VProc if required with the option The default value is CPU For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the computer vision CV board CV22 Walnut and imx274_mipi are used as examples Run the following Stream live mode draw on stream rtsp 0.0.2 Video output VOUT live mode draw on VOUT high-definition multimedia interface HDMI® If there is no display on the stream or the display is not fluent check the following two points If the display is not fluent use a larger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents For certain situations users may need to use TensorFlow model with the NV12 input which includes two input files Y data and UV data It can be used to convert the models with the NV12 input and generate the cavalry binary at present This section provides the explanations with mobilenetv2_nv12 as below Converts image from jpg to NV12 by the conversion tool for DRA calculation Generates Cavalry binary by quick_start.sh Deploys and tests the converted cavalry binary on the board Images can be converted to NV12 from jpg or JPEG using the tool convert_jpg2nv12 in 3 CFlite Python Tools After conversion separated Y and UV data files will be generated in specified folders specifies the folder that contains the input jpeg images and specifies the output folder paths used to store the converted Y and UV data files seperately and specifies the resize resolution for NV12 this should be the same with the converted Y image s resolutuion specifies that the generated UV data is not interleaved users can use to generate interleaved UV data For MobileNetv2 this option should be set to 0 to match the network input This section introduces how to generate the cavalry binary for tf model with mobilenetv2_nv12 First generate the Cavalry binary as shown below This model is not included in the cnngen package and is only used as an example of YUV input The content of the Yaml file is as follows Second edit as shown below This is because the UV format in IAV does not match with the NCHW input format generated by the CV toolchain so users need to use dram_format 1 to tell the VP reading the UV data with the interleave mode Finally generate the cavalry binary using the following commands The output is in out/tensorflow/demo_networks/tf_mobilenetv2_nv12/ The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _tf_mobilenetv2_nv12.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_nv12/ chip chip _cavalry_tf_mobilenetv2_nv12 For X86 simulator model desc json file tf_mobilenetv2_nv12.json is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_nv12/ chip build_target _parser/ ades command tf_mobilenetv2_nv12_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_nv12/ chip chip _ades_tf_mobilenetv2_nv12 Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Current default output data format is float32 For CV7x please use od fp16 in command as it does not support FP32 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example to deploy TensorFlow models with the NV12 input users can refer to mobilenetv2_nv12 for detail usage please refer to the following referenced chapters 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application Refer to 2 CNNGen Conversion for how to generate The is included in the path of EVK If it does not exist find it in Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use Test_eazyai can get YUV data from the canvas buffer Test_eazyai can get YUV data from the pyramid buffer If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in such as 7 If the overlay buffer size is not enough it can be added by changing the size in The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 2 For details please refer to The option specifies the live mode The option specifies that the result will be presented in the form of a box The option specifies the input YUV canvas buffer ID The option specifies the channel ID and pyramid layer of the pyramid buffer The option specifies the output stream ID to draw drawing on HDMI in default The option specifies the yuv input from iav This section provides users with the steps for evaluating the pixel accuracy for the segmentation network including steps for resource preparation running the regression tool and obtaining results The DeepLabv3 is used as an example to introduce all test steps and tools For other CNN segmentation networks use this example as a reference for accuracy regression The original DeepLab model can be downloaded from the following address For more information on this model refer to The original mIOU accuracy is 71.83 after training according to the author’s statement with test scale 1.0 no left-right flip To reproduce this mIOU value follow the instructions on the GitHub page to set up Tensorflow and download the VOC2007 dataset A readme file is also provided to help users download VOC2007 train_validation and the test dataset as well as setup ground truth images Generate with the default input size of 5 1 3 x513 Use to check the model file’s status As shown above the original model includes some input variables as well as unsupported operators which are not allowed for inference on the VP Because inference on the VP requires constant input and all supported operators this section includes steps to ensure proper processing All of the tools included in this section are only for reference and do not apply to all networks Users must determine the correct portion of the graph to run on the VP on their own The remaining should run using other methods For more information about the network pre-processing please refer to 2 Pre-Process Model The default model is downloaded from http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz The file frozen_inference_graph.pb in the package is renamed to deeplabv3_mnv2_dm05_pascal_trainval_frozen_inference_graph.pb The input size after preprocess in this network is frozen to 5 1 3 x513 A Python script is provided to help users maintain consistent network input size constant and eliminate the unsupported nodes which calls the graph_surgery.py tool from its program user_path py Please notice is necessary for running the demo on the board It generates a new pb file under the output path and named as Because this command is already merged into the makefile workflow users can continue immediately to the next section and begin converting the model using the CNNGen tool Generate with custom input size If the size other than 5 1 3 x513 is needed please refer to the following step to export another model If the original FOV of the image is not NxN for a 5 1 3 x513 network the scale preprocess with the change of the aspect ratio is required But the change of the aspect ratio in image will make the result worse considerablely in the trained deeplabv3 model So it s better to freeze the network to an input size which has the same aspect ratio with the image FOV The following steps show how to export mobilenetv2_coco_voc_trainaug with an input size of 7 2 0 x405 which has the same aspect ratio with the FOV of 1 9 2 0 x1080 Clone tensorflow model project from https://github.com/tensorflow/models.git Download model from model zoo http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz Work in the folder of the tensorflow model project Add PYTHONPATH Decompress deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz and copy files to tensorflow/models/research/deeplab Export model deeplabv3_mnv2_pascal_train_aug_720x405.pb is generated under tensorflow/models/research/deeplab/export Copy deeplabv3_mnv2_pascal_train_aug_720x405.pb to CNNGen Sample Package Generate cavalry binary The output is in out/tensorflow/demo_networks/tf_deeplab_v3/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _tf_deeplab_v3.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_deeplab_v3/ chip chip _cavalry_tf_deeplab_v3 For X86 simulator model desc json file tf_deeplab_v3.json is in the cnngen output folder out/tensorflow/demo_networks/tf_deeplab_v3/ chip build_target _parser/ ades command tf_deeplab_v3_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_deeplab_v3/ chip chip _ades_tf_deeplab_v3 Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 3 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _tf_deeplab_v3.bin The deeplabv3.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 3 CNNGen Conversion for how to generate tf_deeplab_v3.json and tf_deeplab_v3_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/tensorflow/demo_networks/deeplabv3/dra_img/image1.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents Accuracy Mode After compiling run an accuracy test Before executing the command ensure that the network between the PC and the board is valid Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Locate the accuracy report file from the following folder path user_path txt The result binary is stored in the following folder path user_path The complete test result for the converted network bases on VOC07 train-validation dataset and test dataset 6 3 2 images Pixel Accuracy avg 92.24 Mean Pixel Accuracy avg 79.06 Mean Intersection over Union avg 68.30 Frequency Weighted Intersection over Union avg 86.47 For more information about this accuracy test tool please refer to fs_accuracy_tool Download the original model of VGG-SSD from the following address The mAP of the original model of VGG-SSD is 2 2 Refer to the following address for details Use tf_print_graph_summary.py to check the model file’s status As shown above the original model includes some input variables as well as unsupported operators which are not allowed for inference on the VP Because inference on the VP requires constant input and all supported operators this section includes steps to ensure proper processing All of the tools included in this section are only for reference and do not apply to all networks Users must determine the correct portion of the graph to run on the VP on their own The remaining should run using other methods To view the location of the supported operators follow the steps below Show the graph with Tensorboard Visit the URL above to view the graph and try to find the main inference flow Otherwise there will be so many unsupported operators that it cannot be processed easily Try to identify if there is any offline flow This means that the flow has no relationship with the input and stays the same regardless of the input When the main inference thread is found use to cut it and only keep the main inference thread Use the command to check if there are any unsupported operators in the surgery model To deal with unsupported operators in this stage use the instructions below If the operators are in the beginning or end use to cut them again If there are one or two unsupported operators in the middle check if there are other supported operators that could replace them If there are no suitable operators for replacement users can use the custom node in Caffe For TensorFlow and ONNX contact the Ambarella support team for more details If many unsupported operators just locate in the middle users can split the model to three parts with such as the example of pvanet With the above analysis the VP input and output can be found below Users can also find the offline flow for the binary below Because from the graph has no relationship with the input users can generate it using offline tools run it with one image and then dump the output The output and should be used as the input of the final calculation and include boxes decode NMS and others which run on Arm Then perform the following steps To remove the unsupported operator and use constant input with VP input and output can run entirely on the VP Print the graph The information above includes the model used by the CNNGen Samples It is found in the following path tensorflow/demo_networks/mobilenetv2_ssd/models/frozen_mobilenet_300_v2_ssd_opt.pb To generate a constant binary with an offline tool such as the PriorBox in Caffe SSD Split the network with only anchor boxes output kept Run the above splited network in PC to generate anchor boxes For the input any image should be OK as is not related with input content only be related with input resolution The command above generates the offline binary used by the CNNGen Samples It can also be found in the following tensorflow/demo_networks/mobilenetv2_ssd/models/anchor_boxes.bin To write Arm code to implement the boxes decode NMS and others locate the sample code from the SDK package Analyze the input pre-process flow for instructions on how to convert it The original network summary should be as follows The output Squeeze means there are 1 9 1 7 candidate boxes 4 means start_x start_y end_x end_y The output Postprocessor/convert_scores means there are 1 9 1 7 candidate boxes 9 1 means the confidence of every classes this model is trained by 9 1 classes with COCO Then if users convert it in default configuration without “ot” or “os” the generated dimension after the parser should be as follows It is not easy to handle in Arm as there are different alignment rules in different chips So users can use below post process option to make it easy in ea_cvt_tf_mobilenetv2_ssd.yaml For the first Output Parser will convert 1 4 1 1 9 1 7 to 1 1 1 9 1 7 4 with “ot 0 2 3 1 ” then to 1 1 1 7 6 6 8 with “os 1 1 1 7 6 6 8 ” Actually users can only use “os 1 1 1 7 6 6 8 ” to convert 1 4 1 1 9 1 7 to 1 1 1 7 6 6 8 but the content layout should be different as follow For “ot” and “os” final content is as below which the Arm code is based on For only “os” final content is as below The same post process for the output So there are two reasons that users need to use “ot” and “os” in above examples The dimension 1 1 1 N is easy to be handled in Arm side the VP has padding with different bytes in different chips so default dimension 1 4 1 1 9 1 7 or 1 9 1 1 1 9 1 7 will result in lots of paddings with useless data and not easy to be handled The Arm code in packages/data_process is designed with this dimension If users can write the Arm code by themselves they can do anything they want do not need to follow above design flow with “ot” and “os” Generate the Cavalry binary as shown below The output is in out/tensorflow/demo_networks/tf_mobilenetv2_ssd/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _tf_mobilenetv2_ssd.bin is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_ssd/ chip chip _cavalry_tf_mobilenetv2_ssd For X86 simulator model desc json file tf_mobilenetv2_ssd.json is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_ssd/ chip build_target _parser/ ades command tf_mobilenetv2_ssd_ades.cmd is in the cnngen output folder out/tensorflow/demo_networks/tf_mobilenetv2_ssd/ chip chip _ades_tf_mobilenetv2_ssd Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Start CVflow Engine For below Dummy and File Mode with CVflow Chip Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 4 CNNGen_Conversion for how to generate _cavalry&lt;version&gt; _tf_mobilenetv2_ssd.bin The ssd_tf.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Simulator Refer to 4 CNNGen_Conversion for how to generate tf_mobilenetv2_ssd.json and tf_mobilenetv2_ssd_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ Users can find in cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/models/ This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use Before running model _cavalry&lt;version&gt; _tf_mobilenetv2_ssd.bin users need to verify whether the parameters num_class output_score_name output_loc_name tf_scale_factors in the ssd_tf.lua file are correct If not users need to modify them OpenMP is used in data process library to process the loops in parallel which will benefit from multiple cores for performance Please specify the number of parallel threads for Arm tasks through the default is the maximum core number of the chip File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/tensorflow/demo_networks/mobilenetv2_ssd/dra_img/dog.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 For this TensorFlow network it is RGB input not BGR which is the case for the Caffe SSD model The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_cnngen_onnx_demos</field>
    <field name="url">d6/d99/fs_cnngen_onnx_demos.html</field>
    <field name="keywords"></field>
    <field name="text">ONNX Demos ONNX Demos This chapter describes the ONNX CNNGen demos The ByteTrack is for Multi-object tracking(MOT) which is similar to FairMOT Unlike FairMOT the output of the backbone network of ByteTrack does not include re-ID features For more details please refer to GitHub The following contents demonstrate how to export ByteTrack open neural network exchange ONNX models from the public source project and how to run the ONNX model with the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with MIT License The steps are below Download the source code project from here Go to the source project directory and install the required packages Download pretrained model Download the pretrained models from the link below and place them in pretrained/ bytetrack_s_mot17.pth.tar bytetrack_m_mot17.pth.tar bytetrack_l_mot17.pth.tar bytetrack_x_mot17.pth.tar Refer to the following steps to modify the input size of network Take bytetrack_s as an example Export the ONNX model bytetrack_s bytetrack_m bytetrack_l bytetrack_x The input size of the exported model needs to be 3 2 aligned This model only detects person The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package Take bytetrack_s as an example Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_bytetrack_s.bin is in the cnngen output folder out/onnx/demo_networks/onnx_bytetrack_ chip chip _cavalry_onnx_bytetrack_* For X86 simulator model desc json file onnx_bytetrack_* json is in the cnngen output folder out/onnx/demo_networks/onnx_bytetrack_ build_target _parser/ ades command onnx_bytetrack_*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_bytetrack_ chip chip _ades_onnx_bytetrack_* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Copy files to the secure digital SD card for the EVK test For example place files on the SD card with the following structure Please rename the chip _cavalry version _bytetrack.bin generated by CFlite to bytetrack_cavalry* bin and place it in the directory above File mode Load Cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following commads For the file mode using image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/bytetrack/dra_img/palace_frame.jpg in and create as the output directory Live mode a Initialize the environment on the CV board The cv5_timn and imx274_mipi are used as examples b Run the following commands take bytetrack_s as an example Stream live mode draw on stream rtsp 0.0.2 Video output VOUT live mode draw on VOUT high definition multimedia interface HDMI® If there is no display on the stream or the display is not fluent check the following points If the display is not fluency use a larger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be increased by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For more details refer to the EazyAI Library application programming interface API related content in the Linux software development kit SDK Doxygen documents CenterNet models an object as a single point the center point of its bounding box The CenterNet detectors use the key point estimation to find center points and regress to all of the other object properties such as size three-dimensional location orientation and pose It is end-to-end differentiable simpler faster and more accurate than corresponding bounding box based detectors The following sections demonstrate how to export CenterNet ONNX model from PyTorch and run the ONNX model in the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a project in GitHub with MIT license The steps are shown as below Download the source code project from here Install the required Python packages and build libraries following the steps in the following link Download ctdet_coco_dlav0_1x.pth from here and put it under the models folder of the source project ctdet_coco_dlav0_1x.pth is the pre-trained weight file of the DLA backbone There are several implementations of the CenterNet backbone hourglass DLA Deep Layer Aggregation DLA+DCN Deformable Convolutional Network ResNet and ResNet+DCN DLA is chosen as the sample for the following reasons Hourglass is too heavy and the performance is low on CV chips DCN is not supported by the CV toolchain The pre-trained ResNet model is not provided by the source project Create a Python3 source file named export_ctdet_onnx_model.py for example with the following code Copy export_ctdet_onnx_model.py to the src folder of the source project and run the following command The ONNX model file ctdet_coco_dlav0_1x.onnx will be generated under the src folder Run the following command to do the graph surgery The final model file ctdet_coco_dlav0_1x_384x384.onnx is generated for converting by the CV toolchain Run the following command to verify that there are no unsupported operators The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_centernet.bin is in the cnngen output folder out/onnx/demo_networks/onnx_centernet/ chip chip _cavalry_onnx_centernet For X86 simulator model desc json file onnx_centernet.json is in the cnngen output folder out/onnx/demo_networks/onnx_centernet/out_ build_target _parser/ ades command onnx_centernet_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_centernet/ chip chip _ades_onnx_centernet Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_centernet.bin The centernet.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_centernet.json and onnx_centernet_ades.cmd Create a text file named label_coco_80.txt and fill it with the following content Copy files to SD card on the EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/centernet/dra_img/33887522274_eebd074106_k.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents DDRNet is a street scene segmentation network with the high performance and accuracy Please follow the steps below to download and export the PYTorch model to ONNX model Download the PYTorch model file Download the model file from For example download the DDRNet_23_slim on Cityscapes val mIoU 77.8 URL Download and modify the source code Convert to ONNX Please ensure that the user current environment follows the requirment described in The model file will be generated under user_path It also reads the example image in dra_img and output a gray-scale result torch_out.jpg in DDRNet.pytorch Use the graph surgery tool to optimize the model The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples 3 For EVK the cavalry binary chip _cavalry version _onnx_ddrnet.bin is in the cnngen output folder out/onnx/demo_networks/onnx_ddrnet/ chip chip _cavalry_onnx_ddrnet For X86 simulator model desc json file onnx_ddrnet.json is in the cnngen output folder out/onnx/demo_networks/onnx_ddrnet/out_ build_target _parser/ ades command onnx_ddrnet_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_ddrnet/ chip chip _ades_onnx_ddrnet Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool As this DDRNet has one simple input port and one simple output port it could reuse the Deeplab_v3 s board application Please refer to 3 Build Unit Test For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_ddrnet.bin The ddrnet.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_ddrnet.json and onnx_ddrnet_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/ddrnet/dra_img/frankfurt_000000_000294_leftImg8bit.png in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents fb_num Specify the cycle buffer number of frame buffer default is 2 Certain situations might require users to blur an individual s face in order to maintain their privacy and avoid potential legal consequences This chapter explains how to deploy the face detection and blur function demo In this demo the ONNX Fast Generic Face Detector FGFD network is used to detect the face using VP DSP then performs the blur function Users can download the original model of FGFD from the following address The pre-trained model file version-RFB-320.onnx is used to generate the Cavalry binary for FGFD The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_fgfd.bin is in the cnngen output folder out/onnx/demo_networks/onnx_fgfd/ chip chip _cavalry_onnx_fgfd For X86 simulator model desc json file onnx_fgfd.json is in the cnngen output folder out/onnx/demo_networks/onnx_fgfd/out_ build_target _parser/ ades command onnx_fgfd_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_fgfd/ chip chip _ades_onnx_fgfd Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Uint Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_fgfd.bin The fgfd.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_fgfd.json and onnx_fgfd_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/FGFD/dra_img/2.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Normal demo on stream Blur demo on stream In Blur mode the lua file blur_strength needs to be changed to 0 and the default value of blur_strength is 0 In Normal mode the lua file blur_strength needs to be changed to 1 Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 color_table resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents The DSP supports inserting blurred areas onto streams In this demo the result of FGFD is sent to the DSP as a structure including x y width and height using the ioctl command Because the blur function supports up to 1 6 blurred areas per stream the FGFD result will be filtered by sizes of detected face boxes when more than 1 6 faces are detected Users can specify the color of a blurred area before starting the encoder using the following command The option specifies the U and V values of color and specifies the color table of the blur function For more details please refer to the document in included in SDK For details on using refer to Chapter 9 of Ambarella_CV*_UG_Flexible_Linux_SDK* pdf To do multiple object tracking FairMOT consists of two homogeneous branches to predict pixel-wise object scores and re-ID features with an anchor-free and single-shot deep network The following sections demonstrate how to export FairMOT ONNX model with dlav0 backbone which doesn t use DCNv2 from PyTorch and run the ONNX model in the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step please contact the Ambarella support team for assistance The ONNX model is exported from a project in GitHub with MIT license Due to the pretrained model with dlav0 is not provided by the author the following steps start with how to train the network with the dlav0_34 backbone on the dataset CrowdHuman and MOT17 The whole steps are shown as below Clone the source project from here Clone DCNv2 if the network with dla_34 will be evaluated on PC tested with commit c7f778f28b84c66d3af2bf16f19148a07051dac1 Install requirements Compile DCNv2 if network with dla_34 will be evaluated on PC Download dataset Download MOT17 dataset from here Download CrowdHuman dataset from here CrowdHuman_train01.zip CrowdHuman_train02.zip CrowdHuman_train03.zip CrowdHuman_val.zip annotation_train.odgt annotation_val.odgt Prepare the dataset in the following structure Unzip MOT17.zip then move all folders under unzipped to DATASET_ROOT Unzip CrowdHuman_train01.zip CrowdHuman_train02.zip and CrowdHuman_train03.zip then move all image files under unzipped folder to Unzip CrowdHuman_val.zip then move all image files under unzipped folder to Generate dataset labels Copy CNNGEN_SAMPLES_PACKAGE_ROOT py to FAIRMOT_ROOT then run the following commands Copy CNNGEN_SAMPLES_PACKAGE_ROOT py to FAIRMOT_ROOT then run the following commands Download dlav0_34 COCO pretrained model ctdet_coco_dlav0_1x.pth from here and copy it to FAIRMOT_ROOT The pretrained model is from the CenterNet project at Change dataset configurations Change the following item in FAIRMOT_ROOT json to the actual one Change the following item in FAIRMOT_ROOT json to the actual one Modify FAIRMOT_ROOT py Launch training Copy CNNGEN_SAMPLES_PACKAGE_ROOT sh and CNNGEN_SAMPLES_PACKAGE_ROOT sh to FAIRMOT_ROOT and launch the training process Change in mot17_on_crowdhuman_dlav0_34.sh to the actual one The training will take about 1 hour on 4 GeForce RTX 2 0 8 0 Ti gpus Change in crowdhuman_dlav0_34.sh to the actual one The training will take about 7 hours on 2 GeForce RTX 2 0 8 0 Ti gpus can t be used in crowdhuman_dlav0_34.sh or else it will cause error The trained model will be generated at FAIRMOT_ROOT pth After the training is done copy CNNGEN_SAMPLES_PACKAGE_ROOT py to FAIRMOT_ROOT and run the following commands The ONNX model file model_last_288x160.onnx will be generated under the FAIRMOT_ROOT folder Quit conda and initialize the CV toolchain environment Do graph surgery The file fairmot_dlav0_34_288x160_mot17_on_crowdhuman.onnx will be generated It can be put under CNNGEN_SAMPLE_PACKAGE_ROOT Use the following command to verify there are no unsupported operators The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_fairmot.bin is in the cnngen output folder out/onnx/demo_networks/onnx_fairmot/ chip chip _cavalry_onnx_fairmot For X86 simulator model desc json file onnx_fairmot.json is in the cnngen output folder out/onnx/demo_networks/onnx_fairmot/out_ build_target _parser/ ades command onnx_fairmot_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_fairmot/ chip chip _ades_onnx_fairmot Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_fairmot.bin The fairmot.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Simulator Refer to 2 CNNGen Conversion for how to generate onnx_fairmot.json and onnx_fairmot_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The raw.bin is used as an input without the right preprocess and postprocess The real image is used as an input with the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/fairmot/dra_img/mot_frame.jpg in and create as the output directory For the file mode with raw.bin as input place the raw.bin such as cvflow_cnngen_samples/out/onnx/demo_networks/onnx_fairmot/dra_image_bin/mot_frame.bin in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Stream live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents The LFFD network is a general detection framework that is applicable to one class detection such as face detection pedestrian detection head detection vehicle detection and so on The following sections show how users can export the LFFD ONNX model from MXNet and work with it in the Ambarella CNNGen samples package The ONNX model is exported from a GitHub project with the MIT license The steps are provided as below Download the source code project from here Install the following dependent Python packages ONNX 1.3.0 is required to export the ONNX model from MXNet or else the error will appear ONNX runtime 1.0.0 is required to run the Ambarella toolchain on the exported ONNX model A newer version of ONNX is needed when MXNet upgrades relate to exporting the slice operator A future version of Ambarella toolchain may not support the older versions of ONNX Use the following Python code to export the ONNX model for the head detection The following model file will be generated and can be converted with the Ambarella toolchain It is in the CNNGen sample package mxnet_exported_lffd_head_720x480.onnx The file symbol_10_160_17L_4scales_v1_deploy-symbol.json in the code is renamed from symbol_10_160_17L_4scales_v1_deploy.json under head_detection/symbol_farm The file symbol_10_160_17L_4scales_v1_deploy-800000.params in the code is renamed from train_10_160_17L_4scales_v1_iter_800000.params under head_detection/saved_model/configuration_10_160_17L_4scales_v1_2019-09-20-13-08-26 input_shape can be adjusted to a different size There is no need to do graph surgery on the exported ONNX model However users can try it by running the following command The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_lffd.bin is in the cnngen output folder out/onnx/demo_networks/onnx_lffd/ chip chip _cavalry_onnx_lffd For X86 simulator model desc json file onnx_lffd.json is in the cnngen output folder out/onnx/demo_networks/onnx_lffd/out_ build_target _parser/ ades command onnx_lffd_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_lffd/ chip chip _ades_onnx_lffd Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_lffd.bin The lffd.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_lffd.json and onnx_lffd_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/lffd/dra_img/00034000_640x480.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents RetinaFace is a practical single-stage SOTA face detector which is initially introduced in arXiv technical report and then accepted by CVPR 2 0 2 0 The following sections demonstrate how to export an RetinaFace ONNX model from the public source project implemented with MXNet and how to run the ONNX model in the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with MIT license The steps are shown as below The source code project is from here Only the pre-trained model files are used when exporting the model Install the required Python packages Download pre-trained model retinaface_mnet025_v1.zip The download link is derived from the source code Write a Python3 source file to convert model from MXNet to ONNX The detailed example can be found in cnngen_samples_package/onnx/demo_networks/retinaface/script/export_retinaface_onnx_model.py Extract the model files and put the model folder together with export_retinaface_onnx_model.py Modify _op_translations.py in the installed MXNet package path For example local/lib/python3.6 py The original export_onnx module in mxnet 1.6.0 does not support the slice operator Add the following functions to support exporting the slice operator The original export_onnx module in mxnet 1.6.0 has problem to export the BatchNorma operator to ONNX in opset 9 and above The new function in mxnet 1.7.0 has been improved but it is not released in pip yet Use the following content from mxnet 1.7.0 to replace the original convert_batchnorm function Run the following command to generate the ONNX model by replacing some unsupported operators without retrain The ONNX model file mxnet_exported_retina_face_640x640.onnx will be generated under the folder out_dir UpSampling is replaced with Deconvolution Crop is replaced with slice SoftmaxActivation is replaced with softmax Optional Run the following command to do graph surgery Run the following command to verify that there are no unsupported operators The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_retinaface.bin is in the cnngen output folder out/onnx/demo_networks/onnx_retinaface/ chip chip _cavalry_onnx_retinaface For X86 simulator model desc json file onnx_retinaface.json is in the cnngen output folder out/onnx/demo_networks/onnx_retinaface/out_ build_target _parser/ ades command onnx_retinaface_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_retinaface/ chip chip _ades_onnx_retinaface Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;verion&gt; _onnx_retinaface.bin The retinaface.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_retinaface.json and onnx_retinaface_ades.cmd Copy files to SD card for EVK test For example place files on the SD card with the following structure This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/retinaface/dra_img/ in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Normal demo Blur demo Query YUV with pyramid manual feed Query YUV with canvas feed EFM demo Run Normal demo Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI Blur and Overlay draw on stream rtsp 0.0.2 with DSP blur high score faces and stream overlay low score faces The Blur mode includes Blur and Blur and Overlay modes The lua file enable_blur needs to be changed to 1 and the default value of enable_blur is 0 And set the value of conf_threshold and blur_conf_threshold according to note 2 3 Their values range from 0 to 1 In Blur mode user needs to set the value of blur_conf_threshold less than conf_threshold under lua file In Blur and Overlay mode user needs to set the value of blur_conf_threshold greater than conf_threshold under lua file In this case if the score is between conf_threshold and blur_conf_threshold it is the Overlay mode If score is greater than blur_conf_threshold it is the Blur mode Query YUV with pyramid manual feed Query YUV with canvas manual feed EFM demo Take CV22 Walnut and imx274_mipi for example Overlay draw on stream rtsp 0.0.2 with overlay only Overlay and Blur draw on stream rtsp 0.0.2 with DSP blur high score faces and stream overlay low score faces In Overlay and Blur mode the lua file enable_blur needs to be changed to 1 and the default value of enable_blur is 0 If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents The model is from the official repository for the paper Robust High-Resolution Video Matting with Temporal Guidance Robust video matting RVM is specifically designed for robust human video matting Unlike existing neural models that process frames as independent images RVM uses a recurrent neural network NN to process videos with temporal memory RVM can perform matting in real-time on any video without additional inputs The following sections demonstrate how to export an RVM Open Neural Network Exchange ONNX model from the public source project implemented with PyTorch as well as how to convert the ONNX model in the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with GPL-3.0 license The steps are as shown below Clone the source code project from GitHub Install the required Python packages Download the pre-trained model rvm_mobilenetv3.pth Switch to the onnx branch to export the ONNX model Go to the RobustVideoMatting folder and checkout the onnx branch Modify model/model.py using the comments in the following code Change export_onnx.py to the following code Run the following command to export the ONNX model This command is an example to set input with to 1 9 2 0 x1080 and downsample ratio with 0.25 The file rvm_mobilenetv3_1920x1080_ratio_0.25.onnx will be generated Perform graph surgery The file rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx will be generated This file is used to generate the Cavalry binary model Run the following command to verify that there are no unsupported operators Switch to the master branch to generate dynamic range analysis DRA data Go to the RobustVideoMatting folder and checkout the master branch Modify inference.py using the comments in the following code Run the following command to generate DRA data for converting rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx The DRA data will be generated in the dra_data folder This command is an example to generate DRA data for input 1 9 2 0 x1080 downsample ratio 0.25 The sample video used as the input source can be downloaded from GoogleDrive The Cavalry binary files can be generated with the CNNGen sample package Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_robust_video_matting.bin is in the cnngen output folder out/onnx/demo_networks/onnx_robust_video_matting/ chip chip _cavalry_onnx_robust_video_matting For X86 simulator model desc json file onnx_robust_video_matting.json is in the cnngen output folder out/onnx/demo_networks/onnx_robust_video_matting/out_ build_target _parser/ ades command onnx_robust_video_matting_ades.cmd is in the cnngen output folder out/onnx/demo_networks/onnx_robust_video_matting/ chip chip _ades_onnx_robust_video_matting Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the evaluation kit EVK binary with make For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference Copy files to the SD card on the computer vision CV board The model name must be cavalry_robust_video_matting.bin for read by the test_video_matting demo by default For example place files on the SD card with the following structure Users should prepare the images by themselves background.png is used for the background in below live mode demo src_n.jpg is used for the input in below file mode demo Initialize environment on the CV board CV22 Walnut and imx274_mipi are used as an example If there is no display on the stream or the display is not fluent use a greater value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be expanded by changing the size in reallocate_mem overlay 0 x04000000 Run the demo Stream live mode draw on stream rtsp 0.0.2 Use default parameters With specific alpha thresh background RGB color and blend mode Blend mode 0 green background 1 half green background 2 moving green background 3 green foreground 4 half green foreground 5 half green foreground and background 6 image background With specific model parameters Video output VOUT live mode draw on VOUT HDMI File mode Only JPG image file inputs are placed under the JPEG image file results are generated under This live demo is not ready on CV72 please try file mode first This chapter gives a brief introduction about transformer and explains how to deploy the live demo for it Transformer is a new approach for computer vision CV task comparing with general CNN solution It was commonly used in the natural language processing NLP domain The most significant feature of it is the usage of the “attention” module Transformer has obtained great success in language processing domain and led researchers to adapt it to computer vision tasks Related works includes the following Vision Transformer ViT for image classification This network divides the image into 2 D patches These patches are regarded as words and are input to the encoder of the transformer for analysis DETR for detection which uses the CNN to extract feature The points in the feature map are then regarded as a sequence of words SETR for segmentation It uses the ViT as the encoder of the image and then uses CNN to decode and generate the segmentation result However as the computer vision task has larger scale and higher resolution comparing with word processing the model’s adaptability to different vision task and computation complexity still remains a question Based on these two points SWIN transformer has proposed an hierarchical feature representation and a shifted window based multi-head self-attention SWIN-MSA mechanism The hierarchical feature representation is alike to the general CNN down-sampling procedure this allows the network to be better adapted to the variant scale The SWIN-MSA mechanism could divide the image into non-overlapped windows so that self-attention is calculated in local windows for efficiency The shifted window also introduces cross-window connections to enlarge the model power The SWIN transformer currently support different kinds of vison tasks including Image classification Object detection Semantic segmentation etc For future development of transformer model effectiveness and performance efﬁciency are two important points In current CV field CNN is still the main choice while transformer becomes more and more popular In the future the transformer may replace CNN’s position or the transformer may co-exist with CNN and develop collaboratively with CNN For more information please refer to the paper of SWIN transformer Prepare source code Download pytorch weight Generate ONNX model An ONNX model swin_tiny_patch4_window7_224.onnx will be generated in the current folder Graph surgery The pre-trained model file swin_tiny_patch4_window7_224_modified.onnx is used to generate the Cavalry binary Test environment is Torch 1.8.0 Torchvision 0.9.0 ONNX 1.6.0 Onnxruntime 1.6.0 The Cavalry binary files can be generated with the CNNGen sample package The output is in out/onnx/demo_networks/swin_tiny/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _swin_tiny.bin is in the cnngen output folder out/onnx/demo_networks/onnx_swin_tiny/ chip chip _cavalry_onnx_swin_tiny For X86 simulator model desc json file swin_tiny.json is in the cnngen output folder out/onnx/demo_networks/swin_tiny/out_ build_target _parser/ ades command swin_tiny_ades.cmd is in the cnngen output folder out/onnx/demo_networks/swin_tiny/ chip chip _ades_swin_tiny Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig For live mode currently as this network has not been supported in CFlite Python Inference library please refer to 5 Run C Inference In this example the camera module imx274 and CV2 board are used Copy files to SD card on the CV board Copy the imagenet_1000.txt and swin_tiny_cavalry.bin to EVK for example folder The imagenet_1000.txt is located in the folder path ambarella/unit_test/private/cv_test/imagenet The _cavalry&lt;version&gt; _swin_tiny.bin is in out/onnx/demo_networks/swin_tiny/ _cavalry_swin_tiny/ _cavalry&lt;version&gt; _swin_tiny.bin Run the live demo on the CV board The performance of this network is arround 59.4ms on CV2 platform with default DRA setting Test clock is gclk_ddr 1 5 6 0 0 0 0 0 0 0 Hz gclk_vision 9 1 2 0 0 0 0 0 0 Hz The classification result will be shown on HDMI OSD YOLOv3 is the third version of a popular object detection algorithm YOLO – You Only Look Once The published model recognizes 8 0 different classes in images and videos but most importantly it is super-fast and nearly as accurate as Single Shot MultiBox SSD The following sections demonstrate how to export YOLOv3 YOLOv3 SPP and YOLOv3 Tiny ONNX models from PyTorch and how to handle the ONNX models in the Ambarella CNNGen samples package The ONNX model is exported from a project in Github with GPL v3.0 license The steps are as below Download source code project from here Download pre-trained weight files yolov3.pt yolov3-spp.pt and yolov3-tiny.pt from here and put it under the weights folder in the project directory Install the following dependent Python packages torch and torchvision can be installed by the guidance in onnx 1.6.0 and onnxruntime 1.1.1 are installed in test Use pillow 7.0.0 to avoid package conflict torch 1.4.0 is used in the test If newer version of torch is used the output names may be different from 2 9 0 3 5 2 4 1 4 yolov3 2 9 8 3 6 0 4 2 2 yolov3 spp and 7 5 1 2 7 yolov3 tiny The output names should be updated with the names of the final Conv nodes in the networks Modify the code in the source project models.py detect.py Run the following command under the project directory to export the ONNX models YOLOv3 YOLOv3 SPP YOLOv3 Tiny The ONNX model files are generated under the weights folder Run the following command to do graph surgery on the ONNX models YOLOv3 YOLOv3 SPP YOLOv3 Tiny The following model files which can be converted with Ambarella toolchain are generated yolov3-416x416-out-290-352-414-surgery.onnx yolov3-spp-416x416-out-298-360-422-surgery.onnx yolov3-tiny-416x416-out-75-127-surgery.onnx These files have been put in the CNNGen sample package The steps above are only for reference users need to modify it if needed which may be resulted by different environments such as the input name in step 7 To convert PyTorch to ONNX the above installation may break the environment of CNNGen tool such as graph_surgery.py If users find some problems please use the installation script in CNNGen Toolchain package to recover the environment The Cavalry binary files can be generated with the CNNGen sample package For YOLOv3 generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov3/ For YOLOv3 SPP generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov3_spp/ For YOLOv3 Tiny generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov3_tiny/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_yolov* bin is in the cnngen output folder out/onnx/demo_networks/onnx_yolov chip chip _cavalry_onnx_yolov* For X86 simulator model desc json file onnx_yolov* json is in the cnngen output folder out/onnx/demo_networks/onnx_yolov build_target _parser/ ades command onnx_yolov*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_yolov chip chip _ades_onnx_yolov* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK User should use Cooper Linux SDK package not this CVflow CNNGen Samples package and this application must be included in EVK firmware Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run Above is using live streaming option fsync_off can disable frame sync which means the result may not be applied to the right frame For Vout display please use option dd HDMI and remove fsync_off which is only for streaming In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_yolov3* bin The yolov3* lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_yolov3* json and onnx_yolov3*_ades cmd Please follow below steps to run inference Copy files to SD card for EVK test For example place files on the SD card with the following structure tiny_anchors.txt has the following content inside 1 0 1 4 2 3 2 7 3 7 5 8 8 1 8 2 1 3 5 1 6 9 3 4 4 3 1 9 Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode only for CVflow® performance test The real image is used as an input with the right preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/yolov3/dra_img/bus.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information Users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream without frame sync machine rtsp 0.0.2 For stream live mode option fsync_off disables frame sync If need to enable frame sync users should enable encode dummy in eazyai_video.sh VOUT live mode draw on VOUT HDMI This version of YOLOv5 is a public release from Ultralytics on GitHub The following contents demonstrate how to export YOLOv5 ONNX models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with GNU General Public License v3.0 license The steps are as below Download source code project from here Go to the source project directory and install the required packages Export the ONNX model with the script under the source project directory with opset_version 1 1 Edit export.py Export YOLOv5s Export YOLOv5m Export YOLOv5l Export YOLOv5x Do graph surgery on the ONNX model by cutting off the Detect() layers YOLO layers YOLOv5s YOLOv5m YOLOv5l YOLOv5x If layer_compare.py in the CV toolchain is used under the source project directory the name of the folder utils should be changed because it conflicts with a package name in the CV toolchain The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package For YOLOv5s generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov5s/ For YOLOv5m generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov5m/ For YOLOv5l generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov5l/ For YOLOv5x generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov5x/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_yolov5* bin is in the cnngen output folder out/onnx/demo_networks/onnx_yolov chip chip _cavalry_onnx_yolov5* For X86 simulator model desc json file onnx_yolov5* json is in the cnngen output folder out/onnx/demo_networks/onnx_yolov5 build_target _parser/ ades command onnx_yolov*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_yolov5 chip chip _ades_onnx_yolov5* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as below User should use Cooper Linux SDK package not this CVflow CNNGen Samples package and this application must be included in EVK firmware Build the X86 Simulator Binary with make Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Accuracy Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For EVK Board Refer to 2 CNNGen Conversion for how to generate onnx_yolov5*_cavalry bin The yolov5* lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_yolov5* json and onnx_yolov5*_ades cmd Please follow below steps to run inference Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode for test CVflow® performance The image is used as input with preprocess and postprocess The raw.bin is used as an input without the right preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/yolov5/dra_img/bus.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable Vproc if needed with option default value is cpu For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents This version of YOLOv7 is a public release from Ultralytics on GitHub The following contents demonstrate how to export YOLOv7 open neural network exchange ONNX models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with GNU General Public License version 3 2 9 June 2 0 0 7 The steps are below Download the source code project from here Go to the source project directory and install the required packages Download the weights of standard models and tiny models then place them under the source project directory YOLOv7_tiny YOLOv7 YOLOv7-X YOLOv7-W6 YOLOv7-E6 YOLOv7-D6 YOLOv7-E6E Export the ONNX model with the script under the source project directory with opset_version 1 1 Edit export.py Export YOLOv7_tiny ONNX export success saved as yolov7_tiny.onnx Export YOLOv7 ONNX export success saved as yolov7.onnx If users require other types of YOLOv7 models they can export them to the ONNX model according to the weight such as YOLOv7-X YOLOv7-W6 etc Perform graph surgery on the ONNX model by cutting off the Detect() layers YOLO layers YOLOv7_tiny YOLOv7 When using the AMBA toolchain graph_surgery.py it is necessary to match the onnx version with the toolchain Therefore it is recommended that users use onnx 1.6.0 If layer_compare.py in the CV toolchain is used under the source project directory the name of the folder utils should be changed because it conflicts with the package name in the CV toolchain The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package For YOLOv7_tiny generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov7_tiny/ For YOLOv7 generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolov7/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_yolov* bin is in the cnngen output folder out/onnx/demo_networks/onnx_yolov chip chip _cavalry_onnx_yolov* For X86 simulator model desc json file onnx_yolov* json is in the cnngen output folder out/onnx/demo_networks/onnx_yolov build_target _parser/ ades command onnx_yolov*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_yolov chip chip _ades_onnx_yolov* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as shown below User should use Cooper Linux SDK package not this CVflow CNNGen Samples package and this application must be included in EVK firmware Build X86 Simulator binary with make Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For the EVK board Refer to 2 CNNGen Conversion for information on how to generate onnx_yolov7*_cavalry bin The yolov7* lua is included in the path of the EVK If it does not exist it can be found in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for information on how to generate onnx_yolov7* json and onnx_yolov7*_ades cmd Please follow below steps to run inference Copy files to the secure digital SD card for the EVK test For example place files on the SD card with the following structure Users can find label_coco_80.txt in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users must keep the file path consistent during using File mode For the X86 simulator Run ADES mode The raw.bin is used as an input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess Run Acinference mode The raw.bin is used as an input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess For the EVK board Load Cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode to test CVflow® performance The image is used as an input with preprocess and postprocess The raw.bin is used as an input without the corrcet preprocess or postprocess For the file mode using image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/yolov7/dra_img/bus.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable VProc if required with the option The default value is CPU For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board CV22 Walnut and imx274_mipi are used as examples Run the following Stream live mode draw on stream rtsp 0.0.2 Video output VOUT live mode draw on VOUT high definition multimedia interface HDMI® If there is no display on the stream or the display is not fluent check the following two points If the display is not fluency use a larger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be increased by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For more details refer to the EazyAI Library application programming interface API related content in the Linux software development kit SDK Doxygen documents This version of YOLOv8 is a public release from Ultralytics on GitHub The following contents demonstrate how to export YOLOv8 open neural network exchange ONNX models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX model is exported from a source project in GitHub with GNU AFFERO GENERAL PUBLIC LICENSE Version 3 1 9 November 2 0 0 7 The steps are below Download the source code project from here Set up conda environment to export model to avoid damaging the environment of CNNGen toolchain Export the ONNX model Detection YOLOv8n YOLOv8s YOLOv8m YOLOv8l YOLOv8x Segmentation YOLOv8n YOLOv8s YOLOv8m YOLOv8l YOLOv8x If conda is not used please revert to the CNNGen toolchain related environment after exporting the model The exported model is yolov8 n|s|m|l|x onnx or yolov8 n|s|m|l|x Perform graph surgery on the ONNX model taking yolov8s.onnx as an example The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package For Detection take Yolov8s as an example For Segmentation take Yolov8s-seg as an example Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples n|s|m|l|x _ det|seg For EVK the cavalry binary chip _cavalry version _onnx_yolov* bin is in the cnngen output folder out/onnx/demo_networks/onnx_yolov8 chip chip _cavalry_onnx_yolov8* For X86 simulator model desc json file onnx_yolov* json is in the cnngen output folder out/onnx/demo_networks/onnx_yolov8 build_target _parser/ ades command onnx_yolov8*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_yolov8 chip chip _ades_onnx_yolov8* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build the EVK binary as shown below Build X86 Simulator binary with make Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the x86 binary Then the executable file test_eazyai can be found in cvflow_cnngen_samples_package For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI In the following examples the camera module imx274_mipi and cv22_walnut board are used The test_eazyai is used for the following example refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications 4 EazyAI Inference C Library 5 EazyAI Postprocess C Library 6 EazyAI Unit Test 7 EazyAI Live Application For the EVK board Refer to 2 CNNGen Conversion for information on how to generate onnx_yolov8 n|s|m|l|x _ det|seg _cavalry.bin The yolov8_ det|seg lua is included in the path of the EVK If it does not exist it can be found in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for information on how to generate onnx_yolov8* json and onnx_yolov8*_ades cmd Please follow below steps to run inference Copy files to the secure digital SD card for the EVK test For example place files on the SD card with the following structure Users can find label_coco_80.txt in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs Users must keep the file path consistent during using File mode For the X86 simulator take yolov8s as an example Run ADES mode The raw.bin is used as an input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess Run Acinference mode The raw.bin is used as an input without the preprocess or postprocess The image is used as an input with the correct preprocess and postprocess For the EVK board take yolov8s as an example Load Cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run the following Dummy mode to test CVflow® performance The image is used as an input with preprocess and postprocess The raw.bin is used as an input without the corrcet preprocess or postprocess For the file mode using image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/yolov8/dra_img/bus.jpg in and create as the output directory Option isrc default preprocess is based on OpenCV users can enable VProc if required with the option The default value is CPU For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board The cv22_walnut and imx274_mipi are used as examples Run the following commands take yolov8s as an example Stream live mode draw on stream rtsp 0.0.2 Video output VOUT live mode draw on VOUT high definition multimedia interface HDMI® For Segmentation OpenMP is enabled by default and in lua can be set for the thread number of parallel processing If there is no display on the stream or the display is not fluent check the following points If the display is not fluency use a larger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not large enough it can be increased by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For more details refer to the EazyAI Library application programming interface API related content in the Linux software development kit SDK Doxygen documents YOLOX is an anchor-free version of YOLO with a simpler design but better performance! It aims to bridge the gap between research and industrial communities For more details please refer to GitHub The following contents demonstrate how to export YOLOX ONNX models from the public source project implemented with PyTorch and how to run the ONNX models with the Ambarella CNNGen samples package If the current CNNGen samples package does not include this conversion step contact the Ambarella support team for assistance The ONNX models are exported from the source project in GitHub with Apache-2.0 license The steps are as below Download the source project from here The steps were tested with the following commit Go to the source project directory and install the required packages It s ok if there is an installing error related to the package Download weights of standard models and light models and put them under the source project directory YOLOX-s YOLOX-m YOLOX-l YOLOX-x YOLOX-Darknet53 YOLOX-Nano YOLOX-Tiny Export ONNX models from standard models and light models The reference is this guide Export YOLOX-s Export YOLOX-m Export YOLOX-l Export YOLOX-x Export YOLOX-Darknet53 Export YOLOX-Nano Export YOLOX-Tiny Do a graph surgery on the ONNX models to eliminate unused initializer constantify shapes eliminate deadend and fold constants YOLOX-s YOLOX-m YOLOX-l YOLOX-x YOLOX-Darknet53 YOLOX-Tiny YOLOX-Nano The Cavalry binary files converted from the ONNX models can be generated with the CNNGen sample package For YOLOX-s generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_s/ For YOLOX-m generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_m/ For YOLOX-l generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_l/ For YOLOX-x generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_x/ For YOLOX-Darknet53 generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_darknet/ For YOLOX-Nano generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_nano/ For YOLOX-Tiny generate the Cavalry binary using the following commands The output is in out/onnx/demo_networks/onnx_yolox_tiny/ Current default output data format is float32 and even some netowrk use FP32 as input For CV7x please use ac in command for self-adaption which will switch to FP16 as it does not support FP32 The cnngen output folder is in cvflow_cnngen_samples For EVK the cavalry binary chip _cavalry version _onnx_yolox_* bin is in the cnngen output folder out/onnx/demo_networks/onnx_yolox_ chip chip _cavalry_onnx_yolox_* For X86 simulator model desc json file onnx_yolox_* json is in the cnngen output folder out/onnx/demo_networks/onnx_yolox_ build_target _parser/ ades command onnx_yolox_*_ades cmd is in the cnngen output folder out/onnx/demo_networks/onnx_yolox_ chip chip _ades_onnx_yolox_* Current DRA strategy is default means use mixed fix8 fix16 and FP16(CVflow v3 only) to blance performance and accuracy For best performance please use ds fx8 in command For best accuracy please use ds fx16 or ds fp16 in command The ds fp16 is only for CVflow V3 Please use cvb to enable CVflowbackend convert flow For CV3x please enable CVflowbackend if GVP should be used The python scripts locate at cvflow_cnngen_samples For detailed script usage please refer to 2 EazyAI Python Tools For how to generate a new convert configuration yaml file users can manually modify based on above files also they can use the configuration generation tool in 2.2 EazyAI Configuration Tool Users can use quick dummy convert which is only for performance evaluation without above configuration file For detail please refer to 2.3 EazyAI Convert Tool Build Unit Test for EVK Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI 3 EazyAI Simulator to build x86 binary Then the executable file test_eazyai can be found in SDK For detailed script usage please refer to 2.4 EazyAI Inference Tool and 2.5 CVflow Simple Inference Tool Check if EVK is alive and start CVflow engine for below Dummy and File mode with CVflow Chip For CV2x and CV5x users need to run it at once to boot CVflow engine But for CV7x this command is not MUST have users can use it check if EVK is alive Also it is not needed for the inferecne on Simulator and Original framework Dummy Mode Only For Chip File Mode Above is using CVflow option ip ip_address is needed to find the chip for other model users can remove it For Simulator please useoption p ades and p acinf For Original Framework please useoption p orig Remove iy to let this application run without postprocess Live mode Start CVflow engine and DSP video pipeline Below demo used CV22 Walnut and imx274_mipi as the examples Run For Vout display please use option dd HDMI In the following examples the camera module imx274 and CV22 board are used The test_eazyai is used for the following example please refer to the following referenced chapters for detailed usage of test_eazyai 2 Applications sec_ref_eazyai For EVK Board Refer to 2 CNNGen Conversion for how to generate _cavalry&lt;version&gt; _onnx_yolox_* bin The yolox.lua is included in the path of EVK If it does not exist find it in cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua For X86 Refer to 2 CNNGen Conversion for how to generate onnx_yolox_* json and onnx_yolox_*_ades cmd Please follow below steps to run inference Copy files to SD card for EVK test For example place files on the SD card with the following structure Users can find in cvflow_cnngen_samples/library/eazyai/unit_test/resource/ This file saving method is only an example The file can be placed freely according to the user s needs users need to keep the file path consistent during use File mode For X86 Simulator Run Ades mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess Run Acinference mode The raw.bin is used as input without the preprocess and postprocess The image is used as an input with the right preprocess and postprocess For EVK Board Load cavalry Only CV2x and CV5x need to boot up cavalry manually for other chips users do not need to run this command Run Dummy mode for CVflow® performance test The image is used as an input with the right preprocess and postprocess The raw.bin is used as input without the preprocess and postprocess For the file mode with image as input place the test image such as cvflow_cnngen_samples/onnx/demo_networks/yolox/dra_img/bus.jpg in and create as the output directory For specific parameter information users can enter the command test_eazyai and press entry to view Live mode Initialize the environment on the CV board Use CV22 Walnut and imx274_mipi for examples Run Streams live mode draw on stream rtsp 0.0.2 VOUT live mode draw on VOUT HDMI If there is no display on the stream or the display is not fluency check the following two points If the display is not fluency use bigger value in enc_dummy_latency 4 such as 7 If the overlay buffer size is not enough it can be added by changing the size in reallocate_mem overlay 0 x04000000 The overlay buffer size taken by each feature in ea_display_feature_e is around 1 0 2 4 resolution enc-dummy-latency 5 For details please refer to EazyAI Library API related content in Linux SDK Doxygen documents</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">fs_history_updates</field>
    <field name="url">db/d40/fs_history_updates.html</field>
    <field name="keywords"></field>
    <field name="text">Historical Updates Historical Updates Key Updates History Before 1.8.0 The following revision history table summarizes changes contained in this document SDK Version Updated Date Modification 1.6.2 2 0 2 3 0 7 2 8 • Added Page EazyAI Framework • Added Library Page CVflow Lite API • Updated Section 4 Accuracy Evaluation • Updated Section 1 0 x86 Simulator • Updated most of the exmaples with EazyAI Python tools in Caffe Demos Tensorflow Demos and ONNX Demos • Added Section ByteTrack • Updated Section 1 4 Eazyai Video rename quick_ipc.sh to eazyai_video.sh 1.6.1 2 0 2 3 0 5 1 2 • Added Section 1 CVflow Gen2 for CV2x CV5x • Added Section 2 CVflow Gen3 for CV72 • Added Section sec_ov_how_to_start • Added Section EazyAI Framework • Updated Section 5 EazyAI Unit Test Delete the option class_num and calculate it directly from the label.txt for post-processing • Added Section YOLOv8 • Added Section sub_sec_eazyai_simulator_qs • Added Section 1 6 Cavalry Profiler • Added Section sub_sec_sample_usage • Added Section 1 8 CVflow Layer Profiler • Updated Section 2.2.3 Run Inference Test • Added Section 1 9 CVFlowBackend • Added Section 5.2 CVFlowBackend • Added Section 15.5 Linux SDK CVflow FAQ • Updated Section 1 7 VP Reload and Clock Modification • Updated Section 2.3 Utils Library • Updated Section 6 Performance Table 1.6.0 2 0 2 3 0 3 1 6 • Add support for CV72 • Add Section sub_sec_ov_intro_gen3 • Updated Section sec_ov_how_to_start • Updated Section 3 Environment Setting • Add Section 10.4 CV72 Limitation • Updated Page fs_cnngen_samples_package • Updated Section 5 Quick Shell Script • Updated Section 4 CNNGUI • Updated Section Stream overlay buffer size of live mode calculation method reference eazyai_cvflow_components_display • Added Section sub_sec_eazyai_simulator_qs • Added Section YOLOv7 • Updated Section Light Yolo • Updated Section sub_sec_caffe_yolo_v2_yolov3_demo • Updated Section 4 CNNGUI • Updated Section 5 Quick Shell Script • Added Section 6.7 cvflow_perf_eval.py • Updated Section 13.4 Register Custom Operator • Updated Section sub_sec_eazyai_simulator_qs • Updated Section sub_sec_light_yolo_run_inference • Updated Section sub_sec_caffe_qrcode_run_inference • Updated Section sub_sec_caffe_yolo_v3_sp50_full_run_inference • Updated Section sub_sec_onnx_yolov3_run_inference 1.5.3 2 0 2 2 0 9 1 6 • Added support for awesome CSS theme in doxygen document • Added Chapter fs_cnngen_lib which includes EazyAI Simulator and Data Process • Added Section 2 Applications • Added Section 3 Libraries • Added Section Robust Video Matting • Added Section HF-Net • Added Section QR Code • Added Section 7.4 OpenMP for Post Processing • Added Section 4.2 Convert YUV to RGB • Removed Chapter Blade Runner Liveness Demo • Updated Section Face Detection and Blur Function • Updated Section RetinaFace • Updated Section FairMOT • Updated Section Bodypix • Updated Section YOLOX • Updated Section YOLOv5 • Updated Section YOLOv3 • Updated Section RetinaFace • Updated Section LFFD • Updated Section Face Detection and Blur Function • Updated Section Regression for DeepLabv3 • Updated Section CenterNet • Updated Section SSD MobileNetV1 • Updated Section Mobilenetv2_NV12 • Updated Section CGPP Deployment for Yolov3 • Updated Section DDRNet • Updated Section sec_framework_select • Updated Section 6.2 Network Cost • Updated Section 6.4 Cache • Added Section 6.10 Network Bandwidth • Updated Section 5 Quick Shell Script • Added Section 1 0 x86 Simulator • Update Section 4 CNNGUI 1.5.2 2 0 2 2 0 4 1 5 • Added Section SWIN Transformer • Updated Section 7.2 Batch for Vproc and NNCtrl • Updated Section 6 Performance Table 1.5.1 2 0 2 2 0 3 1 0 • Updated Section 3 Converter • Updated Section 1 2 layer_compare.py • Updated Section 15.1 UG CNNGen User Guide • Updated Section sub_sec_deploy_call_seq • Updated Section 5.3 Alignment • Added Section 9.1 Cavalry Log • Updated Section 10.1 Primitive Code • Updated Section 1 8 VProc Performance • Updated Section 6 Performance Table • Updated Section 2 Layer_compare • Updated Section subsec_prepare_mode_graph_description_file 1.5.0 2 0 2 1 1 2 1 0 • Updated Section Car License Plate Detection and Recognition • Updated Section Bodypix • Updated Section Light Yolo • Updated Section RetinaFace • Updated Section 1 6 Cavalry Profiler • Updated Section 1 4 Eazyai Video • Updated Section SSD MobileNetV1 • Updated Section 6 Performance Table • Refine CV2x to CVflow in the whole documents and add CV5x related contents • Add a new search engine • Updated Section 4 CNNGUI • Updated Section 1 0 Compatibility • Added Section 6.9 CV5x DRAM Limitation • Added Section 1 9 DRAM Bandwidth Statistics • Added Section YOLOX Since CVflow® CNNGen Samples 1.5 it will be independent with CV2x and CV5x Linux SDK package Users may use different SDK versions or different chips so there may be some new features in this document that the old or different SDK cannot support If so please refer to above key updates or contact the Ambarella support team for assistance CV2x Key Updates History The following revision history table summarizes changes contained in this document SDK Version Updated Date Modification 3.0.7 2 0 2 1 1 0 0 8 • Updated Section sub_sec_deploy_ssd_app • Updated Section 1 4 Eazyai Video • Updated Section sub_sec_caffe_mobilenetv1_ssd_run_with_live_mode • Added Section DDRNet • Added Section 8.5 Tensorflow 2.x • Updated Section 7 Accuracy Table • Added Section 1 8 VProc Performance • Added Section sub_sec_tf_remove_node • Added Section LSTM Warp-CTC with One Shot • Updated Section 4 CNNGUI 3.0.6 2 0 2 1 0 7 2 8 • Updated Section Face Recognition • Updated Section MTCNN • Updated Section LFFD • Updated Section CenterNet • Updated Section FairMOT • Updated Section RetinaFace • Updated Section YOLOv5 • Updated Section Regression for DeepLabv3 • Added Chapter Blade Runner Liveness Demo • Added Section sub_sec_compare_classification_network_result • Updated Section 3 Environment Setting • Added Section 1 7 Pruning Test Tools • Updated Section 1 1 NN Model Protection • Updated Section 1 2 Unit License Protection • Updated Section 13.1 Manually Resume • Added Section 13.2 How to Handle Big DAG • Added Section 1 7 VP Reload and Clock Modification • Updated Section 6 Performance Table • Updated Section sec_cnngen_sample_network • Updated Section 4 CNNGUI • Added Section 1 4 Eazyai Video • Updated Section 6.1 DRA Version History • Updated Section 7.2 Batch for Vproc and NNCtrl • Updated Section 6.8 Auto Recycle • Added Section 4.1 Rotate • Added Section Audio with DS_CNN_KWS • Updated Section Car License Plate Detection and Recognition • Updated Section 10.3 SDK API 3.0.5 2 0 2 1 0 4 2 0 • Added Section Hand Landmark • Added Section 4 Input Process • Added Section sec_ov_how_to_start • Updated Section 1 3 VP Task Priority • Added Section 13.1 Manually Resume 3.0.4 2 0 2 1 0 3 2 0 • Added Section 1 6 Cavalry Profiler • Updated Section 1 4 Eazyai Video • Added Section 7.3 Crop and Resize • Updated Section sec_deploy_eazyai • Added Section FairMOT • Added Section 4 CNNGUI • Added Section 11.3 Mismatch Between SDK And Cavalry_gen • Updated Section 6.1 AMA • Updated Section sub_sec_sample_usage • Updated Section 4 Data Format • Updated Section 5.1 Random • Updated Section 2 caffeparser.py • Added Section 3 Post Process • Added Section 6.8 Auto Recycle 3.0.3 2 0 2 1 0 1 2 0 • Updated Section Work Flow • Updated Section 3 Environment Setting • Updated Section 1 4 eval_surgery.py • Updated Section sec_deploy_file_mode • Updated Section 5.3 Alignment • Updated Section sub_sec_sample_toolchain_config • Updated Section 2 Pre-Process Model • Updated Section 6.6 CV2 High Address DRAM • Updated Section 6.7 Print Cavalry Memory Summary • Updated Section 6.1 AMA • Added Section Bodypix • Added Section 1 4 Eazyai Video • Updated Section 1 5 HDMI YUV Input • Updated Section 5 DRA List Generation 3.0.2 2 0 2 0 1 1 2 0 • First Doxygen release for CNNGen Development • Refine all the DSP cases to use eazyai_video.sh • Added Section fs_deep_learning_dg • Added Section 8.5 Tensorflow 2.x • Added Section 13.3 Replace Subgraph • Added Section 16.2 keras_converter • Added Section 16.3 torch2onnx • Updated Section 5 Output Process • Added Section 6.3 CMA Allocate Fail • Added Section 6.4 Cache • Added Section 6.5 DVI Memory Protection • Added Section 8.1 Frame Buffer on VOUT • Added Section 8.2 Blur on Stream • Added Section 9.2 VAS Problems • updated Section sec_deploy_eazyai • updated Section 1 1 NN Model Protection • Updated Section 6 Performance Table • Updated Section sec_cnngen_sample_network • Updated Section 7 Run on Board • Updated Section sub_sec_caffe_mobilenetv1_ssd_run_with_file_mode • Updated Section sub_sec_tf_mobilenetv2_ssd_run_with_file_mode • Updated Section MTCNN • Updated Section Regression for DeepLabv3 • Updated Section onnx_retinaface_build_evk_binary • Updated Section onnx_retinaface_run_on_the_board • Added Chapter fs_tools_demos • Added Chapter YOLOv5 • Updated Section Car License Plate Detection and Recognition • Added Section Light Yolo 3.0 2 0 2 0 0 8 2 8 • Added Section sec_framework_select • Added Section 8.4 NCHW Model • Updated Section 11.1 Compatibility • Updated Section 1 2 layer_compare.py • Updated Section 1 3 Pre-Process Model Tools • Added Section 15.4 UG Dynamic Range Analysis • Updated Section sub_sec_deploy_call_seq • Updated Section sub_sec_deploy_ssd_app • Updated Section 7 Performance Optimization • Updated Section 8 Show the Result • Updated Section 1 0 Compatibility • Added Section sec_deploy_eazyai • Added Section 1 1 NN Model Protection • Added Section 1 2 Unit License Protection • Added Section 1 3 VP Task Priority • Updated Chapter fs_cnngen_samples_package • Updated Section sub_sec_caffe_mobilenetv1_ssd_run_with_file_mode • Updated Section sub_sec_tf_mobilenetv2_ssd_run_with_live_mode • Updated Section 3 Build EVK Binary • Updated Section caffe_mtcnn_run_on_the_board • Updated Section 3 Convert and Compile • Updated Section sub_sec_fd_blur_build_evk_binary • Updated Section sub_sec_fd_blur_run_on_board • Updated Section 6 DSP Blur Function • Updated Section Face Recognition • Updated Section 3 Build EVK Binary • Updated Section caffe_face_landmarks_68_run_on_the_board • Updated Section onnx_lffd_build_evk_binary • Updated Section onnx_lffd_run_on_the_board • Updated Section onnx_centernet_build_evk_binary • Updated Section onnx_centernet_run_on_the_board • Added Section RetinaFace • Added Chapter PVANET Demo 2.5.8 2 0 2 0 0 6 2 4 • Updated Section sec_accuarcy_compile • Updated Section sec_accuarcy_workflow • Added Section CenterNet • Updated Section sub_sec_caffe_mobilenetv1_ssd_run_with_file_mode • Updated Section sub_sec_tf_mobilenetv2_ssd_run_with_file_mode 2.5.7 2 0 2 0 0 5 2 0 • Updated Section 6 Parsers • Added Section 6.1 DRA Version History • Updated Section 8.3 TFlite • Updated Section 7 Accuracy Table • Updated Section 5 Quick Shell Script • Updated Section 6 Performance Table • Updated Section 2 Generate Cavalry Binary • Updated Section onnx_lffd_run_on_the_board • Updated Section Car License Plate Detection and Recognition • Added Section sec_tf_model_with_nv12_input 2.5.6 2 0 2 0 0 3 2 6 • Updated Section 6 Parsers • Updated Section 8.2 Input Transpose • Added Section 8.3 TFlite • Updated Section 1 0 x86 Simulator • Updated Section 1 3 Pre-Process Model Tools • Updated Section 5 Output Process • Added Section 9 VP Hang Debug • Added Section 1 0 Compatibility • Updated Section 3 Build EVK Binary • Updated Section Face Detection and Blur Function • Updated Section 3 Build EVK Binary • Updated Section caffe_mobilefacenets_run_v1_on_the_board • Updated Section 3 Build EVK Binary • Updated Section caffe_face_landmarks_68_run_on_the_board • Added Section ResNet50 with Json Pre-Processing • Added Section ResNet50 with DRAv3 • Added Section YOLOv3 • Added Section LFFD • Added Section Car License Plate Detection and Recognition 2.5.5 2 0 2 0 0 2 1 4 • Updated Section sub_sec_sample_tool_fraw_yolov3 • Updated Section sub_sec_sample_tool_draw_ssd • Added Section Face Detection and Blur Function • Added Section Face Recognition • Added Section Face Alignment • Added Section Regression for DeepLabv3 2.5.3 2 0 1 9 1 1 1 5 • Updated Section 6 Parsers • Updated Section 1 2 layer_compare.py • Updated Section 1 3 Pre-Process Model Tools • Added Section sub_sec_sample_best_perf • Added Section sub_sec_sample_dra_set • Updated Section 5 Quick Shell Script Deleted old Section 7.2 • Added Section CGPP Deployment for Yolov3 2.5.4 2 0 1 9 1 2 2 0 • Updated Section 8 TensorFlow Parser • Updated Section SSD Mobilenetv2 2.5.2 2 0 1 9 1 0 2 5 • Updated Section 6 Parsers • Added Section 7 Accuracy Table • Updated Chapter fs_cnngen_samples_package • Updated Section 6 Performance Table • Added Section Deployment for Quantization Retrained Yolov3 2.5.1 2 0 1 9 0 9 2 7 • Updated Section 6 Performance Table • Updated Section 4 SDK Preparation 2.5 2 0 1 9 0 8 1 6 • Revised for the CV chip family • Updated Section 5 Quick Shell Script • Added Section LSTM Warp-CTC</field>
  </doc>
  <doc>
    <field name="type">page</field>
    <field name="name">index</field>
    <field name="url">index.html</field>
    <field name="keywords"></field>
    <field name="text">CVflow CNNGen Developmen The docuement of CVflow CNNGen Development provides detailed information of NN convert with CNNGen toolchain and NN deployment with Cooper SDK for the CVflow® Vector Processor VP a key vision processing module for executing neural networks efficiently in Ambarella’s family of computer vision CV chips Key Updates The following revision history table summarizes changes contained in this document SDK Version Updated Date Modification 1.8 2 0 2 3 1 1 0 2 • Refine the whole structure of this documents • Remove Section for Makefile system • Added Chapter Quick Start • Added Section 2.1 EazyAI Helper Tool • Update section 2.4.2 Enbale GPU For Simulator • Added Section 2.7 CVflow Layer Profiler Tool • Update section 6 Parsers • Update section 1 0 x86 Simulator The SDK versions listed in the table reflects the current SDK version x.x or patch release x.x.x A patch number that is not sequential indicates that the previous patch release did not apply to this document Users may use different SDK versions so there may be some new features in this document that the old SDK cannot support If so please refer to above key updates or ask Ambarella support team for assistance For the old SDK update history please refer to fs_hist_updates License Copyright c 2 0 2 0 Ambarella International LP This file and its contents are protected by intellectual property rights including without limitation U.S and/or foreign copyrights This Software is also the confidential and proprietary information of Ambarella International LP and its licensors You may not use reproduce disclose distribute modify or otherwise prepare derivative works of this Software or any portion thereof except pursuant to a signed license agreement or nondisclosure agreement with Ambarella International LP or its authorized affiliates In the absence of such an agreement you agree to promptly notify and return this Software to Ambarella International LP This file includes sample code and is only for internal testing and evaluation If you distribute this sample code whether in source object or binary code form it will be without any warranty or indemnity protection from Ambarella International LP or its affiliates THIS SOFTWARE IS PROVIDED AND ANY EXPRESS OR IMPLIED WARRANTIES INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF NON-INFRINGEMENT MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL AMBARELLA INTERNATIONAL LP OR ITS AFFILIATES BE LIABLE FOR ANY DIRECT INDIRECT INCIDENTAL SPECIAL EXEMPLARY OR CONSEQUENTIAL DAMAGES INCLUDING BUT NOT LIMITED TO PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES LOSS OF USE DATA OR PROFITS COMPUTER FAILURE OR MALFUNCTION OR BUSINESS INTERRUPTION HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY WHETHER IN CONTRACT STRICT LIABILITY OR TORT INCLUDING NEGLIGENCE OR OTHERWISE ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE</field>
  </doc>
</add>
