<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: Deployment</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d7/d53/fs_deployment.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Deployment </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The following illustrates the deployment process.</p>
<div class="image">
<img src="../../development_flow.jpg" alt=""/>
<div class="caption">
Deployment Flow</div></div>
   <hr  />
<h1><a class="anchor" id="sec_deploy_evk_binary"></a>
1 Prepare the Binary</h1>
<p>Users must prepare a binary that can run Cavalry. This section describes the basic steps to prepare the binary.</p>
<h2><a class="anchor" id="sub_sec_deploy_full_sdk"></a>
1.1 Full Software Development Kit (SDK)</h2>
<p>Compile the binary as follows: </p><pre class="fragment">build $ cd boards/cv*_*nut
build $ make sync_build_mkcfg
build $ make cv*_ipcam_config
build $ make -j 8
</pre><h2><a class="anchor" id="sub_sec_deploy_eva_sdk"></a>
1.2 Evaluation Software Development Kit (SDK)</h2>
<p>Use the binary in the SDK package. For more details, refer to the <em>Ambarella CV UG Flexible Linux EVK Getting Started Guide</em> document.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_applications"></a>
2 Applications</h1>
<h2><a class="anchor" id="sub_sec_deploy_app_location"></a>
2.1 Location</h2>
<p>Different types of demos are divided into the following four folders. Users can find the required demo code in the corresponding folder. </p><pre class="fragment">&lt;Linux SDK Package&gt;/ambarella
    |--unit_test        # Only can run on CVflow chip
    |  |--private
    |     |--cv_test    # Basic CVflow unit test
    |--app              # Only can run on CVflow chip
    |  |--ai_cam
    |     |--cvflow     # CVflow application with pure cvflow APIs of Vproc and NNCTrl
    |     |--cv0        # ARM Neon accelerator
    |--packages
    |  |--eazyai
    |     |--unit_test  # Basic CVflow unit test with easy deployment APIs of EazyAI
    |                   # Can run file mode both on CVflow chip and x86 Simulator
    |                   # Only can run dummy and live mode on CVflow chip
    |                   # It is included both in Linux SDK package and CVflow CNNGen Samples package
    |     |--apps       # complex EazyAI CVflow applications, only can run on CVflow chips
</pre><p>For EazyAI-related information, refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html">EazyAI Library API</a>.</p>
<h2><a class="anchor" id="sub_sec_deploy_file_mode"></a>
2.2 File Mode</h2>
<p>Images must be processed for different networks. The processing methods correspond with their network training inputs, with the following considerations:</p>
<ul>
<li>Input type, channel number, RGB, BGR, YUV, and more</li>
<li>Meaning, normalization, or non-normalization</li>
</ul>
<h3><a class="anchor" id="subb_sec_deploy_in_file"></a>
2.2.1 Prepare a Test Input with Image</h3>
<p>Use <code>gen_image_list.py</code> to prepare the test input binary. For more details, refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_dra_list_gen">5 DRA List Generation</a>, which provides an example similar to the following which uses You only look once (YOLO) real-time object detection system. </p><pre class="fragment">build $ gen_image_list.py -f test_image/ -o img_dra_list.txt -ns -e jpg -c 0 -d 0,0 -r 416,416 -bf dra_bin/ -bo dra_list.txt
</pre><h3><a class="anchor" id="subb_sec_deploy_in_live"></a>
2.2.2 Prepare Test Input with Sensor</h3>
<p>Use the <b>VProc</b> library to prepare the test input with the sensor. </p><pre class="fragment">board # eazyai_video.sh --stream_A 4K --hdmi 4K --enc_dummy_latency 4 --reallocate_mem overlay,0x01600000 --vsrc_mode 3840x2160
board # test_yuvcap -b 0 -Y -f /tmp/chan0_canvas0_4K_60frame_NV12.yuv -F 2 -r 1
board # eazyai_video.sh --idle
</pre><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li><code>-b --buffer</code> select canvas buffer id</li>
<li><code>-Y --yuv</code> capture YUV data from source buffer</li>
<li><code>-f --filename [?.yuv]</code> filename to store output yuv</li>
<li><code>-F --format [0~6]</code> YUV420 data format for encode buffer, 0: IYUV(I420), 1: YV12, 2: NV12, 3:YU16, 4:YV16, 5:NV16, 6:YUV444. Default is IYUV format</li>
<li><code>-r --frames [0|1~n]</code> frame counts to capture, 0: countless frames</li>
</ol>
</dd></dl>
<ol type="1">
<li>Convert 4K NV12 to 4K RGB. <pre class="fragment"> board # test_vproc_yuv2rgb -b /usr/local/vproc/vproc.bin -i 4k_canvas0_3840x2160.yuv -o 4k.rgb –s 3840x2160 -r 0
</pre></li>
<li>Resize the 4K RGB to 1080p RGB. <pre class="fragment"> board # test_vproc_resize -b /usr/local/vproc/vproc.bin -i 4k.rgb –s 3840x2160x3 -o 416_416.rgb –S 416x416x3 -c 0
</pre></li>
</ol>
<p>For more details, refer to the <em>VProc Doxygen</em> document in the Linux SDK package.</p>
<h3><a class="anchor" id="subb_sec_deploy_run_inference"></a>
2.2.3 Run Inference Test</h3>
<p>At present, there are two basic unit tests: <code>test_nnctrl</code> and <code>test_eazyai</code>. <code>test_eazyai</code> is a further encapsulation of <code>test_nnctrl</code>, which is more simple and convenient to use. This test can support the x86 simulator.</p>
<dl class="section note"><dt>Note</dt><dd>Do not run the test in serial port with lots of print messages, which will result unstable system and bad performance.</dd></dl>
<p>An example using the command is shown below. Flash the binary to the evaluation kit (EVK) board and run as follows:</p>
<ol type="1">
<li>Load Cavalry, only for the EVK board (the x86 simulator does not require this step). <pre class="fragment">  board # modprobe cavalry
  board # cavalry_load -f /lib/firmware/cavalry.bin -r
</pre></li>
<li>Run Inference with <code>test_nnctrl</code>.<ol type="a">
<li>Dummy mode, only for CVflow performance test.<ol type="i">
<li>Show each Arm and CVflow run time. <pre class="fragment">board # test_nnctrl -b &lt;cavalry_gen_generated.bin&gt; -e -c --in &lt;input_0_layer_name&gt; -t &lt;vp running times, 0: infinite loop&gt;
</pre></li>
<li>Show average Arm and CVflow run time. <pre class="fragment">board # test_nnctrl -b &lt;cavalry_gen_generated.bin&gt; --in &lt;input_0_layer_name&gt; -t &lt;vp running times, 0: infinite loop&gt; \
    --print-interval &lt;Used together with `-t`, printing once every x cycles, and cannot exceed `-t` when `-t` is not 0.&gt;
</pre></li>
</ol>
</li>
<li><p class="startli">The image is used as an input with the correct preprocess and postprocess. </p><pre class="fragment">   board # test_nnctrl -b &lt;cavalry_gen_generated.bin&gt; --in &lt;input_0_layer_name&gt;=&lt;input_0.bin&gt; --in &lt;input_1_layer_name&gt;=&lt;input_1.bin&gt; --out &lt;output_0_layer_name&gt;=&lt;output_0.bin&gt; --out &lt;output_1_layer_name&gt;=&lt;output_1.bin&gt; -v -e
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li><code>-b cavalry_gen_generated.bin</code>: this *.bin is generated by <em>cavalry_gen</em> in the CNNGen tools.</li>
<li>For <code>--in</code> and <code>--out</code>, users should know the first and last output names. This information can be found in "cavalry_gen -v" when generating the "cavalry.bin". The input binary is the image, and the image output is saved to the analysis. The network output will also be generated with 32-byte or 128-byte alignment in dynamic random access memory (DRAM), which is decided by different chips. When saving to the binary, <code>test_nnctrl</code> will <b>remove the padding</b>. For more details, refer to <a class="el" href="../../d7/d53/fs_deployment.html#sub_sec_deploy_out_align">5.3 Alignment</a>.</li>
<li><code>-v</code> prints the debug messages</li>
<li><code>--dump</code> saves all the outputs in <code>/tmp</code>. It cannot change the current patch. If required, users can modify the code manually.</li>
<li><code>-e</code>, prints the inference time: the <b>vp_time</b> and <b>arm_time</b>, <b>arm_time</b> will include the <b>vp_time</b> and API call time.</li>
<li>The arm_time is found by adding vp_time and application programming interface (API) call time.</li>
<li>The total time is arm_time, and the pure vector processor (VP) time is vp_time.</li>
<li><code>-c</code> mmap cache memory. The default is noncache.</li>
<li><code>t --iter</code> Iteration of run network, loop forever (0xffffffff) if set 0. Default is 1.</li>
<li><p class="startli"><code>--print-interval</code> shows average Arm and CVflow run time. <code>--print-interval</code> option is depends on option <code>-t</code>.</p>
<p class="startli">a. <code>--print-interval</code> should smaller than total iteration times. e.g. </p><pre class="fragment">board # test_nnctrl -b cavalry.bin --in data -t 50 --print-interval 10 (ok)
board # test_nnctrl -b cavalry.bin --in data -t 0 --print-interval 10  (ok)
board # test_nnctrl -b cavalry.bin --in data -t 10 --print-interval 50 (error)
</pre><p class="startli">b. <code>--print-interval</code> option support MODE_SERIAL mode (run_mode = 0) only. e.g. </p><pre class="fragment">board # test_nnctrl -b cavalry.bin --in data -t 0 --print-interval 10 -m 0 (ok)
board # test_nnctrl -b cavalry.bin --in data -t 0 --print-interval 10 -m 1 (error)
</pre></li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li><p class="startli">Run Inference with <code>test_eazyai</code></p>
<p class="startli">Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a> for the detailed information.</p>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_deploy_live_mode"></a>
2.3 Live Mode</h2>
<h3><a class="anchor" id="subb_sec_deploy_call_seq"></a>
2.3.1 Calling Sequence</h3>
<p>The following diagram illustrates the calling sequence for the custom node APIs.</p>
<div class="image">
<img src="../../live_demo_api_flow.jpg" alt=""/>
<div class="caption">
Figure 2-1. Custom Node API Calling Sequence.</div></div>
<p><b>VProc</b> is a processing library based on the Cavalry driver, which utilizes the CVflow VP to perform the following functions:</p>
<ul>
<li>YUV image to RGB image</li>
<li>Image resizing</li>
<li>Mean value subtraction</li>
<li>Data convert</li>
<li>Region of interest (ROI) crop</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Some users may find that the VProc resize results are different from those resulting from open source libraries. There are different resize methods in this API.<ul>
<li>For the same method in OpenCV, users can use “RESIZE_SIN_STEP” to match the result between Vproc and OpenCV.</li>
<li>“RESIZE_MUL_STEPS” has minor differences from “RESIZE_SIN_STEP”. If users do not care about this difference, they can ignore this flag and use the default.</li>
</ul>
</li>
<li>Both methods use a bilinear interpolation method, but with different strategies to approach the targeted resize ratio. For example, assuming that there is a request to resize an image from 1920x1080 to 640x480:<ul>
<li>“RESIZE_SIN_STEP” uses a one-shot way directly to handle the resize, as OpenCV does.</li>
<li>“RESIZE_MUL_STEPS” proceeds through a few steps, such as 1920x1080 -&gt; 960x540 -&gt; 640x480 in this case.</li>
</ul>
</li>
<li>They are designed to meet different requirements for different stages. Generally, the difference between these two methods are minor and users can choose one of them. For additional information on VProc, refer to the <em>VProc Library in SDK Doxygen document</em>.<ul>
<li><b>NNCtrl</b> is used to load the library with the Cavalry driver. For more details about NNCtrl, refer to <em>NNCtrl library in SDK</em> Doxygen document.</li>
<li><b>Cavalry_mem</b> is used to configure CV memory. For more details, refer to <em>Cavalry Memory Library in SDK Doxygen document</em>.</li>
</ul>
</li>
<li>The <b>Cavalry</b> driver for CV’s VP includes a set of Linux device drivers that encapsulate the lower-level complexities of the CV’s VP functionalities. The drivers conform to the standard Linux driver model, exposing a series of APIs that can be invoked by input / output control (<b>IOCTL()</b>) system calls. These APIs are used by applications to configure and control the CV VP in detail. For more details, refer to <em>Cavalry Driver in SDK Doxygen document</em>.</li>
<li>The image audio video (<b>IAV</b>) driver includes a set of Linux device drivers that encapsulate the lower-level complexities of the CV digital signal processor (DSP) core functionalities. The drivers conform to the standard Linux driver model, exposing a series of APIs that can be invoked by <b>IOCTL()</b> system calls. These APIs are used by applications to configure and control the CV system on chip (SoC) in detail, including controls of image, audio, and video (audio control is new in this version). For more details, refer to <em>IAV Driver in SDK Doxygen document</em>.</li>
</ul>
</dd></dl>
<p>For information on the continuous memory allocator**(CMA) memory**, refer to <em>Ambarella CV DG Flexible Linux SDK CMA Driver</em>.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>When deploying a network, if the network output memory will be accessed by the application, the <b>cache enable</b> will include 10x its normal speed.</li>
<li>When DSP boots up, it competes for bandwidth with VP. As the DSP has a higher priority than VP, the VP performance will decrease.</li>
<li>To unify the interface in user applications between CV5x and CV2x, since CV5x SDK 0.5 and CV2x SDK 3.0.9, IAV_INFO_CANVAS is used to query the YUV buffer base.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="subb_sec_deploy_test_nnctrl_live"></a>
2.3.2 test_nnctrl_live</h3>
<p>This application is used for all common networks with only VP tasks. It pre-processes the input, sending the final results to an on-screen display (OSD) server.</p>
<p>Flash the binary to the EVK board and run as follows: </p><pre class="fragment">    board # test_nnctrl_live

    test_nnctrl_live usage:

    Data Flow: Buffer_YUV-&gt;YUV2RGB-&gt;Resize-&gt;Sub_mean-&gt;Scale-&gt;Neural_Network-&gt;Socket_Send


    -b --cavalry-bin    Network binary, generated by cavalry_gen
    --in                Input port name, initial network input, specify after network
    --out               Output port name,
                        for multi output network, output order is command order, specify after network
    -p --socket-port    Set socket port for output data, default is 27182
    -s --source         Set live input data source = canvas_buffer/pyramid_buffer,
                        0=canvas_buffer, 1=pyramid_buffer
    -i --source-id      Set live input data canvas/pyramid id,
                        canvas_buffer 0:Main/1:Second/2:Thrid/3:Fourth/4:Fifth,
                        pyramid layer 0/1/2/3/4/5
    -t --rgb-type       YUB2RGB output data color space type, 0=RGB, 1=BGR, default is BGR
    -m --submean-bin    Path to Sub_mean mean file, data format should be fp16,
                        with/without 32byte padding
    -r --scale-ratio    Scale ratio after submean, default is 0.00390625
    -n --no-desc        Do not send data description before first frame output data,
                        description format is total_out_num(uint32_t)/len_out1_name(uint32_t)/
                        out1_name(char *)/out1_data_desc(struct data_desc)/
                        len_out2_name/out2_name/out2_data_desc/...
    -h --help           print help info

    Example:
        test_nnctrl_live -p 27182 -b cavalry_gen_generated.bin --in data --out conv59 --out conv67 --out conv75 -s 0 -i 0 -t 0 -m mean.bin -r 0.00390625
</pre><p>The live mode data flow is described as below: </p><pre class="fragment">Buffer_YUV-&gt;YUV2RGB-&gt;Resize-&gt;Sub_mean(Optional)-&gt;Scale(Optional)-&gt;-&gt;Neural_Network-&gt;Socket_send-&gt;OSD_server_display
</pre><p>Certain steps may be unnecessary if the user already has the submean and scale in the Caffe Model. Convert the output data format of the resize to the neural network's input data format using no option “-m” and “-r”.</p>
<p>For a detailed example, refer to Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_yolo_v2_deploy_with_live_mode">4 Deploy with Live Mode</a>.</p>
<p>The common applications are described as follows:</p>
<ul>
<li><b>Buffer_YUV</b>: enables the user to specify the input data from canvas / pyramid buffers with “-s”, and sets the input buffer ID or pyramid layer ID with “-i”.</li>
<li><b>YUV2RGB</b>: supports RGB and BGR color space output. The default is BGR for Caffe models; users can change the default using the “-t” option. “-v” prints a debug message.</li>
<li><b>Resize</b>: data input size is the same as the YUV2RGB output; data output size information is derived from cavalry.bin.</li>
<li><b>Sub_mean</b> (Optional): data input is FP16. The data source comes from the data format conversion and after resizing, <b>vproc_scale_ext()</b> converts FIX8 to FP16. Users can remove the data format conversion if there is no data overflow after the submean and scale. The data format of the mean file should be FP16, and in the same dimension as the NN's input dimension. If the input of the NN is the BGR color space, the mean file should also be the BGR color space.</li>
<li><b>Scale</b> (Optional): data input is FP16 from the submean; output data is for the NN's input. If the output data format is different than the input data format, input data is converted to the output data format. The scale factor can be '1' if users do not want to change the input data range. Note that there can be a data overflow if the NN's input data format represents a lower range compared to the input data format.</li>
<li><b>Neural_network</b>: the input is from scale, and the network is from cavalry.bin. Execute on the VP; users can specify the cavalry.bin generated by the CNNgen tool named cavalry_gen. Then, users can set the input and output name, which should be the same as Caffe model’s layer name. Because the output is sent to a socket server for a parser in the osd_server, if there is more than one, the order of the output will be the same as the order of the parameters.</li>
<li><b>Socket_send</b>: NN output is sent to osd_server with a specified socket port; "–p" option can specify the socket port, which should be the same port in osd_server.</li>
<li><b>OSD_server</b>: receives data from the NN and parses it. Draws an overlay on a screen connected by a high-definition multimedia interface (HDMI®) cable. This enables the users to write their own OSD server for the network.<ul>
<li><em>osd_server_yolov2.cpp</em> for Yolov2 original mode in the CNNGen Samples Package, for the model trained by the user. This is different from the original model, and some code should be modified.</li>
<li><em>osd_server_yolov3.c</em> for Yolov3, Tiny_Yolov3 and Yolov5 original mode in the CNNGen Samples Package, for the model trained by the user. As this is also different from the original model, some code should be modified.</li>
<li><em>osd_server_imagenet.cpp</em> for any classification networks.</li>
</ul>
</li>
</ul>
<h3><a class="anchor" id="subb_sec_deploy_test_eazyai"></a>
2.3.3 test_eazyai</h3>
<p>This application is a unify application to run different networks in live mode.</p>
<p>For more detail, refer to Section <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a>.</p>
<h3><a class="anchor" id="subb_sec_deploy_pvanet_app"></a>
2.3.4 PVANET Application</h3>
<p>The <em>test_pvanet_live.cpp</em> is more complex as it requires coordination between <b>Arm</b> and <b>VP</b> tasks. For more details, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#fs_caffe_pvanet">PVANET Demo</a>.</p>
<h2><a class="anchor" id="sub_sec_deploy_easy_add"></a>
2.4 Add New Network</h2>
<p>The EazyAI unit test enables users to add support for file and live mode of a new network easily without modifying the unit test code. For more detail, refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a>.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_libraries"></a>
3 Libraries</h1>
<p>This chapter mainly introduces some libraries required for running the model, mainly including VProc, NNCtrl, and EazyAI.</p>
<h2><a class="anchor" id="sub_sec_deploy_vproc"></a>
3.1 Vproc Library</h2>
<p>VProc stands for CVflow VP processing, which is desinged to manage some operations or mathematical calculations whose performance could be boosted by utilizing Ambarella CVflow chips. Most heavy calculations are carried out on the CVflow VP.</p>
<p>The VProc library is based on the Cavalry driver. Users should first enable the Cavalry driver and then enable VProc library. Follow the commands below to build the library: </p><pre class="fragment">  build $ make menuconfig
      [*] Ambarella Package Configuration  ---&gt;
          [*]   Build Ambarella vp process library
</pre><p>If a user wants to verify some pre-defined algorithms (for example, CVfilter in the VProc library), follow the commands below to build the unit tests: </p><pre class="fragment">  build $ make menuconfig
      [*] Ambarella Unit Test Configuration  ---&gt;
          [*]   Ambarella Private Linux Unit test configs  ---&gt;
              [*]   Build CV unit tests  ---&gt;
              [*]   Build vproc algorithm unit test
</pre><dl class="section note"><dt>Note</dt><dd>For more details, refer to VProc-related content in the <em>Linux SDK Doxygen documents</em>.</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_nnctrl"></a>
3.2 NNCtrl Library</h2>
<p>NNCtrl is used for users to control the neural network.</p>
<dl class="section note"><dt>Note</dt><dd>For more details, refer to NNCtrl-related content in the <em>Linux SDK Doxygen documents</em>.</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_eazyai"></a>
3.3 EazyAI Library</h2>
<p>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a> for the detailed information.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_input_process"></a>
4 Input Process</h1>
<p>During the training and deployment stages, users must perform preprocessing, which includes decoding images to RGB or BGR data, converting YUV to RGB or BGR data, resizing to the resolution required by networks, and more. There are many different preprocessing methods which may affect the final network result.</p>
<p>Two examples in the Ambarella SDK are as follows:</p>
<ul>
<li><p class="startli">VProc, which is for final deployment</p>
<p class="startli">VProc supports different preprocessing features. The most frequently used is converting YUV to RGB / BGR and resizing. For resizing, VProc supports two different methods: <b>RESIZE_MUL_STEPS</b> and <b>RESIZE_SIN_STEP</b>.</p><ul>
<li>Both methods use Bilinear-Interpolation for resizing. Generally speaking, their differences are minor. If users do not care about minor effects on the final result, they can choose either method or use the default.</li>
<li><p class="startli"><b>RESIZE_MUL_STEPS</b> performs resizing with multiply steps, as follows: </p><pre class="fragment">    1920x1080 -&gt; 960x540 -&gt; 640x480 in this case
</pre><p class="startli">This method is smoother. It leaves more surrounding details, but is relatively blurry.</p>
</li>
<li><b>RESIZE_SIN_STEP</b> uses the one-shot method to manage resizing, which is similar to the method in <b>OpenCV</b>. This method retains more edges and increased sharpness, so the final result is clearer.</li>
</ul>
</li>
<li><p class="startli">gen_image_list.py prepares image dynamic range analysis (DRA).</p>
<p class="startli">gen_image_list.py uses OpenCV to process the image, as follows:</p><ul>
<li>Decode the image to RAW BGR data</li>
<li>Convert the image to Float32</li>
<li>Perform the resizing operations</li>
<li>Convert to a different data format if it is set in the command line</li>
</ul>
</li>
</ul>
<p>If users use the image binary, which is generated by gen_image_list.py for the network inference test, the result may be different from the VProc preprocessing results.</p>
<p>Even in VProc, the two methods below may also cause different results:</p>
<ul>
<li>Convert YUV to RGB, then to perform resizing</li>
<li>Resize YUV first, then perform RGB conversion</li>
</ul>
<p>Ambarella suggests that the preprocessing method in the deployment stage should be the same as the method used in the training stage.</p>
<h2><a class="anchor" id="sub_sec_deploy_input_process_rotate"></a>
4.1 Rotate</h2>
<p>VProc includes one common rotate API <b>vproc_rotate()</b> which can perform some basic rotations.</p>
<p>For rotation of any degree, users can use the API in VProc seen below: </p><pre class="fragment">This API performs warp affine transformation on the image.

@param src Input vector.
@param dst Output vector.
@param waf_cfg Warp affine configuration.
@return 0 = success, -1 = error.
         -WARP_TABLE_OVERFLOW means if encounter a warp table conversion error.

AMBA_API int vproc_warp_affine(IN vect_desc_t *src, OUT vect_desc_t *dst,
                IN warpaffine_cfg_t *waf_cfg);
</pre><p>Users can get the affine Matrix with an OpenCV API, such as getRotationMatrix2D(), and feed it to this API. Then, VProc can perform rotations of any degree.</p>
<p>For a detailed example, refer to the function <em>hand_landmark_run()</em> of <code>ambarella/unit_test/private/cv_test/hand_landmark/src/hand_landmark.cpp</code> in the demo <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sec_tf_hand_landmark">Hand Landmark</a>.</p>
<h2><a class="anchor" id="sub_sec_deploy_input_process_convert_yuv2rgb"></a>
4.2 Convert YUV to RGB</h2>
<p>CNNGen toolchain enables users to add network preprocess for color conversion from YUV to RGB. Similar steps are used for other preprocesses, such as mean, scale, resize, warp, and more.</p>
<p>The basic workflow is as follows:</p><ol type="1">
<li>CNNGen toolchain supports conversions of both NV12 and IYUV to RGB.<ul>
<li>NV12 format (U and V are interleaved buffers): <pre class="fragment">  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  UVUVUVUVUVUVUVUVUVUVUVUVUVUV
  UVUVUVUVUVUVUVUVUVUVUVUVUVUV
</pre></li>
<li>IYUV format (U and V are planar buffers): <pre class="fragment">  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  YYYYYYYYYYYYYYYYYYYYYYYYYYYY
  UUUUUUUUUUUUUU
  UUUUUUUUUUUUUU
  VVVVVVVVVVVVVV
  VVVVVVVVVVVVVV
</pre></li>
</ul>
</li>
<li>Add color conversion preprocess using a JavaScript Object Notation (JSON) file.<ul>
<li>Two inputs are required: one luma image of the shape (1, 1, height, width), and a chroma image of the shape (1, 2, height/2, width/2) with uint8.</li>
<li>Use gen_image_list.py to generate <code>dra_y_list.txt</code> and <code>dra_uv_list.txt</code>, which contain the lists of Y and UV image paths for DRA.</li>
<li>The CSC node performs a conversion from YUV to RGB. The name must match the original network's input layer, as seen in the following example: <div class="fragment"><div class="line">Example:</div>
<div class="line">{</div>
<div class="line"><span class="stringliteral">&quot;inputs&quot;</span>:</div>
<div class="line">[</div>
<div class="line">  {</div>
<div class="line">  <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_Y&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_y_list.txt&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;shape&quot;</span>: [1, 1, &lt;height&gt;, &lt;width&gt;],</div>
<div class="line">  <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">  <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span></div>
<div class="line">  },</div>
<div class="line">  {</div>
<div class="line">  <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_UV&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_uv_list.txt&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;shape&quot;</span>: [1, 2, &lt;height/2&gt;, &lt;width/2&gt;],</div>
<div class="line">  <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">  <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;UV&quot;</span></div>
<div class="line">  }</div>
<div class="line">],</div>
<div class="line"><span class="stringliteral">&quot;operators&quot;</span>:</div>
<div class="line">[</div>
<div class="line">  {</div>
<div class="line">  <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;COLOR_CONVERT&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;&lt;original network input name&gt;&quot;</span>,</div>
<div class="line">  <span class="stringliteral">&quot;attr&quot;</span>:</div>
<div class="line">  {</div>
<div class="line">      <span class="stringliteral">&quot;code&quot;</span>: <span class="stringliteral">&quot;YUV2RGB_NV12&quot;</span></div>
<div class="line">  },</div>
<div class="line">  <span class="stringliteral">&quot;inputs&quot;</span>: [<span class="stringliteral">&quot;&quot;</span>input_Y<span class="stringliteral">&quot;, &quot;</span>input_UV<span class="stringliteral">&quot;],</span></div>
<div class="line"><span class="stringliteral">  &quot;</span>dataformat<span class="stringliteral">&quot;: &quot;</span>0,0,0,0<span class="stringliteral">&quot;</span></div>
<div class="line"><span class="stringliteral">  }</span></div>
<div class="line"><span class="stringliteral">]</span></div>
<div class="line"><span class="stringliteral">}</span></div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>For NV12 and IYUV, CNNGen toolchain manages the inputs using different channels. Set the input_UV shape as <b>[1, 2, height/2, width/2]</b> in the JSON file.</dd></dl>
For a detailed example of preparing a JSON file, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_prepare_the_json_file">2 Prepare the JSON File</a>. For information about JSON files, refer to the CNNGen tool guide <em>Ambarella CV UG JSON Preprocessing Format</em>.</li>
</ul>
</li>
<li>As configured in the JSON file, CNNGen toolchain converts the input of the original network to input_Y and input_UV in the following shapes:<ul>
<li>Input_Y shape is [1, 1, height, width]</li>
<li>Input_UV shape is [1, 2, height / 2, width / 2]</li>
</ul>
</li>
<li>By default, input_UV's <b>dram_format is set to 1</b>, and VP reads UV input with <b>interleave mode</b>. Interleaved UV input** is required, which means that UV is grouped together in dynamic random-access memory (DRAM), which is for NV12 format. The interleave mode reading is shown below: <div class="image">
<img src="../../vp_read_interleaved_mode.jpg" alt=""/>
<div class="caption">
Figure 4-1. VP Reading Interleave Mode.</div></div>
 Further, users can set dram_format to 0. This tells VP to read in normal mode.</li>
<li>When <b>dram_format</b> is set to 1, as mentioned in step 4, the UV input is interleaved and has shape of [1, 1, height/2, width]. CNNGen toolchain automatically sets UV input pitch to be the same as the Y input, which corresponds to the shape of interleaved UV input.<ul>
<li>For example, assuming the original network's input is [1, 3, 224, 200] for RGB, the input's information in <code>cavalry_info.txt</code> is as below:<ul>
<li>Input_Y: [1, 1, 224, 200], <em>pitch: 224</em>;</li>
<li>Input_UV: [1, 2, 112, 100], <em>pitch: 224</em>.</li>
</ul>
</li>
</ul>
</li>
<li>EazyAI / test_nnctrl:<ul>
<li>Gets the input pitch from the Cavalry binary</li>
<li>Checks dram_format. If '1': test_nnctrl / EazyAI fuse the depth dim to width dim in order to read the interleaved UV input correctly.</li>
</ul>
</li>
<li>If users want CVflow to read UV input with normal mode, which is for IYUV, they must set the <b>dram_format</b> to 0 in vas and generate the Cavalry binary. Then, VP will read UV input in <b>planar</b> format. The steps are shown below: <div class="fragment"><div class="line">1. Edit out_&lt;network_name&gt;/out_&lt;network_name&gt;_parser/&lt;network_name&gt;.vas as shown below:</div>
<div class="line">VP_input(&lt;UV input node <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>&gt;, data_format(0, 0, 0, 0), vector(1, 2, &lt;UV data height&gt;, &lt;UV data width&gt;),</div>
<div class="line">    dram_format = 0, <span class="comment">// add this line</span></div>
<div class="line">    dram_rotate = 0,</div>
<div class="line">    rt_config = 0,</div>
<div class="line">    VP_cnngen_demangled_id(<span class="stringliteral">&quot;&lt;UV input node name&gt;&quot;</span>),</div>
<div class="line">    __cnngen_tracker = { 3 });</div>
<div class="line"> </div>
<div class="line">2. Generate the cavalry binary <span class="keyword">using</span> the following commands:</div>
<div class="line">build $ rm -fr out_&lt;network name&gt;/out_&lt;network name&gt;_parser/vas_output</div>
<div class="line">build $ cd out_&lt;network name&gt;/out_&lt;network name&gt;_parser &amp;&amp; vas -<span class="keyword">auto</span> -show-progress &lt;network name&gt;.vas</div>
<div class="line">build $ cd ../..</div>
<div class="line">build $ rm -fr cavalry_&lt;network name&gt;</div>
<div class="line">build $ mkdir cavalry_&lt;network name&gt;</div>
<div class="line">build $ cavalry_gen -d out_&lt;network name&gt;/out_&lt;network name&gt;_parser/vas_output -f cavalry_&lt;network name&gt;/cavalry_&lt;network name&gt;.bin -p cavalry_&lt;network name&gt;</div>
<div class="line">    -v &gt; cavalry_&lt;network name&gt;/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
</div><!-- fragment --> The output Cavalry binary is <code>cavalry_[network name]/cavalry_[network name].bin</code>.</li>
</ol>
<ol type="1">
<li>Assume the original network's input is "data" and its shape is [1, 3, 224, 200], which corresponds to images with the size of 224x200 in RGB format. The following are two examples on CV22 of preprocessing for color conversion of NV12 and IYUV to RGB separately:</li>
</ol>
<p><b>Example: NV12 to RGB on CV22</b></p>
<ol type="1">
<li>Prepare the JSON file as shown below: <div class="fragment"><div class="line">build $ vi example_nv12.json</div>
<div class="line">{</div>
<div class="line">    <span class="stringliteral">&quot;inputs&quot;</span>:</div>
<div class="line">    [</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_Y&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_y_list.txt&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;shape&quot;</span>: [1, 1, 224, 200],</div>
<div class="line">      <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">      <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span></div>
<div class="line">      },</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_UV&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_uv_list.txt&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;shape&quot;</span>: [1, 2, 112, 100],</div>
<div class="line">      <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">      <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;UV&quot;</span></div>
<div class="line">      }</div>
<div class="line">    ],</div>
<div class="line">    <span class="stringliteral">&quot;operators&quot;</span>:</div>
<div class="line">    [</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;COLOR_CONVERT&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;data&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;attr&quot;</span>:</div>
<div class="line">      {</div>
<div class="line">          <span class="stringliteral">&quot;code&quot;</span>: <span class="stringliteral">&quot;YUV2RGB_NV12&quot;</span></div>
<div class="line">      },</div>
<div class="line">      <span class="stringliteral">&quot;inputs&quot;</span>: [<span class="stringliteral">&quot;&quot;</span>input_Y<span class="stringliteral">&quot;, &quot;</span>input_UV<span class="stringliteral">&quot;],</span></div>
<div class="line"><span class="stringliteral">      &quot;</span>dataformat<span class="stringliteral">&quot;: &quot;</span>0,0,0,0<span class="stringliteral">&quot;</span></div>
<div class="line"><span class="stringliteral">      }</span></div>
<div class="line"><span class="stringliteral">    ]</span></div>
<div class="line"><span class="stringliteral">}</span></div>
</div><!-- fragment --></li>
<li>Convert the model to cavalry binary using the following commands: <div class="fragment"><div class="line">build $ rm -fr ./out_parser</div>
<div class="line">build $ onnxparser.py -m example_network.onnx -o example_network -of ./out_parser -pp example_nv12.json -odst <span class="stringliteral">&quot;o:output|odf:fp32&quot;</span></div>
<div class="line">build $ rm -fr ./out_parser/vas_output</div>
<div class="line">build $ cd ./out_parser; vas -<span class="keyword">auto</span> -show-progress ./example_network.vas; cd ../</div>
<div class="line">build $ cavalry_gen -d ./out_parser/vas_output -f ./cavalry_output/cavalry_example_network.bin -p ./out_parser -v &gt; ./cavalry_output/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
</div><!-- fragment --> Check the port information of UV input in <code>cavalry_info.txt</code>: <div class="fragment"><div class="line">build $ vi ./cavalry_output/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
<div class="line">...</div>
<div class="line">input_id: 0 ... dim: (P, D, H, W) = ( 1, 1, 224, 200)   pitch: 224 ... dram_format: 0 ...</div>
<div class="line">input_id: 1 ... dim: (P, D, H, W) = ( 1, 2, 112, 100)   pitch: 224 ... dram_format: 1 ...</div>
<div class="line">...</div>
</div><!-- fragment --> The shape of input_UV is [1,2,112,100], dram_format is 1. So the pitch is <b>224</b> on CV22, which corresponds to the shape of interleaved UV input [1, 1, 112, <b>200</b>].</li>
<li>Use images in NV12 format as input files for test_nnctrl / EazyAI. <div class="fragment"><div class="line">board # test_nnctrl -b cavalry_example_network.bin -e --in input_Y=&lt;NV12_image_Y&gt;.bin --in input_UV=&lt;NV12_image_UV&gt;.bin</div>
</div><!-- fragment --><ul>
<li>The UV input should be <b>interleaved</b> which has shape of [1, 1, 112, 200];</li>
<li>For test_nnctrl / EazyAI, it checks the dram_format is 1, fuse the depth dim to width dim as [1, 1, 112, 200], and get the pitch (224) from the cavalry. Then it can read the UV input file correctly.</li>
<li>For VP, as the dram_format is 1, it reads the UV input with interleave mode, which converts the UV input's shape from [1, 1, 112, 200] to [1, 2, 112, 100].</li>
<li>Lastly, VP converts the YUV input to RGB and feed it to the network.</li>
</ul>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>In this example, the pitch is 224, as CV22 will align with 32 bytes. For detailed information about the alignment, refer to <a class="el" href="../../d7/d53/fs_deployment.html#sub_sec_deploy_out_align">5.3 Alignment</a>.</dd></dl>
<p><b>Example: IYUV to RGB on CV22</b></p>
<ol type="1">
<li>Prepare the JSON file as shown below: <div class="fragment"><div class="line">build $ vi example_iyuv.json</div>
<div class="line">{</div>
<div class="line">    <span class="stringliteral">&quot;inputs&quot;</span>:</div>
<div class="line">    [</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_Y&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_y_list.txt&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;shape&quot;</span>: [1, 1, 224, 200],</div>
<div class="line">      <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">      <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span></div>
<div class="line">      },</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_UV&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;work dir&gt;/dra/dra_uv_list.txt&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;shape&quot;</span>: [1, 2, 112, 100],</div>
<div class="line">      <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">      <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;UV&quot;</span></div>
<div class="line">      }</div>
<div class="line">    ],</div>
<div class="line">    <span class="stringliteral">&quot;operators&quot;</span>:</div>
<div class="line">    [</div>
<div class="line">      {</div>
<div class="line">      <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;COLOR_CONVERT&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;data&quot;</span>,</div>
<div class="line">      <span class="stringliteral">&quot;attr&quot;</span>:</div>
<div class="line">      {</div>
<div class="line">          <span class="stringliteral">&quot;code&quot;</span>: <span class="stringliteral">&quot;YUV2RGB_NV12&quot;</span></div>
<div class="line">      },</div>
<div class="line">      <span class="stringliteral">&quot;inputs&quot;</span>: [<span class="stringliteral">&quot;&quot;</span>input_Y<span class="stringliteral">&quot;, &quot;</span>input_UV<span class="stringliteral">&quot;],</span></div>
<div class="line"><span class="stringliteral">      &quot;</span>dataformat<span class="stringliteral">&quot;: &quot;</span>0,0,0,0<span class="stringliteral">&quot;</span></div>
<div class="line"><span class="stringliteral">      }</span></div>
<div class="line"><span class="stringliteral">    ]</span></div>
<div class="line"><span class="stringliteral">}</span></div>
</div><!-- fragment --></li>
<li>Convert the model to the Cavalry binary as follows: <div class="fragment"><div class="line">build $ rm -fr ./out_parser</div>
<div class="line">build $ onnxparser.py -m example_network.onnx -o example_network -of ./out_parser -pp example_iyuv.json -odst <span class="stringliteral">&quot;o:output|odf:fp32&quot;</span></div>
</div><!-- fragment --> Set <code>dram_format</code> to 0 in vas: <div class="fragment"><div class="line">Edit out_parser/example_network.vas as below.</div>
<div class="line">VP_input(&lt;UV input node <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>&gt;, data_format(0, 0, 0, 0), vector(1, 2, &lt;UV data height&gt;, &lt;UV data width&gt;),</div>
<div class="line">    dram_format = 0, <span class="comment">// modify this line</span></div>
<div class="line">    dram_rotate = 0,</div>
<div class="line">    rt_config = 0,</div>
<div class="line">    VP_cnngen_demangled_id(<span class="stringliteral">&quot;&lt;UV input node name&gt;&quot;</span>),</div>
<div class="line">    __cnngen_tracker = { 3 });</div>
</div><!-- fragment --> Convert the model to cavalry binary using the following commands: <div class="fragment"><div class="line">build $ rm -fr ./out_parser/vas_output</div>
<div class="line">build $ cd ./out_parser; vas -<span class="keyword">auto</span> -show-progress ./example_network.vas; cd ../</div>
<div class="line">build $ cavalry_gen -d ./out_parser/vas_output -f ./cavalry_output/cavalry_example_network.bin -p ./out_parser -v &gt; ./cavalry_output/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
</div><!-- fragment --> Then check the port information of UV input in <code>cavalry_info.txt</code>: <div class="fragment"><div class="line">build $ vi ./cavalry_output/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
<div class="line">...</div>
<div class="line">input_id: 0 ... dim: (P, D, H, W) = ( 1, 1, 224, 200)   pitch: 224 ... dram_format: 0 ...</div>
<div class="line">input_id: 1 ... dim: (P, D, H, W) = ( 1, 2, 112, 100)   pitch: 128 ... dram_format: 0 ...</div>
<div class="line">...</div>
</div><!-- fragment --> The shape of input_UV is [1,2,112,100], pitch is 128 and dram_format is 0.</li>
<li>Use images in IYUV format as input files for test_nnctrl / EazyAI. <div class="fragment"><div class="line">board # test_nnctrl -b cavalry_example_network.bin -e --in input_Y=&lt;IYUV_image_Y&gt;.bin --in input_UV=&lt;IYUV_image_UV&gt;.bin</div>
</div><!-- fragment --><ul>
<li>The UV input should be <b>planar</b> which has shape of [1, 2, 112, 100];</li>
<li>For test_nnctrl / EazyAI, it checks the dram_format is 0, then read the input file with shape of [1, 2, 112, 100], and get the pitch (128) from the cavalry.</li>
<li>For VP, as the dram_format is 0, it reads the UV input with normal mode, which reads the UV input with shape of [1, 2, 112, 100].</li>
<li>Lastly, VP converts the YUV input to RGB and feed it to the network.</li>
</ul>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>In this example, the pitch is 128, as CV22 aligns with 32 bytes. For detailed information about the alignment, refer to <a class="el" href="../../d7/d53/fs_deployment.html#sub_sec_deploy_out_align">5.3 Alignment</a>.</dd></dl>
<p>For a detailed example, refer to the demo <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_cgpp_deployment_for_yolov3">CGPP Deployment for Yolov3</a>.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_out_process"></a>
5 Output Process</h1>
<p>Occasionally, users will be required to perform some postprocessing for the VP result so it can be easily processed by Arm.</p>
<h2><a class="anchor" id="sub_sec_deploy_out_random"></a>
5.1 Random</h2>
<p>The parser sets the output format using DRA and the established network configuration. Check the format in the VAS code with the string data_format (*, *, *, *) using the layer “VP_output”. For data format details, refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_data_format">4 Data Format</a>.</p>
<p>The examples below demonstrate possible <b>fix-points</b>:</p>
<ul>
<li>data_format(0, 0, 0, 0), int8 0~255</li>
<li>data_format(1, 0, 7, 0), -1~1</li>
<li>data_format(0, 0, 8, 0), 0~1</li>
<li>data_format(1, 0, 0, 0), if the DRAM value is 255, the real data is -1 with <b>signed char</b>, then the data is -1.0</li>
<li>data_format(1, 0, 2, 0), if DRAM value is 255, the real data is -1 with <b>signed char</b>, then the data is -1 * 1.0 / 2^2 = -0.25</li>
<li>data_format(1, 0, 9, 0), if DRAM value is 255, the real data is -1 with <b>signed char</b>, then the data is -1 * 1.0 / 2^9 = -0.001953125</li>
</ul>
<p>If <b>FP16</b> is (1, 1, 0, 4), it is equal with FP16 which is defined in IEEE-754, as shown below.</p>
<ul>
<li>Sign bit: 1 bit</li>
<li>Exponent width: 5 bits</li>
<li>Significant precision:： 11 bits</li>
</ul>
<p>The Ambarella side is different from the standard side above. One more parameter is added: “exponent offset” tunes its expoffset field for underflow and overflow.</p>
<p>Users must parse FP16 in two steps.</p>
<ol type="1">
<li>Parse FP16 as (1, 1, 0 ,4); users can use the header file “half.hpp”, which is in the Arm “ComputeLibrary” library, to convert. Users can also follow IEEE-754 and write code to parse it.</li>
<li>If the expoffset is not 0, such as -6, use (FP16 value in memory which means the value parsed by 1, 1, 0, 4) * 2^((-6*2)+1).</li>
</ol>
<h2><a class="anchor" id="sub_sec_deploy_out_mandatory"></a>
5.2 Mandatory</h2>
<p>For easier analysis, Ambarella suggests using FP16 or FP32 (preferred) for mandatory output.</p>
<ul>
<li>Set the standard FP16 mandatory output using the data_format(1, 1, 0, 4), real value = (FP16 value in memory)</li>
<li>Set the standard FP32 mandatory output using the data_format(1, 2, 0, 7), real value = (FP 32 value in memory)</li>
<li>Transpose and Reshape</li>
</ul>
<h4><a class="anchor" id="autotoc_md26"></a>
Data Format</h4>
<p>This can be set using the following commands in “-odst” option with <b>ONet</b>. </p><pre class="fragment">build $ caffeparser.py -p /home/work/cnngen/caffe/demo_networks/mtcnn/Onet/models/det3.prototxt \
                       -m /home/work/cnngen/caffe/demo_networks/mtcnn/Onet/models/det3.caffemodel \
                       -i /home/work/cnngen/out/caffe/demo_networks/onet/dra_list.txt \
                       -o onet \
                       -of /home/work/cnngen/out/caffe/demo_networks/onet/out_fix8_full \
                       -iq -idf 0,0,0,0 -c coeff-force-fx8,coeff-force-fx8
                       -odst "o:conv6-3|odf:fp16" -odst "o:conv6-2|odf:fp16" -odst "o:prob1|odf:fp16"
Alternatively:
build $ caffeparser.py -p /home/work/cnngen/caffe/demo_networks/mtcnn/Onet/models/det3.prototxt \
                       -m /home/work/cnngen/caffe/demo_networks/mtcnn/Onet/models/det3.caffemodel \
                       -i /home/work/cnngen/out/caffe/demo_networks/onet/dra_list.txt \
                       -o onet \
                       -of /home/work/cnngen/out/caffe/demo_networks/onet/out_fix8_full \
                       -iq -idf 0,0,0,0 -c coeff-force-fx8,coeff-force-fx8
                       -odst "o:conv6-3|odf:fp32" -odst "o:conv6-2|odf:fp32" -odst "o:prob1|odf:fp32"
</pre><h4><a class="anchor" id="autotoc_md27"></a>
Data Dimension</h4>
<p>More postprocessing methods can be found in the parser, such as “ot” and “os”. The “ot” example, seen below, has similar usage to “os”.</p>
<p>All VP output widths align with special bytes for different chips, such as 32 bytes in the example below. For alignment details, refer to the next section.</p>
<ol type="1">
<li>Output (1,1,2,28); the output format is fix8 (1 byte), the real output is as follows: <pre class="fragment"> YYYYYYYYYYYYYYYYYYYYYYYYYYYY(YYYY)
 YYYYYYYYYYYYYYYYYYYYYYYYYYYY(YYYY)
</pre></li>
</ol>
<p>The last 4 bytes are padding.</p>
<ol type="1">
<li>Output (1,1,2,15); the output format is float32 (4 bytes) as (1,1,2,60), the real output is as follows: <pre class="fragment"> YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
 YYYYYYYYYYYYYYYYYYYYYYYYYYYY(YYYY)
 YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY
 YYYYYYYYYYYYYYYYYYYYYYYYYYYY(YYYY)
</pre></li>
</ol>
<p>The last 4 bytes are padding.</p>
<p>First, use Tensorflow mobilenetv2 as an example. </p><pre class="fragment">build $ tf_print_graph_summary.py -p frozen_mobilenet_224_v2_opt.pb
Graph summary:
Total number of ops: 314
Number of non constant ops: 156
Inputs (1):
  input (1, 224, 224, 3)
Outputs (1):
  MobilenetV2/Predictions/Softmax [1, 1001]
</pre><p>For the case above, the output [1, 1001, 1, 1] total size is 1001 * 32 bytes; as it aligns with 32 bytes, Arm cannot effectively process it as users must get the next class’s confidence with 32 bytes of padding.</p>
<p>More postprocess functions are in the parser. The command below switches the channel and width. </p><pre class="fragment">Build tfparser.py -p frozen_mobilenet_224_v2_opt.pb -isrc "is:1,3,224,224|iq|idf:0,0,0,0|im:127.5,127.5,127.5|ic:128.000000000|i:input=dra_image_bin/dra_bin_list.txt"  -o tf_mobilenet_v2 -of out_tf_mobilenet_v2_parser -c act-force-fx8,coeff-force-fx8 -odst "o:MobilenetV2/Predictions/Softmax|ot:0,3,2,1|odf:fp32"
</pre><p>The output should be (1,1,1,1001), meaning that the width is 1001. Users can visit all class’s confidence one by one in the output DRAM.</p>
<h2><a class="anchor" id="sub_sec_deploy_out_align"></a>
5.3 Alignment</h2>
<p>In the CV chip family, the CVflow® engine can accelerate NNs. For the best performance and special hardware design, network data must align with different pitches, which includes both input data and output data. The pitch alignment rule varies based on the type of chip (due to differences in the chips' hardware designs).</p>
<ul>
<li>CV25, CV22, and CV2 align with 32 bytes.</li>
<li>CV28 aligns with 64 bytes.</li>
<li>CV5 aligns with 128 bytes.</li>
<li>CV72 aligns with 128 bytes.</li>
<li>Future chips may align with larger pitches.</li>
</ul>
<p>Users must pay attention to this pitch alignment rule. For most networks, CVflow results cannot be directly used; Arm must perform some post-process based on pitch alignment rules.</p>
<p>For liveview NN applications, Ambarella suggests that the user must use the pitch from network output structure. The structure is defined in <code>nnctrl.h</code> as shown below. </p><pre class="fragment">struct io_dim {
   …
    uint32_t pitch;  // The pitch of the width
    ….
};
</pre><p>For <code>test_nnctrl</code>, the network output will also be generated with 32 / 64 / 128 byte alignment in DRAM, which is decided by different chips. When saving to binary, <code>test_nnctrl</code> will <b>remove the padding</b>.</p>
<dl class="section note"><dt>Note</dt><dd>DSP also has alignment mismatch issues between different chips. Refer to the basic Linux SDK document for more details.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_cavalry_dram"></a>
6 Cavalry DRAM</h1>
<p>Cavalry utilizes the following two methods to manage Cavalry memory:</p>
<ol type="1">
<li><b>CMA</b>: the same method used for DSP, this method is very flexible. The advantage of this method is that Arm can use the free memory in both DSP and VP. However, for some low cost-products with small DRAM, as CMA enables Arm to use Cavalry's memory, when Cavalry requires it, it must to get the memory back. Sometimes, users cannot get the memory back from Arm. This case is mentioned in <a class="el" href="../../d7/d53/fs_deployment.html#sub_sec_deploy_cma_alloc">6.3 CMA Allocate Fail</a>.</li>
<li><b>AMA</b>: different fom <b>CMA</b>'s flexibility, using this method the VP's free memory cannot be used by Arm. For more details, refer to <a class="el" href="../../d7/d53/fs_deployment.html#sub_sec_deploy_cavalry_ama">6.1 AMA</a>. Since SDK 3.0.2, <b>AMA</b> is the default method.</li>
</ol>
<h2><a class="anchor" id="sub_sec_deploy_cavalry_ama"></a>
6.1 AMA</h2>
<p>The following uses CV25M as an example. Its total size is 1 GB, and it is shared by DSP, Arm, and VP. Users can configure it as seen below if necessary. </p><pre class="fragment">build $ make menuconfig
  [*] Ambarella Firmware Configuration -&gt;
    Memory Options -&gt;
      RTOS memory options -&gt;
         (0x10000000) DSP DRAM start address
         (0x30000000)) CV DRAM start address
</pre><p>The configuration above indicates the points below.</p>
<ul>
<li>CV: 0x30000000 ~ 0x40000000, 256 MB</li>
<li>DSP: 0x10000000 ~ 0x30000000, 512 MB</li>
<li>Arm: 0x00000000 ~ 0x10000000, 256 MB</li>
</ul>
<p>For the CV25M EVK, users can check the information seen below.</p>
<ol type="1">
<li>CV: 0x30000000 ~ 0x40000000, 256 MB <pre class="fragment"># cat /proc/ambarella/cavalry_cma
mmb   phys[start-end]        size        kern-virt[start-end]                   ref_cnt  cache  recycle  pid
0000  0x30000000-0x30400000  0x00400000  0x00000000882309a2-0x00000000ef98cf83
----

Free list count: 0, max contiguous free size: 0xfc00000
Total Cavalry memory size: 0x10000000 (about 256MB), used size: 0x400000 (about 4MB)

Cavalry Status: Stopped
</pre></li>
<li>DSP: 0x10000000 ~ 0x30000000, 512 MB <pre class="fragment"># cat /proc/ambarella/cma
mmb   phys[start-end]        size        virt[start-end]                        attr    buf_id      ref_cnt  dma_buf  usage
0     0x10000000-0x24081000  0x14081000  0x00000000bb3d586a-0x0000000086ba9bd7  0x0003  22          0        0        IAV_PART_PREALLOC
|-3   0x10000000-0x24000000  0x14000000  0x00000000bb3d586a-0x00000000e2e606ef  0x0001  0           0        0        IAV_PART_DSP
|-4   0x24000000-0x24001000  0x00001000  0x00000000e2e606ef-0x000000006e31e29c  0x0003  16          0        0        IAV_PART_DSP_DEF_CMD
|-6   0x24001000-0x24081000  0x00080000  0x000000006e31e29c-0x0000000086ba9bd7  0x0001  18          0        0        IAV_PART_DSP_LOG
11    0x24082000-0x24084000  0x00002000  0x000000004a918a02-0x00000000133a4308  0x0000  7           0        0        IAV_PART_QUANT
1     0x24100000-0x2518E000  0x0108E000  0x00000000dc405139-0x000000005f902df2  0x0003  23          0        0        IAV_PART_PREALLOC2
|-2   0x24100000-0x2410B000  0x0000B000  0x00000000dc405139-0x00000000eb9f0da0  0x0003  20          0        0        IAV_PART_DSP_RSV
|-5   0x2410B000-0x2460B000  0x00500000  0x00000000eb9f0da0-0x000000002959b7f4  0x0001  17          0        0        IAV_PART_DSP_UCODE
|-12  0x2460B000-0x2510B000  0x00B00000  0x000000002959b7f4-0x000000007dbd943e  0x0001  8           0        0        IAV_PART_IMG
|-14  0x2510B000-0x2510F000  0x00004000  0x000000007dbd943e-0x00000000e1508f2b  0x0003  15          0        0        IAV_PART_BSH
|-15  0x2510F000-0x2518E000  0x0007F000  0x00000000e1508f2b-0x000000005f902df2  0x0003  21          0        0        IAV_PART_IAV_RSV
7     0x25200000-0x25C00000  0x00A00000  0x00000000a53e317e-0x0000000019b2e9c8  0x0000  1           0        0        IAV_PART_BSB
8     0x25C00000-0x26411000  0x00811000  0x00000000a1f5a4c2-0x000000009b6dfe1f  0x0000  4           0        0        IAV_PART_OVERLAY
9     0x26500000-0x26D20000  0x00820000  0x000000002b49e207-0x0000000002b952ea  0x0000  5           0        0        IAV_PART_QPMATRIX
10    0x26E00000-0x275E0000  0x007E0000  0x00000000c4433f67-0x000000005b4ffe64  0x0000  6           0        0        IAV_PART_WARP
13    0x27600000-0x28A00000  0x01400000  0x0000000042281989-0x00000000a360cbc7  0x0000  9           0        0        IAV_PART_MASK

Total mmb size: 0x18722000, about 392MB, total 512MB
</pre></li>
<li>Arm: 0x00000000 ~ 0x10000000, 256 MB <pre class="fragment"># free
              total        used        free      shared  buff/cache   available
Mem:        1017896      705044      280356         216       32496      287576
Swap:             0           0           0
</pre></li>
</ol>
<p>The leftover Arm memory is 280356 / 1024 = 273 MB, but it is not equal to 256 MB. This is because the CMA buffer is used, meaning that Arm can use the free CMA memory in DSP. For VP, if AMA is enabled, Arm cannot use the VP memory. If disabled, Arm can use the memory.</p>
<p>Since SDK 3.0.2, AMA is the default; it can only use the free CMA buffer in DSP. As mentioned in the log above, DSP free memory is (512 - 392) = 120 MB. The total leftover memory is (256 + 120) = 376 MB, but because the Linux kernel costs more than 100 MB, the total free is 273 MB.</p>
<p>For more details, refer to <em>DRAM_Optimization</em> in Linux SDK Doxygen document.</p>
<h2><a class="anchor" id="sub_sec_deploy_cma_nn_cost"></a>
6.2 Network Cost</h2>
<p>Users can run <code>test_nnctrl</code> to estimate the Cavalry DRAM cost. </p><pre class="fragment">board # test_nnctrl -b yolo_v3_cavalry_fix8_sp50.bin --in data
Cavalry Bin: [0][yolo_v3_cavalry_fix8_sp50.bin]
INPUT: [data]
Total neural network num: 1
Input: 0 [data] dim: (1, 3, 416, 416), pitch: 416 (752, 16), dram_fmt: 0, bitvector: 0, data_fmt: (0, 0, 8, 0), size: 519168, phys: 0xb2cf2140, virt: 0x7fbd6f8140
Network [0] [yolo_v3_cavalry_fix8_sp50.bin] total memory size: 48796352, dvi: 40836988, blob: 7958912. Bandwidth size: 73300860
</pre><p>A DRAM cost estimation includes the following:</p>
<ul>
<li><b>Total Memory Size</b>: the total memory, which will cost 48796352 / 1024 / 1024 = 46.5 MB.</li>
<li><b>DVI</b>: the original network model size is 40836988/ 1024 / 1024 = 38.9 MB. It is close to (original model size / (Quantitative level Float 32 / (fix 8 or fix 16)) * (Sparse percentage)); for example, if fix 8 and sp 50 is used, the size is approximately the original model size / (32 / 8) / 2. If using fix 16, it is approximately the original model size / (32 / 16) / 2. If it is mixed, it is approximately original model size / (2~4) / 2. If users want a rough estimate, they can use this size, as it is much greater than the blob size.</li>
<li><b>Blob</b>: the temp data from running the network. Temp input and output for every DAG is 7958912 / 1024 / 1024 = 7.6 MB.</li>
<li><b>Total Memory Size = DVI + Blob</b>.</li>
<li>Users can run all networks in CNNGen samples to see each network’s final memory cost for reference or perform a rough estimate of memory cost.</li>
</ul>
<p>If users need to get a rough cost, DVI size (Cavalry Binary Size) can be used for DRAM cost as BLOB memory is much lower than DVI in most of the times, user can just evaluate a rough BLOB memory size and add it with the DVI size.</p>
<p>If users need to get a rough DRAM cost for different resolution based on current tested resolution, they can refer to follows.</p>
<div class="fragment"><div class="line">board # test_nnctrl -b yolo_v3_sp50_cavalry.bin --in data=dog.bin -e</div>
<div class="line">Input: 0 [data] dim: (1, 3, 960, 960), pitch: 960, dram_fmt: 0, bitvector: 0, data_fmt: (0, 0, 8, 0), loop_cnt: 1, size: 2764800, phys: 0xb2b14000, virt: 0x7f9fd7f000</div>
<div class="line">Network [0] [yolo_v3_sp50_cavalry.bin] total memory size: 103142144, dvi: 40843404, blob: 62165760. Bandwidth size: 301434944. Extra_high_mem size: 0</div>
<div class="line"> </div>
<div class="line">board # test_nnctrl -b yolo_v3_sp50_cavalry.bin --in data=dog.bin -e</div>
<div class="line">Input: 0 [data] dim: (1, 3, 416, 416*, pitch: 416, dram_fmt: 0, bitvector: 0, data_fmt: (0, 0, 8, 0), loop_cnt: 1, size: 519168, phys: 0xb2b0e000, virt: 0x7f99afd000</div>
<div class="line">Network [0] [yolo_v3_sp50_cavalry.bin] total memory size: 51679616, dvi: 40837408, blob: 10727808. Bandwidth size: 83483648. Extra_high_mem size: 0</div>
</div><!-- fragment --><p>As above example, total cost of 960x960 is about 98 MB (<b>103142144 / 1024 / 1024</b>), and 416x416 is 50 MB (<b>51679616 / 1024 / 1024</b>). And DVI size will not be affected by the input resolution, only blob size will be affected.</p>
<p>So the estimated calculation formula is follows.</p>
<div class="fragment"><div class="line">DRAM cost <span class="keywordflow">for</span> <span class="keyword">new</span> input resolution = dvi cost  + blob cost * <span class="keyword">new</span> input resolution / old input resolution</div>
</div><!-- fragment --><p>Then DRAM cost for 960x960 is about (40843404 + 10727808 * 960 * 960 / 416 / 416) / 1024 / 1024 = <b>93.4 MB</b>, which is close to real DRAM cost <b>98 MB</b>.</p>
<h2><a class="anchor" id="sub_sec_deploy_cma_alloc"></a>
6.3 CMA Allocate Fail</h2>
<p>In Arm's high workload, Arm Linux may allocate memory from free CMA memory, but when the CMA memory is required, it will get it back from Arm Linux. Sometimes, the Cavalry memory interface <b>alloc_cache_recycle()</b> will report “EBUSY”, meaning that CMA requires time to take the memory back; it does not indicate a failure or lack of memory. With this error, the user’s application must wait and try again.</p>
<h2><a class="anchor" id="sub_sec_deploy_cache"></a>
6.4 Cache</h2>
<p>Cache is a high-speed buffer between the CPU and DRAM to address the problem of data read/write speed mismatch in the system.</p>
<p>In a multi-core system, the following two factors may cause the cache inconsistency issue that leads the CPU or I/O devices to read the wrong data.</p>
<ul>
<li>As the CPU writes data to the cache, the contents change. However, as the DRAM is not written immediately, the data of DRAM is not changed.</li>
<li>The I/O processor or I/O device writes data to the main memory, the contents of DRAM are modified, but the contents of cache are not.</li>
</ul>
<p>To solve the problem of incorrect data reading caused by the cache inconsistency, cavalry provides the API <code>cavalry_mem_sync_cache()</code> with below flags.</p>
<ul>
<li>Clean flag, check the dirty bit of the memory cache line. If the dirty bit is 1, write the contents of the cache line back to the next-level storage and set the dirty bit to 0.</li>
<li>Invalid flag, check the valid bit of the memory cache line. If the valid bit is 1, set it to 0 to discard the cached contents.</li>
</ul>
<p>The combination of <b>clean cache</b> and <b>invalidate cache</b> operations above is called the <b>flush cache</b> process. Users can decide which operation should be executed according to the direction of data access. The detailed usages are listed below:</p>
<ul>
<li>If the CPU writes data to the DRAM first, and then DMA or other devices access the DRAM, users must execute <b>clean cache</b> first to ensure that the DMA or other devices access DRAM accurately.</li>
<li>If the CPU reads the DRAM first, DMA or other devices do not need to execute <b>clean cache</b> before accessing it. Once DMA or other devices re-write the DRAM, users need to execute <b>invalidate cache</b>. So that the CPU will read the data from the DRAM directly instead of the data in the cache.</li>
</ul>
<p>However, if both CPU and DMA access a block of DRAM concurrently, Ambarella suggests users execute <b>flush cache</b> before read/write such block of DRAM to avoid the cache inconsistency issue.</p>
<p>Then for NNCtrl and Vproc, users needs to decide if to enable cache, use NNCtrl as an example.</p>
<ul>
<li>First, NNCTRL can only report the size that network need without allocation as below.<ul>
<li><code>no_mem=0</code> means to return the size with input and output</li>
<li><code>no_mem=1</code> means to return the size without input and output.</li>
</ul>
</li>
<li>Then, user should use this size to allocate memory with cavalry_mem api by themselves, so "cache_en" is used to control if to apply memory with cache disable or enable.</li>
</ul>
<p>The best suggestions are as follows:</p><ul>
<li>Use no_mem=1 to exclude input and output, then apply network memory with cache disable.</li>
<li>Apply input and output with cache enable or disable for different cases; refer to the example below.<ul>
<li>For output, Ambarella suggests to enable cache, as it always required for Arm to read and process.</li>
<li>Input depends on individual use cases. Input from DSP or VProc that is sent directly to NNCtrl means that Arm will not visit this memory, then it is better to disable cache. If input should go through Arm, the flow will be DSP -&gt; VProc -&gt; Arm -&gt; NNCtrl. In this case it is better to enable cache.</li>
</ul>
</li>
</ul>
<p>Users must remember that cache enable is only an advantage for Arm read.</p>
<h2><a class="anchor" id="sub_sec_deploy_mprotect"></a>
6.5 DVI Memory Protection</h2>
<p>Libnnctrl can support <b>mprotect</b> function for the DVI memory of 'NET.bin`.</p>
<p>With library, user applications cannot write data on <code>NET.bin</code> DVI memory after calling the API <b>nnctrl_load_net()</b>.</p>
<p>It will trigger a <b>segmentation fault</b> if the user application writes data on DVI memory, which can protect VP from a VP hang, as broken DVI memory will result in a VP hang.</p>
<p>Debugging memory trash is performed easily when <b>segmentation fault</b> is reported.</p>
<p>By default, this feature is enabled. Users can disable it by setting “struct net_cfg .no_dvi_mprot” when calling the API <b>nnctrl_init_net()</b>.</p>
<dl class="section note"><dt>Note</dt><dd>This feature only can prevent the destructive behavior of Arm, but cannot prevent the destructive behavior of CVflow and DSP.</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_cv2_high_addr_dram"></a>
6.6 CV2 High Address DRAM</h2>
<p>For CV2, there are 32 address lines enabling users to access 0 - 4 GB address space. However, due to the fact that some addresses are reserved for the chip's registers, Arm can only access 0 ~ 3.5 GB of DRAM space. 0.5 GB memory is wasted when hardware uses a 4-GB DRAM size.</p>
<p>DSP and VP can visit this wasted 0.5 GB.</p>
<ul>
<li>For how to use this in DSP cases, refer to "OTHER - DSP Private DRAM" of the Feature Sets Document in SDK Doxygen documents.</li>
<li>For how to use this in VP cases, refer to the following.</li>
</ul>
<p>There are five different types of VP DRAM cost, as follows:</p>
<ol type="1">
<li>DVI memory, which stores the real network</li>
<li>Blob memory, which is used when the network is running</li>
<li>Network input and output</li>
<li>Commands between VP and Cavalry driver</li>
<li>VP firmware</li>
</ol>
<p>Users can only transfer the memory in 1 and 2 to the 0.5 GB. For 3, 4, and 5, users should place it in CMA, which is visible for Arm.</p>
<h4><a class="anchor" id="autotoc_md29"></a>
Example: Place DVI and blob memory to high address</h4>
<pre class="fragment">test_nnctrl -b yolov3_sp50.bin --in data=dog.bin --out conv59 --out conv67 --out conv75 -k -e --high-mem 0xe0000000
</pre><p>As described above, VP and DSP use this DRAM. Currently there are no protection for combined usage of DSP and VP, so users should be cautious if they must use this memory. Ensure that VP and DSP will not visit the same address. Ambarella suggests that:</p>
<ul>
<li>Users run networks one by one to get the “actual size” returned by the NNCtrl library, and record the size for each network.</li>
<li>Users pack all networks together and group them as “VP high memory”. Then, calculate the total size (VP_total_size) for all networks.</li>
<li>Users split the high 512 MB into two parts, DSP and VP, where “DSP_total_size + VP_total_size = 512MB”.</li>
</ul>
<h2><a class="anchor" id="sub_sec_deploy_show_cavalry_cma"></a>
6.7 Print Cavalry Memory Summary</h2>
<p>Users can use the command below to check the Cavalry CMA cost. </p><pre class="fragment">board # cat /proc/ambarella/cavalry_cma
mmb   phys[start-end]        size        kern-virt[start-end]                   ref_cnt  cache  recycle  pid
0000  0x60000000-0x60400000  0x00400000  0x00000000f090c13c-0x0000000066683cb3
----

Free list count: 0, max contiguous free size: 0x1fc00000
Total Cavalry memory size: 0x20000000 (about 512MB), used size: 0x400000 (about 4MB)

Cavalry Status: Started
</pre><h2><a class="anchor" id="sub_sec_deploy_show_cavalry_auto_recycle"></a>
6.8 Auto Recycle</h2>
<p>Since SDK 2.5.7, auto recycle is enabled by default in the API <b>cavalry_mem_alloc()</b>.</p>
<p>When the application process exits, Cavalry will recycle the unused memory automatically. However, in some situations, that memory cannot be recycled.</p>
<p>For example, users create a main process. Then in this main process, users fork a new sub-process. When there are issues in the application (such as "segment fault", which makes the main process stop, but does not stop the sub-process), the memory cannot be recycled as this memory is locked by the sub-process. The only method is to kill this sub-process, then the memory will be recycled.</p>
<p>Users can use the command below to free all unused Cavalry memory. </p><pre class="fragment">board # recover_cv_mem.sh
</pre><p>If users want to recycle the memory by themselves, they must use the API <b>cavalry_mem_alloc_persist()</b>.</p>
<h2><a class="anchor" id="sub_sec_deploy_cv5x_dram_limitation"></a>
6.9 CV5x DRAM Limitation</h2>
<p>For CV5x can support a maximum of 32 GB of DRAM.</p>
<ul>
<li>As Arm is 64 bits, it can access the full DRAM.</li>
<li>Becuse CVflow and DSP are 32 bits, they can only visit 4 GB of DRAM.</li>
</ul>
<p>There are some limitations for cases when data is interactive between CVflow and DSP, or when DRAM is greater than 4 GB. For more details, refer to <em>Ambarella CV5x DG Flexible Linux SDK*.* DRAM Optimization</em>.</p>
<h2><a class="anchor" id="sub_sec_deploy_nn_bw"></a>
6.10 Network Bandwidth</h2>
<p>Also the result can match the calculated result from test_nnctrl, we do both to make sure everything is right.</p>
<p>There are two methods to test the network bandwidth (MB/s).</p>
<ul>
<li>test_nnctrl</li>
<li>DDR statistics driver, for detail, please refer to <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_dram_bw_stat">19 DRAM Bandwidth Statistics</a></li>
</ul>
<p>Here <code>test_nnctrl</code> is used to check the network bandwidth cost.</p>
<div class="fragment"><div class="line">board $ test_nnctrl -b cv22_cavalry2.2.8.3_nn.bin --in normalized_input_image_tensor=cat.bin –e -w</div>
<div class="line">Cavalry Bin: [0][cv22_cavalry2.2.8.3_nn.bin]</div>
<div class="line">INPUT: [normalized_input_image_tensor]</div>
<div class="line">Count [100] Dags: 1 / 1, ticks: 7715168, avg_vp_time: 6278.62 us, total_vp_time: 627862.00 us, total_arm_time: 627896 us</div>
<div class="line">VP BW: 10193 MB/s = 6710886592 B / 627862 us</div>
<div class="line">Input: 0 [normalized_input_image_tensor] dim: (1, 3, 320, 320), pitch: 320, dram_fmt: 0, bitvector: 0, data_fmt: (0, 0, 0, 0), loop_cnt: 1, size: 307200, phys: 0x60748000, virt: 0x7f875d9000</div>
<div class="line">Network [0] [cv22_cavalry2.2.8.3_mnet_v3.bin] total memory size: 5698752, dvi: 3419024, blob: 2258080. Bandwidth size: 7900800. Extra_high_mem size: 0</div>
<div class="line">Net_id: 0, Dags: 6 / 6, vp_ticks: 52144, vp_time: 4243 us, arm_time: 4291 us</div>
<div class="line">==== Check Bandwidth ====</div>
<div class="line">Total VP Margin BW: 9683 MB/s = 10193 MB x (100 - 5)%</div>
<div class="line">Net [0] BW : 1775 MB/s = 7900800 B / 4243 us</div>
<div class="line">Check BW Result: pass</div>
</div><!-- fragment --><p>Users can get <code>Bandwidth size: 7900800</code>, then to use this value and inference time to get the bandwidth, such as follows.</p>
<div class="fragment"><div class="line">Net [0] BW : 1775 MB/s = 7900800 B / 4243 us</div>
</div><!-- fragment --><p>For roughly estimating, users can the bandwidth size to calculate the whole cost. For example, <b>NET[1]</b> run <b>30 fps</b>, <b>Net[2]</b> run <b>15 fps</b>, then its total bandwidth size in <b>1s</b> is as below.</p>
<div class="fragment"><div class="line">Total BW size = NET[1] BW Size * 30 + Net [2] BW Size * 15</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="sec_deploy_perf_opt"></a>
7 Performance Optimization</h1>
<h2><a class="anchor" id="sub_sec_deploy_mem_cp"></a>
7.1 Memory Copy</h2>
<p>There are two methods for memory copy: <b>GDMA copy</b> and <b>VP copy</b> (in the Vproc library).</p>
<p>The <b>GDMA copy</b> can save the CPU cycle and is managed by the IAV driver. Although GDMA copy can also be used for Cavalry memory copy, Cavalry memory is not managed by the IAV driver. Therefore, users should use the direct physical address (as shown below): </p><pre class="fragment">struct iav_gdma_copy {
    union {
        unsigned long src_offset;
        unsigned long src_addr;  // Use this one for cavalry
    };
    union {
        unsigned long dst_offset;
        unsigned long dst_addr; // Use this one for cavalry
    };
    u16 src_pitch;
    u16 dst_pitch;
    u16 src_use_phys : 1;
    u16 dst_use_phys : 1;
    u16 reserved : 14;
    u16 width;
    u32 height;
    u32 src_mmap_type; // For cavalry, do not need to set it
    u32 dst_mmap_type; // For cavalry, do not need to set it
};
</pre><p>When using <b>VP copy</b>, ensure that the buffer size is greater than <b>200 KB</b>, as the VP loads DAGs to run the copy. If the buffer size is smaller, loading DAGs can be slower than using memcopy in Linux. For more details, refer to the API <b>vproc_copy()</b> in the <em>VPRoc Library in SDK Doxygen Documents</em>.</p>
<p>The EazyAI library supports <b>VP copy</b> in <b>ea_tensor_memcpy()</b>.</p>
<p>The following table shows the time taken by different copy mode.</p>
<table class="doxtable">
<caption>Table 7-1. Time taken by Different Copy Mode. </caption>
<tr>
<th align="center" rowspan="2">Mode </th><td align="center" colspan="2">10KB=10240B=0x2800B </td><td align="center" colspan="2">100KB=102400B=0x19000B </td><td align="center" colspan="2">200KB=204800B=0x32000B </td><td align="center" colspan="2">1MB=1048576B=0x100000B </td><td align="center" colspan="2">2MB=2097152B=0x200000B </td><td align="center" colspan="2">5MB=5242880B=0x500000B </td><td align="center" colspan="2">10MB=10485760B=0xA00000B </td></tr>
<tr>
<td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td><td align="center">pure copy</td><td align="center">with 4K encoding </td></tr>
<tr>
<th>vproc_memcpy </th><td align="center">0.129 ms</td><td align="center">0.189 ms </td><td align="center">0.159 ms</td><td align="center">0.241 ms </td><td align="center">0.187 ms</td><td align="center">0.264 ms </td><td align="center">0.405 ms</td><td align="center">0.511 ms </td><td align="center">0.716 ms</td><td align="center">1.064 ms </td><td align="center">1.543 ms</td><td align="center">1.994 ms </td><td align="center">2.982 ms</td><td align="center">4.017 ms </td></tr>
<tr>
<th>GDMA copy </th><td align="center">0.056 ms</td><td align="center">0.066 ms </td><td align="center">0.208 ms</td><td align="center">0.244 ms </td><td align="center">0.392 ms</td><td align="center">0.438 ms </td><td align="center">1.805 ms</td><td align="center">3.203 ms </td><td align="center">3.612 ms</td><td align="center">4.285 ms </td><td align="center">8.963 ms</td><td align="center">10.932 ms </td><td align="center">17.901 ms</td><td align="center">25.624 ms </td></tr>
<tr>
<th>memory copy </th><td align="center">0.104 ms</td><td align="center">0.148 ms </td><td align="center">0.893 ms</td><td align="center">1.111 ms </td><td align="center">1.777 ms</td><td align="center">3.731 ms </td><td align="center">9.094 ms</td><td align="center">11.857 ms </td><td align="center">18.178 ms</td><td align="center">25.728 ms </td><td align="center">45.493 ms</td><td align="center">67.777 ms </td><td align="center">90.825 ms</td><td align="center">135.632 ms </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The data above is tested on <b>CV22_WALNUT</b>.</li>
<li>The configuration file used by <b>"pure copy"</b> is <b>cv22_ipcam_config</b> and <b>no</b> other app is running in the background.</li>
<li>The configuration file used by <b>"with 4K encoding"</b> is <b>cv22s66_ipcam_config</b> and <b>DSP 4K encoding</b> is running in the background.</li>
<li>Users can refer to the following steps to get the test results: <pre class="fragment">  board # dd if=/dev/zero of=./test_10k bs=10k count=1
  board # eazyai_video.sh --vsrc_mode 3840x2160 --stream_A 3840x2160
  board # test_encode --show-stream-info (Only for check, show stream info)
  board # test_vproc_memcpy -i test_10k -o out/test_10k_out -t (get vproc_memcpy time)
  board # test_memcpy -s 0x2800 (get GDMA_copy and memory_copy time)
</pre></li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_batch"></a>
7.2 Batch for Vproc and NNCtrl</h2>
<p>The <b>vproc_resize_vect_batch()</b> in the VProc library can perform the image resizing in batches and <code>net_loop_cnt</code> in the NNCtrl library can run network in batches, saving DAG load time in the VP. This results in optimized performance.</p>
<p>There are two variables which must be configured for the batch mode, such as shown below. </p><pre class="fragment">struct net_cfg {
    ……
    IN uint32_t net_loop_cnt;  // Network loop run internal count.
          Blob memory size will be increased to "blob * net_loop_cnt".
          The port size is "port * net_loop_cnt" and the address is continuous.
          It will save (N-1) dvi loading time

    ……
};

struct net_run_cfg {
    uint32_t net_loop_cnt;  // Changes the net_loop_cnt on-the-fly.
          This value cannot be larger than the count specified while using nnctrl_init_net()

    ….
};
</pre><ul>
<li>The first is used for network initialization to allocate the resource for the worst case.</li>
<li>The second is used to define the real batch number when running the network.</li>
</ul>
<p>For example, users can set the first one to 10; when running the network, they can set the second one to any value which is less than 10.</p>
<p>Users can use the commands below to check the performance between single run and batch.</p><ul>
<li>Single run: <pre class="fragment">  board # test_nnctrl -b mobilenetv1_ssd_cavalry.bin --batch 1 --in data -e
  Net_id: 0, Dags: 6 / 6, vp_time: 7035.81 us
</pre></li>
<li>Batch with 10: <pre class="fragment">  board # test_nnctrl -b mobilenetv1_ssd_cavalry.bin --batch 10 --in data -e
  Net_id: 0, Dags: 6 / 6, vp_time: 64850 us
</pre></li>
</ul>
<dl class="section note"><dt>Note</dt><dd>There is another loop_cnt in the following structure, which is only for long short-term memory (LSTM). In LSTM, the network has internal loops. If users set it to 4, and net_loop_cnt to 10, it can perform looping with 4 x 10. In most of the cases, users are not required to use it. <pre class="fragment">    struct io_dim {
        …;
        uint32_t reserved_2[10];  // Reserved field
    };
</pre></dd></dl>
<p>For this batch mode example, users can refer to <b>ea_crop_resize_vproc()</b> in the EazyAI library, which uses batch mode.</p>
<h2><a class="anchor" id="sub_sec_deploy_reszie_crop"></a>
7.3 Crop and Resize</h2>
<p>Users can perform crop and resize in DSP or VP, using the methods below.</p>
<p>For more details, refer to <em>Basic Pyramid and Basic Canvas in SDK doxygen feature_sets Documents</em>.</p>
<ul>
<li>For single buffers, users can set the canvas buffer's resolution close to network's input resolution in Lua. In addition, the smallest canvas buffer resolution is 320x240.</li>
<li>For multiple resolution buffers, users can use a pyramid buffer, which has six layers for different resolutions. Users can also use two canvas buffers, but this may waste DRAM bandwidth. It is assumed that the main buffer is 4KP in DSP.<ul>
<li>Two Canvas buffers (480p and 720p) converts 4KP to 480p or converts 4KP to 720p.</li>
<li>Pyramid buffers convert 4KP to 720p, then convert 720p to 480p.</li>
</ul>
</li>
<li>DSP buffers can support manual feed to lock buffers that will not be deleted when in use. Users are not required to perform memory copy, which increases DRAM bandwidth.</li>
</ul>
<p>CVflow APIs are more flexible than those for DSP; DSP can only generate the frames in defined frames per second (fps). VP can generate the frames when required, meaning that DSP may waste some DRAM bandwidth.</p>
<p>Users can use one canvas buffer, then use VP to perform resizing and cropping at any time. If users do not use this process, it saves bandwidth. If users use VP to perform resizing and cropping, it will increase VP loading.</p>
<h2><a class="anchor" id="sub_sec_deploy_openmp"></a>
7.4 OpenMP for Post Processing</h2>
<p>For some networks' postprocessing, there are many calculations that run in serial mode but have no dependencies. Then users can enable <b>OpenMP(Open Multi-Processing)</b> to accelerate it. OpenMP** performs these calculations in parallel through multiple threads. For detailed usage, refer to sub_sec_caffe_mobilenetv1_ssd_run_with_live_mode.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_show_result"></a>
8 Show the Result</h1>
<p>CV chips include three interfaces or methods showing the final results:</p>
<ul>
<li>Frame buffer in VOUT (<em>test_smartfb.c</em>), which cannot perform a frame synchronization</li>
<li>Overlay in stream (<em>test_overlay.c</em>), which cannot perform a frame synchronization</li>
<li>Encode the final result to H.264 / H.265 metadata and MJPEG user data (<em>test_custom_sei.c</em>), which can insert the result in the correct frame with the frame synchronization on presentation time stamp (PTS)</li>
<li>Perform blur on detected objects (<em>test_blur.c</em>) by DSP</li>
</ul>
<p>EazyAI library supports some of the methods above in <code>ea_display.c</code>.</p>
<h2><a class="anchor" id="sub_sec_deploy_fb_vout"></a>
8.1 Frame Buffer on VOUT</h2>
<p>There are some differences between different chips:</p>
<ul>
<li>CV25, CV22, and CV2 have two VOUT devices, fb0 and fb1. fb0 is for digital VOUT and fb1 is for HDMI / composite video broadcast signal (CVBS) VOUT.</li>
<li>CV28 has one VOUT device, fb0, to show frame buffer on VOUT.</li>
<li>CV5 has three VOUT devices, fb0, fb1, and fb2. fb2 is used as default for HDMI VOUT.</li>
</ul>
<p>The SmartFB library will switch automatically for different chips.</p>
<h2><a class="anchor" id="sub_sec_deploy_blur"></a>
8.2 Blur on Stream</h2>
<p>CV2 / CV22 / CV25 has two VOUT controllers: VOUTA (LCD / digital) and VOUTB (HDMI / CVBS). CV28 has one VOUT controller: VOUTA for CVBS / MIPI DSI®.</p>
<p>CV2 / CV22 / CV25 chips have two VOUT mixers: "VOUT mixer a" and "VOUT mixer b". CV28 has one VOUT mixer, which is shared by VOUT and blur.</p>
<p>The table below lists some use cases for the VOUT mixer.</p>
<a class="anchor" id="table_mixer_configuration"></a>
<table class="doxtable">
<caption>Table 8-1. VOUT Mixer Configuration.</caption>
<tr>
<th align="center">Chip </th><th align="center">Mixer </th><th>Default VOUT </th><th>Default Blur </th></tr>
<tr>
<td rowspan="2">CV2/CV22/CV25 </td><td>VOUT mixer a </td><td>LCD / digital </td><td>Blur </td></tr>
<tr>
<td>VOUT mixer b </td><td>HDMI / CVBS </td><td>HDMI / CVBS </td></tr>
<tr>
<td>CV28 </td><td>VOUT mixer a </td><td>CVBS / MIPI_DSI </td><td>Blur </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>By default, "VOUT A" owns "VOUT mixer a" and "VOUT B" owns "VOUT mixer b" whether or not they are functioning. Blur cannot own the VOUT mixer when "VOUT A / VOUT B" are in the working state. Users can specify the "VOUT mixer a" or "VOUT mixer b" to blur or VOUT manually.</li>
<li>If the blur is enabled, it will use "VOUT mixer a" by default.</li>
<li>It is suggested to close the VOUT device whose mixer is used by blur.</li>
</ul>
</dd></dl>
<p>The following cases show how to use blur:</p>
<p><b>Draw on stream</b> (For CV2 / CV22 / CV25 / CV28) </p><pre class="fragment">board # eazyai_video.sh --stream_A 1080p --blur --enc_dummy_latency 3
board # test_fd_blur -m /sdcard/onnx_fgfd_cavalry.bin -l 2 -c 0 --top_k 200 --nms_threshold 0.5 --conf_threshold 0.7 --blur_strength 0 -s 0
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li>As HDMI VOUT is not required, users can delete the canvas buffer whose type is <em>"prev"</em> in Lua.</li>
<li>"--blur-enable 1" is used to enable stream blur insertion; "--blur-vout-mixer 0" is used to specify "VOUT mixer a" for blur, which is the default setting.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_vp_hang"></a>
9 VP Hang Debug</h1>
<p>If the VP is hanging, but the reason cannot be identified, there are three possible reasons:</p>
<ul>
<li>Core voltage is correct for chip specification, but VP will not be stable if core voltage is too low.</li>
<li>There are some issues in the DAGs, which are generated by the VAS compiler.</li>
<li>The user application has some issues, which destroy DAGs in DRAM.</li>
</ul>
<h2><a class="anchor" id="sub_sec_deploy_cavalry_log"></a>
9.1 Cavalry Log</h2>
<p><code>cavalry_log</code> can be used when VP is hanging, which can help catch some useful information. For detailed usage, refer to <em>Ambarella CV* UG Flexible Linux SDK*.* Code Building and Debug Environment</em>.</p>
<p>Follow the example below when a VP hang issue is encountered.</p>
<ol type="1">
<li>Save the printed error message of NNCtrl related interfaces to <code>app_log.txt</code> and share the <code>app_log.txt</code> to Ambarella.</li>
<li>Run the command below and share the full log <code>dmesg_log.txt</code> to Ambarella. Do not use <code>dmesg -c</code> to clean the log, the full message is required. <pre class="fragment"> board # dmesg &gt; dmesg_log.txt
</pre></li>
<li>Run the command below to check the interrupt. <pre class="fragment"> board # cat /proc/interrupts  | grep cavalry
 40:          5          0          0          0     GIC-0 185 Edge      cavalry_vp
 43:          6          0          0          0     GIC-0 188 Edge      cavalry_sched
</pre></li>
<li>Run the command below to trigger VP. The command below sends a message to VP. If it is alive, the interrupt number will increase. <pre class="fragment"> board # cavalry_log -u /lib/firmware/cavalry.bin -d 4
</pre></li>
<li>Run the command below to check the interrupt again to see if the VP is alive. If the following message is received, VP is alive. <pre class="fragment"> board # cat /proc/interrupts  | grep cavalry
 40:          6          0          0          0     GIC-0 185 Edge      cavalry_vp
 43:          7          0          0          0     GIC-0 188 Edge      cavalry_sched
</pre></li>
<li>Run the command below again and save the printed information to <code>cavalry_log.txt</code>, and then share <code>cavalry_log.txt</code> file to Ambarella. <pre class="fragment"> board # cavalry_log -u /lib/firmware/cavalry.bin -l
</pre></li>
<li>Send the <code>app_log.txt</code>, <code>dmesg_log.txt</code>, and <code>cavalry_log.txt</code> to Ambarella support team.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Please do not use <code>-d 4</code> in step 6 as <code>cavalry_log</code> with <code>-d 4</code> will return immediately without dumping any log.</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_vas_problem"></a>
9.2 VAS Problems</h2>
<p>Users can perform the steps below to check if there are issues with VAS.</p>
<ol type="1">
<li>When VP hangs, users can utilize the tool below to dump all necessary information into a single binary on their own board. <pre class="fragment"> board # cavalry_core_dump -d /tmp/dump.bin
</pre></li>
<li>Then, users can use the binary to be <em>replayed</em> on the Ambarella EVK to determine if the problem is due to the VP. In addition, after running the following command, <code>dmesg -c</code> can also get more information. <pre class="fragment">    board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r
    board # cavalry_core_dump -r /tmp/dump.bin
</pre></li>
<li>If there are some problems in <em>replayed</em>, dump the binary below, then report this issue and send <code>vpstatus_dump.bin</code> below to the Ambarella support team. <pre class="fragment"> board # vpstatus -d /tmp/vpstatus_dump.bin
</pre></li>
</ol>
<h2><a class="anchor" id="sub_sec_deploy_dram_problem"></a>
9.3 DRAM Problems</h2>
<p>For DRAM problems, users can use <b>shmoo</b> tool to perform DRAM tuning, then use the cases below to check DRAM stability.</p>
<ol type="1">
<li><p class="startli">DSP stream diff: </p><pre class="fragment"> board # init.sh --imx274_mipi
 board # test_aaa_service -a &amp;
 board # test_encode -i 1080p -f 60 --hdmi 480p -X --extra-buf 1 -A --smax 1080p -B --smax 1080p
 board # test_encode -A -H 1080p --bitrate 400000 -b 0 -e -B -H 1080p -b 0 --bitrate 400000 -e
 board # memtester-arm 50 &gt; /dev/null &amp;
 board # bandwidth-arm-thread --thread 4 --quick &gt; /dev/null &amp;
 board # test_stream --nofile --dram-en 1 --dram-mode 0
</pre><p class="startli">If DRAM is not stable, an example of a diff message shows. </p><pre class="fragment">Stream B and stream A's type[3, 3] or size[57, 81] is different, statistic count: 89
</pre><p class="startli">Users can also add “<em>memtester-arm 50 &gt; /dev/null &amp;</em>” to give DRAM more stress on the Arm side.</p>
<dl class="section note"><dt>Note</dt><dd>The example above is based on CV22; the commands of different boards are different. For example, CV28 does not have direct HDMI output; it can only use HDMI with "mipi_dsi" device.</dd></dl>
</li>
<li><p class="startli">VP CRC output check together with DSP case </p><pre class="fragment"> board # eazyai_video.sh --stream_A 1080p --stream_B 1080p --fourth_source_buf_res 1920x1080
 board # memtester-arm 50 &gt; /dev/null &amp;
 board # bandwidth-arm-thread --thread 4 --quick &gt; /dev/null &amp;
 board # test_nnctrl -c -k -t 0 -m 1 -b mobilenetv1_ssd_cavalry.bin --in data=dog.bin -b mobilenetv1_ssd_cavalry.bin --in data=dog.bin -b mobilenetv1_ssd_cavalry.bin --in data=dog.bin
</pre><p class="startli">This indicates that users run three SSDs in multi-thread. Ambarella recommends that users use their own network. Using the mobilnetv1_ssd as an example, after running the last command, this application will be blocked. If no print message appears, that means it is functioning.</p>
<p class="startli">If DRAM is not stable, CRC error report occurs, that means with the same input, the network output is different. Also if DRAM is not stable, VP will hang.</p>
<p class="startli">For details on DRAM tuning, refer to <em>Ambarella CV S6LM UG Flexible Linux SDK2.5 DRAM Tuning</em>.</p>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_deploy_dag_check"></a>
9.4 DAG Check in DRAM</h2>
<p>Sometimes, user applications can destroy the DVI binary in DRAM due to incorrect operation. DVI includes weights and bias. If it is destroyed, a VP hang may occur.</p>
<p>Users can refer to the option "--dump" in <code>test_nnctrl</code> for how to dump the DVI binary into files, then compare the dumped DVI files with the original DVI file in VAS output to find if this DVI binary has been broken.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_compatibility"></a>
10 Compatibility</h1>
<h2><a class="anchor" id="sub_sec_deploy_primitive_code"></a>
10.1 Primitive Code</h2>
<p>The primitive code, generated by the parser, is similar to the C code which is compiled by GCC.</p>
<ol type="1">
<li>For CV2x chips, as CNNGen will use the same parser for different chips, users are not required to run a DRA for different CV2x chips. They can use the same analysis for chips in the same architecture and the same usage for CV5x chips.</li>
<li>But for different architectures, such as CV2x and CV5x, the primitive code is not compatible. The means, the primitive code which is generated by CV2x parser cannot be directly used on CV5x chips. Of course, the difference is very small, the performance is almost the same, but minor accuracy may be lost. If users are not concerned about this, they can also use CV2x's primitive code on CV5x.</li>
<li>For CV72, it is totally different with different Parser and VAS compiler.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Ambarella suggests that users <b>save the parser output for their networks</b>. It will be easier for users to use the same network in different Ambarella chips in future. USers must recompile the parser output with the correct VAS compiler. Saving the final Cavalry binary is not enough; currently, only CV22 and CV25 share the same Cavalry binary.</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_vas"></a>
10.2 VAS</h2>
<p>The difference starts in the VAS stage.</p>
<ul>
<li>For CV2x, the detailed difference is as follows:<ul>
<li>CV25 and CV22 use the same VAS compiler.</li>
<li>CV2, CV22, and CV28 use different VAS compilers.</li>
</ul>
</li>
<li>All CV5x chips use the same VAS compiler, different from CV2x.</li>
<li>CV72 chip uses different VAS compiler with CV2x and CV5x.</li>
<li><b>Cavalry_gen</b> is a packaging tool for VAS built binaries. And CV2x and CV5x are using cavalry_gen v2, and CV72 is using cavalry_gen v3.</li>
</ul>
<p><b>NNCtrl</b> checks the following:</p>
<h4><a class="anchor" id="autotoc_md34"></a>
Example 1. On the CV22 board, the commands below show chip versions in NET.bin.</h4>
<pre class="fragment"># test_nnctrl -b fc.bin --bin-version
[fc.bin] Net Binary Cavalry Gen Version: 2.2.8.2
VDG Chip Arch: cv2
</pre><h4><a class="anchor" id="autotoc_md35"></a>
Example 2. On the CV22 board, the commands below report error messages when running CV2 NET.bin</h4>
<pre class="fragment"># test_nnctrl -b fc.bin --in input -e
[LIBNN  ERR] Compiled ARCH is: 1 - cv22, but VDG ARCH in NET.bin is: 2 - cv2
[LIBNN  ERR] Please update NET.bin with CNNGen cv22 Env
[LIBNN  ERR] Check VDG ARCH error in NET.bin
</pre><h4><a class="anchor" id="autotoc_md36"></a>
Example 3. On the CV22 board, the commands below can force bypass chip check.</h4>
<pre class="fragment"># test_nnctrl -b fc.bin --in input -e --no-chip-check
[LIBNN WARN] Bypass Check Chip. Debug Only!
Network [0] [fc.bin] total memory size: 9740288, dvi: 296660, blob: 9437184. Bandwidth size: 9733844
Net_id: 0, Dags: 1 / 1, vp_ticks: 243473, vp_time: 19813 us, arm_time: 19847 us
</pre><p>For chips that use different VAS compilers:</p>
<ul>
<li>In one CNNGen toolchain, if users use different VAS to compile the same primitive code which is generated by the same parser, the results of CV chips should be bit-perfect matches for most cases, except for those that require convolution or operator splitting. Even in those cases, the gap would be in the range of rounding difference.</li>
<li>In one situation, for example, users used old CNNGen to convert a network on CV25, then they were required to use a new chip which used different VAS compiler, but this tool is only supported in the latest CNNGen. If users use the latest VAS to compile old primitive code, Ambarella cannot currently promise this compatibility.<ul>
<li>There may be speed / accuracy improvement in the latest CnnGen that will be missed if using old primitive graphs.</li>
<li>Tracking target compatibility is tedious for different VAS and primitive versions.</li>
<li>Newer versions of primitive definition may be compatible to earlier versions in syntax but not in semantics.</li>
<li>Mixing earlier primitive and later VAS are not a flow that will be QA’ed in Ambarella side.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_deploy_sdk_api"></a>
10.3 SDK API</h2>
<p>All CV chips share the same APIs. At the application code level, they can use the same code with a different Cavalry deployment binary between CV2x and CV5x with cavalry <b>v2</b>.</p>
<p>The NNCtrl, VProc, and cavalry_mem libraries, which are used for the network deployment, are all <b>binary level compatible</b>. The application which is compiled with the old SDK can be used directly on the new SDK.</p>
<p>For CV72, it has similar APIs for <b>code level compatible</b>, but not <b>binary level compatible</b> with CV2x and CV5x as it is based on cavalry <b>v3</b>.</p>
<dl class="section note"><dt>Note</dt><dd>Ambarella does not recommend using the application developed with SDK 2.5 directly on SDK 3.0 as Cavalry from SDK 3.0 and SDK 2.5 are very different; SDK 3.0 has multiple advantageous features that SDK 2.5 does not have.</dd></dl>
<p>IAV APIs used for basic video features, are currently only <b>code level compatible</b>. The application developed with the old SDK can be used with re-compiling on the new SDK.</p>
<h2><a class="anchor" id="sub_sec_deploy_cv72_limit"></a>
10.4 CV72 Limitation</h2>
<p>For CV72, users must load cavalry firmware when DSP is in <b>INIT</b> status.</p>
<div class="fragment"><div class="line">board # cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --><p>And it only can generate FP16 output with CVflow engine, but EazyAI library will convert it to FP32 automatically with ARM resource when doing inference.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_nn_protect"></a>
11 NN Model Protection</h1>
<p>In a normal deployment, the user must save the model to NAND. The application will load it to DRAM, then to VP internal memory, then execute it.</p>
<ul>
<li>Saving to NAND is dangerous. There are many methods to analyse NAND data.</li>
<li>Saving to DRAM is not as dangerous, but is not considered safe as it can be visited by Arm applications.</li>
<li>VP internal memory, which can be visited only by the VP processor, is very safe because it is a special hardware flow only for Ambarella.</li>
</ul>
<p>For <b>old SDK, SDK 2.0.3, and SDK 2.5</b>:</p>
<ul>
<li>No encryption; save the original model to NAND, load to DRAM and VP. This is very dangerous.</li>
<li>Users perform encryption and save to NAND. When loading, customers decrypt the encrypted model to DRAM, then load to VP memory. This method is less dangerous.</li>
</ul>
<p>Both old methods above have risk, as NAND can be copied easily and DRAM can be visited by a hacker’s Arm applications.</p>
<p>Since <b>SDK 3.0.6</b>, NN model protection methods are introducted in VP. Some points to keep in mind:</p>
<ul>
<li>The NN model which is used for final production is encrypted by users' own private key. With secure flow, users can share the private key to VP, and VP will use its own private key to encrypt this key and save it to NAND. The private key used by the VP is different in every chip and on the same chip if used by different users.</li>
<li>When required, VP will decrypt to get users' private key and use it to decrypt the ciphertext NN model, then load it to DRAM. In DRAM, it is ciphertext.</li>
<li>Only the VP processor can see the real plaintext model.</li>
<li>All important private keys are in VP memory, which Arm cannot visit.</li>
</ul>
<p>As VP must perform decryption both in loading and running, some performance lost will be lost, but the amount lost is negligable.</p>
<ul>
<li>For the running time, performance lost is at the microseconds level. For example, yolov3 is about 303 us.</li>
<li>For the loading time, performance lost is at the millisecond level; loading occurrs during network initialization. For example, yolov3 is about 400 ms.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Users should load Cavalry before DSP bootup for encryption mode.</li>
<li>For more details on API, refer to the <em>Cavalry document in Doxygen</em> which is included in the SDK package and <em>Ambarella UG Flexible Linux SDK NN Model Protection</em>.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_per_unit"></a>
12 Unit License Protection</h1>
<p>Unit license protection (ULP) is a mechanism of restricting the software so that it can only run on licensed devices. With this mechanism, software vendors are able to release and track their software more precisely.</p>
<p>To get the per unit license (PUL), there are two core security bases: "device identity" and "license
checking". "Device identity" means that the device owns a unique ID. "License checking" has two possible approaches:</p>
<ul>
<li>License checking by connecting with an internet server.</li>
<li>License checking via the device hardware.</li>
</ul>
<p>The first approach requires that the device has Internet access when checking licenses. As some embedded devices do not have Internet access, it is not ideal for all devices. License checking using the device hardware is more appropriate for devices without Internet access.</p>
<p>Before <b>SDK 3.0.6</b>, this feature uses the DSP for key verifification with the following conditions:</p><ul>
<li>DSP should be in IDLE mode.</li>
<li>If the PUF-level 256-bit unique ID (in the Ambarelle chip) is used, it is easy to fake this ID.</li>
</ul>
<p>Since <b>SDK 3.0.6</b>, this feature will use the VP for key verification.</p><ul>
<li>VP does not have as many limitation as DSP.</li>
<li>With VP, the unique ID is not the the PUF-level 256-bit unique ID. VP uses public key and private key mechanisms. The private key is stored in VP, and users use the public key to verify the certificate which signed the private key to confirm the chip's legitimacy. This cannot be faked.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For more details, refer to <em>Ambarella UG Flexible Linux SDK Unit License Protection by CVflow</em>.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_task_priority"></a>
13 VP Task Priority</h1>
<p>In multi-network deployments, there may be some issues which users must manage for high-efficiency usage of VP resources.</p>
<p>For example: suppose there are two networks; one is very big and the other is small. If users allow the system to decide by itself, the sample network may be called in far less frequently, which will result in low-efficiency VP usage.</p>
<p>An old SDK version provides early quit and resume APIs to control the VP task priority in user applications, but it is difficult to use this for the purpose of controlling all logics in the application level.</p>
<p>In the latest SDK 3.0, there is an easy method to set the API for VP task priority, which means that the VP will control scheduling. In the user application level, only the network priority must be set.</p>
<p>For example, there are two networks as follows:</p>
<ul>
<li>Net 0 - large net, which is around 41 ms; the priority is set to 0 (lowest).</li>
<li>Net 1 - small net, which is around 2.5 ms; the priority is set to 31 (highest).</li>
</ul>
<p>If net 0 is running and net 1 is activated, net 1 will take the hardware resources as there should be only one network that can run at the same time. Net 0 will stop and wait for net 1 to finish. This result is reasonable, as net 1 has higher priority. But users may find that the running time of net 0 is prolonged to 44 ms, and the running time of net 1 jumps to ~16 ms.</p>
<p>As shown in the the following log, note that the vp_time is from "vp_time_us" in the application. </p><pre class="fragment">running net, id = 0
running net, id = 1
Got result: Net_id = 1, Frame_id = 0, vp_time: 16.50830 ms
Got result: Net_id = 0, Frame_id = 0, vp_time: 43.90284 ms
running net, id = 1
running net, id = 0
Got result: Net_id = 1, Frame_id = 1, vp_time: 2.45955 ms
Got result: Net_id = 0, Frame_id = 1, vp_time: 40.95597 ms
</pre><p>This is because "vp_time_us" indicates that the time that the network spends in the VP (note that it is not a pure computing time) has included the waiting time.</p>
<p>For “Got result: Net_id = 0, Frame_id = 0, vp_time: 43.90284 ms”, as Net 0 is waiting for Net 1, the time is the running time of "Net 1" and "Net 0".</p>
<p>For “Got result: Net_id = 1, Frame_id = 0, vp_time: 16.50830 ms”, when Net 1 comes, Net 0 does not stop immediately as it must finish the current DAG which is running. This time should include the running time of "Net 1" and "the DAG in Net 0 which should be finished before stopping".</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can use <em>cavalry_top</em> or <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_cavalry_profiler">16 Cavalry Profiler</a> to check VP usage efficiency.</li>
<li>For more details on the API, refer to the <em>Cavalry document in Doxygen</em>, which is included in the SDK package.</li>
<li>For usage examples, refer to Section <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_hyperlpr">Car License Plate Detection and Recognition</a>.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_deploy_manual_resume"></a>
13.1 Manually Resume</h2>
<p>Most networks require pre-processing. Users must call VProc for pre-processing, such as YUV to RGB convert, resize, and so on. Ambarella suggests that users set these pre-processing tasks to use the same priority as the networks. However, an issue can occur, as follows:</p>
<p>For example, A1 is the YUV to RGB for network A, A2 is the RGB resize for network A, A3 is network A, and B is network B. If users run them in two threads, the flow will be as follows: </p><pre class="fragment">One DAG in B -&gt; A1 -&gt; One DAG in B -&gt; A2 -&gt; One DAG in B -&gt; A3
</pre><p>The VProc APIs will take more time, as it includes the time of the DAG in B.</p>
<p>If users want to run A1, A2, and A3 together without being interrupted by B, they can use the flag “no_auto_resume” for network B, meaning that B will not be resumed automatically. Users must resume it manually. Users can then control the flow as shown below. </p><pre class="fragment">One DAG in B -&gt; A1 -&gt; A2 -&gt; A3 -&gt; B
</pre><p>Refer to the image below for how to use the manual resume APIs.</p>
<div class="image">
<img src="../../manual_resume_flow.png" alt=""/>
<div class="caption">
Figure 14-1. Manual Resume Flow.</div></div>
<p>For detailed examples, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_hyperlpr">Car License Plate Detection and Recognition</a>.</p>
<h2><a class="anchor" id="sub_sec_deploy_big_dag"></a>
13.2 How to Handle Big DAG</h2>
<p>Halting DAGs when running from the hardware perspective is not supported. Some large DAGs will block the small nets, which have higher priority. If a single DAG is too computationally intensive, users can split the large DAG into smaller DAGs.</p>
<p>First, users can check if there are some large DAGs, as shown below: </p><pre class="fragment">board # test_nnctrl -b onnx_yolov5m_cavalry.bin --in images --single-run -e
Net_id: 0, Dag_id: 1, vp_ticks: 141042, vp_time: 11478 us. [Run Single Dag]
Net_id: 0, Dag_id: 2, vp_ticks: 80226, vp_time: 6528 us. [Run Single Dag]
Net_id: 0, Dag_id: 3, vp_ticks: 75888, vp_time: 6175 us. [Run Single Dag]
Net_id: 0, Dag_id: 4, vp_ticks: 57969, vp_time: 4717 us. [Run Single Dag]
Net_id: 0, Dag_id: 5, vp_ticks: 22110, vp_time: 1799 us. [Run Single Dag]
Net_id: 0, Dag_id: 6, vp_ticks: 26844, vp_time: 2184 us. [Run Single Dag]
Net_id: 0, Dag_id: 7, vp_ticks: 31077, vp_time: 2529 us. [Run Single Dag]
Net_id: 0, Dag_id: 8, vp_ticks: 20210, vp_time: 1644 us. [Run Single Dag]
Net_id: 0, Dag_id: 9, vp_ticks: 18315, vp_time: 1490 us. [Run Single Dag]
Net_id: 0, Dag_id: 10, vp_ticks: 16650, vp_time: 1354 us. [Run Single Dag]
Net_id: 0, Dag_id: 11, vp_ticks: 36684, vp_time: 2985 us. [Run Single Dag]
Net_id: 0, Dag_id: 12, vp_ticks: 59714, vp_time: 4859 us. [Run Single Dag]
Net_id: 0, Dag_id: 13, vp_ticks: 57982, vp_time: 4718 us. [Run Single Dag]
Net_id: 0, Dag_id: 14, vp_ticks: 23624, vp_time: 1922 us. [Run Single Dag]
Net_id: 0, Dag_id: 15, vp_ticks: 28951, vp_time: 2356 us. [Run Single Dag]
Net_id: 0, Dag_id: 16, vp_ticks: 19573, vp_time: 1592 us. [Run Single Dag]
Net_id: 0, Dag_id: 17, vp_ticks: 17734, vp_time: 1443 us. [Run Single Dag]
Net_id: 0, Dag_id: 18, vp_ticks: 8707, vp_time: 708 us. [Run Single Dag]
Net_id: 0, Dags: 18 / 18, vp_ticks: 743300, vp_time: 60489 us, arm_time: 61186 us
</pre><p>There are two methods to perform this force splitting.</p>
<ul>
<li><p class="startli">Split with specific primitives:</p>
<p class="startli">To configure a specific DAG split, users can use the option “-prim-split-point” with the output tensor's name to tell the VAS compiler how to split, as shown below: </p><pre class="fragment"> Primitives 0 ~ 3

 Primitive 4
 VP_reshape(input,
            VP_tensor(temp, data_format(1, 1, 11, 0), vector(1, 1, 1, 2450)),
            w = 2450,
            h = 1,
            d = 1,
            p = 1,
            __cnngen_tracker = { 11 }
           );

 Primitive 5
 VP_reshape(temp,
            VP_tensor(temp2, data_format(1, 1, 11, 0), vector(1, 2450, 1, 1)),
            w = 1,
            h = 1,
            d = 2450,
            p = 1,
            __cnngen_tracker = { 12 }
           );

 Primitive 6 ~ n
</pre><ul>
<li>Single split: <pre class="fragment">   build $ vas -auto -show-progress -prim-split-point temp example.vas
</pre></li>
<li>Multiple split: <pre class="fragment">   build $ vas -auto -show-progress -prim-split-point temp,temp2 example.vas
</pre></li>
</ul>
</li>
</ul>
<ol type="1">
<li>Since version 2.4.0, the option "-pre-split-num-prim &lt;n&gt;" splits the input VAS program into multiple VAS programs so that each VAS program includes &lt;n&gt; primitives, as shown in the following command. <pre class="fragment"> build $ vas -auto -show-progress -pre-split-num-prim 7 example.vas
</pre></li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_deploy_eazyai_video"></a>
14 Eazyai Video</h1>
<p>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_vi_sh">3 Eazyai Video</a> for the detailed information.</p>
<hr  />
<h1><a class="anchor" id="sec_deploy_yuv_input"></a>
15 HDMI YUV Input</h1>
<p>Users may be required to check CNN algorithm results for specific videos; LT6911 is suggested in this case. This module's full name is "Lontium Semi, LT6911C, HDMI to MIPI DSI / CSI, QFN64P".</p>
<p>With this module, users can connect computer HDMI output to the CV EVK sensor interface. The computer HDMI output resource will be used as a sensor input. This is not limited to computer HDMI output; any device which has HDMI output can function. Then, users can play video files on their devices and CV EVK board can receive these video frames as sensor inputs, as seen in the images below.</p>
<div class="image">
<img src="../../lt6911_flow" alt=""/>
<div class="caption">
Figure 15-1. Flow.</div></div>
<p>Users can use the commands below to enable this module in EVK, using Yolov3 below as an example. </p><pre class="fragment">board # eazyai_video.sh --vin lt6911 --hdmi 720p
board # osd_server_yolov3 -p 27182 -f /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua &amp;
board # test_nnctrl_live -p 27182 -b yolo_v3_cavalry_fix8_sp50.bin -s 0 -i 1 --in data --out conv59 --out conv67 --out conv75  -t 0 -r 0.00390625
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li>If LT6911 is unstable, delete the LT6911 default microcontroller unit (MCU) code using the command "test_vin -w 0xffff".</li>
<li>If the YUV input is selected, set the mode to "0" for the “vsrc_id” option in the Lua script.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_cavalry_profiler"></a>
16 Cavalry Profiler</h1>
<p>For the deployment of multiplication algorithms, it is important for users to write applications that use the VP resource effectively. Poor logic may result in the waste of VP resources.</p>
<p>Users can use <em>cavalry_top</em> to check the idle rate of VP processor. <em>cavalry_top</em> is similar with <em>top</em> in Linux. </p><pre class="fragment">board # cavalry_top -u 100 -c 10000
</pre><p>This command is to sample with interval 100us and doing 10000 times, so it means during the duration (10000 x 100us = 1s), do sampling 10000 with interval 100us.</p>
<p>As it has limited information for users to debug, the <b>Cavalry Profiler</b> provides more detailed information with Web UI.</p>
<p>Use the <em>test_fdfr_v1</em> demo below on CV22_Walnut as an example. For details on this demo, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_mobilefacenets">Face Recognition</a>.</p>
<ol type="1">
<li><p class="startli">Generate cavalry_profiler_fdfrv1.bin.</p>
<p class="startli">After test_fdfr_v1 is running, execute the following command. </p><pre class="fragment">board # cavalry_profile -b cavalry_profiler_fdfrv1.bin
</pre></li>
<li><p class="startli">Open the Web UI.</p>
<p class="startli">Users should open the "cavalry_profiler.html" in <code>SDK/ambarella/app/utility/cavalry_profiler</code> with the browser. Any browser application can be used as it is implemented with HTML5; however, old IE browser versions should not be used. Ambarella suggests using the latset version of the Chrome browser. Users can see the following interface:</p>
<div class="image">
<img src="../../Cavalry_Profiler.jpg" alt=""/>
<div class="caption">
Figure 17-1. Cavalry Profiler.</div></div>
</li>
<li><p class="startli">Show the VP information:</p>
<p class="startli">Drag "cavalry_profiler_fdfrv1.bin" to the prompted button on the web, then the VP profile data will be analyzed and shown. Users can move the mouse and hover over an element to see related information.</p>
<div class="image">
<img src="../../Cavalry_Profiler_fdfrv1_histogram_with_note.jpg" alt=""/>
<div class="caption">
Figure 16-2. Cavalry Profiler Histogram Note.</div></div>
<p class="startli">By analyzing the above figure and the application's source code, users can get the following information:</p><ul>
<li>Part 1: VProc uses the VP resource for converting YUV to RGB and resizing RGB.</li>
<li>Part 2: VProc uses the VP resource to prepare the input data with different resolutions for the eight pnets.</li>
<li>Part 3: pnet is using the VP resource to run eight pnet models.</li>
<li>Part 4: the post-processing of pnet and pre-processing of rnet is running on Arm.</li>
<li>Part 5: rnet is using the VP resource.</li>
<li>Part 6: onet is using the VP resource.</li>
<li>Part 7: mobilefacenet is using the VP resource for face recognition.</li>
<li>Part 8: the application is waiting for the next frame as it used the block API in IAV.</li>
<li>1 ~ 8 is a full loop for this application to analyze one frame.</li>
</ul>
<p class="startli">For how to use the VP resource effectively, the most important principle is to run VP without stopping. Users can run Arm tasks and CV tasks at the same time, as they are different and independent cores.</p>
<p class="startli">Through the above analysis, for <em>test_fdfr_v1</em>, users can optimize the logic so that part 4 of the first frame and part 7 of the second frame can run at the same time, which can improve the efficiency of the VP resource.</p>
</li>
<li><p class="startli">Show the VP information separately by PID.</p>
<p class="startli">Users can click <b>"Show by PID"</b>, which is marked by the red box in the figure above. Then, users can see the information as follows.</p><ul>
<li>PID:652 is for face detection which includes the pre-process with VProc, Pnet, rnet, and onet.</li>
<li>PID:651 is for face recognition with mobilefacenet.</li>
</ul>
<div class="image">
<img src="../../Cavalry_Profiler_fdfrv1_histogram_by_PID.jpg" alt=""/>
<div class="caption">
Figure 17-3. Cavalry Profiler Histogram by PID.</div></div>
</li>
<li><p class="startli">Check the utilization rate.</p>
<p class="startli">When scrolling down, users can see the utilization rate of VP for different tasks; currently it supports <b>CVflow / feature extrating (FEX) / feature match (FMA)</b>.</p>
</li>
</ol>
<div class="image">
<img src="../../Cavalry_Profiler_fdfrv1_chart.jpg" alt=""/>
<div class="caption">
Figure 17-4. Cavalry Profiler Chart. </div></div>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Only CV2 contains FEX / FMA, and FEX / FMA is for stereo.</li>
<li>Different processes will be displayed in different colors.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_vp_clock"></a>
17 VP Reload and Clock Modification</h1>
<p>User can use below command to restart VP as below when VP is hang.</p>
<div class="fragment"><div class="line">board # rmmod cavalry; modprobe cavalry; cavalry_load -f /lib/firmware/cavalry.bin</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>In CV72, user only can do it when DSP is in "INIT" stage.</dd></dl>
<p>Users may be required to decrease the VP clock to save power, and can perform the following procedures: </p><pre class="fragment">board # cavalry_load --vp-clk 1/2
Configure VP Clock Percentage: 504/1008
board # cavalry_load --vp-clk 1/4
Configure VP Clock Percentage: 252/1008
</pre><dl class="section note"><dt>Note</dt><dd>Users cannot use the commands above when the VP is running certain tasks.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_vproc_perf"></a>
18 VProc Performance</h1>
<p>There are many common image process and traditional algorithm examples in VProc, which will benefit from the CVflow engine.</p>
<p>For detailed performance, refer to <em>d1/d66/page_lib_vproc_doc.html#sec_vproc in the Doxygen documents of the Linux SDK package</em>.</p>
<dl class="section note"><dt>Note</dt><dd>For network performance, CV2 is approximately 4 x CV22, 16 x CV28M, or more. The traditional algorithm in VProc is decided by different cases, the reason being some traditional operators that VProc used have similar performances among CV28M, CV22, and CV2.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_deploy_dram_bw_stat"></a>
19 DRAM Bandwidth Statistics</h1>
<p>Arm, DSP, and CVflow will use the same double data rate (DDR) controller.</p>
<ul>
<li>Priority, DSP &gt; Arm &gt; CVflow. If DSP runs out of bandwidth, CVflow and Arm will be too slow for DRAM I/O operation. And even ARM have higher priority than CVflow, CVflow will get higher bandwidth than ARM as it is too faster.</li>
<li>Users can use the statistics tool to check the bandwidth usage, provided below. For more details, refer to <em>feature_sets/d5/dd4/fs_others_dram_statistics.html</em> in the Doxygen documents of the Linux SDK package. <pre class="fragment">  [Usage]
    echo duration=M interval=N verbose=0/1 &gt; /proc/ambarella/dram_statistics
      M: statistical time in ms, now M=5000
      N: statistics in every N ms, 0ms means no interval, now N=1000
  [DRAM Info]
    LPDDR4 with 64-bits @ 1560MHz, Burst Size: 64, theory BW: 23803 MB/s
  [Average BW]
    Module        Bandwidth(In MB/s)      Percent
    -----------
    Cortex                    21.098               0.88
    DSP                     1881.053               7.902
    VP                      4182.182              17.569
    GDMA                       0.409               0.01
    ENET                       0.003               0.00
    Other                      0.001               0.00
    All                     6084.749              25.562
  [Minimal BW among each interval]
    Module        Bandwidth(In MB/s)      Percent
    -----------
    [Cortex]                  17.366               0.72
    DSP                     1880.176               7.898
    VP                      4181.658              17.567
    GDMA                       0.000               0.00
    ENET                       0.011               0.00
    Other                      0.000               0.00
    All                     6079.189              25.538
    -----------
    Cortex                    17.366               0.72
    [DSP]                   1880.176               7.898
    VP                      4181.658              17.567
    GDMA                       0.000               0.00
    ENET                       0.011               0.00
    Other                      0.000               0.00
    All                     6079.189              25.538
    -----------
    Cortex                    18.041               0.75
    DSP                     1881.571               7.904
    [VP]                    4176.131              17.544
    GDMA                       0.000               0.00
    ENET                       0.000               0.00
    Other                      0.000               0.00
    All                     6075.745              25.524
    -----------
    Cortex                    17.366               0.72
    DSP                     1880.176               7.898
    VP                      4181.658              17.567
    [GDMA]                     0.000               0.00
    ENET                       0.011               0.00
    Other                      0.000               0.00
    All                     6079.189              25.538
    -----------
    Cortex                    17.899               0.75
    DSP                     1880.911               7.901
    VP                      4187.530              17.591
    GDMA                       0.000               0.00
    [ENET]                     0.000               0.00
    Other                      0.000               0.00
    All                     6086.293              25.568
    -----------
    Cortex                    17.366               0.72
    DSP                     1880.176               7.898
    VP                      4181.658              17.567
    GDMA                       0.000               0.00
    ENET                       0.011               0.00
    [Other]                    0.000               0.00
    All                     6079.189              25.538
    -----------
  [Maximal Total BW among each interval]
    Module        Bandwidth(In MB/s)      Percent
    -----------
    Cortex                    17.899               0.75
    DSP                     1880.911               7.901
    VP                      4187.530              17.591
    GDMA                       0.000               0.00
    ENET                       0.000               0.00
    Other                      0.000               0.00
    All                     6086.293              25.568
  [Interval BW]
    Module        Bandwidth(In MB/s)      Percent
    -----------
    Cortex                    33.855               0.142
    DSP                     1880.745               7.900
    VP                      4183.336              17.574
    GDMA                       2.000               0.08
    ENET                       0.003               0.00
    Other                      0.004               0.00
    All                     6099.921              25.625
    -----------
    Cortex                    17.366               0.72
    DSP                     1880.176               7.898
    VP                      4181.658              17.567
    GDMA                       0.000               0.00
    ENET                       0.011               0.00
    Other                      0.000               0.00
    All                     6079.189              25.538
    -----------
    Cortex                    18.378               0.77
    DSP                     1881.933               7.905
    VP                      4182.277              17.569
    GDMA                       0.000               0.00
    ENET                       0.002               0.00
    Other                      0.004               0.00
    All                     6082.572              25.552
    -----------
    Cortex                    18.041               0.75
    DSP                     1881.571               7.904
    VP                      4176.131              17.544
    GDMA                       0.000               0.00
    ENET                       0.000               0.00
    Other                      0.000               0.00
    All                     6075.745              25.524
    -----------
    Cortex                    17.899               0.75
    DSP                     1880.911               7.901
    VP                      4187.530              17.591
    GDMA                       0.000               0.00
    ENET                       0.000               0.00
    Other                      0.000               0.00
    All                     6086.293              25.568
    -----------
  [Statistics Info]
    ID     ClientName    Request       Burst   MaskBurst
     0           AXI0    1621738     1621733      106482
     1       ARM_DMA0          0           0           0
     2       ARM_DMA1          0           0           0
     3           ENET        180         249          51
     4           FDMA         32         144           4
     5          CANC0          0           0           0
     6          CANC1          0           0           0
     7           GDMA      16384       32768           0
     8          SDXC0          0           0           0
     9          SDXC1          0           0           0
    10     USB20(DEV)          0           0           0
    11    USB20(HOST)          0           0           0
    12          ORCME      28566       57132           0
    13        ORCCODE     836773     1673546           0
    14          ORCVP          0           0           0
    15     ORCL2CACHE     747262      747262       78674
    16           SMEM   54562890   137301866    15063236
    17          VMEM0  100523578   341606968      171110
    18            FEX          0           0           0
    19           BMEM          0           0           0
    BANK  Bank0   Bank1   Bank2   Bank3   Bank4   Bank5   Bank6   Bank7
    Open  3012328 2993616 3027973 3043783 3038687 2993916 3005187 2954373
</pre></li>
</ul>
<hr  />
 </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-cavalry_html_ga37331a8707327be2ea6a7ae00e8322ae"><div class="ttname"><a href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a></div><div class="ttdeci">cavalry_info</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
