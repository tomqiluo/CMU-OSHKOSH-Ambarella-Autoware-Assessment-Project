<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: Tensorflow Demos</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d7/d0a/fs_cnngen_tf_demos.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Tensorflow Demos </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the Tensorflow CNNGen demos.</p>
<hr  />
<h1><a class="anchor" id="sec_tf_audio_ds_cnn_kws"></a>
Audio with DS_CNN_KWS</h1>
<p>DS_CNN_KWS is a tensorflow implementation of keyword spotting model.</p>
<ul>
<li>The published model recognizes 12 different classes with "_slilence", "_unknow_", "yes", "no", "up", "down", "left", "right", "on", "off", "stop" and "go".</li>
<li>The input should be a WAV binary as follows.<ul>
<li>Length is one second</li>
<li>16 kHz</li>
<li>16 bit</li>
<li>1 channel</li>
</ul>
</li>
</ul>
<p>The following sections demonstrate how to convert ds_cnn model with Ambarella CNNgen toolchain and how to run model in file mode with SDK package.</p>
<h2><a class="anchor" id="ds_cnn_kws_preprcess_model"></a>
1 Preprocess Model</h2>
<p>The original model is from a project in Github with Apache License v2.0. The preprocessing steps are as shown below.</p>
<ol type="1">
<li>Download source code project from <a href="https://github.com/ARM-software/ML-KWS-for-MCU">https://github.com/ARM-software/ML-KWS-for-MCU</a> and reset to below commit. <div class="fragment"><div class="line">build $ git reset --hard 8151349b110f4d1c194c085fcc5b3535bdf7ce4a</div>
</div><!-- fragment --></li>
<li>Model can be found at <code>&lt;User_Path&gt;/ML-KWS-for-MCU/Pretrained_models/DS_CNN/DS_CNN_L.pb</code>.</li>
<li><p class="startli">As the WAV decoder node and MFCC node cannot be supported, please use script "model_preprocess.py" to delete these two nodes. </p><div class="fragment"><div class="line">build $ model_preprocess.py -in Reshape_1 -on labels_softmax -<span class="keywordflow">if</span> DS_CNN_L.pb -of &lt;Absolute_Path&gt;/ds_cnn_l_opt.pb</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>"model_preprocess.py" is in <code>&lt;User_Path&gt;/cnngen/tensorflow/test_networks/ds_cnn_kws/script/</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
<h2><a class="anchor" id="ds_cnn_kws_prepare_dra_binaries"></a>
2 Prepare DRA Binaries</h2>
<p>As the model needs audio MFCC features as the input, please prepare it as below for DRA analysis.</p>
<ol type="1">
<li><p class="startli">Run script "gen_mfcc_features.py" to extract MFCC features and to save features in bin file. </p><div class="fragment"><div class="line">build $ gen_mfcc_features.py -i dra_wav/ -o dra_wav.txt -bf dra_bin/ -bo dra_bin.txt</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>Input file must be 16 kHz, 16 bit, 1 channel and one second length audio file with WAV format.</li>
<li>This script will generate DRA binaries list file of source file and features binary file which is for parser.</li>
<li>"gen_mfcc_features.py" is in <code>&lt;User_Path&gt;/cnngen/tensorflow/test_networks/ds_cnn_kws/script/</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
<h2><a class="anchor" id="ds_cnn_kws_generate_cavalry_binary"></a>
3 Generate Cavalry Binary</h2>
<p>Generate the Cavalry binary as shown below:</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/test_networks/ds_cnn_kws/config/ea_cvt_tf_ds_cnn_kws.yaml</div>
</div><!-- fragment --><p>The output is in: <code>out/tensorflow/test_networks/tf_ds_cnn_kws/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/test_networks/tf_ds_cnn_kws</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_ds_cnn_kws.bin</code> is in the cnngen output folder <code>out/tensorflow/test_networks/tf_ds_cnn_kws/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_ds_cnn_kws</code>.</li>
<li>For X86 simulator, model desc json file <b>tf_ds_cnn_kws.json</b> is in the cnngen output folder <code>out/tensorflow/test_networks/tf_ds_cnn_kws/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_ds_cnn_kws_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/test_networks/tf_ds_cnn_kws/&lt;chip&gt;/&lt;chip&gt;_ades_tf_ds_cnn_kws</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Current default output data format is float32. For CV7x, please use <code>-od fp16</code> in command as it does not support FP32.</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_ds_cnn_kws_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/test_networks/tf_ds_cnn_kws/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_ds_cnn_kws/&lt;chip&gt;_cavalry&lt;version&gt;_tf_ds_cnn_kws.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/test_networks/tf_ds_cnn_kws/tf_ds_cnn_kws_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/test_networks/tf_ds_cnn_kws</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_ds_cnn_kws_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_ds_cnn_kws_run_c_inference"></a>
5 Run C Inference</h2>
<ol type="1">
<li>Copy the generated binary model <b>&lt;chip&gt;_cavalry&lt;version&gt;_tf_ds_cnn_kws.bin</b> and <b>silence.bin</b> to SD card and plug the SD card TO EVK.</li>
<li>Run with File mode on EVK board.<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run. <div class="fragment"><div class="line">board # test_nnctrl -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_ds_cnn_kws.bin --in Reshape_1=silence.bin --out labels_softmax=labels_softmax.bin</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Use script "get_result.py" in PC to get the inference result from binary file which is generated at step 2. <div class="fragment"><div class="line">build $ get_result.py -l labels.txt -f labels_softmax.bin</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>labels.txt is in <code>out/tensorflow/test_networks/tf_ds_cnn_kws/cavalry_tf_ds_cnn_kws/</code>.</li>
<li>"get_result.py" is in <code>&lt;User_Path&gt;/cnngen/tensorflow/test_networks/ds_cnn_kws/script/</code>.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_tf_bodypix"></a>
Bodypix</h1>
<p>This live demo is for human body segmentation, human body part segmentation, and human pose detection. The following sections explains how to convert the models and run the live demo.</p>
<p>Please <b>note</b> that the tfjs model of this demo is in a project which has <b>Apache License 2.0</b>. But please ask for detailed information from Tensorflow official public if customers need to use. Ambarella assumes no responsibility for this. Address of this public project is as follows: <a href="https://github.com/tensorflow/tfjs-models/tree/master/body-pix">https://github.com/tensorflow/tfjs-models/tree/master/body-pix</a> , <a href="https://github.com/google-coral/project-bodypix">https://github.com/google-coral/project-bodypix</a></p>
<h2><a class="anchor" id="sub_sec_tf_bodypix_download_model_file"></a>
1 Download and Export Model File</h2>
<p>Users could refer to <code>cnngen_sample_package/tensorflow/demo_networks/bodypix/scripts/get_convert_model.sh</code> for the tfjs models <b>downloading</b> and the pb file <b>converting</b>. Tensorflow pb models will be generated under: <code>cnngen_samplepackge/tensorflow/demo_networks/bodypix/models</code>.</p>
<dl class="section note"><dt>Note</dt><dd>By default, the get_convert_model.sh script installs tensorflowjs. However, <b>the installation of tensorflowjs upgrades tensorflow version to 2.1.X.</b> Please read this script and decide whether the command fits users' environment. After the execution of this script, users may need to <b>downgrade the tensorflow version back</b> to the CNNGen toolchain default setting before using the CNNGen tool. The default tensorflow version could be found in the toolchain installation script: <code>Ambarella_Toolchain_CNNGen_&lt;ver&gt;_&lt;date&gt;/Ubuntu-&lt;ver&gt;/ubuntuToolChain-&lt;ver&gt;-&lt;date&gt;.ubuntu-&lt;ver&gt;</code>, search key word "tensorflow" in that file. <div class="fragment"><div class="line">build $ cd &lt;user_path&gt;/cnngen_sample_package/tensorflow/demo_networks/bodypix/scripts/</div>
<div class="line">build $ cat get_convert_model.sh</div>
<div class="line">build $ sh get_convert_model.sh</div>
<div class="line">build $ ls ../models/</div>
<div class="line">         bodypix_mobilenet_s16_sp25.pb bodypix_mobilenet_s16_full.pb</div>
</div><!-- fragment --></dd></dl>
<p>The previous script will download mobilenet based bodypix tfjs model and resnet50 based bodypix tfjs model. For mobilenet based bodypix, it will also download three kinds of weight including <b>sp25, sp50, full model</b>. But for resnet50, only <b>full</b> version is available. Users could also choose stride 32, 16, or 8 for these models. As there are many choices, the <em>get_convert_model.sh</em> only supports mobilenet full stride 16 version and mobilenet sp25 stride 16 version. If the user has other demands, please refer to the command in that script and generate your own model.</p>
<p>The original model of bodypix is stored here: <a href="https://storage.googleapis.com/tfjs-models/savedmodel/bodypix">https://storage.googleapis.com/tfjs-models/savedmodel/bodypix</a></p>
<p>Followings are some information about the content in get_convert_model.sh, if users are not interested in it, please skip and continue to the next subsection:</p>
<p>The models stored in the previous link is in tfjs format, so it is necessary to convert them to tensorflow frozen pb models. The convert tool could refer to: <a href="https://github.com/patlevin/tfjs-to-tf">https://github.com/patlevin/tfjs-to-tf</a>. Users could use the following command to install this <em>tfjs-graph-converter</em> tool.</p>
<div class="fragment"><div class="line">build $ python3 -m pip install tfjs-graph-converter</div>
</div><!-- fragment --><p>Or users could download the tfjs-to-tf source code and use the following command to generate tensorflow pb frozen model.</p>
<div class="fragment"><div class="line">build $ python3 &lt;user_path&gt;/tfjs-to-tf/tfjs_graph_converter/converter.py tfjs_model.json output.pb --output_format tf_frozen_model</div>
</div><!-- fragment --><p>The tensorflowjs need to be installed before using this tfjs_graph_converter.</p>
<div class="fragment"><div class="line">build $ python3 -m pip install tensorflowjs==1.5.2</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_tf_bodypix_model_preprocessing"></a>
2 Model preprocessing</h2>
<p>The original model is from a project in Github, please refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_bodypix_download_model_file">1 Download and Export Model File</a> for detailed information. The preprocessing steps are as shown below.</p>
<ol type="1">
<li>For full model.<ul>
<li>Graph Surgery for optimization. <div class="fragment"><div class="line">build $ graph_surgery.py tf \</div>
<div class="line">   -p tensorflow/demo_networks/bodypix/models/origin_model/<span class="stringliteral">&quot;bodypix_mobilenet_s16_full.pb&quot;</span> \</div>
<div class="line">   -o tensorflow/demo_networks/bodypix/models/bodypix_mobilenet_s16_full_481_641.pb \</div>
<div class="line">   -isrc <span class="stringliteral">&quot;i:&quot;</span>sub_2<span class="stringliteral">&quot;|is:1,&quot;</span>481,641<span class="stringliteral">&quot;,3&quot;</span> \</div>
<div class="line">   -t <span class="stringliteral">&quot;ConstantifyShapes&quot;</span> \</div>
<div class="line">   -on <span class="stringliteral">&quot;float_segments,float_part_heatmaps,float_heatmaps,float_short_offsets,MobilenetV1/displacement_fwd_2/BiasAdd,MobilenetV1/displacement_bwd_2/BiasAdd&quot;</span></div>
</div><!-- fragment --></li>
<li>Generate input files for the post-processing model. <div class="fragment"><div class="line">build $ python3 tools/accuracy_tool/pc_module/run_tensorflow_model.py \</div>
<div class="line">   --pbfile tensorflow/demo_networks/bodypix/models/bodypix_mobilenet_s16_full_481_641.pb \</div>
<div class="line">   --inode sub_2 \</div>
<div class="line">   --onode float_part_heatmaps \</div>
<div class="line">   --folder tensorflow/demo_networks/bodypix/dra_img/ \</div>
<div class="line">   --extension jpg \</div>
<div class="line">   --mean 127.5 --scale 0.007843137254902 \</div>
<div class="line">   --input_shape 481,641 --isbgr 0 \</div>
<div class="line">   --out tensorflow/demo_networks/bodypix/postproc_dra_bin \</div>
<div class="line">   --out_data_format fp32</div>
<div class="line">build $ gen_image_list.py</div>
<div class="line">   -f tensorflow/demo_networks/bodypix/postproc_dra_bin -ns \</div>
<div class="line">   -o tensorflow/demo_networks/bodypix/postproc_dra_bin/dra_bin_list.txt</div>
</div><!-- fragment --></li>
<li>Generate post-processing model. <div class="fragment"><div class="line">build $ python3 tensorflow/demo_networks/bodypix/scripts/post_process_tf.py \</div>
<div class="line">     --pbfile tensorflow/demo_networks/bodypix/models/bodypix_mobilenet_s16_full_481_641_post_proc.pb \</div>
<div class="line">     --input_h_w 481,641 \</div>
<div class="line">     --inode part_seg \</div>
<div class="line">     --onode argmax \</div>
<div class="line">     --stride 16</div>
</div><!-- fragment --></li>
<li>Yaml info(<code>tensorflow/demo_networks/bodypix/config/</code>).<ol type="a">
<li>Bodypix. <div class="fragment"><div class="line"><span class="preprocessor"># This yaml is used to quantify the model by tool &quot;eazyai_cvt.py&quot;.</span></div>
<div class="line"> </div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: tf_bodypix</div>
<div class="line"><a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a>: out/tensorflow/demo_networks</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>: tensorflow/demo_networks/bodypix/models/bodypix_mobilenet_s16_full_481_641.pb</div>
<div class="line"> </div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path: tensorflow/demo_networks/bodypix/dra_img/</div>
<div class="line">    in_file_ext: jpg</div>
<div class="line">    out_shape: 1,3,481,641</div>
<div class="line">    out_data_format: uint8</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color: RGB</div>
<div class="line"> </div>
<div class="line">input_nodes:</div>
<div class="line">  sub_2:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line">    mean: 127.5</div>
<div class="line">    std: 128</div>
<div class="line"> </div>
<div class="line">output_nodes:</div>
<div class="line">  float_heatmaps:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,21607</div>
<div class="line">  float_short_offsets:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,43214</div>
<div class="line">  MobilenetV1/displacement_fwd_2/BiasAdd:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,40672</div>
<div class="line">  MobilenetV1/displacement_bwd_2/BiasAdd:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,40672</div>
<div class="line">  float_segments:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    shape: 1,31,41,1</div>
<div class="line">  float_part_heatmaps:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    shape: 1,31,41,24</div>
<div class="line"> </div>
<div class="line">cnngen_convert:</div>
<div class="line">  dra_option:</div>
<div class="line">    strategy: <span class="keyword">auto</span></div>
</div><!-- fragment --></li>
<li><p class="startli">Bodypix post_proc.</p>
<p class="startli">For Bodypix post_proc, user should modify data_prepare(the step2: Generate input files for the post-processing model.), input, output of yaml. </p><div class="fragment"><div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path: tensorflow/demo_networks/bodypix/postproc_dra_bin/dra_bin_list.txt</div>
<div class="line">    in_file_ext: bin</div>
<div class="line">    out_shape: 1,24,31,41</div>
<div class="line">    out_data_format: fp32</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: UseOriginalFiles</div>
<div class="line"> </div>
<div class="line">input_nodes:</div>
<div class="line">  part_seg:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line"> </div>
<div class="line">output_nodes:</div>
<div class="line">  argmax:</div>
<div class="line">    data_format: uint8</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,480,640</div>
</div><!-- fragment --></li>
</ol>
</li>
</ul>
</li>
<li><p class="startli">For sp25 model.</p>
<p class="startli">Please refer to the first part <b>For full model</b> of chapter <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_bodypix_model_preprocessing">2 Model preprocessing</a>. It only needs to change the model to sp25:<code>bodypix_mobilenet_s16_sp25.pb</code> and follow the steps in sequence.</p>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_tf_bodypix_generate_cavalry_binary"></a>
3 Generate Cavalry Binary</h2>
<ul>
<li><p class="startli">Generate the Cavalry binary.</p>
<p class="startli">Generate the Cavalry binary as shown below:</p>
</li>
</ul>
<div class="fragment"><div class="line">build $ eazyai_cvt -c tensorflow/demo_networks/bodypix/config/ea_cvt_tf_bodypix.yaml</div>
<div class="line">build (cv2x &amp; cv5x) $ eazyai_cvt -c tensorflow/demo_networks/bodypix/config/ea_cvt_tf_bodypix_post_proc.yaml</div>
<div class="line">build (cv7x &amp; cv3) $ eazyai_cvt -c tensorflow/demo_networks/bodypix/config/ea_cvt_tf_bodypix_post_proc_for_cv7x_cv3.yaml</div>
</div><!-- fragment --><p>The <code>tf_bodypix_post_proc</code> model uses bin input in this demo, the cv7x/cv3 does not support the fp32 input/output format. To facilitate conversion, different data formats of input and yaml are provided for tf_bodypix_post_proc. The output is in: <code>out/tensorflow/demo_networks/tf_bodypix/</code>. The output is in: <code>out/tensorflow/demo_networks/tf_bodypix_post/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_bodypix</code> and <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_bodypix_post</code>.</li>
<li>For EVK, the tf_bodypix cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_bodypix</code>. The tf_bodypix_post cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix_post.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_bodypix_post</code>.</li>
<li>For X86 simulator, the tf_bodypix model desc json file <b>tf_bodypix.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>, ades command <b>tf_bodypix_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix/&lt;chip&gt;/&lt;chip&gt;_ades_tf_bodypix</code>. The tf_bodypix_post model desc json file <b>tf_bodypix_post.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix_post/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>, ades command <b>tf_bodypix_post_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_bodypix_post/&lt;chip&gt;/&lt;chip&gt;_ades_tf_bodypix_post</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<ul>
<li>The relationship between input resolution and output size.</li>
</ul>
<ol type="1">
<li><p class="startli">About <b>changing the input resolution</b>:</p>
<p class="startli">When the user need to change the input resolution, it is also <b>necessary to change the output size</b>. As this network has multiple outputs, the flattened size could be calculated as:</p><ul>
<li>output_h = floor(input_h / stride) + 1, output_w = floor(input_w / stride) + 1</li>
<li>pose_keypoint_num = 17, pose_line_num = 16</li>
<li>float_heatmaps: output_h * output_w * pose_keypoint_num</li>
<li>float_short_offsets: output_h * output_w * pose_keypoint_num * 2</li>
<li>MobilenetV1/displacement_fwd_2/BiasAdd: output_h * output_w * pose_line_num * 2</li>
<li>MobilenetV1/displacement_bwd_2/BiasAdd: output_h * output_w * pose_line_num * 2</li>
</ul>
<p class="startli">For example: A 481x641 input resolution has a feature output resolution as (31, 41), so <code>float_heatmaps is 31 * 41 * 17 = 21607</code>, <code>float_short_offsets is float_heatmaps * 2 = 43214</code>, <code>MobilenetV1/displacement_fwd_2/BiasAdd is 31 * 41 * 2 * 16 = 40672</code></p>
<p class="startli">Here are some output sizes corresponding to other resolutions in yaml:</p><ol type="a">
<li>input: 321x481 <div class="fragment"><div class="line">float_heatmaps:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,11067</div>
<div class="line">float_short_offsets:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,22134</div>
<div class="line">MobilenetV1/displacement_fwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,20832</div>
<div class="line">MobilenetV1/displacement_bwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,20832</div>
</div><!-- fragment --></li>
<li>input: 481x641 <div class="fragment"><div class="line">float_heatmaps:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,21607</div>
<div class="line">float_short_offsets:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,43214</div>
<div class="line">MobilenetV1/displacement_fwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,40672</div>
<div class="line">MobilenetV1/displacement_bwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,40672</div>
</div><!-- fragment --></li>
<li>input: 721x1281 <div class="fragment"><div class="line">float_heatmaps:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,63342</div>
<div class="line">float_short_offsets:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,126684</div>
<div class="line">MobilenetV1/displacement_fwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,119232</div>
<div class="line">MobilenetV1/displacement_bwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,119232</div>
</div><!-- fragment --></li>
<li>input: 1057x1921 <div class="fragment"><div class="line">float_heatmaps:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,137819</div>
<div class="line">float_short_offsets:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,275638</div>
<div class="line">MobilenetV1/displacement_fwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,259424</div>
<div class="line">MobilenetV1/displacement_bwd_2/BiasAdd:</div>
<div class="line">  data_format: fp32</div>
<div class="line">  transpose: 0,2,3,1</div>
<div class="line">  shape: 1,1,1,259424</div>
</div><!-- fragment --></li>
</ol>
</li>
<li><p class="startli">About <b>output permuting</b>:</p>
<p class="startli">OpenCV uses H,W,C as data dimension order, while CNNGen uses (P,)C,H,W by default.In order to facilitate the post-processing with OpenCV, the CNNGen output is transposed to H,W,C order using <em>"it"</em>:0,2,3,1. </p><dl class="section note"><dt>Note</dt><dd>Ambarella recommends to use 481,641 resolution full mobilenet model to reach 15 fps on CV25M (VP 504 MHz) or 721,1281 resolution sp25 mobilenet model to reach 10+ fps on CV25M (VP 504 MHz). Users could try other sparse ratio or resolution for different requirement.</dd></dl>
<p>In the post-processing work-flow, a tensorflow pb model is real-time generated and converted to cavalry binary. This model mainly contains two tensorflow operators: <em>"resize"</em> and <em>"argmax"</em>. These two operators could be fairly speeded up if they are calculated by VP part instead of Arm. The DRA image of this model is generated by the backbone network with the help of run_tensorflow_model.py under: <code>cnngen_sample_package/tools/accuracy_tool/pc_module</code>.</p>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_tf_bodypix_build_binary"></a>
4 Build SDK Binary</h2>
<p>Choose the bodypix application in the menuconfig.</p>
<div class="fragment"><div class="line">build $ make cv22_ipcam_config</div>
<div class="line">build $ make menuconfig</div>
<div class="line">   [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">      [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">         [*] Build EazyAI applications  ---&gt;</div>
<div class="line">            [*] Build Bodypix EazyAI apps</div>
<div class="line">build $ make test_bodypix</div>
</div><!-- fragment --><p>Download the binary to the board.</p>
<h2><a class="anchor" id="sub_sec_tf_bodypix_run_python_inference"></a>
5 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_bodypix/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_bodypix/&lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix.bin</div>
<div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_bodypix_post/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_bodypix_post/&lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix_post.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_bodypix/tf_bodypix_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/demo_networks/tf_bodypix</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_bodypix_post/tf_bodypix_post_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/demo_networks/tf_bodypix_post</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_bodypix_run_c_inference">6 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_bodypix_run_c_inference"></a>
6 Run C Inference</h2>
<p>Place the generated binary model &lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix.bin and &lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix_post.bin to the board. Run:</p>
<div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 7 --reallocate_mem overlay,0x04009000 --save_dir /root</div>
<div class="line">board # test_bodypix -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix.bin --in sub_2 --out float_segments --out float_part_heatmaps</div>
<div class="line">    --out float_heatmaps --out float_short_offsets --out MobilenetV1/displacement_fwd_2/BiasAdd --out MobilenetV1/displacement_bwd_2/BiasAdd \</div>
<div class="line">   -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix_post.bin --in part_seg --out argmax --part_enable --pose_enable</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>This live demo is not ready on CV72, please try file mode first.</dd></dl>
<p>The result could be observed from the stream or VOUT, depending on the show mode that the user chosen by the parameter <em>draw_mode</em>. A Lua script is generated under <em>/root/run_ipc.lua</em>. Users could modify this Lua if needed and reuse this Lua script as:</p>
<div class="fragment"><div class="line">board # eazyai_video.sh --customer_lua run_ipc.lua --hdmi 1080p --stream_A 1080p --enc_dummy_latency 7 --reallocate_mem overlay,0x04009000</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>If detection result is shown on the stream overlay, then the users could adjust the <em>enc_dummy_latency</em> value [0 ~ 7]. The setting of this value depends on the VP + Arm processing total time. For example, if <em>enc_dummy_latency</em> is set as 7 and fps is set to 30, considering there are 2 frame latency kept for vDSP depth and 1 frame kept for assurance, then the available calculation time window for VP + ARM processing is (7 - 2 - 1) * 33.3ms, which equals to 133.2ms.</dd></dl>
<p>Options:</p><ul>
<li>part_enable: enables body part detection. The default is disabled.</li>
<li>seg_thres: segmentation score threshold setting (-10 ~ 10), the default value is 0. As this threshold is set before the sigmoid function, the range is larger. When it is set as 0, it equals to 0.5 after sigmoid.</li>
<li>pose_enable: enables pose detection. The default is disabled.</li>
<li>stride: stride of the backbone network. The default value is 16.</li>
<li>refine_steps: keypoint refinement iteration number. The default value is 10. The larger this value is, the larger the size of the detected human skeleton is and the more stretched the posture is.</li>
<li>pose_max_num: detects maximum number of pose, the default value is 5.</li>
<li>pose_thres: poses score threshold, the default value is 0.4.</li>
<li>nms_radius: keypoint NMS filtering radius, the default value is 20.0.</li>
<li>circle_buf_num: the number of circular_buffer, the default value is 2. When the number is 1, it means serial processing.</li>
<li>draw_mode: sets the result showing method, 0: VOUT, 1: Stream, the default value is 1. If setting the draw mode to VOUT, the user could use the "enc-dummy" parameter of the test_encode to roughly control the detection result's synchronization effect with the orginal image; If setting draw mode to stream, as the "frame sync" method is applied in this application, please ensure this "enc-dummy" parameter is large enough to allow DSP to apply the detection result to the stream.</li>
<li>mask_trans: segmentation mask transparency, the default value is 210. The larger this value is, the less transparent the mask is.</li>
<li>draw_pose_num: the num of pose drawn on overlay, the default value is 5. This number doesn't affect pose detection result. It only draws this number of poses in score decrease order.</li>
<li>text_ratio: sets the overlay text scale ratio, the default value is 0.6. Users could enlarge this parameter while using higher output resolution, so that the text is large enough to be seen on the screen.</li>
<li>x_offset: sets the overlay text y offset, the default value is 30. When the text_ratio is very large, some part of the title may exceed the top edge, users could use this parameter to adjust the title horizontal position.</li>
<li>y_offset: sets the overlay text y offset, the default value is 50. Use this parameter to adjust the title vertical position.</li>
<li>background_file: gives a 8 bits per pixel three-channel image as the background. Person segmentation is shown this background above. There are some sample images under <em> ambarella/unit_test/private/cv_test/bodypix/background_image_samples </em>.</li>
<li>crop_enable: while this parameter is enabled, the boarder of the feature map is discarded. The discarded boarder width equals to stride / 2. The default is disabled.</li>
<li>roi_w, roi_h: while this parameter is set as non-zero value, the detection is only focused on part of the display window. The default value is 0.</li>
</ul>
<p>The Lua script also needs to be adjusted correspondingly, while using this ROI feature. For example, set the previous Lua script as:</p>
<div class="fragment"><div class="line">board # vi /root/run_ipc.lua</div>
<div class="line">    ... ...</div>
<div class="line">   chan_0 = {</div>
<div class="line">   ... ...</div>
<div class="line">   main = {</div>
<div class="line">         max_output = {0, 0}, -- output width</div>
<div class="line">         input      = {0, 0, 0, 0}, -- full VIN</div>
<div class="line">         output     = {0, 0, 1920, 1080},</div>
<div class="line">   },</div>
<div class="line">   second = {</div>
<div class="line">   max_output = {0, 0}, -- output width</div>
<div class="line">   input      = {320, 120, 1280, 960}, -- Set ROI to start at left-top point (320,120),</div>
<div class="line">   ROI width is 1280, height is 960. The left and right 320 pixels of the feature map are zero padded,</div>
<div class="line">   and the top 120 pixels are zero padded. The bottom part of the feature map always remains.</div>
<div class="line">   output     = {0, 0, 720, 480},</div>
<div class="line">   ... ...</div>
<div class="line"> </div>
<div class="line">   canvas = {</div>
<div class="line">         {</div>
<div class="line">            type = <span class="stringliteral">&quot;encode&quot;</span>,</div>
<div class="line">            size = {0, 0},</div>
<div class="line">            source = {<span class="stringliteral">&quot;chan_0.main&quot;</span>,},</div>
<div class="line">            extra_dram_buf = 0,</div>
<div class="line">         },</div>
<div class="line">         {</div>
<div class="line">            type = <span class="stringliteral">&quot;encode&quot;</span>,</div>
<div class="line">            size = {0, 0},</div>
<div class="line">            source = {<span class="stringliteral">&quot;chan_0.second&quot;</span>,},</div>
<div class="line">            extra_dram_buf = 0,</div>
<div class="line">         },</div>
<div class="line">   ... ...</div>
<div class="line"> </div>
<div class="line">board # eazyai_video.sh --customer_lua run_ipc.lua --hdmi 1080p --stream_A 1080p --enc_dummy_latency 8 --reallocate_mem overlay,0x04009000</div>
<div class="line">board # test_bodypix -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_bodypix.bin --in sub_2 --out float_segments --out float_part_heatmaps --out float_heatmaps \</div>
<div class="line">   --out float_short_offsets --out MobilenetV1/displacement_fwd_2/BiasAdd --out MobilenetV1/displacement_bwd_2/BiasAdd \</div>
<div class="line">   -i 1 --roi_w 1280 --roi_h 960</div>
</div><!-- fragment --><p>The previous command uses the first canvas as the input buffer of the network, and the ROI is set to [1280,960].</p>
<hr  />
<h1><a class="anchor" id="sec_tf_hand_landmark"></a>
Hand Landmark</h1>
<p>This live demo is for palm detection and hand landmark detection. It is a two-stage demo, the first network is a palm detection network, which outputs the bounding box and seven key-points for each detected palm; the second network is a hand landmark detection network, which outputs 21 3D key-points for each input hand image.</p>
<p>Address of this public project is as follows: <a href="https://github.com/google/mediapipe/tree/master/modules">https://github.com/google/mediapipe/tree/master/modules</a> , <a href="https://google.github.io/mediapipe/solutions/hands">https://google.github.io/mediapipe/solutions/hands</a></p>
<h2><a class="anchor" id="sub_sec_tf_hand_landmark_download_model_file"></a>
1 Download the Model Files</h2>
<p>Users could directly use: </p><div class="fragment"><div class="line">git clone https:<span class="comment">//github.com/google/mediapipe.git</span></div>
</div><!-- fragment --><p> to download the complete google mediapipe project. Then the model files could be found under: <code>&lt;user_path&gt;/mediapipe/mediapipe/modules/palm_detection/palm_detection.tflite</code> and <code>&lt;user_path&gt;/mediapipe/mediapipe/modules/hand_landmark/hand_landmark.tflite</code></p>
<p>or use the following commands to download the related model files only: </p><div class="fragment"><div class="line">wget https:<span class="comment">//github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection.tflite</span></div>
<div class="line">wget https:<span class="comment">//github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark.tflite</span></div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_tf_hand_landmark_generate_cavalry_binary"></a>
2 Generate Cavalry Binary</h2>
<p>Generate the Cavalry binary as shown below: </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/palm_detection/config/ea_cvt_tf_palm_detection.yaml</div>
<div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/hand_landmark/config/ea_cvt_tf_hand_landmark.yaml</div>
</div><!-- fragment --><p>The outputs are generated in: <code>out/tensorflow/demo_networks/tf_palm_detection/</code> and <code>out/tensorflow/demo_networks/tf_hand_landmark/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen <b>tf_palm_detection</b> output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_palm_detection</code>, the cnngen <b>tf_hand_landmark</b> output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_hand_landmark</code>.</li>
<li>For EVK, the <b>tf_palm_detection</b> cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_palm_detection.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_palm_detection/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_palm_detection</code>, the <b>tf_hand_landmark</b> cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_hand_landmark.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hand_landmark/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_hand_landmark</code>.</li>
<li>For X86 simulator, model <b>tf_palm_detection</b> desc json file <b>tf_palm_detection.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_palm_detection/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_palm_detection_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_palm_detection/&lt;chip&gt;/&lt;chip&gt;_ades_tf_palm_detection</code>. The model <b>tf_hand_landmark</b> desc json file <b>tf_hand_landmark.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hand_landmark/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_hand_landmark_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hand_landmark/&lt;chip&gt;/&lt;chip&gt;_ades_tf_hand_landmark</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<p>About <b>the coverage_th</b>: The default DRA coverage_th is 0.90. Enlarging this ratio could let the DRA quantization result tend to use more fx16 data format. In this case, the detection result would be more accurate but the calculation speed would be decreased. In this example, users could enlarge this ratio to 0.95 to get a better detection result. Users are also encouraged to use other coverage_th ratios, based on users' different requirements for accuracy or performance. It can be modified in ea_cvt_tf_hand_landmark.yaml. </p><div class="fragment"><div class="line">dra_option:</div>
<div class="line">  version: 2</div>
<div class="line">    dra_v2:</div>
<div class="line">      coverage_th: 0.95</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_tf_hand_landmark_build_binary"></a>
3 Build SDK Binary</h2>
<p>Choose the hand landmark application in the menuconfig. </p><div class="fragment"><div class="line">build $ make cv22_ipcam_config</div>
<div class="line">build $ make menuconfig</div>
<div class="line">   [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">      [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">         [*] Build EazyAI applications  ---&gt;</div>
<div class="line">            [*] Build Hand Landmark EazyAI apps</div>
<div class="line">build $ make test_hand_landmark</div>
</div><!-- fragment --><p>Download the binary to the board.</p>
<h2><a class="anchor" id="sub_sec_tf_hand_landmark_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_hand_landmark/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_hand_landmark/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hand_landmark.bin</div>
<div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_palm_detection/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_palm_detection/&lt;chip&gt;_cavalry&lt;version&gt;_tf_palm_detection.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_hand_landmark/tf_hand_landmark_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/demo_networks/tf_hand_landmark</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_palm_detection/tf_palm_detection_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/demo_networks/tf_palm_detection</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_hand_landmark_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_hand_landmark_run_c_inference"></a>
5 Run C Inference</h2>
<p>Place the generated binary model &lt;chip&gt;_cavalry&lt;version&gt;_tf_palm_detection.bin and &lt;chip&gt;_cavalry&lt;version&gt;_tf_hand_landmark.bin to the board. Run: </p><div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --fb_num 4 --pyramid_manual_map 0x07 --pyramid_input_buf_id 0 --pyramid_item_num 8 --pyramid_scale_type 2 --pyramid_layer_1_rescale_size 256x256 --reallocate_mem overlay,0x04009000 --stream_A 1080p --enc_dummy_latency 3</div>
<div class="line">board # test_hand_landmark -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_palm_detection.bin  --in input --out regressors --out classificators -b &lt;chip&gt;_cavalry&lt;version&gt;_tf_hand_landmark.bin --in input_1 --out Identity --out Identity_1 --out Identity_2</div>
</div><!-- fragment --><p>The result could be observed from the stream or VOUT, depending on the show mode chosen by the parameter <code>draw_mode</code>. </p><dl class="section note"><dt>Note</dt><dd>As different CV platforms could have different CV performance, if the detection result is shown on the stream overlay, then users may need to adjust the <em>enc_dummy_latency</em> value [0 ~ 7] to let the detection result be applied to the correct image frame.</dd></dl>
<ul>
<li>For how to use eazyai_video.sh, please refer to <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_eazyai_video">14 Eazyai Video</a></li>
<li>For how to adjust this value and other overlay related parameters, please refer to sub_sec_tf_hand_landmark_run_live_demo</li>
<li>For how to adjust the pyramid buffer related parameters, please refer to subsec_hyperlpr_run</li>
</ul>
<p>Some useful command line options:</p>
<p><code>det_thres</code>: Users could use this option to set the palm detection score threshold. The larger this ratio is the more accurate the detection result will be but the less palms are detected. The default value is 0.6.</p>
<p><code>nms_thres</code>: Users could use this option to set NMS IOU filtering threshold. The smaller this ratio is, the less neighbor bounding boxes will be accepted by the algorithm. The default value is 0.2.</p>
<p><code>pose_thres</code>: Users could use this option to set the hand landmark detection threshold. The larger this ratio is the more accurate the detection result will be but the less landmarks are detected. The default value is 0.9.</p>
<hr  />
<h1><a class="anchor" id="sec_tf_hfnet"></a>
HF-Net</h1>
<p>HF-Net is a monolithic deep neural network (DNN) for descriptor extraction. It was introduced in the CVPR 2019 paper <a href="https://arxiv.org/abs/1812.03506">From Coarse to Fine: Robust Hierarchical Localization at Large Scale</a>.</p>
<p>The following sections demonstrate how to export an HF-Net TensorFlow model from the public source project implemented with TensorFlow, as well as how to convert the model in the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_hfnet_export_model"></a>
1 Export Model</h2>
<p>The TensorFlow model is exported from a <a href="https://github.com/ethz-asl/hfnet">source project</a> in GitHub with a license from Massachusetts Institute of Technology (MIT). The steps are shown as below.</p>
<ol type="1">
<li>Install the Python packages required by HF-Net in <a href="https://github.com/ethz-asl/hfnet/blob/master/setup/requirements.txt">here</a>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Only <b>tensorflow</b> is necessary for freezing the pre-trained model.</dd></dl>
<ol type="1">
<li>Download the package for the trained model from <a href="http://robotics.ethz.ch/~asl-datasets/2019_CVPR_hierarchical_localization/hfnet_tf.tar.gz">here</a>, and extract it. <div class="fragment"><div class="line">build $ tar -xzvf hfnet_tf.tar.gz</div>
</div><!-- fragment --></li>
<li><p class="startli">Create a Python file <code>freeze_model.py</code> with the following source code to freeze the whole model. </p><div class="fragment"><div class="line"><span class="keyword">import</span> tensorflow as tf</div>
<div class="line">from tensorflow.python.saved_model <span class="keyword">import</span> tag_constants</div>
<div class="line">from tensorflow.python.framework.graph_util <span class="keyword">import</span> convert_variables_to_constants</div>
<div class="line"><span class="keyword">import</span> tensorflow.contrib.resampler</div>
<div class="line"> </div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> = <span class="stringliteral">&quot;saved_models/hfnet&quot;</span></div>
<div class="line">session = tf.Session()</div>
<div class="line">net_input = tf.placeholder(tf.float32, shape=(1, None, None, 1), <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=<span class="stringliteral">&quot;gray_image&quot;</span>)</div>
<div class="line">tf.saved_model.loader.load(</div>
<div class="line">      session, [tag_constants.SERVING], str(<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>),</div>
<div class="line">      clear_devices=True)</div>
<div class="line"> </div>
<div class="line">output_node_names=[<span class="stringliteral">&#39;global_descriptor&#39;</span>, <span class="stringliteral">&#39;keypoints&#39;</span>, <span class="stringliteral">&#39;local_descriptors&#39;</span>]</div>
<div class="line"> </div>
<div class="line">output_graph_def = convert_variables_to_constants(session, session.graph_def, output_node_names=output_node_names)</div>
<div class="line">with tf.gfile.FastGFile(<span class="stringliteral">&#39;hfnet_frozen_graph.pb&#39;</span>, mode=<span class="stringliteral">&#39;wb&#39;</span>) as f:</div>
<div class="line">      f.write(output_graph_def.SerializeToString())</div>
</div><!-- fragment --><p class="startli">Then, run the following command. </p><div class="fragment"><div class="line">build $ python3 freeze_model.py</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The file <code>hfnet_frozen_graph.pb</code> will be generated.</dd></dl>
</li>
<li><p class="startli">View the model in <b>tensorboard</b>, and identify the input and output nodes for local descriptor detection. </p><div class="fragment"><div class="line">build $ python3 import_pb_to_tensorboard.py --model_file hfnet_frozen_graph.pb --log_dir tb_log</div>
<div class="line">build $ tensorboard --logdir=tb_log</div>
</div><!-- fragment --><p class="startli">For local descriptor detection only, the input node is <b>pred/strided_slice_3:0</b>, the output nodes are <b>pred/local_head/detector/DepthToSpace</b> and <b>pred/local_head/descriptor/l2_normalize</b>.</p>
<dl class="section note"><dt>Note</dt><dd>The tool <b>import_pb_to_tensorboard.py</b> in CNNGen toolchain requires "import tensorflow.contrib.resampler" to function on the HF-Net model. Use the following source code and run as a Python file.<ul>
<li>import_pb_to_tensorboard.py <pre class="fragment">  #!/usr/bin/env python3
  # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
  # ================================
  """Imports a protobuf model as a graph in Tensorboard."""

  from __future__ import absolute_import
  from __future__ import division
  from __future__ import print_function

  import argparse
  import sys

  from tensorflow.core.framework import graph_pb2
  from tensorflow.python.client import session
  from tensorflow.python.framework import importer
  from tensorflow.python.framework import ops
  from tensorflow.python.platform import app
  from tensorflow.python.platform import gfile
  from tensorflow.python.summary import summary
  import tensorflow.contrib.resampler

  def import_to_tensorboard(model_file, log_dir):
  """View an imported protobuf model (`.pb` file) as a graph in Tensorboard.
  Args:
      model_file: The protobuf (`pb`) model to visualize
      log_dir: The location for the Tensorboard log to begin visualization from.
  Usage:
      Call this function with your model file and desired log directory.
      Launch Tensorboard by pointing it to the log directory.
      View your imported `.pb` model as a graph.
  """
  with session.Session(graph=ops.Graph()) as sess:
      with gfile.FastGFile(model_file, "rb") as f:
      graph_def = graph_pb2.GraphDef()
      graph_def.ParseFromString(f.read())
      importer.import_graph_def(graph_def)

      pb_visual_writer = summary.FileWriter(log_dir)
      pb_visual_writer.add_graph(sess.graph)
      print("Model Imported. Visualize by running: "
          "tensorboard --logdir={}".format(log_dir))
      pb_visual_writer.close()

  def main(unused_args):
  import_to_tensorboard(FLAGS.model_file, FLAGS.log_dir)

  if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.register("type", "bool", lambda v: v.lower() == "true")
  parser.add_argument(
      "--model_file",
      type=str,
      default="",
      required=True,
      help="The protobuf (\'pb\') model to visualize.")
  parser.add_argument(
      "--log_dir",
      type=str,
      default="",
      required=True,
      help="The location for the Tensorboard log to begin visualization from.")
  FLAGS, unparsed = parser.parse_known_args()
  app.run(main=main, argv=[sys.argv[0]] + unparsed)
</pre></li>
</ul>
</dd></dl>
</li>
<li><p class="startli">Create a Python file <code>freeze_model_part.py</code> with the following source code to freeze the required part of the model. </p><div class="fragment"><div class="line"><span class="keyword">import</span> tensorflow as tf</div>
<div class="line">from tensorflow.python.saved_model <span class="keyword">import</span> tag_constants</div>
<div class="line">from tensorflow.python.framework.graph_util <span class="keyword">import</span> convert_variables_to_constants</div>
<div class="line"><span class="keyword">import</span> tensorflow.contrib.resampler</div>
<div class="line"> </div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> = <span class="stringliteral">&quot;saved_models/hfnet&quot;</span></div>
<div class="line">session = tf.Session()</div>
<div class="line"> </div>
<div class="line">net_input = tf.placeholder(tf.float32, shape=(1, None, None, 1), <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=<span class="stringliteral">&quot;gray_image&quot;</span>)</div>
<div class="line">tf.saved_model.loader.load(</div>
<div class="line">   session, [tag_constants.SERVING], str(<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>),</div>
<div class="line">   clear_devices=True,</div>
<div class="line">   input_map={<span class="stringliteral">&#39;pred/strided_slice_3:0&#39;</span>: net_input})</div>
<div class="line"> </div>
<div class="line">output_node_names=[<span class="stringliteral">&#39;pred/local_head/detector/DepthToSpace&#39;</span>, <span class="stringliteral">&#39;pred/local_head/descriptor/l2_normalize&#39;</span>]</div>
<div class="line">output_graph_def = convert_variables_to_constants(session, session.graph_def, output_node_names=output_node_names)</div>
<div class="line">with tf.gfile.FastGFile(<span class="stringliteral">&#39;hfnet_frozen_graph_part.pb&#39;</span>, mode=<span class="stringliteral">&#39;wb&#39;</span>) as f:</div>
<div class="line">   f.write(output_graph_def.SerializeToString())</div>
</div><!-- fragment --><p class="startli">Then, run the following command. </p><div class="fragment"><div class="line">build $ python3 freeze_model_part.py</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>The file <code>hfnet_frozen_graph_part.pb</code> will be generated.</dd></dl>
</li>
<li><p class="startli">Create a Python file <code>optimize_graph.py</code> to optimize the model in order to avoid errors in running the OpenCV DNN. </p><div class="fragment"><div class="line">from tensorflow.python.saved_model <span class="keyword">import</span> tag_constants</div>
<div class="line">from tensorflow.python.tools <span class="keyword">import</span> freeze_graph</div>
<div class="line">from tensorflow.python <span class="keyword">import</span> ops</div>
<div class="line">from tensorflow.tools.graph_transforms <span class="keyword">import</span> TransformGraph</div>
<div class="line"><span class="keyword">import</span> tensorflow as tf</div>
<div class="line"><span class="keyword">import</span> os</div>
<div class="line"><span class="keyword">import</span> argparse</div>
<div class="line"> </div>
<div class="line">def get_graph_def_from_file(graph_filepath):</div>
<div class="line">   tf.compat.v1.reset_default_graph</div>
<div class="line">   with ops.Graph().as_default():</div>
<div class="line">      with tf.io.gfile.GFile(graph_filepath, <span class="stringliteral">&#39;rb&#39;</span>) as f:</div>
<div class="line">         graph_def = tf.compat.v1.GraphDef()</div>
<div class="line">         graph_def.ParseFromString(f.read())</div>
<div class="line">         return graph_def</div>
<div class="line"> </div>
<div class="line">def optimize_graph(in_graph, out_graph, input_names, output_names, <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>):</div>
<div class="line">   graph_def = get_graph_def_from_file(in_graph)</div>
<div class="line">   optimized_graph_def = TransformGraph(</div>
<div class="line">      graph_def,</div>
<div class="line">      input_names,</div>
<div class="line">      output_names,</div>
<div class="line">      <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>)</div>
<div class="line">   tf.io.write_graph(optimized_graph_def,</div>
<div class="line">      logdir=<span class="stringliteral">&#39;./&#39;</span>,</div>
<div class="line">      as_text=False,</div>
<div class="line">      <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=out_graph)</div>
<div class="line">   print(<span class="stringliteral">&#39;Graph optimized!&#39;</span>)</div>
<div class="line"> </div>
<div class="line">parser = argparse.ArgumentParser(add_help=False)</div>
<div class="line">parser.add_argument(<span class="stringliteral">&#39;--in_graph&#39;</span>, required=True)</div>
<div class="line">parser.add_argument(<span class="stringliteral">&#39;--out_graph&#39;</span>, required=True)</div>
<div class="line">parser.add_argument(<span class="stringliteral">&#39;--inputs&#39;</span>, required=True, nargs=<span class="stringliteral">&#39;+&#39;</span>)</div>
<div class="line">parser.add_argument(<span class="stringliteral">&#39;--outputs&#39;</span>, required=True, nargs=<span class="stringliteral">&#39;+&#39;</span>)</div>
<div class="line">parser.add_argument(<span class="stringliteral">&#39;--transforms&#39;</span>, required=True, nargs=<span class="stringliteral">&#39;+&#39;</span>)</div>
<div class="line">args = parser.parse_args()</div>
<div class="line"> </div>
<div class="line">optimize_graph(args.in_graph,</div>
<div class="line">         args.out_graph,</div>
<div class="line">         args.inputs,</div>
<div class="line">         args.outputs,</div>
<div class="line">         args.<a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>)</div>
</div><!-- fragment --><p class="startli">Then, run the following command. </p><div class="fragment"><div class="line">build $ python3 optimize_graph.py \</div>
<div class="line">      --in_graph hfnet_frozen_graph_part.pb \</div>
<div class="line">      --out_graph hfnet_frozen_graph_part_optimized.pb \</div>
<div class="line">      --inputs gray_image \</div>
<div class="line">      --outputs pred/local_head/detector/DepthToSpace pred/local_head/descriptor/l2_normalize \</div>
<div class="line">      --<a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a> <span class="stringliteral">&quot;remove_nodes(op=Identity)&quot;</span> merge_duplicate_nodes strip_unused_nodes <span class="stringliteral">&quot;fold_constants(ignore_errors=true)&quot;</span> fold_batch_norms fold_old_batch_norms</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The file <code>hfnet_frozen_graph_part_optimized.pb</code> will be generated.</dd></dl>
</li>
<li>Perform a graph surgery. <div class="fragment"><div class="line">build $ graph_surgery.py tf -p hfnet_frozen_graph_part_optimized.pb -o hfnet_320x240_surgery.pb -isrc <span class="stringliteral">&#39;i:gray_image|is:1,240,320,1&#39;</span> \</div>
<div class="line">      -on pred/local_head/detector/DepthToSpace,pred/local_head/descriptor/l2_normalize \</div>
<div class="line">      -t ConstantifyShapes,GraphTransform{<span class="stringliteral">&quot;remove_nodes(op=Identity) fold_constants&quot;</span>}</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The file <code>hfnet_320x240_surgery.pb</code> will be generated. This file is used to generate the Cavalry binary model.</dd></dl>
</li>
<li>Run the following command to verify that there are no unsupported operators. <div class="fragment"><div class="line">build $ python3 tf_print_graph_summary.py -p hfnet_320x240_surgery.pb</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_tf_hfnet_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>Generate the Cavalry binary as shown below:</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/hfnet/config/ea_cvt_tf_hfnet.yaml</div>
</div><!-- fragment --><p>The output is in: <code>out/tensorflow/demo_networks/tf_hfnet/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_hfnet</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hfnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_hfnet</code>.</li>
<li>For X86 simulator, model desc json file <b>tf_hfnet.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hfnet/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_hfnet_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_hfnet/&lt;chip&gt;/&lt;chip&gt;_ades_tf_hfnet</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_hfnet_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build a unit test for the EVK. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">        -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">          [*] Build Ambarella custom postprocess library with hfnet</div>
<div class="line">        [*] Build EazyAi unit tests</div>
<div class="line">  build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li><p class="startli">Build a unit test for the X86 simulator.</p>
<p class="startli">Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <code>test_eazyai</code> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</p>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_hfnet_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_hfnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_hfnet/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_hfnet/tf_hfnet_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/tensorflow/demo_networks/tf_hfnet</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 2 \</div>
<div class="line">        -cb out/tensorflow/demo_networks/tf_hfnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_hfnet/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin \</div>
<div class="line">        -pn hfnet -pl &lt;usr_path&gt;/hfnet/config/hfnet.lua -dm 1 -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_hfnet_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example; refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the EVK board:<ul>
<li>Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_hfnet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin</code>.</li>
<li>The <code>hfnet.lua</code> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of the EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ul>
</li>
<li>For X86 simulator:<ul>
<li>Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_hfnet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <code>tf_hfnet.json</code> and <code>tf_hfnet_ades.cmd</code>.</li>
</ul>
</li>
</ul>
</dd></dl>
<ul>
<li><p class="startli">Copy files to the SD card for the EVK test.</p>
<p class="startli">For example, place files on the SD card with the following structure: </p><div class="fragment"><div class="line">/sdcard/hfnet</div>
<div class="line">|--model</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|       1341847980.722988.jpg</div>
<div class="line">|       1341847980.722988.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This file saving method is only an example. The file can be placed freely according to the user's requirements. Users must keep the file path consistent during use.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For the X86 simulator:<ol type="a">
<li>Run ADES mode.<ol type="i">
<li>The raw.bin is used as input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">    --model_path &lt;usr_path&gt;/out_tf_hfnet_parser/tf_hfnet.json \</div>
<div class="line">    --ades_cmd_file &lt;usr_path&gt;/ades_tf_hfnet/tf_hfnet_ades.cmd \</div>
<div class="line">    --isrc <span class="stringliteral">&quot;i:gray_image=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">    --output_dir &lt;usr_path&gt;/hfnet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n hfnet \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_hfnet_parser/tf_fhnet.json \</div>
<div class="line">     --lua_file hfnet.lua --queue_size 1 --thread_num 1 \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:gray_image=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/hfnet/out \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_tf_hfnet/tf_hfnet_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run acinference mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_hfnet_parser/tf_hfnet.json \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:gray_image=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/hfnet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n hfnet \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_hfnet_parser/tf_fhnet.json \</div>
<div class="line">     --lua_file hfnet.lua --queue_size 1 --thread_num 1 \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:gray_image=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/hfnet/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For the EVK board:<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li><p class="startli">Run the following:</p><ol type="i">
<li>Dummy mode, only for CVflow performance test: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/hfnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin</div>
</div><!-- fragment --></li>
<li>The image is used as an input, with preprocess and postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 1 -n hfnet \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/hfnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin  \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/hfnet.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:gray_image=/sdcard/hfnet/in|t:jpg&quot;</span> \</div>
<div class="line">     --output_dir /sdcard/hfnet/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the correct preprocess or postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/hfnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin  \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:gray_image=/sdcard/ssd/in|t:raw&quot;</span> \</div>
<div class="line">     --output_dir /sdcard/hfnet/out/</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with the image as an input, place the test image (such as <code>cvflow_cnngen_samples/tensorflow/demo_networks/hfnet/dra_img/1341847980.722988.jpg</code>) in <code>/sdcard/hfnet/in</code>, and create <code>/sdcard/hfnet/out</code> as the output directory.</li>
<li>For the option <b>&ndash;isrc"|d:vp"</b>, the default preprocess is based on OpenCV; users can enable VProc if required with the option <b>"d:vp"</b>. The default value is CPU.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b> and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode:<ol type="1">
<li>Initialize the environment on the computer vision (CV) board. CV22 Walnut and imx274_mipi are used as examples. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li><p class="startli">Run the following:</p><ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>): <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 1 -n hfnet \</div>
<div class="line">    --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/hfnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin  \</div>
<div class="line">    --lua_file /usr/share/ambarella/eazyai/lua/hfnet.lua</div>
</div><!-- fragment --></li>
<li>Video output (VOUT) live mode (draw on VOUT high-definition multimedia interface (HDMI)) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 1 -n hfnet \</div>
<div class="line">    --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/hfnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_hfnet.bin  \</div>
<div class="line">    --lua_file /usr/share/ambarella/eazyai/lua/hfnet.lua</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluent, check the following two points:<ul>
<li>If the display is not fluent, use a larger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_tf_model_with_mobilenetv2_nv12"></a>
Mobilenetv2_NV12</h1>
<p>For certain situations, users may need to use TensorFlow model with the NV12 input, which includes two input files, Y data, and UV data. It can be used to convert the models with the NV12 input and generate the cavalry binary at present. This section provides the explanations with mobilenetv2_nv12 as below:</p>
<ul>
<li>Converts image from jpg to NV12 by the conversion tool for DRA calculation.</li>
<li>Generates Cavalry binary by quick_start.sh.</li>
<li>Deploys and tests the converted cavalry binary on the board.</li>
</ul>
<h2><a class="anchor" id="sub_sec_convert_DRA_image"></a>
1 Convert DRA Image</h2>
<p>Images can be converted to <b>NV12</b> from <b>jpg</b> or <b>JPEG</b> using the tool <code>convert_jpg2nv12</code> in <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_tools">3 CFlite Python Tools</a>. After conversion, separated Y and UV data files will be generated in specified folders. </p><div class="fragment"><div class="line">build $ convert_jpg2nv12 --jpeg &lt;jpg images path&gt; --y &lt;output Y path&gt; --uv &lt;output UV path&gt; --width &lt;resize width&gt; --height &lt;resize height&gt; --interleave 0</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>"--jpeg" specifies the folder that contains the input jpeg images.</li>
<li>"--y" and "--uv" specifies the output folder paths used to store the converted Y and UV data files seperately.</li>
<li>"--width" and "--height" specifies the resize resolution, for NV12, this should be the same with the converted Y image's resolutuion.- "--interleave 0" specifies that the generated UV data is not interleaved, users can use "--interleave 1" to generate interleaved UV data. For MobileNetv2, this option should be set to 0 to match the network input.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_nv12_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>This section introduces how to generate the cavalry binary for tf model with mobilenetv2_nv12.</p>
<p>First, generate the Cavalry binary as shown below:</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/mobilenetv2_nv12/config/ea_cvt_tf_mobilenetv2_nv12.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>This model is not included in the cnngen package and is only used as an example of YUV input.</li>
<li>The content of the Yaml file is as follows: <div class="fragment"><div class="line"><span class="preprocessor"># This yaml is used to quantify the model by tool &quot;eazyai_cvt.py&quot;.</span></div>
<div class="line"> </div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: tf_mobilenetv2_nv12</div>
<div class="line"><a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a>: ./out/tensorflow/demo_networks</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>: tensorflow/demo_networks/mobilenetv2_nv12/models/mobilenet_nv12_float_reconstructed.pb</div>
<div class="line"> </div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path: tensorflow/demo_networks/mobilenetv2_nv12/dra_img/</div>
<div class="line">    in_file_ext: jpg</div>
<div class="line">    out_shape: 1,1,224,224</div>
<div class="line">    out_data_format: uint8</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color: grayscale</div>
<div class="line">  dra_data_2:</div>
<div class="line">    in_path: tensorflow/demo_networks/mobilenetv2_nv12/dra_img/</div>
<div class="line">    in_file_ext: jpg</div>
<div class="line">    out_shape: 1,2,112,112</div>
<div class="line">    out_data_format: uint8</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenUVFromImage</div>
<div class="line">        arguments:</div>
<div class="line">          uv_format: NV12</div>
<div class="line"> </div>
<div class="line">input_nodes:</div>
<div class="line">  x.1:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line">    mean: 0.0</div>
<div class="line">    std: 1.0</div>
<div class="line">  x:</div>
<div class="line">    data_prepare: dra_data_2</div>
<div class="line">    mean: 0.0</div>
<div class="line">    std: 1.0</div>
<div class="line"> </div>
<div class="line">output_nodes:</div>
<div class="line">  dense_1/BiasAdd:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1000,1,1</div>
<div class="line"> </div>
<div class="line">cnngen_convert:</div>
<div class="line">  dra_option:</div>
<div class="line">    strategy: <span class="keyword">auto</span></div>
</div><!-- fragment --></li>
</ol>
</dd></dl>
<p>Second, edit "out/tensorflow/demo_networks/tf_mobilenetv2_nv12/out_tf_mobilenetv2_nv12_parser/&lt;network_name&gt;.vas" as shown below. This is because the UV format in IAV does not match with the NCHW input format generated by the CV toolchain, so users need to use <b>dram_format = 1</b> to tell the VP reading the UV data with the interleave mode. </p><div class="fragment"><div class="line">VP_input(&lt;UV input node <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>&gt;, data_format(0, 0, 0, 0), vector(1, 2, &lt;UV data height&gt;, &lt;UV data width&gt;),</div>
<div class="line">    dram_format = 1, <span class="comment">// modify or add this line</span></div>
<div class="line">    dram_rotate = 0,</div>
<div class="line">    rt_config = 0,</div>
<div class="line">    VP_cnngen_demangled_id(<span class="stringliteral">&quot;&lt;UV input node name&gt;&quot;</span>),</div>
<div class="line">    __cnngen_tracker = { 3 });</div>
</div><!-- fragment --><p>Finally, generate the cavalry binary using the following commands: </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/mobilenetv2_nv12/config/ea_cvt_tf_mobilenetv2_nv12.yaml -rc</div>
</div><!-- fragment --><p>The output is in: <code>out/tensorflow/demo_networks/tf_mobilenetv2_nv12/</code>. </p><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_mobilenetv2_nv12</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_nv12.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_nv12/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_nv12</code>.</li>
<li>For X86 simulator, model desc json file <b>tf_mobilenetv2_nv12.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_nv12/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_mobilenetv2_nv12_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_nv12/&lt;chip&gt;/&lt;chip&gt;_ades_tf_mobilenetv2_nv12</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Current default output data format is float32. For CV7x, please use <code>-od fp16</code> in command as it does not support FP32.</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_nv12_build_unit_test"></a>
3 Build Unit Test</h2>
<div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">   [*]   Ambarella Package Configuration  ---&gt;</div>
<div class="line">      -*-   Build Ambarella EAZYAI library   ---&gt;</div>
<div class="line">         [*]   Build EazyAI unit tests</div>
<div class="line">            -*-   Build Ambarella custom postprocess library   ---&gt;</div>
<div class="line">               [*]   Build Ambarella custom postprocess library with mobilenetv2_nv12</div>
<div class="line">        [*]   Build EazyAI unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_tf_mobilenetv2_nv12_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_mobilenetv2_nv12/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_nv12/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_nv12.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_mobilenetv2_nv12/tf_mobilenetv2_nv12_cvt_summary.yaml \</div>
<div class="line">        -iy tensorflow/demo_networks/mobilenetv2_nv12/config/ea_inf_tf_mobilenetv2_nv12.yaml -pwd ./out/tensorflow/demo_networks/tf_mobilenetv2_nv12</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 3 \</div>
<div class="line">        -cb out/tensorflow/demo_networks/tf_mobilenetv2_nv12/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_nv12/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_nv12.bin \</div>
<div class="line">        -pn mobilenetv2_nv12 -pl &lt;usr_path&gt;/mobilenetv2_nv12/config/mobilenetv2_nv12.lua -dm 0 \</div>
<div class="line">        -lp &lt;usr_path&gt;/mobilenetv2_ssd/config/imagenet_1000.txt -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_nv12_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <code>test_eazyai</code> is used for the following example, to deploy TensorFlow models with the NV12 input, users can refer to mobilenetv2_nv12, for detail usage please refer to the following referenced chapters.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_mobilenetv2_nv12_cnngen_conversion">2 CNNGen Conversion</a> for how to generate "cavalry_mobilenetv2_nv12.bin".</li>
<li>The "mobilenetv2_nv12.lua" is included in the path "/usr/share/ambarella/eazyai/lua" of EVK. If it does not exist, find it in "cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua".</li>
</ol>
</dd></dl>
<ul>
<li>Copy files to SD card for EVK test. For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/mobilenetv2_nv12/</div>
<div class="line">|--model</div>
<div class="line">|     cavalry_mobilenetv2_nv12.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|     imagenet_1000.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find "imagenet_1000.txt" in "cvflow_cnngen_samples/library/eazyai/unit_test/resource/".</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>Test_eazyai can get YUV data from the <b>canvas buffer</b>. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --img_scale --reallocate_mem overlay,0x01200000</div>
<div class="line">board # test_eazyai -m 0 -d 0 -c 1 -s 0 --yuv --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/mobilenetv2_nv12/model/cavalry_mobilenet_nv12.bin \</div>
<div class="line">      --label_path /sdcard/mobilenetv2_nv12/labels/imagenet_1000.txt -n mobilenetv2_nv12 --lua_file mobilenetv2_nv12.lua</div>
</div><!-- fragment --></li>
<li>Test_eazyai can get YUV data from the <b>pyramid buffer</b>. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --pyramid_input_buf_id 0 --pyramid_scale_type 2 --pyramid_layer_1_rescale_size 448x448 \</div>
<div class="line">      --pyramid_manual_map 0x04 --enc_dummy_latency 4 --img_scale  --reallocate_mem overlay,0x01200000</div>
<div class="line">board # test_eazyai -m 0 -d 0 -p 0,2 -s 0 --yuv --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/mobilenetv2_nv12/model/cavalry_mobilenet_nv12.bin \</div>
<div class="line">      --label_path /sdcard/mobilenetv2_nv12/labels/imagenet_1000.txt -n mobilenetv2_nv12 --lua_file mobilenetv2_nv12.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.<ul>
<li>If the display is not fluency, use bigger value in "--enc_dummy_latency 4", such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in "--reallocate_mem overlay,0x04000000".</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (1024 + resolution * enc-dummy-latency * 2). For details, please refer to "EazyAI Library API related content in Linux SDK Doxygen documents".</li>
</ul>
</li>
<li>The option "-m 0" specifies the live mode.</li>
<li>The option "-d 0" specifies that the result will be presented in the form of a box.</li>
<li>The option "-c 1" specifies the input YUV canvas buffer ID.</li>
<li>The option "-p 0,2" specifies the channel ID and pyramid layer of the pyramid buffer.</li>
<li>The option "-s 0" specifies the output stream ID to draw, drawing on HDMI in default.</li>
<li>The option "--yuv" specifies the yuv input from iav.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ul>
<h1><a class="anchor" id="sec_tf_deeplab_v3"></a>
Regression for DeepLabv3</h1>
<p>This section provides users with the steps for evaluating the pixel accuracy for the segmentation network, including steps for resource preparation, running the regression tool, and obtaining results. The DeepLabv3 is used as an example to introduce all test steps and tools. For other CNN segmentation networks, use this example as a reference for accuracy regression.</p>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_context"></a>
1 Context</h2>
<p>The original DeepLab model can be downloaded from the following address: <a href="http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz">http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz</a></p>
<p>For more information on this model, refer to: <a href="https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</a></p>
<p>The original mIOU accuracy is <b>71.83%</b> after training (according to the authors statement with test scale=1.0, no left-right flip). To reproduce this mIOU value, follow the instructions on the <b>GitHub</b> page to set up Tensorflow and download the VOC2007 dataset. A readme file is also provided to help users download VOC2007 train_validation and the test dataset, as well as setup ground truth images.</p>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_export_model"></a>
2 Export Model</h2>
<ul>
<li><p class="startli">Generate with the default input size of 513x513.</p>
<p class="startli">Use <b>"tf_print_graph_summary.py"</b> to check the model files status:</p>
</li>
</ul>
<div class="fragment"><div class="line">build $ tf_print_graph_summary.py -p user_downloaded.pb</div>
<div class="line">--------------------------------------------------------------------------------</div>
<div class="line">Graph summary:</div>
<div class="line">Total number of ops: 963</div>
<div class="line">Number of non constant ops: 593</div>
<div class="line">Inputs (1):</div>
<div class="line">ImageTensor (1, ?, ?, 3)</div>
<div class="line">Outputs (1):</div>
<div class="line">SemanticPredictions [1, None, None]</div>
<div class="line">Issue          |          Problematic Ops          |    Recommendation</div>
<div class="line">Unsupported</div>
<div class="line">operators     |  Equal,GreaterEqual, LogicalAnd |   Cut graph using graph surgery</div>
<div class="line">found</div>
<div class="line">Kernel input</div>
<div class="line">to Conv2D is  |                                     |   ConstantifyShapes</div>
<div class="line">not constant</div>
</div><!-- fragment --><p>As shown above, the original model includes some input variables as well as unsupported operators which are not allowed for inference on the VP. Because inference on the VP requires constant input and all supported operators, this section includes steps to ensure proper processing. All of the tools included in this section are only for reference, and do not apply to all networks. Users must determine the correct portion of the graph to run on the VP on their own. The remaining should run using other methods. For more information about the network pre-processing, please refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_mobilenetv2_ssd_preprocess_model">2 Pre-Process Model</a>.</p>
<p>The default model is downloaded from <a href="http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz">http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz</a>. The file <b>frozen_inference_graph.pb</b> in the package is renamed to <b>deeplabv3_mnv2_dm05_pascal_trainval_frozen_inference_graph.pb</b>. The input size after preprocess in this network is frozen to 513x513.</p>
<p>A Python script is provided to help users maintain consistent network input size constant and eliminate the unsupported nodes, which calls the <b>graph_surgery.py</b> tool from its program: <code>&lt;user_path&gt;/cnngen_sample/tensorflow/demo_networks/deeplab_v3/script/do_surgery.py</code>.</p>
<div class="fragment"><div class="line">build $ cd &lt;user_path&gt;/cnngen_sample/tensorflow/demo_networks/deeplab_v3/script</div>
<div class="line">build $ do_surgery.py \</div>
<div class="line">   --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>=<span class="stringliteral">&quot;original_pb_file&quot;</span> \</div>
<div class="line">   --out_dir=output_folder_name \</div>
<div class="line">   --input_node=<span class="stringliteral">&quot;ExpandDims&quot;</span> \</div>
<div class="line">   --input_size=<span class="stringliteral">&quot;513,513&quot;</span> \</div>
<div class="line">   --output_node=<span class="stringliteral">&quot;ArgMax&quot;</span></div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Please notice "(0,0,0,0) Output Node Format" is necessary for running the demo on the board.</li>
</ul>
</dd></dl>
<p>It generates a new pb file under the output path and named as <b>"frozen_inference_graph_surgery.pb"</b>. Because this command is already merged into the makefile workflow, users can continue immediately to the next section and begin converting the model using the CNNGen tool.</p>
<ul>
<li><p class="startli">Generate with custom input size.</p>
<p class="startli">If the size other than 513x513 is needed, please refer to the following step to export another model. If the original FOV of the image is not NxN for a 513x513 network, the scale preprocess with the change of the aspect ratio is required. But the change of the aspect ratio in image will make the result worse considerablely in the trained deeplabv3 model. So it's better to freeze the network to an input size which has the same aspect ratio with the image FOV. The following steps show how to export <b>mobilenetv2_coco_voc_trainaug</b> with an input size of 720x405, which has the same aspect ratio with the FOV of 1920x1080.</p><ol type="1">
<li>Clone tensorflow model project from <a href="https://github.com/tensorflow/models.git">https://github.com/tensorflow/models.git</a>. <div class="fragment"><div class="line">commit 080347bc9056fdb8f0a2236ccdb5bfef1cdf0cca</div>
</div><!-- fragment --></li>
<li>Download model from model zoo <a href="http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz">http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz</a>.</li>
<li>Work in the folder of the tensorflow model project.<ol type="a">
<li>Add PYTHONPATH. <div class="fragment"><div class="line"><span class="preprocessor"># From tensorflow/models/research/</span></div>
<div class="line"><span class="preprocessor">build $ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim</span></div>
</div><!-- fragment --></li>
<li>Decompress <b>deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz</b> and copy files to <code>tensorflow/models/research/deeplab</code>. <div class="fragment"><div class="line">tensorflow/models/research/deeplab</div>
<div class="line">|-- deeplabv3_mnv2_pascal_train_aug</div>
<div class="line">| |-- frozen_inference_graph.pb</div>
<div class="line">| |-- model.ckpt-30000.data-00000-of-00001</div>
<div class="line">| |__ model.ckpt-30000.index</div>
<div class="line">|-- export_model.py</div>
<div class="line">|__ export</div>
</div><!-- fragment --></li>
<li>Export model. <div class="fragment"><div class="line"><span class="preprocessor"># From tensorflow/models/research/deeplab</span></div>
<div class="line"><span class="preprocessor">build $ python3 export_model.py --checkpoint_path deeplabv3_mnv2_pascal_train_aug/model.ckpt-30000 --crop_size=405 --crop_size=720 --export_path export/deeplabv3_mnv2_pascal_train_aug_720x405.pb</span></div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><b>deeplabv3_mnv2_pascal_train_aug_720x405.pb</b> is generated under <code>tensorflow/models/research/deeplab/export</code>.</dd></dl>
</li>
<li>Copy <b>deeplabv3_mnv2_pascal_train_aug_720x405.pb</b> to <code>{CNNGen Sample Package}/tensorflow/demo_networks/deeplab_v3/models</code>.</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_cnngen_conversion"></a>
3 CNNGen Conversion</h2>
<p>Generate cavalry binary. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy tensorflow/demo_networks/deeplab_v3/config/ea_cvt_tf_deeplab_v3.yaml</div>
</div><!-- fragment --><p> The output is in: <code>out/tensorflow/demo_networks/tf_deeplab_v3/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_deeplab_v3</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_deeplab_v3/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_deeplab_v3</code>.</li>
<li>For X86 simulator, model desc json file <b>tf_deeplab_v3.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_deeplab_v3/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_deeplab_v3_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_deeplab_v3/&lt;chip&gt;/&lt;chip&gt;_ades_tf_deeplab_v3</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_build_unit_test"></a>
4 Build Unit Test</h2>
<p>Build Unit Test for EVK </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">        -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">          [*] Build Ambarella custom postprocess library with deeplabv3</div>
<div class="line">             [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_tf_deeplab_v3_run_python_inference"></a>
5 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_deeplab_v3/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_deeplab_v3/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_deeplab_v3/tf_deeplab_v3_cvt_summary.yaml \</div>
<div class="line">        -iy tensorflow/demo_networks/deeplab_v3/config/ea_inf_tf_deeplab_v3.yaml -pwd ./out/tensorflow/demo_networks/tf_deeplab_v3</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/tensorflow/demo_networks/tf_deeplab_v3/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_deeplab_v3/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin \</div>
<div class="line">        -pn deeplabv3 -pl &lt;usr_path&gt;/deeplab_v3/config/deeplabv3.lua -dm 1 -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_run_c_inference"></a>
6 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_deeplab_v3_cnngen_conversion">3 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin</b>.</li>
<li>The <b>deeplabv3.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li><p class="startli">For X86:</p>
<p class="startli">Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_deeplab_v3_cnngen_conversion">3 CNNGen Conversion</a> for how to generate <b>tf_deeplab_v3.json</b> and <b>tf_deeplab_v3_ades.cmd</b>.</p>
</li>
</ol>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/deeplabv3</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        image1.jpg</div>
<div class="line">|        image2.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">      --model_path &lt;usr_path&gt;/out_tf_deeplab_v3_parser/tf_deeplab_v3.json \</div>
<div class="line">      --ades_cmd_file &lt;usr_path&gt;/ades_tf_deeplab_v3/tf_deeplab_v3_ades.cmd \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:ExpandDims=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">      --output_dir &lt;usr_path&gt;/deeplab_v3/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_deeplab_v3_parser/tf_deeplab_v3.json \</div>
<div class="line">     --lua_file deeplabv3.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:ExpandDims=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/deeplab_v3/out \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_tf_deeplab_v3/tf_deeplab_v3_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">    --model_path &lt;usr_path&gt;/out_tf_deeplab_v3_parser/tf_deeplab_v3.json \</div>
<div class="line">    --isrc <span class="stringliteral">&quot;i:ExpandDims=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">    --output_dir &lt;usr_path&gt;/deeplab_v3/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">    --model_path &lt;usr_path&gt;/out_tf_deeplab_v3_parser/tf_deeplab_v3.json \</div>
<div class="line">    --lua_file deeplabv3.lua \</div>
<div class="line">    --isrc <span class="stringliteral">&quot;i:ExpandDims=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">    --output_dir &lt;usr_path&gt;/deeplab_v3/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li><p class="startli">Run the following.</p><ol type="i">
<li>Dummy mode, only for CVflow performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/deeplabv3/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/deeplabv3/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/deeplabv3.lua \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:ExpandDims=/sdcard/deeplabv3/in|t:jpg|d:vp|c:rgb&quot;</span> \</div>
<div class="line">      --output_dir /sdcard/deeplabv3/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/deeplabv3/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:ExpandDims=/sdcard/deeplabv3/in|t:raw&quot;</span> \</div>
<div class="line">      --output_dir /sdcard/deeplabv3/out/</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/tensorflow/demo_networks/deeplabv3/dra_img/image1.jpg</code>) in <code>/sdcard/deeplabv3/in</code>, and create <code>/sdcard/deeplabv3/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 1 -r -n deeplabv3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/deeplabv3/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/deeplabv3.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 1 -r -n deeplabv3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/deeplabv3/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_deeplab_v3.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/deeplabv3.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, check the following two points.<ul>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_deeplab_v3_run_accuracy_test"></a>
7 Run Accuracy Test</h2>
<ul>
<li><p class="startli">Accuracy Mode After compiling, run an accuracy test. Before executing the command, ensure that the network between the PC and the board is valid. </p><div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_deeplab_v3/tf_deeplab_v3_cvt_summary.yaml \</div>
<div class="line">        -iy tensorflow/demo_networks/deeplab_v3/config/ea_inf_acc_tf_deeplab_v3.yaml -pwd ./out/tensorflow/demo_networks/tf_deeplab_v3</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
<p>Locate the accuracy report file from the following folder path: <code>&lt;user_path&gt;/cnngen_sample/out/tensorflow/demo_networks/tf_deeplab_v3/accuracy/report/cv_segmentation_result.txt</code>.</p>
<p class="startli">The result binary is stored in the following folder path: <code>&lt;user_path&gt;/cnngen_sample/out/tensorflow/demo_networks/tf_deeplab_v3/accuracy/io_bin/cv_out/</code>.</p>
<p class="startli">The complete test result for the converted network bases on VOC07 train-validation dataset and test dataset (632 images):</p><ul>
<li>Pixel Accuracy avg: 92.24%</li>
<li>Mean Pixel Accuracy avg: 79.06%</li>
<li>Mean Intersection over Union avg: 68.30%</li>
<li>Frequency Weighted Intersection over Union avg: 86.47%</li>
</ul>
<p class="startli">For more information about this accuracy test tool, please refer to fs_accuracy_tool.</p>
</li>
</ul>
<hr  />
<h1><a class="anchor" id="sec_tf_mobilenetv2_ssd"></a>
SSD Mobilenetv2</h1>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_download_model_file"></a>
1 Download Model File</h2>
<p>Download the original model of VGG-SSD from the following address: <a href="http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz">http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz</a></p>
<dl class="section note"><dt>Note</dt><dd>The mAP of the original model of VGG-SSD is 22%. Refer to the following address for details. <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></dd></dl>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_preprocess_model"></a>
2 Pre-Process Model</h2>
<p>Use <b> tf_print_graph_summary.py</b> to check the model files status: </p><div class="fragment"><div class="line">build $ tf_print_graph_summary.py -p frozen_inference_graph.pb</div>
<div class="line">--------------------------------------------------------------------------------</div>
<div class="line">Graph Summary</div>
<div class="line">Total number of ops: 7975</div>
<div class="line">Number of non constant ops: 5403</div>
<div class="line">Inputs (1):</div>
<div class="line">image_tensor (?, ?, ?, 3)</div>
<div class="line">Outputs (6):</div>
<div class="line">detection_classes None</div>
<div class="line">Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/TopKV2 [None]</div>
<div class="line">Postprocessor/unstack None</div>
<div class="line">detection_boxes None</div>
<div class="line">num_detections None</div>
<div class="line">detection_scores None</div>
<div class="line">Unsupported op types (26):</div>
<div class="line">[<span class="stringliteral">&#39;Greater&#39;</span>, <span class="stringliteral">&#39;TensorArraySizeV3&#39;</span>, <span class="stringliteral">&#39;Rank&#39;</span>, <span class="stringliteral">&#39;Switch&#39;</span>, <span class="stringliteral">&#39;Merge&#39;</span>, <span class="stringliteral">&#39;TopKV2&#39;</span>, <span class="stringliteral">&#39;Fill&#39;</span>, <span class="stringliteral">&#39;Where&#39;</span>, <span class="stringliteral">&#39;TensorArrayGatherV3&#39;</span>, <span class="stringliteral">&#39;TensorArrayWriteV3&#39;</span>, <span class="stringliteral">&#39;All&#39;</span>, <span class="stringliteral">&#39;Enter&#39;</span>, <span class="stringliteral">&#39;Tile&#39;</span>, <span class="stringliteral">&#39;Gather&#39;</span>, <span class="stringliteral">&#39;Size&#39;</span>, <span class="stringliteral">&#39;TensorArrayScatterV3&#39;</span>, <span class="stringliteral">&#39;TensorArrayV3&#39;</span>, <span class="stringliteral">&#39;ZerosLike&#39;</span>, <span class="stringliteral">&#39;Exit&#39;</span>, <span class="stringliteral">&#39;Less&#39;</span>, <span class="stringliteral">&#39;NextIteration&#39;</span>, <span class="stringliteral">&#39;NonMaxSuppressionV2&#39;</span>, <span class="stringliteral">&#39;Equal&#39;</span>, <span class="stringliteral">&#39;Range&#39;</span>, <span class="stringliteral">&#39;LoopCond&#39;</span>, <span class="stringliteral">&#39;TensorArrayReadV3&#39;</span>]</div>
<div class="line">--------------------------------------------------------------------------------</div>
</div><!-- fragment --><p>As shown above, the original model includes some input variables as well as unsupported operators which are not allowed for inference on the VP. Because inference on the VP requires constant input and all supported operators, this section includes steps to ensure proper processing. All of the tools included in this section are only for reference, and do not apply to all networks. Users must determine the correct portion of the graph to run on the VP on their own. The remaining should run using other methods.</p>
<p>To view the location of the supported operators, follow the steps below.</p>
<ol type="1">
<li>Show the graph with Tensorboard: <div class="fragment"><div class="line">build $ import_pb_to_tensorboard.py --model_file frozen_inference_graph.pb --log_dir test_log</div>
<div class="line">Model Imported. Visualize by running: tensorboard --logdir=test_log</div>
<div class="line">build $ tensorboard --logdir=test_log</div>
<div class="line">TensorBoard 1.10.0 at http:<span class="comment">//test:6006 (Press CTRL+C to quit)</span></div>
</div><!-- fragment --></li>
<li>Visit the URL above to view the graph, and try to find the main inference flow. Otherwise, there will be so many unsupported operators that it cannot be processed easily.</li>
<li>Try to identify if there is any offline flow. (This means that the flow has no relationship with the input, and stays the same regardless of the input.)</li>
<li>When the main inference thread is found, use <b>"graph_surgery.py"</b> to cut it and only keep the main inference thread.</li>
<li>Use the <b>"tf_print_graph_summary.py"</b> command to check if there are any unsupported operators in the surgery model. To deal with unsupported operators in this stage, use the instructions below.<ul>
<li>If the operators are in the beginning or end, use <b>"graph_surgery.py"</b> to cut them again.</li>
<li>If there are one or two unsupported operators in the middle, check if there are other supported operators that could replace them. If there are no suitable operators for replacement, users can use the custom node in <b>Caffe</b>. For <b>TensorFlow</b> and <b>ONNX</b>, contact the Ambarella support team for more details.</li>
<li>If many unsupported operators just locate in the middle, users can split the model to three parts with <b>"graph_surgery.py"</b>, such as the example of pvanet.</li>
</ul>
</li>
</ol>
<p>With the above analysis, the VP input and output can be found below. </p><div class="fragment"><div class="line">Input: Preprocessor/mul</div>
<div class="line">Output: Squeeze,Postprocessor/convert_scores</div>
</div><!-- fragment --><p>Users can also find the offline flow for the binary below. </p><div class="fragment"><div class="line">Offline: Preprocessor/mul</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>Because <code>"Concatenate/concat"</code> from the graph has no relationship with the input, users can generate it using offline tools, run it with one image, and then dump the output.</li>
<li>The output <code>"Squeeze,Postprocessor/convert_scores"</code> and <code>"Concatenate/concat"</code> should be used as the input of the final calculation, and include boxes decode, NMS, and others which run on Arm.</li>
</ul>
</dd></dl>
<p>Then, perform the following steps.</p>
<ol type="1">
<li>To remove the unsupported operator and use constant input with VP input and output: <div class="fragment"><div class="line">build $ graph_surgery.py tf -p frozen_inference_graph.pb -o frozen_mobilenet_300_v2_ssd_opt.pb -isrc <span class="stringliteral">&quot;i:Preprocessor/mul|is:1,300,300,3&quot;</span> -on Squeeze,Postprocessor/convert_scores -t ConstantifyShapes</div>
<div class="line"></div>
<div class="line">2019-05-21 20:09:33.698768: I tensorflow/tools/graph_transforms/transform_graph.cc:317 Applying sort_by_execution_order</div>
<div class="line">Saved graph def to frozen_mobilenet_300_v2_ssd_opt.pb</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><b>"frozen_mobilenet_300_v2_ssd_opt.pb"</b> can run entirely on the VP.</dd></dl>
<b>Print the graph:</b> <div class="fragment"><div class="line">build $ tf_print_graph_summary.py -p frozen_mobilenet_300_v2_ssd_opt.pb</div>
<div class="line">from ._conv <span class="keyword">import</span> register_converters as _register_converters</div>
<div class="line">--------------------------------------------------------------------------------</div>
<div class="line">Graph Summary</div>
<div class="line">Total number of ops: 438</div>
<div class="line">Number of non constant ops: 249</div>
<div class="line">Inputs (1):</div>
<div class="line">Preprocessor/mul (1, 300, 300, 3)</div>
<div class="line">Outputs (2):</div>
<div class="line">Squeeze [1, 1917, 4]</div>
<div class="line">Postprocessor/convert_scores [1, 1917, 91]</div>
<div class="line">Unsupported op types: None</div>
<div class="line">--------------------------------------------------------------------------------</div>
</div><!-- fragment --> The information above includes the model used by the CNNGen Samples. It is found in the following path: <code>tensorflow/demo_networks/mobilenetv2_ssd/models/frozen_mobilenet_300_v2_ssd_opt.pb</code>.</li>
<li>To generate a constant binary with an offline tool, such as the PriorBox in Caffe SSD:<ul>
<li>Split the network with only anchor boxes output kept. <div class="fragment"><div class="line">build $ graph_surgery.py tf -p frozen_inference_graph.pb -o frozen_mobilenet_300_v2_ssd_opt.pb -isrc <span class="stringliteral">&quot;i:Preprocessor/mul|is:1,300,300,3&quot;</span> -on Concatenate/concat -t ConstantifyShapes</div>
</div><!-- fragment --></li>
<li>Run the above splited network in PC to generate anchor boxes. For the input, any image should be OK as "Concatenate.concat_0.bin" is not related with input content, only be related with input resolution. <div class="fragment"><div class="line">build $ tf_inference.py -p frozen_mobilenet_300_v2_ssd_opt.pb -isrc <span class="stringliteral">&quot;i:Preprocessor/mul|is:1,300,300,3|ifile:../../dra_img/dog.jpg&quot;</span> -odst <span class="stringliteral">&quot;o:Concatenate/concat|ofile:Concatenate.concat_0.bin&quot;</span></div>
<div class="line">build $ ls -l Concatenate.concat_0.bin</div>
<div class="line">-rw-rw-r-- 1 784 500 30672 Sep 27 06:11 Concatenate.concat_0.bin</div>
</div><!-- fragment --> The command above generates the offline binary used by the CNNGen Samples. It can also be found in the following: <code>tensorflow/demo_networks/mobilenetv2_ssd/models/anchor_boxes.bin</code>.</li>
</ul>
</li>
<li>To write Arm code to implement the boxes decode, NMS, and others, locate the sample code from the SDK package: <code>"ambarella/packages/data_process/"</code>.</li>
<li>Analyze the input pre-process flow for instructions on how to convert it.</li>
</ol>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_post_process"></a>
3 Post Process</h2>
<p>The original network summary should be as follows. </p><div class="fragment"><div class="line">Build $ tf_print_graph_summary.py -p frozen_mobilenet_300_v2_ssd_opt.pb</div>
<div class="line">Graph summary:</div>
<div class="line">Total number of ops: 438</div>
<div class="line">Number of non constant ops: 249</div>
<div class="line">Inputs (1):</div>
<div class="line">    Preprocessor/mul (1, 300, 300, 3)</div>
<div class="line">Outputs (2):</div>
<div class="line">    Squeeze [1, 1, 1917, 4]</div>
<div class="line">    Postprocessor/convert_scores [1, 1, 1917, 91]</div>
</div><!-- fragment --><ol type="1">
<li>The output <code>Squeeze</code> means there are 1917 candidate boxes, 4 means [start_x, start_y, end_x, end_y].</li>
<li>The output <code>Postprocessor/convert_scores</code> means there are 1917 candidate boxes, 91 means the confidence of every classes, this model is trained by 91 classes with COCO.</li>
</ol>
<p>Then if users convert it in default configuration without ot or os, the generated dimension after the parser should be as follows. </p><div class="fragment"><div class="line">VP_typeconv(Postprocessor__convert_scores_,</div>
<div class="line">            VP_tensor(Postprocessor__convert_scores_typeconv, data_format(1, 2, 0, 7), vector(1, 91, 1, 1917)),</div>
<div class="line">            strong_zero = 1,</div>
<div class="line">            disable_reserved = 1,</div>
<div class="line">            __cnngen_tracker = { 454 }</div>
<div class="line">           );</div>
<div class="line">VP_typeconv(Squeeze_pre_trans___sq___,</div>
<div class="line">            VP_tensor(Squeeze_typeconv, data_format(1, 2, 0, 7), vector(1, 4, 1, 1917)),</div>
<div class="line">            strong_zero = 1,</div>
<div class="line">            disable_reserved = 1,</div>
<div class="line">            __cnngen_tracker = { 455 }</div>
<div class="line">           );</div>
</div><!-- fragment --><p>It is not easy to handle in Arm as there are different alignment rules in different chips.</p>
<p>So users can use below post process option to make it easy in ea_cvt_tf_mobilenetv2_ssd.yaml. </p><div class="fragment"><div class="line">output_nodes:</div>
<div class="line">  Squeeze:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,7668</div>
<div class="line">  Postprocessor/convert_scores:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose: 0,2,3,1</div>
<div class="line">    shape: 1,1,1,174447</div>
</div><!-- fragment --><ol type="1">
<li>For the first Output "Squeeze": Parser will convert [1, 4, 1, 1917] to [1, 1, 1917, 4] with ot:0,2,3,1, then to [1, 1, 1, 7668] with os:1,1,1,7668. <div class="fragment"><div class="line">7668 = 1917 * 4</div>
</div><!-- fragment --> Actually, users can only use os:1,1,1,7668 to convert [1, 4, 1, 1917] to [1, 1, 1, 7668], but the content layout should be different as follow.<ul>
<li>For ot and os, final content is as below which the Arm code is based on. <div class="fragment"><div class="line">box1_start_x, box1_start_y, box1_end_x, box1_end_y, box2_start_x, box2_start_y, box2_end_x, box2_end_y </div>
</div><!-- fragment --></li>
<li>For only os, final content is as below. <div class="fragment"><div class="line">box1_start_x, box2_start_x, box3_start_x, box4_start_x, . box1_start_y, box2_start_y, box3_start_y, box4_start_y</div>
</div><!-- fragment --></li>
</ul>
</li>
<li>The same post process for the output "Postprocessor/convert_scores": <div class="fragment"><div class="line">174447 = 1917 * 91</div>
</div><!-- fragment --></li>
</ol>
<p>So there are two reasons that users need to use ot and os in above examples.</p><ol type="1">
<li>The dimension [1, ,1 , 1, N] is easy to be handled in Arm side, the VP has padding with different bytes in different chips, so default dimension [1, 4, 1, 1917] or [1, 91, 1, 1917] will result in lots of paddings with useless data and not easy to be handled.</li>
<li>The Arm code in packages/data_process is designed with this dimension. If users can write the Arm code by themselves, they can do anything they want, do not need to follow above design flow with ot and os.</li>
</ol>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_cnngen_conversion"></a>
4 CNNGen_Conversion</h2>
<p>Generate the Cavalry binary as shown below:</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -c tensorflow/demo_networks/mobilenetv2_ssd/config/ea_cvt_tf_mobilenetv2_ssd.yaml</div>
</div><!-- fragment --><p>The output is in: <code>out/tensorflow/demo_networks/tf_mobilenetv2_ssd/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/tensorflow/demo_networks/tf_mobilenetv2_ssd</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</code> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_ssd/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_ssd</code>.</li>
<li>For X86 simulator, model desc json file <b>tf_mobilenetv2_ssd.json</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_ssd/&lt;chip&gt;/out_&lt;build_target&gt;_parser/</code>. ades command <b>tf_mobilenetv2_ssd_ades.cmd</b> is in the cnngen output folder <code>out/tensorflow/demo_networks/tf_mobilenetv2_ssd/&lt;chip&gt;/&lt;chip&gt;_ades_tf_mobilenetv2_ssd</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_build_unit_test"></a>
5 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">        -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">          [*] Build Ambarella custom postprocess library with ssd</div>
<div class="line">      [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li><p class="startli">Build Unit Test for X86 Simulator</p>
<p class="startli">Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</p>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_run_python_inference"></a>
6 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/tensorflow/demo_networks/tf_mobilenetv2_ssd/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_ssd/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/tensorflow/demo_networks/tf_mobilenetv2_ssd/tf_mobilenetv2_ssd_cvt_summary.yaml \</div>
<div class="line">        -iy tensorflow/demo_networks/mobilenetv2_ssd/config/ea_inf_tf_mobilenetv2_ssd.yaml -pwd ./out/tensorflow/demo_networks/tf_mobilenetv2_ssd</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/tensorflow/demo_networks/tf_mobilenetv2_ssd/&lt;chip&gt;/&lt;chip&gt;_cavalry_tf_mobilenetv2_ssd/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin \</div>
<div class="line">        -pn ssd -pl &lt;usr_path&gt;/mobilenetv2_ssd/config/ssd_tf.lua -dm 0 -lp &lt;usr_path&gt;/mobilenetv2_ssd/config/label_coco_91.txt \</div>
<div class="line">        -ei &lt;usr_path&gt;/mobilenetv2_ssd/models/anchor_boxes.bin -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_tf_mobilenetv2_ssd_run_c_inference"></a>
7 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_mobilenetv2_ssd_cnngen_conversion">4 CNNGen_Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</b>.</li>
<li>The <b>ssd_tf.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li><p class="startli">For X86 Simulator:</p>
<p class="startli">Refer to <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html#sub_sec_tf_mobilenetv2_ssd_cnngen_conversion">4 CNNGen_Conversion</a> for how to generate <b>tf_mobilenetv2_ssd.json</b> and <b>tf_mobilenetv2_ssd_ades.cmd</b>.</p>
</li>
</ol>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/ssd</div>
<div class="line">|--model</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</div>
<div class="line">|</div>
<div class="line">|--anchor_boxes</div>
<div class="line">|       anchor_boxes.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|       label_coco_91.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|       dog.jpg</div>
<div class="line">|       dog.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_coco_91.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>Users can find <b>"mobilenet_priorbox_fp32.bin"</b> in <code>cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/models/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
<p>Before running model <b>&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</b>, users need to verify whether the parameters <b>num_class, output_score_name, output_loc_name, tf_scale_factors</b> in the <b>ssd_tf.lua</b> file are correct. If not, users need to modify them.</p>
<dl class="section note"><dt>Note</dt><dd><pre class="fragment">  _nn_arm_nms_config_ = {
          conf_threshold = 0.2,    -- Confidence threshold
          nms_threshold = 0.3,     -- NMS threshold
          log_level = 2,           -- 0 none, 1 error, 2 notice, 3 debug, 4 verbose.
          keep_top_k = 100,
          nms_top_k = 50,
          background_label_id = 0,
          unnormalized = 0,
          num_class = 91,          -- class num
          thread_num = 4,
          output_score_name = "Postprocessor/convert_scores",
          output_loc_name = "Squeeze",
          tf_scale_factors = {10, 10, 5, 5},
          ssd_debug = 0,
  }
</pre></dd>
<dd>
OpenMP is used in data process library to process the loops in parallel which will benefit from multiple cores for performance. Please specify the number of parallel threads for Arm tasks through "--thread_num", the default is the maximum core number of the chip.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_mobilenetv2_ssd_parser/tf_mobilenetv2_ssd.json \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_tf_mobilenetv2_ssd/tf_mobilenetv2_ssd_ades.cmd \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/tf_ssd_mobilenetv2/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_mobilenetv2_ssd_parser/tf_mobilenetv2_ssd.json \</div>
<div class="line">     --lua_file ssd_tf.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">     --extra_input &lt;usr_path&gt;/anchor_boxes.bin \</div>
<div class="line">     --label_path &lt;user path&gt;/label_coco_91.txt  \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/tf_ssd_mobilenetv2/out \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_tf_mobilenetv2_ssd/tf_mobilenetv2_ssd_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_mobilenetv2_ssd_parser/tf_mobilenetv2_ssd.json</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/tf_ssd_mobilenetv2/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_tf_mobilenetv2_ssd_parser/tf_mobilenetv2_ssd.json \</div>
<div class="line">     --lua_file ssd_tf.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">     --extra_input &lt;usr_path&gt;anchor_boxes.bin \</div>
<div class="line">     --label_path &lt;user path&gt;label_coco_91.txt \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/tf_ssd_mobilenetv2/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li><p class="startli">For EVK Board:</p><ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/ssd_tf.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=/sdcard/ssd/in|t:jpg|d:vp|c:rgb&quot;</span> \</div>
<div class="line">     --extra_input /sdcard/ssd/anchor_boxes/anchor_boxes.bin \</div>
<div class="line">     --label_path /sdcard/ssd/labels/label_coco_91.txt \</div>
<div class="line">     --output_dir /sdcard/ssd/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:Preprocessor/mul=/sdcard/ssd/in|t:raw&quot;</span> \</div>
<div class="line">     --output_dir /sdcard/ssd/out/</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/tensorflow/demo_networks/mobilenetv2_ssd/dra_img/dog.jpg</code>) in <code>/sdcard/ssd/in</code>, and create <code>/sdcard/ssd/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li><p class="startli">Run</p><ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n ssd \</div>
<div class="line">    --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin \</div>
<div class="line">    --lua_file /usr/share/ambarella/eazyai/lua/ssd_tf.lua \</div>
<div class="line">    --extra_input /sdcard/ssd/anchor_boxes/anchor_boxes.bin \</div>
<div class="line">    --label_path /sdcard/ssd/label/label_coco_91.txt</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n ssd \</div>
<div class="line">    --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/&lt;chip&gt;_cavalry&lt;version&gt;_tf_mobilenetv2_ssd.bin \</div>
<div class="line">    --lua_file /usr/share/ambarella/eazyai/lua/ssd_tf.lua \</div>
<div class="line">    --extra_input /sdcard/ssd/anchor_boxes/anchor_boxes.bin \</div>
<div class="line">    --label_path /sdcard/ssd/label/label_coco_91.txt</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, check the following two points.<ul>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>For this TensorFlow network, it is RGB input, not BGR which is the case for the Caffe SSD model.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>. </li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaigen-layercompare_html_ga4d5bb0c360b13429f65cd327c8d0aa12"><div class="ttname"><a href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a></div><div class="ttdeci">model_path</div></div>
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga81f22c9cd9a33cc05e5a1657974438bd"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a></div><div class="ttdeci">work_dir</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga8179f95715172cfcd3a44cd038a81a9f"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a></div><div class="ttdeci">transforms</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
