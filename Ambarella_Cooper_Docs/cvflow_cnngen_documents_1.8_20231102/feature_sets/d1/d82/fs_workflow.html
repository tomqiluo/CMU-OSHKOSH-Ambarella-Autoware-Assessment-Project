<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: Work Flow</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d1/d82/fs_workflow.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Work Flow </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Ambarella CNNGen converts networks on a PC and then runs them on the CV board. In <b>Ubuntu 2004,</b> the user provides the model files to CNNGen which then converts the files and generates DAGs that can run on the CV board.</p>
<p>The basic workflow is as follows:</p>
<ul>
<li>Select suitable network with suitable framework.</li>
<li>Model Optization with quantization and pruning if needed.</li>
<li>Convert the network to Ambarella format.</li>
<li>Evalute the accuracy to see if it can meet with the requirement.</li>
<li>Deploy with Cooper SDK.</li>
</ul>
<div class="image">
<img src="../../workflow.jpg" alt=""/>
<div class="caption">
Workflow</div></div>
   <hr  />
<h1><a class="anchor" id="sec_pre_cnn_model"></a>
1 Network and Framework Selection</h1>
<h2><a class="anchor" id="sub_sec_network_selection"></a>
1.1 Network Selection</h2>
<p>User should follow below points for differe CVflow chips.</p><ol type="1">
<li>Floow the chip computing power abality to select the lighter or higher networks.</li>
<li>Select the networks which will benefit more from CVflow, fro detail, please refer to <em>Ambarella_CV*_DG_Flexible_Linux_SDK*.*_CVflow_FAQ</em>.</li>
<li>Choose the networks which will benefit more from weights pruning as CVflow chip will benefit from weights pruning.</li>
<li>Better not to use public quantization networks, differnt quantization will result big accuracy drop.</li>
</ol>
<h2><a class="anchor" id="sub_sec_framework_support"></a>
1.2 Frameworks Selection</h2>
<p>Currently, three mainstream frameworks are supported as follows.</p>
<ul>
<li>Caffe (Will not be maintained anymore since CNNGen 2.5.4 and 3.5.4 with Ubuntu2004)</li>
<li>Tensorflow 1.x</li>
<li>ONNX</li>
</ul>
<p>For other frameworks, users should handle as follows.</p>
<ul>
<li>For Tensorflow 2.x models, users needs to convert them to tflite, then to use CNNGen toolchain to convert them to Tensorlfow 1.x, then to Ambarella format.</li>
<li>For Tflite models, users need to use CNNGen toolchain to convert them to Tensorlfow 1.x, then to Ambarella format.</li>
<li>For Keras models, users need to convert them to Tensorflow 1.x, then to use CNNGen toolchain for Ambarella format.</li>
<li>For Pytorch and others, users needs to convert it to ONNX, then to use CNNGen toolchain for Ambarella format.</li>
</ul>
<p>There are some examples in fs_cnngen_samples_package.</p>
<ul>
<li>Darknet to Caffe, <code>caffe/demo_networks/yolo_v3/script/darknet2caffe</code></li>
<li>Darknet to Tensorflow, <code>tensorflow/demo_networks/tiny_yolo_v3/models/readme.txt</code></li>
<li>Tflite to Tensorflow, <code>tensorflow/test_networks/yolo_v4/scripts/readme.txt</code></li>
<li>MXNET to ONNX, <code>onnx/demo_networks/retinaface/script/export_retinaface_onnx_model.py</code></li>
<li>Pytorch to ONNX, <code>onnx/demo_networks/yolov5/models/readme.txt</code></li>
</ul>
<h2><a class="anchor" id="sub_sec_framework_retrain"></a>
1.3 Retrain Related Frameworks</h2>
<p>CNNGen will convert Float 32 original model to fix point 8 or 16, and DRA will do dynamic analysis for every operator and choose the right format to reduce the accuracy lost.</p>
<ul>
<li>If users are using full fix16, there will be less accuracy lost, even to be ignored.</li>
<li>If users are using mixed fix16 and fix8, there will be a little accuracy lost, but the performance will be better, it is suggested to use this one.</li>
<li>If users are using full fix 8, the performance will be the best, but there will be more accuracy lost. Of course, it is decided by different networks, if users cannot accept this loss with this setting, quantization fix8 retrain needs to be done.</li>
</ul>
<p>CNNGen has no limitation for which framework users should use, there are some suggestions below.</p>
<ul>
<li>If users do not needs quantization fix8 retrain, they can use whichever framework they prefer, and CNNGen can handle well for every framework.</li>
<li>If users need to use the best performance with fix8, the tool status is as follows.<ul>
<li><b>Caffe</b>, there are some quantization fix8 retrain experience and special tools which can help users to easily retrain, but as everyone knows, the Caffe framework is so old that it cannot cover some types of networks, and it will not be maintain anymore. For some basic networks, such as SSD or Yolov3, Caffe is good.</li>
<li><b>Tensorflow</b>, it is the most flexible framework, but it is so flexible that there will be a little difficulty handling its model file. Also, Ambarella has limited experience with doing quantization fix8 retrain with this framework so users should cover it by themselves.</li>
<li><b>ONNX</b>, is currently the most popular framework as more and more users are using PyTorch and converting the model to ONNX. Ambarella will pay more resource on this for conversion and quantization retrain.</li>
</ul>
</li>
</ul>
<p>So the framework to be used should be decided by which network users would use, and CNNGen tool will not limit the framework users need to use.</p>
<hr  />
<h1><a class="anchor" id="sec_model_opt"></a>
2 Network Optimization</h1>
<p>The following diagram shows the recommended optimization flow of the CNN model before deployment.</p>
<div class="image">
<img src="../../cnn_model_opt_flow.jpg" alt=""/>
<div class="caption">
CNN Model Optimization Workflow</div></div>
   <p>Users can apply Ambarella retrain tools from Ambarella support team to perform pruning and quantization.</p>
<p>To achieve optimal performance on CV, Ambarella recommends pruning the CNN model.</p>
<p>CVflow chip includes a new, faster convolution engine called the inception convolution engine (ICE). However, because ICE supports a fixed-point data format exclusively, the data format must be in either FX16 or FX8. CNNGen converts the modelâ€™s parameters to either the FX16 or FX8 data format. If the original model is based on FX32, accuracy can be lost during conversion (less accuracy is lost with 16-bit). Additionally, accuracy can be lost while achieving the best performance with 8-bit quantization. Therefore, users should first prune, perform quantization, and then retrain the program before using CNNGen.</p>
<h2><a class="anchor" id="sub_sec_1_model_prune"></a>
2.1 Pruning</h2>
<p>Pruning methods include <b>connection</b> pruning, <b>coefficient</b> pruning, and more, such as below.</p>
<div class="image">
<img src="../../sketch_of_pruning.jpg" alt=""/>
<div class="caption">
Sketch of Pruning</div></div>
   <p>The <b>CVflow</b> benefits exclusively from <b>coefficient</b> pruning (sparsification), as it includes a high density of zero-valued coefficients. Because the CVflow execution engine can bypass known results without performing the MAC, there is a direct acceleration when the system is MAC-limited. Users can achieve a low non-zero coefficient density while retaining accuracy and receiving a 3xâ€•4x speed boost on many networks.</p>
<p>Therefore, Ambarella recommends that users prune their CNN models before they are mapped into the CV chip. Additionally, to prevent a loss of accuracy in the final network mAP, Ambarella recommends users retrain the network after pruning. For further information, contact the Ambarella support team.</p>
<dl class="section note"><dt>Note</dt><dd>lots of acceleration engines might not benefit from <b>coefficient</b> pruning. Connection pruning applies a different pruning technique. By reducing the number of convolution kernels processed, connection pruning helps both CVflow and other engines., but it will result big accuracy lost.</dd></dl>
<p>The following describes the steps for pruning the VGG-SSD network.</p>
<ol type="1">
<li>Prepare the original DNN at 100% density.</li>
<li>Use a modified training tool to prune the NN for the lower density.</li>
<li>After pruning, retrain the DNN to match the accuracy of the original DNN.</li>
</ol>
<p>The following table displays the DNN accuracy after both pruning and retraining.</p>
<a class="anchor" id="Accuracy of VGG-SSD after Pruning"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>VGG16+SSD </th><th>Data </th><th>mAP </th><th>Accuracy Lost </th></tr>
<tr align="middle">
<td>100% Density </td><td>FP32 </td><td>76.50% </td><td>0% </td></tr>
<tr align="middle">
<td>18% Density </td><td>FP32 </td><td>76.59% </td><td>+0.09% </td></tr>
<tr align="middle">
<td>13% Density </td><td>FP32 </td><td>76.05% </td><td>-0.45% </td></tr>
<tr align="middle">
<td>8% Density </td><td>FP32 </td><td>75.45% </td><td>-1.05% </td></tr>
<tr align="middle">
<td>5% Density </td><td>FP32 </td><td>74.26% </td><td>-2.24% </td></tr>
</table>
<p>The table above shows how the accuracy can match the original 100% (0~0.45%) by pruning and retraining.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The image input size must be 244x244.</li>
<li>Use VOC0712 full datasets with 20 categories for re-training. Retrain with 22135 images.</li>
<li>Use VOC0712 image for mAP report. Use 4952 images.</li>
<li>The mAP testing images are 4952 images found in the Pascal VOC2007 datasets.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_model_quant"></a>
2.2 Quantization</h2>
<p>Although floating point (32-bit) is used for training and for inference, fixed 8 and fixed 16 multiples are faster and more power-efficient. As a result, all acceleration engines use fixed 8-bit and fixed 16-bit for inference at the edge. The performance improvement for an 8-bit network is approximately twice that of the 16-bit version.</p>
<p>In Ambarella solution, there are two quantization tools to avoid accuracy loss.</p><ol type="1">
<li>Dynamic Range Analysis (DRA), which is a post quantization method without retrain. Please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sub_sec_dra_ver_history">6.1 DRA Version History</a> for detail.</li>
<li><p class="startli">Pre-Quantization Retrain tool, which is a post quantization method without retrain. This tool is not in default release, please ask help from Ambarella support team. Only DRA in #1 cannot cover the accuracy, then Pre-Quantization Retrain is needed. And there are also two method, such as below.</p><ul>
<li>Fake Quantization, may have minor accuracy lost for the final deployment in CVflow chip.</li>
<li>Ambarella Quantization, the same with the final accuracy in CVflow chip.</li>
</ul>
<div class="image">
<img src="../../sketch_of_quant.jpg" alt=""/>
<div class="caption">
Sketch of Quantization</div></div>
   </li>
</ol>
<dl class="section note"><dt>Note</dt><dd>It is not suggest to use quantization midel which is based on public quantization, special for some asymmetric quantization models.</dd></dl>
<p>The following describes the steps for performing quantization for the VGG-SSD network.</p>
<ol type="1">
<li>Prepare the sparse NN.</li>
<li>Use CNNGen DRA tools to quantize from FP to FX8 / FX16.</li>
<li>Retrain with a modified training tool if accuracy loss is too high.</li>
</ol>
<p>The following table shows the DNN accuracy after quantization with CNNGen DRA tool.</p>
<a class="anchor" id="Accuracy of VGG-SSD after Quantization"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>VGG16+SSD </th><th>Data </th><th>mAP </th><th>Accuracy Lost </th></tr>
<tr align="middle">
<td>18% Density </td><td>FP32 </td><td>76.59% </td><td>+0.09% </td></tr>
<tr align="middle">
<td>18% Density </td><td>FX8 </td><td>76.05% </td><td>-0.54% </td></tr>
</table>
<p>This shows that by using the CNNGen DRA tool to quantize the network, the accuracy nearly matches the original level (-0.54%).</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The image input size must be 244x244.</li>
<li>Use VOC0712 full datasets with 20 categories for retraining. Retrain with 22135 images.</li>
<li>Use VOC0712 image for the mAP report. Use 4952 images.</li>
<li>The mAP testing images are the 4952 images that come from the Pascal VOC2007 datasets.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_port_procedure"></a>
3 Converter</h1>
<p>The figure below shows the flow of the porting procedure.</p>
<div class="image">
<img src="../../porting_flow.jpg" alt=""/>
<div class="caption">
Porting Flow</div></div>
   <p>The porting flow shown above is as follows:</p>
<ul>
<li>The CNNGen parser expands the node graph to primitive graph and performs quantization and computation reduction with DRA and custom node.<ul>
<li>DRA determines the data format by data distribution, calculated by the sample images given by the designer.</li>
<li>Custom node provides a serial of specific APIs that enables users to implement their own layers.</li>
<li>Run Acinference in build server to verify results after DRA.</li>
</ul>
</li>
<li>VAS expands the primitive graph to operator graph (DAG) and performs the low-level optimization and DAG splitting.</li>
<li>Users run the VAS generated DAGs on ADES in build server to verify results after Vas compiler.</li>
<li>Users use <em>cavalry_gen</em> script to generate final executive binary and use Ambarellaâ€™s interface to run it on CV board.</li>
</ul>
<p>Users can use <code>eazyai_cvt xxx</code> which inlcudes all above steps with below models. For details, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</p><ol type="1">
<li>Preprocess Mdels in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_preprocess_tools">13 Pre-Process Model Tools</a></li>
<li>Pruning test toools in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_pruning_tool">17 Pruning Test Tools</a></li>
<li>Parser in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_parser">6 Parsers</a></li>
<li>Vas</li>
<li>Cavarly_gen in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_cavalry_gen">11 Cavalry_gen</a></li>
<li>Layer_compare in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_layer_comapre">12 layer_compare.py</a></li>
<li>CVflowbackend in CVFlowBackend</li>
<li>Layer Profiler in CVflow Layer Profiler</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>It is suggested that users need to <b>save the parser output for their networks</b>, for details, please refer to <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_compatibility">10 Compatibility</a>.</dd></dl>
<h2><a class="anchor" id="sub_sec_dra"></a>
3.1 Dynamic Range Analysis(DRA)</h2>
<p>The CNNGen uses dynamic range analysis (DRA) to determine the data format using data distribution. The CNNGen calculates the sample images provided by the designer (Ambarella recommends using between 100 and 200 images for the DRA) to maintain accuracy between the following two cases:</p>
<ul>
<li>Case 1: Customer uses the side-band information input of CNNGen to perform custom quantization on the required layers to maintain NN constraints (such as preservation of accuracy).</li>
<li>Case 2: Customer uses the Ambarella CNNGen tool with DRA to perform quantization on all layers.</li>
</ul>
<div class="image">
<img src="../../dra_flow.jpg" alt=""/>
<div class="caption">
DRA Flow</div></div>
   <h2><a class="anchor" id="sub_sec_accuracy_debug"></a>
3.2 Basic Accuracy Evaluation</h2>
<p>After converting the model using CNNGen, users must confirm the accuracy of the model:</p>
<p>Use layer compare with <code>eazyai_cvt xxx -lc</code> in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a> to check the details. Actually, this tool will call <code>layer_compare.py</code> in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_layer_comapre">12 layer_compare.py</a> to run network infrence with PC, Acinference, and Ades, then to compare every layer's result to see if there is accuracy lost. Note that, this tool's result is only based on one image.</p>
<p>Of course, user also can draw the network result on images to check if it is correct, for this, user can use <code>eazyai_inf xxx</code> in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> to do this inference which will call the tools in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_x86_simulator">10 x86 Simulator</a>.</p>
<hr  />
<h1><a class="anchor" id="fs_accuracy_tool_test"></a>
4 Accuracy Evaluation</h1>
<p>Due to the architectural differences with the x86 platforms, CVflow performs the CNN in the fixed-data type (such as Fix-8 / Fix-16). So besides above basice accuracy test, user need to perform accuracy testing with own dataset for <b>cosine</b>, <b>topn</b> and <b>mAP</b>.</p>
<p>Therefore, there is a little difference between the CVflow accuracy result and the original FP32 accuracy result, short data range may bring accuracy loss. For quantization problems, please refer to <a class="el" href="../../d1/d82/fs_workflow.html#sub_sec_model_quant">2.2 Quantization</a> for detail.</p>
<p>Accuracy evaluation is very important to measure if the convert session is successful.</p><ol type="1">
<li>Based on a big test datasets, good accuracy means this network can cover different scenes with good accuracy.</li>
<li>And close accuracy between original framework and CVflow / Ades / Acinf means the convert session are successful. Different with <code>layer_compare</code> or simple compare with several images between , accuracy is the final measurement indicators for the network.</li>
</ol>
<p>Then EazyAI inference tools <code>eazyai_inf</code> in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> is able to generate four types of network results for this evaluation:</p><ol type="1">
<li>Original Framework FP32 results</li>
<li>Acinference Simulator results</li>
<li>Ades Simulator results</li>
<li>CVflow results</li>
</ol>
<p>For the difference between Acinference and Ades, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_x86_simulator">10 x86 Simulator</a> for detail. And it is more suggested that users can use Original Framework and CVflow to get the accuracy loss as CVflow chip is much faster than Acinference and Ades.</p>
<p>Below chapter includes guidelines for setting up the accuracy evaluation flow.</p>
<h2><a class="anchor" id="sec_accuracy_workflow"></a>
4.1 Workflow</h2>
<p>There are four major parts in the accuracy workflow: data preparation, inference, post-processing, and evaluation.</p>
<div class="image">
<img src="../../cflite_inference_accuracy.jpg" alt=""/>
<div class="caption">
Workflow of the Accuracy Regression Test</div></div>
   <p>Below is the simpe introduction for these modules.</p><ol type="1">
<li>Data Prepare The data preparation module generates the data for network forward inference. It includes converting data from its original format to the inference format. For example, it is able to convert images from jpg format to binary format; it is also able to convert wav format data to binary format. 2 Inference The inference module does the neuron network forward inference, there are four available network inference modes which is metioned in previous:<ul>
<li><b>Original inference</b>: The forward inference is done by the network original platform(x86), which means Caffe/TensorFlow/ONNX.</li>
<li><b>Acinference</b>: The forward inference is done by the AmbaCNN simulator(x86), which is the converted network after the DRA stage. The result after this stage can help the users to know if there is any accuracy lost after network converting and DRA quantization. The result of this mode is very close to the result of the mode <b>cvflow</b>.</li>
<li><b>Ades</b>: The forward inference is done by the Ades simulator(x86), which is the converted network after the VAS stage. The result after this stage can help the users to know if there is accuracy lost after network converting, DRA quantization and VAS compilation. The result of this mode should be bit-perfect with the mode <b>cvflow</b>.</li>
<li><b>CVflow</b>: The forward inference is done by the vector processor(CVflow) of the EVK board. This mode reflects the real inference speed and the real inference result in the deployment.</li>
</ul>
</li>
<li>Post Process The post-process module gets the forward inference result and do the post-processing on the network result. For example, draw segmentation map on the original pictures, return bounding-boxes to the application side, etc.</li>
<li>Evaluation The evaluation module does the network accuracy evaluation. There are four valid modes in evaluation configuration: <b>TopN</b>, <b>Cosine</b>, <b>mAP</b> and <b>mIOU</b>. However, users are responsible for determining whether the new test mode fits the network type. Typically, TopN evaluates classification networks, the Cosine evaluates feature extraction networks, mAP evaluates detection networks and mIOU evaluate segmentation networks.</li>
</ol>
<p>For more information about CFLite modules , please refer to <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_modules">2 CFlite Modules</a>.</p>
<h2><a class="anchor" id="sec_accuracy_run_steps"></a>
4.2 Run steps</h2>
<h3><a class="anchor" id="sub_sec_accuracy_build_the_SDK"></a>
4.2.1 Build the SDK</h3>
<p>Please to <a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_evk_binary">1 Prepare the Binary</a> foe detail.</p>
<h3><a class="anchor" id="sub_sec_accuracy_env_config"></a>
4.2.2 Environment Configuration</h3>
<p>Please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_env_set">3 Environment Setting</a> for detail.</p>
<h3><a class="anchor" id="sub_sec_accuracy_download_dataset"></a>
4.2.3 Download the Test Dataset</h3>
<p>Please refer to <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_dm">1.4 Download Models</a> to download <code>data</code> folder from Ambarella web share, and put is in CNNGen samples <code>ROOT_DIR</code> folder, such as below. </p><div class="fragment"><div class="line">build $ cp -rf data cvflow_cnngen_samples_&lt;version&gt;/</div>
<div class="line">build $ tree cvflow_cnngen_samples_&lt;version&gt;</div>
<div class="line">cvflow_cnngen_samples_1.8/</div>
<div class="line">â”œâ”€â”€ caffe</div>
<div class="line">â”œâ”€â”€ tensorflow</div>
<div class="line">â”œâ”€â”€ onnx</div>
<div class="line">â”œâ”€â”€ xxx</div>
<div class="line">â””â”€â”€ data</div>
<div class="line">build $ cd data; tree data</div>
<div class="line">data/</div>
<div class="line">â”œâ”€â”€ COCO_2014</div>
<div class="line">â”‚Â Â  â””â”€â”€ get_coco_2014.sh</div>
<div class="line">â”œâ”€â”€ ILSVRC_2012</div>
<div class="line">â”‚Â Â  â””â”€â”€ gen_from_downloaded_ilsvrc2012.sh</div>
<div class="line">â””â”€â”€ VOC_07</div>
<div class="line">    â””â”€â”€ get_voc07.sh</div>
<div class="line">build $ cd COCO_2014</div>
<div class="line">build $ ./get_coco_2014.sh</div>
<div class="line">â”œâ”€â”€ dra_img_1 (One image <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â”œâ”€â”€ dra_img_100 (100 images <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â””â”€â”€ test_img_2500 (Test Images)</div>
<div class="line">build $ cd VOC_07</div>
<div class="line">build $ ./get_voc07.sh</div>
<div class="line">â”œâ”€â”€ dra_img_1 (One image <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â”œâ”€â”€ dra_img_100 (100 images <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â”œâ”€â”€ test_img_4952 (Test Images)</div>
<div class="line">â”œâ”€â”€ seg_ground_truth (Ground Truth Files For Segmentation network)</div>
<div class="line">â””â”€â”€ test_seg_632 (Test Images For Segmentation network)</div>
<div class="line">build $ cd ILSVRC_2012</div>
<div class="line">build $ ./gen_from_downloaded_ilsvrc2012.sh</div>
<div class="line">â”œâ”€â”€ dra_img_1 (One image <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â”œâ”€â”€ dra_img_118 (118 images <span class="keywordflow">for</span> DRA in accuracy test)</div>
<div class="line">â””â”€â”€ test_img_1284 (Test Images)</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For ILSVRC_2012, please download <code>ILSVRC2012_img_val.tar</code> manually, and check it md5sum as <code>29b22e2961454d5413ddabcf34fc5622</code>. For this dataset, it requires special register to have the permisson to download it.</li>
<li>The <code>.csv</code> file is the ground truth file for <code>TOPn</code> and <code>mAP</code>, <code>seg_ground_truth</code> is the ground truth file for <code>MIOU</code>.</li>
<li>These are all open source dataset for study and test purpose, so please pay attention to their license statement if not only for study and test.</li>
</ol>
</dd></dl>
<p>Of course, users should have their own dataset for their own network. To use the dataset, prepare the label file in the same format as <em>&lt;dataset_name&gt;_label.csv</em> (located in the same folder as the download script).</p>
<h3><a class="anchor" id="sub_sec_accuracy_convert_network"></a>
4.2.4 Convert With CNNGen toolchain</h3>
<p>Users can use below command to convert the network.</p>
<div class="fragment"><div class="line">build $ python3 eazyai_cvt.py -cy ea_cvt_&lt;network_name&gt;.yaml -ey ea_cvt_ext_&lt;network_name&gt;.yaml</div>
</div><!-- fragment --><p>After the compiling, the result is stored under <em>&lt;work_dir&gt;/&lt;network_name&gt;</em>. There is a convert summary YAML file, which is named as <em>&lt;network_name&gt;_cvt_summary.yaml</em>, generated under the same folder. This summary file is an input configuration for the inference stage in the next step.</p>
<dl class="section note"><dt>Note</dt><dd>For the detail of this convert tool, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</dd></dl>
<h3><a class="anchor" id="sub_sec_accuracy_execution"></a>
4.2.5 Inference and Evaluation</h3>
<p>After compilation, start the accuracy test execution using the following commands for original framework and CVflow.</p>
<div class="fragment"><div class="line">build $ eazyai_inf -ip 10.0.0.2 -cy &lt;network_name&gt;_cvt_summary.yaml -iy ea_inf_&lt;network_name&gt;.yaml --platform orig</div>
<div class="line">build $ eazyai_inf -cy &lt;network_name&gt;_cvt_summary.yaml -iy ea_inf_&lt;network_name&gt;.yaml --platform cvflow</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Please ensuring that the network connection between the PC and EVK is valid when using CVflow.</li>
<li>Using <code>--platform ades</code> and <code>--platform acinf</code> to do simulator inference if needed.</li>
<li>In normal case, only *&lt;network_name&gt;_cvt_summary.yaml* is needed for network forward inference. However,in the accuracy case, a <em>ea_inf_&lt;network_name&gt;.yaml</em> is necessary, since it provides post-processing configuration and evaluation configuration.</li>
<li>IP address is needed for platform <b>cvflow</b>.</li>
<li>Whether to run post-processing or evaluation depends on whether the <em>ea_inf_&lt;network_name&gt;.yaml</em> has the root key "post_process" or "eval_info".<ol type="a">
<li>For <b>TopN</b>, <b>cosine</b>, <b>mIOU</b>, these three mode do not forcedly require post-processing.</li>
<li>For <b>mAP</b>, post-processing is necessary.</li>
</ol>
</li>
<li>For the detail of this convert tool, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a>.</li>
<li>Please use <code>*._no_postfix.csv</code> as the ground truth file. <div class="fragment"><div class="line">data/COCO_2014/coco_val2014_label_no_postfix.csv</div>
<div class="line">data/COCO_2014/coco_val2014_label_tf_no_postfix.csv</div>
<div class="line">data/VOC_07/VOC2007_VGGSSD_GF_21_class_skip_difficult_no_postfix.csv</div>
</div><!-- fragment --></li>
</ol>
</dd></dl>
<p>For <b>cosine</b> evaluation mode, the evaluation will be done during the second inference, while using the option <em>&ndash;compare_folder</em>.</p>
<div class="fragment"><div class="line">build $ eazyai_inf -ip 10.0.0.2 -cy &lt;network_name&gt;_cvt_summary.yaml -iy ea_inf_&lt;network_name&gt;.yaml --platform cvflow</div>
<div class="line">build $ eazyai_inf -cy &lt;network_name&gt;_cvt_summary.yaml -iy ea_inf_&lt;network_name&gt;.yaml --platform orig --compare_folder &lt;previous_inference_result_folder_path&gt;</div>
</div><!-- fragment --><p>Besides the accuracy evaluation result, the network result is alse saved in below locations.</p><ol type="1">
<li>The inference result files can be found in <em>&lt;work_dir&gt;/&lt;network_name&gt;/&lt;chosen_platform&gt;/inference</em>.</li>
<li>The post-processing result files can be found in <em>&lt;work_dir&gt;/&lt;network_name&gt;/&lt;chosen_platform&gt;/post_process</em>.</li>
<li>The evaluation report file can be found in <em>&lt;work_dir&gt;/&lt;network_name&gt;/&lt;chosen_platform&gt;/eval</em>.</li>
</ol>
<h2><a class="anchor" id="sec_accuarcy_usage"></a>
4.3 Usage Demonstration</h2>
<p>Please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_vgg16_ssd">Regression for VGG16-SSD</a> for detailed exmaples.</p>
<h2><a class="anchor" id="sec_accuarcy_notice"></a>
4.4 Some Important Notes</h2>
<p>To test the accuracy for original and CVflow chip in the same baseline, above full accuracy test flow has done below things.</p><ol type="1">
<li>Using the same test datasets</li>
<li>Using the same method for data prepare</li>
<li>Using the same method for post process</li>
<li>Using the same method for accuracy evaluation</li>
</ol>
<p>But users may need to use their own data prepare method and post process, then they can do as below based on current accuracy test flow.</p>
<h3><a class="anchor" id="sub_sec_accuracy_replace_data_prepare"></a>
4.4.1 Replace Data Prepare Module</h3>
<p>Users can develop their own data prepare module and calls the inference module. The return value of the data prepare module is a string of a list file path, in which contains the absolute path of the converted binary files. Therefore, users can starts from changing the calling of this part to use their own method.</p>
<div class="fragment"><div class="line">list_file = data_prepare.run()[0]</div>
</div><!-- fragment --><p>Users can also change from the inference side. The inference module takes a portname-filepath pair to set the file path for each network input port.</p>
<div class="fragment"><div class="line">inferencer.set_in_file_path(inport_name, filepath)</div>
<div class="line">inferencer.run()</div>
</div><!-- fragment --><h3><a class="anchor" id="sub_sec_accuracy_replace_post-processing"></a>
4.4.2 Replace Post Processing Module</h3>
<p>Users can add their own post-processing by adapting to the output of the inference module. The output of the inference module is a dictionary using network output port name as key and the inference result file as value.</p>
<div class="fragment"><div class="line">outport_file_dict = inferencer.get_out_files()[<span class="stringliteral">&#39;out_node&#39;</span>]</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># outport_file_dict is in format:</span></div>
<div class="line">{port_name_1: file_path_1,</div>
<div class="line"> port_name_2: file_path_2</div>
<div class="line"> ...}</div>
</div><!-- fragment --><p>Then users can load these result files and do their own post-processing.</p>
<h2><a class="anchor" id="sec_accuarcy_FAQ"></a>
4.5 FAQ</h2>
<p>Answers for some common questions are as follows.</p>
<ol type="1">
<li>[Question] How does data prepare module convert images to binaries?<ul>
<li>[Answer] The default tool to convert images to binary files is OpenCV.</li>
</ul>
</li>
<li>[Question] How to narrow the accuracy gap between different inference modes?<ul>
<li>[Answer] For different inference modes, ensure that the same tool and environment are used to generate the RGB data. Using different tools or environments to convert the images to binaries may import data difference and cause issues in the binaries data, and ultimately skew the final result.</li>
</ul>
</li>
<li>[Question] Why did <code>cvflow</code> inference mode fail?<ul>
<li>[Answer] Users need to ensure that the network(telnet) connection between the EVK and the PC is valid. And as the CVflow forward inference is done by the <code>test_eazyai</code> application, user also can check if it is available from the EVK side.</li>
</ul>
</li>
<li>[Question] How to debug when some failure happens during inference?<ul>
<li>[Answer] Users can set the option <code>--loglevel</code> as 4 to check the debug log and see if the user can get any clue. If not, please contact Ambarella support.</li>
</ul>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_deploy_nn"></a>
5 Deployment</h1>
<p>Users can use <code>eazyai_inf</code> to do the network inference which will call CVflow chip with enternet port, and generate an app;ication command which can run on EVK for user's reference. As this application will call the EazyAI C library in chip, user can refer to this comand and to be familar with how to write own application wioth EazyAI library.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For more information, refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a>.</li>
<li>For more information of the deployment with Cooper SDK, please refer ro <a class="el" href="../../d7/d53/fs_deployment.html">Deployment</a>.</li>
</ol>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_sample_perf"></a>
6 Performance Table</h1>
<p>The following table lists the network performances used in the CNNGen samples package. The values below are only for <b>reference</b>, and are subject to changes with different toolchain versions. For an accurate value, test it with the CNNGen samples package. As the samples package does not provide all of the model files listed in the table below, users should find and download the relevant versions to CNNGen.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>As shown in the table below, CV2 = 4 x CV22 = 16 x CV28, CV5 = (0.6 ~ 0.8) x CV2, CV72 = (1.2 ~ 1.5) x CV2.</li>
<li>CV25 is the half of CV22 performance, as CV22 Vision clock is 1008 MHz, but CV25 is 504 MHz, they share the same VP hardware.</li>
<li>For CV28 pruning, no rested performance yet, but users can refer to other chips' pruning performance improvement as the ratio should be the same.</li>
<li>CVflow will benefit a lot from pruning, there will be large performance improvement.</li>
<li>Some networks will benefit little from pruning, such as efficientnet and mobilenet as they have less weights.</li>
<li>Although CVflow for CV5 is a little weaker than CV2, CV5 has much more strong Arm, DSP, and sufficient DRAM bandwidth. That mean Arm will help more with SVE support, and high resolution encoding will not affect CVflow performance as there are sufficient DRAM bandwidth.</li>
<li>As CV72 adds new hardware support, for some networks, such transformer networks, non-linear activation, and so on, it will have huge improvement, even 4x, 5x, or 10x.</li>
</ol>
</dd></dl>
<a class="anchor" id="CV22 Performance Table for DRAv2"></a>
<table class="doxtable">
<caption></caption>
<tr align="middle">
<th rowspan="2">CNN Models </th><th colspan="2">CV28 Performance (1x) (ms) </th><th colspan="2">CV22 Performance (~4x) (ms) </th><th colspan="2">CV2 Performance (~16x) (ms) </th><th colspan="2">CV5 Performance (~12.8x) (ms) </th><th colspan="2">CV72 Performance (~24x) (ms) </th></tr>
<tr align="middle">
<th>Full Model </th><th>20% Density </th><th>Full Model </th><th>20% Density </th><th>Full Model </th><th>20% Density </th><th>Full Model </th><th>20% Density </th><th>Full Model </th><th>20% Density </th></tr>
<tr align="middle">
<td>mobilefacenets_fix8_112_96 </td><td>8.3448 </td><td>6.87466 </td><td>2.0846 </td><td>N/A </td><td>0.5955 </td><td>N/A </td><td>0.4779 </td><td>N/A </td><td>0.306 </td><td>N/A </td></tr>
<tr align="middle">
<td>mobilenetv1_ssd_fix8_300_300 </td><td>27.8482 </td><td>13.9781 </td><td>6.9175 </td><td>3.1537 </td><td>1.9493 </td><td>0.9593 </td><td>2.3929 </td><td>1.2895 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>resnet50_fix8_224_224 </td><td>73.4317 </td><td>24.5195 </td><td>18.7301 </td><td>6.0244 </td><td>5.6007 </td><td>2.1567 </td><td>6.5914 </td><td>2.5625 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>vgg19_fix8_224_224 </td><td>336.095 </td><td>90.159 </td><td>88.0988 </td><td>22.3899 </td><td>23.5030 </td><td>7.0427 </td><td>28.971 </td><td>8.5444 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>vgg16_ssd_fix8_300_300 </td><td>506.294 </td><td>168.002 </td><td>122.4476 </td><td>35.4200 </td><td>23.7717 </td><td>7.1844 </td><td>34.761 </td><td>11.727 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>yolov3_fastest_fix8_320_320 </td><td>3.8925 </td><td>3.68744 </td><td>1.1967 </td><td>N/A </td><td>0.5232 </td><td>N/A </td><td>0.6751 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>yolov3_mnetv2_nano_fix8_320_320 </td><td>6.423 </td><td>4.39249 </td><td>1.7658 </td><td>N/A </td><td>0.6142 </td><td>N/A </td><td>0.8235 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>yolov3_fastest_xl_fix8_320_320 </td><td>9.0313 </td><td>7.22087 </td><td>2.5402 </td><td>2.1876 </td><td>0.9035 </td><td>N/A </td><td>1.1873 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>yolov3_mnetv2_lite_fix8_320_320 </td><td>20.5904 </td><td>12.6861 </td><td>5.0584 </td><td>3.1196 </td><td>1.4410 </td><td>N/A </td><td>1.9417 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>tf_yolo_v3_mixed_416_416 </td><td>717.187 </td><td>216.842 </td><td>180.2604 </td><td>47.9553 </td><td>32.6579 </td><td>9.9896 </td><td>47.395 </td><td>14.358 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>tf_deeplab_v3_fix8_513_513 </td><td>74.9494 </td><td>53.3049 </td><td>17.2360 </td><td>12.0236 </td><td>6.5434 </td><td>5.6261 </td><td>7.3219 </td><td>6.5522 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>bodypix_fix8_481_641 </td><td>106.684 </td><td>51.1286 </td><td>25.4898 </td><td>12.3964 </td><td>6.3852 </td><td>3.4461 </td><td>8.1349 </td><td>4.9407 </td><td>3.78 </td><td>1.954 </td></tr>
<tr align="middle">
<td>tf_inceptionv1_fix8_224_224 </td><td>26.9522 </td><td>7.74111 </td><td>6.8038 </td><td>1.9494 </td><td>1.7514 </td><td>0.7449 </td><td>2.2538 </td><td>0.8414 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>tf_inception_v3_fix8_299_299 </td><td>114.902 </td><td>38.673 </td><td>28.9170 </td><td>8.7074 </td><td>6.9894 </td><td>2.6025 </td><td>9.3727 </td><td>3.3619 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>tf_mobilenetv1_fix8_224_224 </td><td>12.6974 </td><td>6.33895 </td><td>3.2801 </td><td>1.6908 </td><td>1.2207 </td><td>0.6947 </td><td>1.4082 </td><td>0.9183 </td><td>0.785 </td><td>0.421 </td></tr>
<tr align="middle">
<td>tf_mobilenetv2_fix8_224_224 </td><td>8.6233 </td><td>5.54697 </td><td>2.4037 </td><td>1.6884 </td><td>1.0785 </td><td>0.7508 </td><td>1.3304 </td><td>0.9978 </td><td>0.679 </td><td>0.466 </td></tr>
<tr align="middle">
<td>tf_mobilenetv2_ssdlite_fix8_300_300 </td><td>23.1544 </td><td>15.4431 </td><td>6.0922 </td><td>4.0318 </td><td>2.3392 </td><td>1.7149 </td><td>3.0763 </td><td>2.2548 </td><td>1.331 </td><td>0.961 </td></tr>
<tr align="middle">
<td>tf_efficientdet_d0_fix8_512_512 </td><td>255.603 </td><td>281.293 </td><td>91.6419 </td><td>N/A </td><td>52.7723 </td><td>N/A </td><td>63.263 </td><td>N/A </td><td>7.961 </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_efficientnet_b0_fix8_224_224 </td><td>41.6688 </td><td>39.4835 </td><td>11.6885 </td><td>11.4160 </td><td>9.3859 </td><td>8.9508 </td><td>9.5599 </td><td>9.2246 </td><td>1.515 </td><td>1.181 </td></tr>
<tr align="middle">
<td>onnx_centernet_fix8_384_384 </td><td>267.942 </td><td>83.3991 </td><td>65.2046 </td><td>17.0723 </td><td>11.9234 </td><td>4.4619 </td><td>18.471 </td><td>5.7173 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_fgfd_fix8_240_320 </td><td>2.6721 </td><td>1.79946 </td><td>0.7421 </td><td>N/A </td><td>0.3235 </td><td>N/A </td><td>0.4339 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_retinaface_fix8_640_640 </td><td>18.5969 </td><td>8.13562 </td><td>4.9085 </td><td>2.4567 </td><td>1.1852 </td><td>N/A </td><td>1.6213 </td><td>N/A </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov3_fix8_416_416 </td><td>660.701 </td><td>172.863 </td><td>166.9117 </td><td>39.9476 </td><td>30.4711 </td><td>9.0585 </td><td>45.180 </td><td>12.521 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov3_spp_fix8_416_416 </td><td>663.897 </td><td>175.592 </td><td>167.5850 </td><td>40.2857 </td><td>31.0350 </td><td>9.2179 </td><td>45.184 </td><td>12.831 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov3_tiny_fix8_416_416 </td><td>52.8347 </td><td>14.2004 </td><td>13.2943 </td><td>3.3210 </td><td>2.7986 </td><td>1.0299 </td><td>3.8731 </td><td>1.3145 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov5s_fix8_416_416 </td><td>86.6313 </td><td>32.111 </td><td>21.9484 </td><td>7.4219 </td><td>5.4054 </td><td>3.4200 </td><td>7.1014 </td><td>4.3617 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov5m_fix8_416_416 </td><td>241.999 </td><td>78.3978 </td><td>60.7822 </td><td>17.7189 </td><td>12.7043 </td><td>6.6894 </td><td>17.424 </td><td>8.891 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov5l_fix8_416_416 </td><td>522.304 </td><td>166.883 </td><td>130.7734 </td><td>37.0621 </td><td>26.1736 </td><td>12.1659 </td><td>36.560 </td><td>16.274 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_yolov5x_fix8_416_416 </td><td>967.496 </td><td>297.953 </td><td>239.9791 </td><td>66.6298 </td><td>49.4302 </td><td>20.6322 </td><td>67.718 </td><td>26.99 </td><td>N/A </td><td>N/A </td></tr>
<tr align="middle">
<td>onnx_fairmot_fix8_160_288 </td><td>107.292 </td><td>32.8526 </td><td>26.8830 </td><td>7.0717 </td><td>6.1175 </td><td>2.0780 </td><td>8.489 </td><td>2.54379 </td><td>4.995 </td><td>1.607 </td></tr>
<tr align="middle">
<td>swin_tiny_mixed_224_224 </td><td>N/A </td><td>N/A </td><td>N/A </td><td>N/A </td><td>59.4 </td><td>N/A </td><td>N/A </td><td>N/A </td><td>14.315</td><td>N/A </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Update CV28 performance as it has big improvement for VAS refinement which will not affect accuracy.</li>
<li>Above performance is based on specific CNNGen toolchain versions, just for reference, please check the real performance with the CVflow CNNGen Samples pacakges as the performacne maybe improved in new CNNGen Toolchain.</li>
<li>More networks' performance will be added in future versions.</li>
</ol>
</dd></dl>
<h1><a class="anchor" id="sec_accuarcy_table"></a>
7 Accuracy Table</h1>
<p>The following table provides network accuracy results based on the CNNGen samples package. The values below are only for reference as the accuracy from different toolchain versions may vary.</p>
<p>The tests provided below only use a small number of images. For an accurate value, users should test with the samples package. As it does not provide all of the files listed in the table below, it is recommended to find and download the relevant files to CNNGen. </p><a class="anchor" id="Accuracy_Table_for_DRAv2"></a>
<table class="doxtable">
<caption></caption>
<tr align="middle">
<th>Framework </th><th>CNN Models </th><th>Method </th><th>EVK DRAv2 Accuracy </th><th>EVK DRAv3 Accuracy </th><th>PC Accuracy </th><th>Input Size </th><th>DRA Stategy </th><th>Quant </th><th>Prune </th><th>Image Numbers </th></tr>
<tr align="middle">
<td rowspan="14">Caffe </td><td rowspan="2">AlexNet </td><td>TOP1 </td><td>0.5537 </td><td>0.5491 </td><td>0.5530 </td><td rowspan="2">227x227 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td><td rowspan="11">ILSVRC_2012 1284 </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.7484 </td><td>0.7874 </td><td>0.7889  </td></tr>
<tr align="middle">
<td rowspan="2">GoogLeNet </td><td>TOP1 </td><td>0.6838 </td><td>0.6815 </td><td>0.6807 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.8497 </td><td>0.8754 </td><td>0.8793  </td></tr>
<tr align="middle">
<td>InsightFace </td><td>Cosine </td><td>0.9892 </td><td>0.9720 </td><td>0.9883 </td><td>112x112 </td><td>Default </td><td>No </td><td>No  </td></tr>
<tr align="middle">
<td rowspan="2">MobileNet V1 </td><td>TOP1 </td><td>0.6332 </td><td>0.6752 </td><td>0.6768 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.8115 </td><td>0.8512 </td><td>0.8800  </td></tr>
<tr align="middle">
<td rowspan="2">ResNet50 </td><td>TOP1 </td><td>0.6900 </td><td>0.6900 </td><td>0.6908 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.8653 </td><td>0.8964 </td><td>0.8956  </td></tr>
<tr align="middle">
<td rowspan="2">SqueezeNet V1 </td><td>TOP1 </td><td>0.5685 </td><td>0.5646 </td><td>0.5748 </td><td rowspan="2">227x227 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.7578 </td><td>0.7616 </td><td>0.7967  </td></tr>
<tr align="middle">
<td>VGG16 SSD </td><td>mAP </td><td>0.7764 </td><td>0.7764 </td><td>0.7797 </td><td>300x300 </td><td>Default </td><td>No </td><td>No </td><td>VOC_07 4952  </td></tr>
<tr align="middle">
<td>Yolov3 </td><td>mAP </td><td>0.5815 </td><td>0.5729 </td><td>0.5873 </td><td>416x416 </td><td>Fix16 </td><td>No </td><td>No </td><td rowspan="2">COCO_2014 2500  </td></tr>
<tr align="middle">
<td>Yolov3 </td><td>mAP </td><td>0.5235 </td><td>0.5244 </td><td>0.5283 </td><td>416x416 </td><td>Default </td><td>Fix8 Quant </td><td><p class="starttd">Sp50 </p>
<p class="endtd"></p>
</td></tr>
<tr align="middle">
<td rowspan="5">ONNX </td><td>Yolov5s </td><td>mAP </td><td>0.4625 </td><td>0.4759 </td><td>0.5084 </td><td>416x416 </td><td>Default </td><td>No </td><td>No </td><td>COCO_2014 2500  </td></tr>
<tr align="middle">
<td rowspan="2">Mobilenet V2 </td><td>TOP1 </td><td>0.6277 </td><td>0.6643 </td><td>0.6680 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td><td rowspan="2">ILSVRC_2012 1284  </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.8287 </td><td>0.8668 </td><td>0.8660  </td></tr>
<tr align="middle">
<td rowspan="2">Swin_tiny </td><td>TOP1 </td><td>0.7617 </td><td>N/A </td><td>0.7889 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td><td rowspan="2">ILSVRC_2012 1284  </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.9167 </td><td>N/A </td><td><p class="starttd">0.9439 </p>
<p class="endtd"></p>
</td></tr>
<tr align="middle">
<td rowspan="3">Tensorflow </td><td>Deeplab V3 </td><td>mIOU </td><td>0.6832 </td><td>0.7009 </td><td>0.7011 </td><td>513x513 </td><td>Default </td><td>No </td><td>No </td><td>VOC 07  </td></tr>
<tr align="middle">
<td rowspan="2">Inception V1 </td><td>TOP1 </td><td>0.6636 </td><td>0.6729 </td><td>0.6706 </td><td rowspan="2">224x224 </td><td rowspan="2">Default </td><td rowspan="2">No </td><td rowspan="2">No </td><td rowspan="2">ILSVRC_2012 1284  </td></tr>
<tr align="middle">
<td>TOP5 </td><td>0.8419 </td><td>0.8614 </td><td>0.8645  </td></tr>
</table>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
