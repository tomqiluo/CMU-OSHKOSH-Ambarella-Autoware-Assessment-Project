<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: EazyAI Framework</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('dd/de5/fs_eazyai.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">EazyAI Framework </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>EazyAI, which means Easy AI, is a complete framework for the debugging phase and the final product development phase.</p>
<div class="image">
<img src="../../eazyai_framework.jpg" alt=""/>
<div class="caption">
Figure 0. EazyAI Framework.</div></div>
<dl class="section note"><dt>Note</dt><dd>Features in dashed boxes have not been supported yet, may be added in future SDK.</dd></dl>
<p>This framework has included lots of useful modules, the only purpose is to make users easier.</p>
<ol type="1">
<li><b>EazyAI CVflow Lite Python Library - CFlite</b>, it provides a series of python APIs to help users to convert and deploy a network on Ambarella chips easily.<ol type="a">
<li>It will call CNNGen toolchain <a class="el" href="../../d2/d67/fs_cnngen.html">CNNGen Toolkits</a> for network convert.</li>
<li>It will call below <b>Module 3 ~ 7</b> for network inference.</li>
<li>Please find its source code in <code>cvflow_cnngen_samples/library/cflite</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cflite">1 EazyAI CVflow Lite Python Library</a> and <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html">CVflow Lite API</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI Python Tools</b>, it provides a series of tools which are based on the APIs in "EazyAI CVflow Lite Python Package" to help users to convert and deploy a network on Ambarella chips easily.<ol type="a">
<li>Please find its source code in <code>cvflow_cnngen_samples/library/cflite/eazyaitools/</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI Video Shell Script</b>, it can help users to setup DSP video pipeline in different chips easily, it is only used in debug stage. Users should use IAV to their own video application in final product developing stage.<ol type="a">
<li>Please find its source code in <code>cooper_linux_sdk/ambarella/packages/eazyai/unit_test/eazyai_video</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_vi_sh">3 Eazyai Video</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI MP Level Inference C Library</b>, it provided the C APIs that helps users to write their final CVflow inference application or Simulator inference application.<ol type="a">
<li>Please find its source code in <code>cvflow_cnngen_samples/library/eazyai</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html">EazyAI Library API</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI NN Postprocess C Library</b>, it provides lots of network post process sample code, such as NMS, bounding boxes decode, and so on.<ol type="a">
<li>Please find its source code in <code>cvflow_cnngen_samples/library/eazyai/unit_test/nn_arm_task</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_arm_postprocess">4 EazyAI Arm Postprocess</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI Unit Test</b>, it provides an unify application which can run network on PC Simulator and CVflow chip.<ol type="a">
<li>Please find its source code in <code>cvflow_cnngen_samples/library/unit_test/test_eazyai.c</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a>, <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_cvflow">5 EazyAI Unit Test</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_simulator">6 EazyAI Simulator Unit Test</a> for detail.</li>
</ol>
</li>
<li><b>EazyAI Live Applications</b>, it provided lots of live network demos which only can work on CVflow chips.<ol type="a">
<li>Please find its source code in <code>cooper_linux_sdk/ambarella/packages/eazyai/apps</code>.</li>
<li>Please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a>, <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html">Caffe Demos</a>, <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html">Tensorflow Demos</a>, and <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html">ONNX Demos</a> for detail.</li>
</ol>
</li>
</ol>
<p>For <b>debug stage</b>, users may need to convert a network quickly and do some quick test, then the tools in <b>Module 2</b> can cover all what they need. For specific convert flow, user also can write their own convert application with the APIs in <b>Module 1</b>.</p>
<p>For <b>final product developing stage</b>, user can use the APIs in <b>Module 4</b> to write their CVflow applications, and they can refer to the sample code in <b>Module 5 ~ 7</b>. As in debug stage, it also uses <b>Module 4 ~ 7</b> to do the network inference, it will be very easy for users to switch between debug stage and final product developing stage.</p>
<hr  />
<h1><a class="anchor" id="sec_eazyai_cflite"></a>
1 EazyAI CVflow Lite Python Library</h1>
<p>With previous solution of <a class="el" href="../../da/dc5/fs_quick_start.html#sec_quick_start">5 Quick Shell Script</a> and sec_cnngen_sample_network , users needs to convert the network in build server, and copy related files to EVK board, then to run the inference. The two old convert flow methods will be deleted in future CNNGen Samples Package which will be replaced by <b>CFlite</b>.</p>
<p>With <b>CFlite</b>, everything is different, users can use python to convert and run CNN on build server directly, users do not need to know how to operate the EVK board as EVK which will always be an external device of PC, it is little similar with developing on a chip which support native compilation.</p>
<p>That means users can do everything in build server, also users can share most of the parameters between convert stage and inference stage.</p>
<p>For detail, please refer to <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html">CVflow Lite API</a> as below.</p>
<ul>
<li><a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_history">0 Revision History</a></li>
<li><a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_introduction">1 Introduction</a></li>
<li><a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_modules">2 CFlite Modules</a></li>
<li><a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_tools">3 CFlite Python Tools</a></li>
<li><a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_api">4 CFlite Python API</a></li>
</ul>
<hr  />
<h1><a class="anchor" id="sec_eazyai_python_tools"></a>
2 EazyAI Python Tools</h1>
<p>This section provides information on the EazyAI Python Tools, below is the full flow to use these tools.</p>
<div class="image">
<img src="../../eazyai_python_tools.jpg" alt=""/>
<div class="caption">
Figure 2. EazyAI Python Tools.</div></div>
<p>And below is the detailed tool list.</p>
<ol type="1">
<li><code>eazyai_helper</code>, which shows supported tools and utils in CFLite.</li>
<li><code>eazyai_cfg</code>, which helps users easily generate the network convert configuration files.</li>
<li><code>eazyai_cvt</code>, which is used for users to convert the network with the network convert configucation files generated by <code>eazyai_cfg.py</code>.</li>
<li><code>eazyai_inf</code>, which is used for network inference.<ol type="a">
<li>CVflow Chip</li>
<li>Ades or Acinference Simulator</li>
<li>Original Framework</li>
</ol>
</li>
<li><code>eazyai_inf_simple_dummy</code>, which is for quick performance inference test on CVflow chip.</li>
<li><code>eazyai_inf_simple_live</code>, which can enable lots of live network demos on CVflow chip.</li>
<li><code>eazyai_video</code>, which is used to setup DSP video pipeline for different chips.</li>
<li><code>eazyai_profiler</code>, which is used to generate a spreadsheet to describe CVflow hardware efficiency for current network.</li>
</ol>
<p>These tools will be install when installing CNNGen toolchain in <a class="el" href="../../da/dc5/fs_quick_start.html#sub_qs_pre_install">1.3 Installation</a>, users can use it as <code>eazyai_cvt</code>, <code>eazyai_inf</code>, and so on directly.</p>
<p>Also they can find them in <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>, for these source code level python applications, users can use it as <code>python3 eazyai_cvt.py</code>, <code>python3 eazyai_inf.py</code>, and so on.</p>
<p>Actually, they are the same.</p>
<h2><a class="anchor" id="subsec_eazyai_python_tools_helper"></a>
2.1 EazyAI Helper Tool</h2>
<p><code>eazyai_helper</code> will help users understand the tools and utils supported by cflite. For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_helper -h</div>
</div><!-- fragment --><p>The following is the terminal information generated by executing the default command to show workflow. </p><div class="fragment"><div class="line">build $ eazyai_helper</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Download Models, DRA Images, Dataset, and Prebuild networks:</span></div>
<div class="line">  -&gt; https:<span class="comment">//amba.egnyte.com/fl/pnAckpTfWI with password [Amba_2023]</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># CFLite Main Work Flow:</span></div>
<div class="line"> ===========================================================================================================================================</div>
<div class="line">    1. Convert Public To Ambarella  ==&gt;  2. Inference                    ==&gt;  3. Profiling        ==&gt;  4. Product Deployment</div>
<div class="line"> ===========================================================================================================================================</div>
<div class="line">    1) eazayi_cfg                     |  1) eazyai_inf_simple_dummy        |  eazyai_profiler       |  eazyai_inf_simple_live -gs</div>
<div class="line">    2) eazayi_cvt                     |  2) eazyai_inf                     |    <span class="keywordflow">for</span> the performance |    or eazyai_inf -gs</div>
<div class="line">                                      |  3) eazyai_inf_simple_live         |    issues              |    to get `test_eazyai` command</div>
<div class="line">                                      |  4) eazyai_video                   |                        |    and refer to its code</div>
<div class="line"> ===========================================================================================================================================</div>
<div class="line"> Note: For the inference application and profiler which will connect real chips, please <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> below command to initialize the chips first.</div>
<div class="line">           build $ eazyai_video -ip 10.0.0.2</div>
<div class="line"> </div>
<div class="line"> Usage: (Please refer to [Quick Start -&gt; EazyAI CFlite Python Tools] of CNNGen Samples prebuild documents)</div>
<div class="line"> </div>
<div class="line">     1. Quick Performance Evaluation</div>
<div class="line">        build $ eazyai_cvt -mp model_file --dummy</div>
<div class="line">        build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb cavalry_model.bin -t 100 (Loop 100 times to get the average inference time)</div>
<div class="line"> </div>
<div class="line">     2. Basic Accuracy Evaluation</div>
<div class="line">        1) Prepare yaml file <span class="keywordflow">for</span> convert</div>
<div class="line">           build $ eazyai_cfg -mp model_file                (Generate ea_cvt_cfg.yaml <span class="keyword">using</span> the user interface)</div>
<div class="line">           build $ eazyai_cfg -mp model_file -fs            (Generate templates and modify them manually)</div>
<div class="line">        2) Convert with or without layer_comapre</div>
<div class="line">           build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac       (With Layer_compare)</div>
<div class="line">           build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac -lc   (Without Layer_compare)</div>
<div class="line">        3) Inference with or without postprocess</div>
<div class="line">           build $ eazyai_inf -cy cflite_cvt_summary.yaml --platform ades                 (Without postprocess and generate binary only)</div>
<div class="line">           build $ eazyai_inf -cy cflite_cvt_summary.yaml -iy ea_inf.yaml --platform ades (Without postprocess and generate <span class="keyword">final</span> result)</div>
<div class="line">           Note: Use `--platform ades / acinf / cvflow / orig` to <span class="keywordflow">switch</span> the platform, and compare its result</div>
<div class="line">                 For `ea_inf.yaml`, pelase refer to the exmaples in the config folder of every network, such as below</div>
<div class="line">                   [tensorflow/demo_networks/deeplab_v3/config/ea_inf_tf_deeplab_v3.yaml] in CNNGen Samples Package</div>
<div class="line">                 For `--platform cvflow`, please <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> below command first</div>
<div class="line">                   build $ eazyai_video -ip 10.0.0.2</div>
<div class="line"> </div>
<div class="line">     3. Full Accuracy Evaluation</div>
<div class="line">        build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac (Convert and generate cflite_cvt_summary.yaml)</div>
<div class="line">        build $ eazyai_video -ip 10.0.0.2</div>
<div class="line">        build $ eazyai_inf -ip 10.0.0.2 --cy cflite_cvt_summary.yaml -iy ea_acc_inf.yaml --platform cvflow</div>
<div class="line">        Note: For `ea_acc_inf.yaml`, pelase refer to the exmaples in the config folder of every network, such as below</div>
<div class="line">                [tensorflow/demo_networks/deeplab_v3/config/ea_inf_acc_tf_deeplab_v3.yaml] in CNNGen Samples Package</div>
<div class="line"> </div>
<div class="line">     4. CVflow Efficiency Evaluation</div>
<div class="line">        build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac (Convert and generate cflite_cvt_summary.yaml)</div>
<div class="line">        build $ eazyai_video -ip 10.0.0.2</div>
<div class="line">        build $ eazyai_profiler -cy cflite_cvt_summary.yaml -op &lt;profiler result output path&gt;</div>
<div class="line"> </div>
<div class="line">     5. Product Deployment</div>
<div class="line">        1) Get the test_eazyai full command from eazyai_inf or eazyai_inf_simple_live.</div>
<div class="line">           build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">           build $ eazyai_inf -cy cflite_cvt_summary.yaml -iy ea_inf.yaml --platform cvflow -gs</div>
<div class="line">           build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 -cb network_cavalry.bin -pn network_name -pl network.lua -dm 1 -dd STREAM1 -gs</div>
<div class="line">        2) Follow the full command and sourece code of test_eazyai to write users<span class="stringliteral">&#39; own application.</span></div>
<div class="line"><span class="stringliteral">           Please refer to [EazyAI Framework -&gt; 4 EazyAI Inference C Library] for detail.</span></div>
</div><!-- fragment --><p>The following is the terminal information generated by executing the command to show the tool list. </p><div class="fragment"><div class="line">build $ eazyai_helper --show_tools</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># CFLite Convert and Inference Tools:</span></div>
<div class="line"> </div>
<div class="line">        1. [eazyai_helper]</div>
<div class="line">           - Show supported tools and utils in CFLite.</div>
<div class="line">        2. [eazyai_cfg]</div>
<div class="line">           - Create the configuration file of network <span class="keywordflow">for</span> eazyai_cvt.</div>
<div class="line">        3. [eazyai_cvt]</div>
<div class="line">           - Convert the network to Ambarella format, and generate summary file <span class="keywordflow">for</span> easy inference with eazyai_inf.</div>
<div class="line">        4. [eazyai_video]</div>
<div class="line">           - Enable CVflow and set up the video pipeline on EVK <span class="keywordflow">for</span> EazyAI inference applications.</div>
<div class="line">        5. [eazyai_inf_simple_dummy]</div>
<div class="line">           - Run inference with dummy input <span class="keywordflow">for</span> quick performance test.</div>
<div class="line">        6. [eazyai_inf]</div>
<div class="line">           - Run network forward inference with different platforms.</div>
<div class="line">        7. [eazyai_inf_simple_live]</div>
<div class="line">           - Run live demos on EVK, which needs to use eazyai_video to set video pipeline first.</div>
<div class="line">        8. [eazyai_profiler]</div>
<div class="line">           - This tool will generate a spreadsheet to describe CVflow hardware efficiency <span class="keywordflow">for</span> current network.</div>
<div class="line"> </div>
<div class="line"># CFLite Utils:</div>
<div class="line"> </div>
<div class="line">        1. [tf_remove_node]</div>
<div class="line">           - Remove specified nodes from Tensorflow models.</div>
<div class="line">        2. [im_reader]</div>
<div class="line">           - Read RGB file to image.</div>
<div class="line">        3. [simple_bincmp]</div>
<div class="line">           - Compare automatically binary files generated by ades and cavalry.</div>
<div class="line">        4. [mtcnn_get_pnet_scales]</div>
<div class="line">           - Calculate mtcnn pnet scales by original image resolution, minimum face size on the image and</div>
<div class="line">             a scaling factor.</div>
<div class="line">        5. [padding_append]</div>
<div class="line">           - Add padding to non-padding binary.</div>
<div class="line">        6. [convert_jpg2nv12]</div>
<div class="line">           - Convert the image dataformat from JPG to NV12.</div>
<div class="line">        7. [im_quant]</div>
<div class="line">           - Do image quant.</div>
<div class="line">        8. [binrotate]</div>
<div class="line">           - Rotate binary file with specific degree</div>
<div class="line">        9. [compare_classification_network_result]</div>
<div class="line">           - Compare and draw the classification network result.</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_python_tools_cfg"></a>
2.2 EazyAI Configuration Tool</h2>
<p><code>eazyai_cfg</code> will help users to generate a right network configuration quickly and correctly. For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_cfg --help</div>
</div><!-- fragment --><ol type="1">
<li>Guide Mode <div class="fragment"><div class="line">build $ eazyai_cfg -mp model_file</div>
<div class="line">Main Menu:</div>
<div class="line">0. Set basic_sec</div>
<div class="line">1. Set data_prepare_sec</div>
<div class="line">2. Set input_nodes_sec</div>
<div class="line">3. Set output_nodes_sec</div>
<div class="line">4. Set advanced options</div>
<div class="line">5. Show all information of Main Menu</div>
<div class="line">6. Quit</div>
</div><!-- fragment --> The applition will guide users to fill network parameters one by one, users can get two full Yaml configuration files easily which will be feed to <code>eazyai_cvt.py</code> for convert directly. a. "network_name__basic.yaml", which includes the basic convert parameters. b. "network_name_full.yaml", which will more special parameters for further tuning that are rarely used for most of the case.</li>
<li>Fast Mode <div class="fragment"><div class="line">build $ eazyai_cfg -mp model_file --fast</div>
</div><!-- fragment --> With this, users get two reference Yaml configuration files quickly, the same files with above boot mode, but in fast mode, the configuration file cannot be used directly as it lacks the right network information, so users need to modify manually.</li>
</ol>
<h3><a class="anchor" id="subsubsec_eazyai_python_tools_cfg_basic"></a>
2.2.1 Basic Convert Parameters</h3>
<div class="fragment"><div class="line"><span class="preprocessor"># Notes:</span></div>
<div class="line"><span class="preprocessor">#   Users use relative | abs path for custom path setting</span></div>
<div class="line"><span class="preprocessor">#   All below custom data formats should follow the definition of CVflow</span></div>
<div class="line"><span class="preprocessor">#   There are three types of parameters, users can pay more attention to the &quot;Essential&quot; parameters.</span></div>
<div class="line"><span class="preprocessor">#   - Essential parameters []!, users must set it, or else convert may fail or accuracy may have problems</span></div>
<div class="line"><span class="preprocessor">#   - Optional parameters []+, users can set it if necessary</span></div>
<div class="line"><span class="preprocessor">#   - Rare parameters []~, users can use it for some special cases</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Basic Information Setting</span></div>
<div class="line"><span class="preprocessor">#   [name]!:        Set the task name, which is used to distinguish different output for different tasks</span></div>
<div class="line"><span class="preprocessor">#   [work_dir]+:    Set the work directory, default is None which means using the current folder</span></div>
<div class="line"><span class="preprocessor">#   [model_path]!:  Set the model path</span></div>
<div class="line"><span class="preprocessor">#   [caffe_prototxt_path]!:  Set the prototxt file path, for Caffe only</span></div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: task_name</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>:</div>
<div class="line">  /user_path/model_file</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Input Binary Setting</span></div>
<div class="line"><span class="preprocessor">#   Prepare input binaries, which can be used for DRA, Inference Test, and so on.</span></div>
<div class="line"><span class="preprocessor">#   [in_path]!:          Set the input path, can be a folder or a text list file</span></div>
<div class="line"><span class="preprocessor">#   [in_file_ext]!:      Set the file extension in &quot;in_path&quot;, such as bin, jpg, png, etc.</span></div>
<div class="line"><span class="preprocessor">#                        This param becomes &quot;Invalid&quot; when &quot;in_path&quot; param is a text list file</span></div>
<div class="line"><span class="preprocessor">#   [out_shape]!:        Set the shape of the binary data file. It should be the same as the NN input node.</span></div>
<div class="line"><span class="preprocessor">#                        If it&#39;s different to the NN input node, a input node reshaping will be triggered.</span></div>
<div class="line"><span class="preprocessor">#   [out_data_format]!:  Set the binary data format. It should be the same with NN input node.</span></div>
<div class="line"><span class="preprocessor">#                        Available choices are uint8(default), float16, float32, or custom.</span></div>
<div class="line"><span class="preprocessor">#   [transforms]!:       Set the python functions for the binary preparation</span></div>
<div class="line"><span class="preprocessor">#                        - Its params are different for different functions</span></div>
<div class="line"><span class="preprocessor">#                        - Its output params are defined in [out_shape] and [out_data_format]</span></div>
<div class="line"><span class="preprocessor">#                        - Use the help option &#39;eazyai_cfg --trans_list&#39; to get the full supported list</span></div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path:</div>
<div class="line">    in_file_ext:</div>
<div class="line">    out_shape:</div>
<div class="line">    out_data_format:</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color:</div>
<div class="line">          mean:</div>
<div class="line">          std:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Input Node Setting</span></div>
<div class="line"><span class="preprocessor">#   The CVflow pre-processing sequence:</span></div>
<div class="line"><span class="preprocessor">#     transpose -&gt; reshape (if the &quot;out_shape&quot; in &quot;data_prepare&quot; cannot match the NN real input) -&gt; (x - mean) / std</span></div>
<div class="line"><span class="preprocessor">#</span></div>
<div class="line"><span class="preprocessor">#   [data_prepare]!: Binding to specific input data prepare node in &quot;data_prepare&quot;</span></div>
<div class="line"><span class="preprocessor">#   [mean]!:         Set the input mean value if the NN input has normalization</span></div>
<div class="line"><span class="preprocessor">#   [std]!:          Set the standard deviation if the NN input has normalization</span></div>
<div class="line"><span class="preprocessor">#   [transpose]+:    Set the input transpose</span></div>
<div class="line"><span class="preprocessor">#   [pp_json]~:      Set the pre/post-processing json file</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:       Set a custom input node name, default is None to use the default naming rules</span></div>
<div class="line"><span class="preprocessor">#                    Only &quot;Valid&quot; when CVflowbackend is used</span></div>
<div class="line">input_nodes:</div>
<div class="line">  input_name1:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line">    mean:</div>
<div class="line">    std:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Output Node Setting</span></div>
<div class="line"><span class="preprocessor">#   The CVFlow post-processing sequence:</span></div>
<div class="line"><span class="preprocessor">#     transpose -&gt; reshape(if [shape] cannot match the transposed output) -&gt; data format converting</span></div>
<div class="line"><span class="preprocessor">#</span></div>
<div class="line"><span class="preprocessor">#   [data_format]!:  Set the output data format</span></div>
<div class="line"><span class="preprocessor">#   [transpose]+:    Set the output Transpose</span></div>
<div class="line"><span class="preprocessor">#   [shape]+:        Set the final output shape. An output node reshaping will be triggered if it&#39;s different to the real NN output shape</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:       Set a custom output node name, default is None to use the default naming rules</span></div>
<div class="line"><span class="preprocessor">#                    Only &quot;Valid&quot; when CVflowbackend is used</span></div>
<div class="line">output_nodes:</div>
<div class="line">  <span class="stringliteral">&#39;output_name1&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
<div class="line">  <span class="stringliteral">&#39;output_name2&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>a. As above files is generated by fast mode, so it lack lots of necessary information, that users need to fill them before using it to convert the network. b. <code>data_prepare: dra_data_1</code> means this network input will use "dra_data_1" to generate the DRA image binaries and list. c. <code>transforms</code> is the method to do data precoess, please run below command to check the support list. <div class="fragment"><div class="line">build $ eazyai_cfg --trans_list</div>
</div><!-- fragment --></dd></dl>
<h3><a class="anchor" id="subsubsec_eazyai_python_tools_cfg_adv"></a>
2.2.2 Full Convert Parameters</h3>
<div class="fragment"><div class="line"><span class="preprocessor"># Notes:</span></div>
<div class="line"><span class="preprocessor">#   Users use relative | abs path for custom path setting</span></div>
<div class="line"><span class="preprocessor">#   All below custom data formats should follow the definition of CVflow</span></div>
<div class="line"><span class="preprocessor">#   There are three types of parameters, users can pay more attention to the &quot;Essential&quot; parameters.</span></div>
<div class="line"><span class="preprocessor">#   - Essential parameters []!, users must set it, or else convert may fail or accuracy may have problems</span></div>
<div class="line"><span class="preprocessor">#   - Optional parameters []+, users can set it if necessary</span></div>
<div class="line"><span class="preprocessor">#   - Rare parameters []~, users can use it for some special cases</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Basic Information Setting</span></div>
<div class="line"><span class="preprocessor">#   [name]!:        Set the task name, which is used to distinguish different output for different tasks</span></div>
<div class="line"><span class="preprocessor">#   [work_dir]+:    Set the work directory, default is None which means using the current folder</span></div>
<div class="line"><span class="preprocessor">#   [model_path]!:  Set the model path</span></div>
<div class="line"><span class="preprocessor">#   [caffe_prototxt_path]!:  Set the prototxt file path, for Caffe only</span></div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: task_name</div>
<div class="line"><a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a>:</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>:</div>
<div class="line">  /user_path/model_file</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Graph Surgery Setting</span></div>
<div class="line"><span class="preprocessor">#   Optional Step, make the original model adapt to the CVFlow operators.</span></div>
<div class="line"><span class="preprocessor">#   [transforms]+:    Apply comma-separated string denoting the transforms sequence, default is None</span></div>
<div class="line"><span class="preprocessor">#   [replace_json]~:  Replace the subgraph which CVFlow does not support or is not effective.</span></div>
<div class="line"><span class="preprocessor">#                     For example: replace_json: ReplaceSubgraph=path/to/config.json</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:        Set a new model name after the Graph Surgery, default is None to use the default naming rules</span></div>
<div class="line">graph_surgery:</div>
<div class="line">  <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">  replace_json:</div>
<div class="line">  rename:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Pruning Setting</span></div>
<div class="line"><span class="preprocessor">#   Optional Step, only for performance rough test, which will break accuracy as the model is not retrained</span></div>
<div class="line"><span class="preprocessor">#   [density]+:  Set the target density ratio</span></div>
<div class="line"><span class="preprocessor">#   [method]~:   NOT supported yet, to be supported in future</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:   Set a new model name after the pruning, default is None to use the default naming rules</span></div>
<div class="line">prune:</div>
<div class="line">  density:</div>
<div class="line">  method:</div>
<div class="line">  rename:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Input Binary Setting</span></div>
<div class="line"><span class="preprocessor">#   Prepare input binaries, which can be used for DRA, Inference Test, and so on.</span></div>
<div class="line"><span class="preprocessor">#   [in_path]!:          Set the input path, can be a folder or a text list file</span></div>
<div class="line"><span class="preprocessor">#   [in_file_ext]!:      Set the file extension in &quot;in_path&quot;, such as bin, jpg, png, etc.</span></div>
<div class="line"><span class="preprocessor">#                        This param becomes &quot;Invalid&quot; when &quot;in_path&quot; param is a text list file</span></div>
<div class="line"><span class="preprocessor">#   [out_shape]!:        Set the shape of the binary data file. It should be the same as the NN input node.</span></div>
<div class="line"><span class="preprocessor">#                        If it&#39;s different to the NN input node, a input node reshaping will be triggered.</span></div>
<div class="line"><span class="preprocessor">#   [out_data_format]!:  Set the binary data format. It should be the same with NN input node.</span></div>
<div class="line"><span class="preprocessor">#                        Available choices are uint8(default), float16, float32, or custom.</span></div>
<div class="line"><span class="preprocessor">#   [transforms]!:       Set the python functions for the binary preparation</span></div>
<div class="line"><span class="preprocessor">#                        - Its params are different for different functions</span></div>
<div class="line"><span class="preprocessor">#                        - Its output params are defined in [out_shape] and [out_data_format]</span></div>
<div class="line"><span class="preprocessor">#                        - Use the help option &#39;eazyai_cfg --trans_list&#39; to get the full supported list</span></div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path:</div>
<div class="line">    in_file_ext:</div>
<div class="line">    out_shape:</div>
<div class="line">    out_data_format:</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color:</div>
<div class="line">          mean:</div>
<div class="line">          std:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Input Node Setting</span></div>
<div class="line"><span class="preprocessor">#   The CVflow pre-processing sequence:</span></div>
<div class="line"><span class="preprocessor">#     transpose -&gt; reshape (if the &quot;out_shape&quot; in &quot;data_prepare&quot; cannot match the NN real input) -&gt; (x - mean) / std</span></div>
<div class="line"><span class="preprocessor">#</span></div>
<div class="line"><span class="preprocessor">#   [data_prepare]!: Binding to specific input data prepare node in &quot;data_prepare&quot;</span></div>
<div class="line"><span class="preprocessor">#   [mean]!:         Set the input mean value if the NN input has normalization</span></div>
<div class="line"><span class="preprocessor">#   [std]!:          Set the standard deviation if the NN input has normalization</span></div>
<div class="line"><span class="preprocessor">#   [transpose]+:    Set the input transpose</span></div>
<div class="line"><span class="preprocessor">#   [pp_json]~:      Set the pre/post-processing json file</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:       Set a custom input node name, default is None to use the default naming rules</span></div>
<div class="line"><span class="preprocessor">#                    Only &quot;Valid&quot; when CVflowbackend is used</span></div>
<div class="line">input_nodes:</div>
<div class="line">  input_name1:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line">    mean:</div>
<div class="line">    std:</div>
<div class="line">    transpose:</div>
<div class="line">    pp_json:</div>
<div class="line">    rename:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Output Node Setting</span></div>
<div class="line"><span class="preprocessor">#   The CVFlow post-processing sequence:</span></div>
<div class="line"><span class="preprocessor">#     transpose -&gt; reshape(if [shape] cannot match the transposed output) -&gt; data format converting</span></div>
<div class="line"><span class="preprocessor">#</span></div>
<div class="line"><span class="preprocessor">#   [data_format]!:  Set the output data format</span></div>
<div class="line"><span class="preprocessor">#   [transpose]+:    Set the output Transpose</span></div>
<div class="line"><span class="preprocessor">#   [shape]+:        Set the final output shape. An output node reshaping will be triggered if it&#39;s different to the real NN output shape</span></div>
<div class="line"><span class="preprocessor">#   [rename]~:       Set a custom output node name, default is None to use the default naming rules</span></div>
<div class="line"><span class="preprocessor">#                    Only &quot;Valid&quot; when CVflowbackend is used</span></div>
<div class="line">output_nodes:</div>
<div class="line">  <span class="stringliteral">&#39;output_name1&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose:</div>
<div class="line">    shape:</div>
<div class="line">    rename:</div>
<div class="line">  <span class="stringliteral">&#39;output_name2&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
<div class="line">    transpose:</div>
<div class="line">    shape:</div>
<div class="line">    rename:</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># CVflow CNNGen Convert Setting</span></div>
<div class="line"><span class="preprocessor">#   [dra_option]+:    DRA configuration to balance the speed and the accuracy while the default setting can&#39;t meet the requirement</span></div>
<div class="line"><span class="preprocessor">#                     - [version]+:   Set the DRA method version which enables [dra_vn], &quot;n&quot; is &quot;2~3&quot;, 3 is stronger but slower in convertion</span></div>
<div class="line"><span class="preprocessor">#                     - [device]+:    Set the GPU device id to speed up the DRA process, only support CUDA 11.4</span></div>
<div class="line"><span class="preprocessor">#                                     default is &#39;-2&#39; for CPU, &#39;-1&quot; for choosing GPU automatically, &quot;0 ~ n&quot; to specify the GPU device id to use</span></div>
<div class="line"><span class="preprocessor">#                     - [strategy]+:  Set DRA main strategy, such as fx8(default), fx16, fp16(Only for CVflowv3), auto.</span></div>
<div class="line"><span class="preprocessor">#                                     &quot;fx8&quot; means fastest, &quot;fx16&quot; for CVflowv2 and &quot;fp16&quot; for CVflowv3 means best accuracy,</span></div>
<div class="line"><span class="preprocessor">#                                     &quot;auto&quot; mode balances between speed and accuracy</span></div>
<div class="line"><span class="preprocessor">#                     - [dra_v2]+:    DRA tuning parameters for DRA version #2</span></div>
<div class="line"><span class="preprocessor">#                                     [coverage_th]+:  Range &quot;0.9 ~ 0.99x&quot;. which controls data range coverage threshold, larger value gets better accuracy</span></div>
<div class="line"><span class="preprocessor">#                                     [coverage_recover_th]~:  Control coverage recovery threshold for 8 bit fixed point, use 0.5 for better accuracy</span></div>
<div class="line"><span class="preprocessor">#                                     [allow_scaling]~:   True | False. Control activation scaling to allow more 8 bit fixed point</span></div>
<div class="line"><span class="preprocessor">#                                     [allow_focused_range]~:  True | False. Control focused range to allow values to be clamped due to data formats</span></div>
<div class="line"><span class="preprocessor">#                     - [dra_v3]+:    DRA tuning parameters for DRA version #3</span></div>
<div class="line"><span class="preprocessor">#                                     DRA_v3 is based on the cumulative quantization. It&#39;s suggested to enable GPU for the heavy computation</span></div>
<div class="line"><span class="preprocessor">#                                     [slope_psnr_end]+: Range &quot;20dB ~ 100dB. Larger PSNR gets better accuracy, tune up it with an increment of 5dB</span></div>
<div class="line"><span class="preprocessor">#                                     [slope_pcc_end]~:  Range &quot;0.9 ~ 0.99x&quot;, Larger PCC gets better accuracy, which is more sensitive for some cases</span></div>
<div class="line"><span class="preprocessor">#                                     [allow_equalization]~:  True | False. Control activation accuracy optimization between cross-channel and depth-wise conv</span></div>
<div class="line"><span class="preprocessor">#   [sideband]~:      Control the quantization data format manually. In this mode, the main function of DRA algorithm is disabled.</span></div>
<div class="line"><span class="preprocessor">#                     - Layer Level:      Use the &#39;layer_control.py&#39; to generate a template, and modify it manually</span></div>
<div class="line"><span class="preprocessor">#                     - Primitive Level:  Modify the &#39;network_sb.json&#39; which is generated by the DRA algorithm, then to feed it back to the parser</span></div>
<div class="line"><span class="preprocessor">#   [vas_option]~:    Set some extra params for VAS compiler in some special cases</span></div>
<div class="line"><span class="preprocessor">#   [cavalry_option]+: Specify the cavalry_gen version, etc.</span></div>
<div class="line"><span class="preprocessor">#                      - [version]+:     Set the cavalry_gen version to match with the SDK version, default is None which uses the latest version</span></div>
<div class="line"><span class="preprocessor">#                      - [user_data]~:   Dump some custom data to the cavalry binary, such as time, parser version, etc.</span></div>
<div class="line"><span class="preprocessor">#                      - [encryption]~:  Set a symmetric encryption key to encrypt the cavalry binary</span></div>
<div class="line"><span class="preprocessor">#   [custom_node]~:   Set for some private operators that CNNGen toolchain cannot support</span></div>
<div class="line"><span class="preprocessor">#                     - [parser]~:  Set the custom node parser which is based on python script</span></div>
<div class="line"><span class="preprocessor">#                     - [dll]~:     Set the custom node dynamic link library &#39;.so&#39;</span></div>
<div class="line"><span class="preprocessor">#   [extra_option]~:  Set some debug options, such as &quot;-d&quot;, &quot;-dinf cerr&quot;, &quot;-ar&quot;, and so on.</span></div>
<div class="line">cnngen_convert:</div>
<div class="line">  dra_option:</div>
<div class="line">    version: 2</div>
<div class="line">    device: -2</div>
<div class="line">    strategy: fx8</div>
<div class="line">    dra_v2:</div>
<div class="line">      coverage_th:</div>
<div class="line">      coverage_recover_th:</div>
<div class="line">      allow_scaling:</div>
<div class="line">      allow_focused_range:</div>
<div class="line">    dra_v3:</div>
<div class="line">      slope_psnr_end:</div>
<div class="line">      slope_pcc_end:</div>
<div class="line">      allow_equalization:</div>
<div class="line">  sideband:</div>
<div class="line">  <a class="codeRef" href="../../../library/d9/d64/group__cflite-eazyaigen-cvb.html#gae0c3a46d1be4129322235ced5c4f1b43">vas_option</a>:</div>
<div class="line">  <a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga9ca4d9b16dfe3a25699a7e93120a8962">cavalry_option</a>:</div>
<div class="line">    version:</div>
<div class="line">    user_data:</div>
<div class="line">    encryption:</div>
<div class="line">  custom_node:</div>
<div class="line">    parser:</div>
<div class="line">    dll:</div>
<div class="line">  extra_option:</div>
</div><!-- fragment --><h3><a class="anchor" id="subsubsec_eazyai_python_tools_cfg_comment"></a>
2.2.3 Convert Parameters Without Comments</h3>
<p>As above, there are lots of help comments in these configuration files that users can refer to, if users do not need them, they can disable it as below. </p><div class="fragment"><div class="line">build $ eazyai_cfg -mp model_file --fast --disable_comment</div>
<div class="line">build $ cat model_file_basic.yaml</div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: model_file</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>:</div>
<div class="line">  /user_path/model_file</div>
<div class="line"> </div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path:</div>
<div class="line">    in_file_ext:</div>
<div class="line">    out_shape:</div>
<div class="line">    out_data_format:</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color:</div>
<div class="line">          mean:</div>
<div class="line">          std:</div>
<div class="line"> </div>
<div class="line">input_nodes:</div>
<div class="line">  input_name1:</div>
<div class="line">    data_prepare: dra_data_1</div>
<div class="line">    mean:</div>
<div class="line">    std:</div>
<div class="line"> </div>
<div class="line">output_nodes:</div>
<div class="line">  <span class="stringliteral">&#39;output_name1&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
<div class="line">  <span class="stringliteral">&#39;output_name2&#39;</span>:</div>
<div class="line">    data_format: fp32</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_python_tools_cvt"></a>
2.3 EazyAI Convert Tool</h2>
<p><code>eazyai_cvt</code> will help users to call CNNGen toochain to convert the network, there are two modes. For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_cvt --help</div>
</div><!-- fragment --><ol type="1">
<li>Dummy Mode <div class="fragment"><div class="line">build $ eazyai_cvt -mp model_file --dummy</div>
<div class="line"><span class="preprocessor">###amba_test: TheoryPerf Result: [0.00095]</span></div>
<div class="line"><span class="preprocessor">### Conversion completed !</span></div>
</div><!-- fragment --> With dummy mode, users only need to feed the model to it, nothing else needed, also the tool will share the theory performation without running on EVK. <dl class="section note"><dt>Note</dt><dd>This performance is only for reference, please run it on EVK for more accurate performance. And this dummy mode is only for performance test.</dd></dl>
</li>
<li>Standard Mode <div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml</div>
</div><!-- fragment --> Users can use <code>eazyai_cfg.py</code> to create <code>ea_cvt_cfg.yaml</code>. With this, users can get a Ambarella model easily, and run it with Simulator or Chip.</li>
<li>Additional Mode <div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ef ea_cvt_ext_cfg.yaml</div>
</div><!-- fragment --> The parameters in <code>ea_cvt_ext_cfg.yaml</code> are finally used when converting. It is used when users do not want to modify <code>ea_cvt_cfg</code>, just to change some parameters through <code>ea_cvt_ext_cfg.yaml</code> to finish some special converting.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>For CVflow v3, such as CV7x, please use FP16 for input and output data format, users can modify the input and output data format in <code>ea_cvt_cfg.yaml</code> manually, or use below <code>--adapt_chip</code> to adapt unsupported parameters. <div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml --adapt_chip</div>
</div><!-- fragment --></dd></dl>
<h2><a class="anchor" id="subsec_eazyai_python_tools_inf"></a>
2.4 EazyAI Inference Tool</h2>
<p><code>eazyai_inf</code> can run the network on CVflow chip, Simulator, and Network Original Framework. For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_inf --help</div>
</div><!-- fragment --><p>For the inference with <code>cvflow</code>, please run below command in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_video">2.6 Video Control Tool</a> to initialize the chips first. </p><div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --><ol type="1">
<li>Inference Without PostProcess <div class="fragment"><div class="line">build $ eazyai_inf -ip 10.0.0.2 -cy ea_cvt_summary.yaml --platform cvflow</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml --platform ades</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml --platform acinf</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml --platform orig</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><code>ea_cvt_summary.yaml</code> is generated by <code>eazyai_cvt.py</code>.</dd></dl>
</li>
<li>Inference With PostProcess <div class="fragment"><div class="line">build $ eazyai_inf -ip 10.0.0.2 -cy ea_cvt_summary.yaml -iy ea_inf.yaml --platform cvflow</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml -iy ea_inf.yaml --platform ades</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml -iy ea_inf.yaml --platform acinf</div>
<div class="line">build $ eazyai_inf -cy ea_cvt_summary.yaml -iy ea_inf.yaml --platform orig</div>
</div><!-- fragment --> Users needs to wrtie <code>ea_inf.yaml</code> manually, such as the example in below.</li>
</ol>
<h3><a class="anchor" id="ssubsec_eazyai_python_tools_inf_yaml"></a>
2.4.1 Inference Yaml File</h3>
<div class="fragment"><div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: network_name</div>
<div class="line"> data_prepare:</div>
<div class="line">     test_data_1:</div>
<div class="line">         in_path: tensorflow/demo_networks/deeplab_v3/dra_img</div>
<div class="line">         in_file_ext: jpg</div>
<div class="line"> input_nodes:</div>
<div class="line">     data:</div>
<div class="line">         data_prepare: test_data_1</div>
<div class="line"> post_process:</div>
<div class="line">     <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: <span class="stringliteral">&quot;network_postprocess_name&quot;</span></div>
<div class="line">     lua: /user_path/network.lua</div>
<div class="line">     extra_input: # Used <span class="keywordflow">for</span> SSD offline boxex</div>
<div class="line"> eval_info:</div>
<div class="line">     test_mode: mAP</div>
<div class="line">     dataset: COCO_2014</div>
<div class="line">     classnum: 80</div>
<div class="line">     groundtruth: /user_path/coco_val2014_label.csv # Replace by sample <span class="keyword">package </span>relative path</div>
</div><!-- fragment --><p> There are four key nodes in above inference yaml file.</p><ol type="1">
<li><code>data_prepare</code> and <code>input_nodes</code> are similar with the definition in <code>eazyai_cfg.py</code>.</li>
<li><code>popt_process</code> is used to set the postprocess parameters.</li>
<li><code>eval_info</code> is used to set the accuracy test parameters.</li>
</ol>
<h3><a class="anchor" id="ssubsec_eazyai_python_tools_inf_gpu"></a>
2.4.2 Enbale GPU For Simulator</h3>
<p>To enable GPU for Simulator inference, users need to modify the LD_LIBRARY_PATH in CNNGen toolchain ENV file, the difference is only to use different libraries as they share the same execute binary.</p>
<ol type="1">
<li>Ades<ol type="a">
<li>Modify <code>LD_LIBRARY_PATH</code> in <code>&lt;toolchain path&gt;/env</code> to switch it to the specific version. <div class="fragment"><div class="line">Line 18 -&gt; # Setting up library search path</div>
<div class="line">Line 19 -&gt; LD_LIBRARY_PATH=...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/VpRef/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:</div>
<div class="line">                           ...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/AdesRuntime/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:...</div>
</div><!-- fragment --></li>
<li>Source the env file. <div class="fragment"><div class="line">build $ source &lt;toolchain path&gt;/env/cv22.env</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Ades with GPU is only supported in CNNGen 3.x for CVflowv3 of CV7x and CV3x.</dd></dl>
</li>
</ol>
</li>
<li>Acinference<ol type="a">
<li>Modify <code>LD_LIBRARY_PATH</code> in <code>&lt;toolchain path&gt;/env</code> to switch it to the specific version. <div class="fragment"><div class="line">Line 18 -&gt; # Setting up library search path</div>
<div class="line">Line 19 -&gt; LD_LIBRARY_PATH=...:${AMBA_CVTOOL_ROOT}/cv2/tv2/release/AmbaCnn/cv2.&lt;toolchain_version&gt;.&lt;system_version&gt;.&lt;cuda_version&gt;/lib:...</div>
</div><!-- fragment --></li>
<li>Source the env file. <div class="fragment"><div class="line">build $ source &lt;toolchain path&gt;/env/cv22.env</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="subsec_eazyai_python_tools_cvflow_inf"></a>
2.5 CVflow Simple Inference Tool</h2>
<p><code>eazyai_inf_simple_dummy</code> and <code>eazyai_inf_simple_live</code> can only work in CVflow chip.</p>
<div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
<div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb cavalry_model.bin -t 10</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -cb cavalry_model.bin -buf BUFFER1 -pn network_name -pl network_postprocess.lua -icf 1 -dm 0 -dd HDMI</div>
</div><!-- fragment --><p>For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy --help</div>
<div class="line">build $ eazyai_inf_simple_live --help</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_python_tools_video"></a>
2.6 Video Control Tool</h2>
<p><code>eazyai_video</code> is used to set up the DSP video pipeline, actually, it wil call <code>eazyai_video.sh</code> of <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_vi_sh">3 Eazyai Video</a> on EVK. </p><div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2 --hdmi_1 1080p</div>
</div><!-- fragment --><p>For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_video --help</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_eazyai_python_tools_profiler"></a>
2.7 CVflow Layer Profiler Tool</h2>
<p><code>eazyai_profiler</code> is used to generate a spreadsheet to describe CVflow hardware efficiency for current network, this spreadsheet will help users understand different bottlenecks in the current network structure.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy ea_cvt_cfg.yaml -ac (Convert and generate cflite_cvt_summary.yaml)</div>
<div class="line">build $ eazyai_video -ip 10.0.0.2</div>
<div class="line">build $ eazyai_profiler -cy cflite_cvt_summary.yaml -op &lt;profiler result output path&gt;</div>
</div><!-- fragment --><p>For detail, please refer to help information in the tool, such as below. </p><div class="fragment"><div class="line">build $ eazyai_profiler --help</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_cvflow_layer_profiler">18 CVflow Layer Profiler</a> for how to read the spreadsheet, this tool is used to simplify the whole flow in that section.</dd></dl>
<h2><a class="anchor" id="subsec_eazyai_python_tools_simple"></a>
2.8 Simple PyTools</h2>
<p>There are lots of tools are provided in CVflow Python Library. For detailed list, please refer to <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_tools">3 CFlite Python Tools</a> in <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cflite">1 EazyAI CVflow Lite Python Library</a>.</p>
<ol type="1">
<li>binrotate<ul>
<li>Command: <div class="fragment"><div class="line">build $ binrotate -i binrotate/cat.bin -r 90 -o binrotate/rotate_cat.bin -width 1024 -height 1024 -chan 3</div>
</div><!-- fragment --></li>
<li>Result: The rotate_cat.bin can be check by "Play the YUV file" and should be rotated with 90 degree.</li>
</ul>
</li>
<li>simple_bincmp<ul>
<li>Command: <div class="fragment"><div class="line">build $ simple_bincmp -ades simple_bincmp/ades_file.bin -cv simple_bincmp/cv_file.bin -l 4</div>
</div><!-- fragment --></li>
<li>Result: Print the check result.</li>
</ul>
</li>
<li>convert_jpg2nv12<ul>
<li>Command: <div class="fragment"><div class="line">build $ convert_jpg2nv12 --jpeg convert_jpg2nv12/jpg_images/ --y convert_jpg2nv12/out_y --uv convert_jpg2nv12/out_uv --width 1024 --height 1024</div>
</div><!-- fragment --></li>
<li>Results: Y and UV files are generated in out_y and out_uv folders.</li>
</ul>
</li>
<li>compare_classification_network_result<ul>
<li>Command: <div class="fragment"><div class="line">build $ compare_classification_network_result -l1 ades_list.txt -l2 pc_list.txt -t compare -c1 red -c2 blue -tc 1 -c 0 -t plot1 -s 1</div>
</div><!-- fragment --></li>
<li>Results: Draw the result figure.</li>
</ul>
</li>
<li>im_reader<ul>
<li>Command: <div class="fragment"><div class="line">build $ im_reader im_reader/cat.bin 1024 1024 3 1 read_cat.jpg</div>
</div><!-- fragment --></li>
<li>Result: Generate the read_cat.jpg .</li>
</ul>
</li>
<li>im_quant<ul>
<li>Command: <div class="fragment"><div class="line">build $ im_quant --imload opencv --mean [127.5,127.5,127.5] --test_img im_quant/image/ --quant sfix8_7 --scale 0.0078125 --out im_quant/output \</div>
<div class="line">        --img_type .jpg --img_rsz [3,1024,1024] --is_bgr 0 --is_pad 0</div>
</div><!-- fragment --></li>
<li>Result: Generate the bin and info txt filese in im_quant/output.</li>
</ul>
</li>
<li>padding_append<ul>
<li>Command: <div class="fragment"><div class="line">build $ padding_append -i padding_append/cat.bin -o padding_append/padding_cat.bin -dim 3,224,224 -fmt 4</div>
</div><!-- fragment --></li>
<li>Result: Generate the padding_append/padding_cat.bin .</li>
</ul>
</li>
<li>tf_remove_node<ul>
<li>Command: <div class="fragment"><div class="line">build $ tf_remove_model_node -i tf_remove_node/frozen_mobilenet_224_opt.pb -o modified_mobilenet_224.pb -n MobilenetV1/Predictions/Softmax</div>
</div><!-- fragment --></li>
<li>Result: Check the modified_mobilenet_224.pb by <code>tf_print_graph_summary</code>, the output should be "MobilenetV1/Predictions/Reshape".</li>
</ul>
</li>
<li>mtcnn_get_pnet_scales<ul>
<li>Command: <div class="fragment"><div class="line">build $ mtcnn_get_pnet_scales --height 1080 --width 1920 --min 60 --factor 0.709</div>
</div><!-- fragment --></li>
<li>Result: It should print below message: <div class="fragment"><div class="line">height=1080, width=1920, min=60, factor=0.709</div>
<div class="line">Scale from</div>
<div class="line">(H,W)</div>
<div class="line">(1080,1920)</div>
<div class="line">to</div>
<div class="line">(H,W)</div>
<div class="line">(216,384)</div>
<div class="line">(154,273)</div>
<div class="line">(109,194)</div>
<div class="line">(77,137)</div>
<div class="line">(55,98)</div>
<div class="line">(39,69)</div>
<div class="line">(28,49)</div>
<div class="line">(20,35)</div>
<div class="line">(14,25)</div>
<div class="line">Done</div>
</div><!-- fragment --></li>
</ul>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_eazyai_vi_sh"></a>
3 Eazyai Video</h1>
<p>Before running the CV demo, users must set up some environments, modify Lua according to requirements, setup DSP, and more. Command settings are different for different chips. Therefore, this setup process can be challenging.</p>
<p>To simplify this process, the shell script named <b>"eazyai_video.sh"</b> can be used. Users must pass a few simple parameters to eazyai_video.sh according to their requirements, then eazyai_video.sh automatically generates the corresponding Lua and sets up the DSP. Eazyai_video.sh can simplify the preparation process to setup DSP function and reduce the probability of human error.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Eazyai_video.sh cannot fully support the features in test_encode; it only supports some common cases for easy DSP setup.</li>
<li>Find this script in <code>&lt;Linux SDK Package&gt;/ambarella/packages/eazyai/unit_test/eazyai_video/eazyai_video.sh</code>.</li>
</ul>
</dd></dl>
<p>Use the following command to show the detailed usage of "eazyai_video.sh".</p>
<div class="fragment"><div class="line">board # eazyai_video.sh --help</div>
</div><!-- fragment --><p>Currently, eazyai_video.sh only supports the following boards:</p>
<ul>
<li>CV2_Chestnut</li>
<li>CV22_Walnut</li>
<li>CV25_Hazenut</li>
<li>CV25M_Pinenut</li>
<li>CV28M_Cashewnut</li>
<li>CV5_Timn</li>
<li>CV52_Crco</li>
<li>CV72_Gage</li>
</ul>
<p>If an unsupported board is used, users can use "eazyai_video.sh" to generate a reference script for a similar board, then modify this script to be suitable for the real used board.</p>
<p>Use "CV22n_Vision" as an example; this board is not supported in "eazyai_video.sh". As "CV22n_Vision" is similar to "CV22_Walnut", users can use the command below to generate a reference script for "CV22_Walnut".</p>
<div class="fragment"><div class="line">board # eazyai_video.sh --gen_ref_script cv22_walnut --hdmi 1080p</div>
</div><!-- fragment --><p>After executing the command above, the script and Lua below will be generated.</p>
<ul>
<li>/tmp/run_eazyai_video/run_eazyai_video.sh</li>
<li>/tmp/run_eazyai_video/run_eazyai_video.lua</li>
</ul>
<p>Users can then modify the above script and Lua to meet the requirements of "CV22n_Vision".</p>
<dl class="section note"><dt>Note</dt><dd>Ambarella does not recommend using the same canvas buffer for algorithm analysis and encoding, as overlay will be added in encoding buffers.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_eazyai_deploy_c_library"></a>
4 EazyAI Inference C Library</h1>
<p><b>EazyAI</b> means Easy AI interface; this library provides easy-to-use APIs for running neural networks on <b>x86 simulator</b> and <b>CVflow chip</b>.</p>
<p>For more details, refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html">EazyAI Library API</a> as below.</p>
<ul>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_history">0 Revision History</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_introduction">1 Introduction</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_cvflow">2 EazyAI Deployment</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_arm_postprocess">4 EazyAI Arm Postprocess</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_cvflow">5 EazyAI Unit Test</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_simulator">6 EazyAI Simulator Unit Test</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_api">7 EazyAI API</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_core_api_test">8 EazyAI Core API Test</a></li>
<li><a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_licence">10 License</a></li>
</ul>
<p>It encapsulates lots of low-level modules in the SDK and some basic functions listed below. It also uses the unified data format “Tensor” to connect them in one pipeline.</p>
<ul>
<li>IAV driver</li>
<li>Cavalry_Mem library</li>
<li>VPROC library</li>
<li>NNCtrl library</li>
<li>SmartFB library</li>
<li>PostProcess Library</li>
<li>x86 Ades Library</li>
<li>x86 Acinference library</li>
<li>The arrange functions in high-level with general conceptions in CV ground, such as preprocess, forward, and post process</li>
<li>Unify unit test for different networks</li>
</ul>
<h4><a class="anchor" id="autotoc_md16"></a>
Example 1: Two Algorithms in Dependence</h4>
<div class="image">
<img src="../../EazyAI_Flow1.png" alt=""/>
<div class="caption">
Figure 4-1. EazyAI Flow Example 1.</div></div>
<h4><a class="anchor" id="autotoc_md17"></a>
Example 2: Two Independent Algorithms</h4>
<div class="image">
<img src="../../EazyAI_Flow2.png" alt=""/>
<div class="caption">
Figure 4-2. EazyAI Flow Example 2.</div></div>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Most demos in the following chapters have been modified to use this library.</li>
<li>If users want to use the same canvas buffer for the two algorithms, Ambarella suggests to query the canvas buffer in main thread and share the frame to the two algorithm threads. Users cannot query the same canvas buffer in the same multiple threads or multiple processes.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_eazyai_cpu_postproc"></a>
5 EazyAI Postprocess C Library</h1>
<p>This library includes lots of network postprocess, users can easily use current post process interface, also it is easy for them to register their own postprocess in to this library.</p>
<p>For detail, please refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_arm_postprocess">4 EazyAI Arm Postprocess</a>.</p>
<p>Currently, it has supported below postprocess.</p>
<div class="fragment"><div class="line">support 16 arm postprocess tasks:</div>
<div class="line">      to_file</div>
<div class="line">      bisenet</div>
<div class="line">      centernet</div>
<div class="line">      deeplabv3</div>
<div class="line">      fgfd</div>
<div class="line">      fairmot</div>
<div class="line">      hfnet</div>
<div class="line">      lffd</div>
<div class="line">      mobilenetv2_nv12</div>
<div class="line">      opennet</div>
<div class="line">      retinaface</div>
<div class="line">      ssd</div>
<div class="line">      yolox</div>
<div class="line">      yolov3</div>
<div class="line">      yolov8</div>
<div class="line">      yolov5</div>
</div><!-- fragment --><p>Also there is a simple library for SSD and Yolo in <a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html">Data Process Library API</a> as below.</p>
<ul>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_history">0. Revision History</a></li>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_overview">1 Overview</a></li>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_proc">2 Dproc Library Process</a></li>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_test">3 Unit Test</a></li>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_api">4 Dproc API</a></li>
<li><a class="elRef" href="../../../library/dc/d4f/page_lib_dproc_doc.html#sec_dproc_lic">5 License</a></li>
</ul>
<hr  />
<h1><a class="anchor" id="sec_eazyai_unit_test"></a>
6 EazyAI Unit Test</h1>
<p>This unit test allow users to run different networks on different platform in one application, but only limit to single network.</p>
<p>For detail, please refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_cvflow">5 EazyAI Unit Test</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_simulator">6 EazyAI Simulator Unit Test</a>.</p>
<ol type="1">
<li>For <b>x86 Simulator</b>:<ol type="a">
<li><p class="startli">Run Ambarella Directed Acyclic Graph (DAG) Emulator System (ADES) mode</p>
<p class="startli">i. The <code>raw.bin</code> is used as an input without the preprocess or postprocess </p><pre class="fragment">    build $ ./build/test_eazyai -m 1 -n to_file
        --model_path &lt;usr_path&gt;/model_parser/model.json \
        --ades_cmd_file &lt;usr_path&gt;/ades_model/model_ades.cmd \
        --isrc "i:&lt;input_name&gt;=&lt;usr_path&gt;|t:raw" \
        --output_dir &lt;usr_path&gt;/model/out
</pre><p class="startli">ii. The image is used as an input with the correct preprocess and postprocess </p><pre class="fragment">    build $ ./build/test_eazyai -m 1 -d 0 -r -n &lt;&gt;model_name&gt; \
        --model_path &lt;usr_path&gt;/out_model_parser/model.json \
        --lua_file &lt;model_name.lua&gt; --queue_size 1 --thread_num 1 \
        --isrc "i:&lt;input_name&gt;=&lt;usr_path&gt;|t:jpg" \
        --output_dir &lt;usr_path&gt;/model/out \
        --ades_cmd_file &lt;usr_path&gt;/ades_model/model_ades.cmd
</pre></li>
<li><p class="startli">Run Acinference mode</p>
<p class="startli">i. The <code>raw.bin</code> is used as an input without the preprocess or postprocess </p><pre class="fragment">    build $ ./build/test_eazyai -m 1 -n to_file
        --model_path &lt;usr_path&gt;/out_model_parser/model.json\
        --isrc "i:&lt;input_name&gt;=&lt;usr path&gt;|t:raw" \
        --output_dir &lt;usr_path&gt;/model/out
</pre><p class="startli">ii. The image is used as an input with the correct preprocess and postprocess </p><pre class="fragment">    build $ ./build/test_eazyai -m 1 -d 0 -r -n ssd \
        --model_path &lt;usr_path&gt;/out_model_parser/model.json \
        --lua_file ssd_caffe.lua --queue_size 1 --thread_num 1 \
        --isrc "i:data=&lt;usr_path&gt;|t:jpg" \
        --output_dir &lt;usr_path&gt;/model/out
</pre><dl class="section note"><dt>Note</dt><dd><ul>
<li>For more details of the x86 simulator executable file, refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> and <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_unit_test_simulator">6 EazyAI Simulator Unit Test</a> to build the x86 binary. Then, the executable file <code>test_eazyai</code> can be found in <code>cvflow_cnngen_samples/library/eazyai/unit_test/build/</code>.</li>
<li>Refer to <em><a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html">Caffe Demos</a>, <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html">Tensorflow Demos</a>, <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html">ONNX Demos</a></em> for information on how to generate and run the <em>model.json</em> and <em>model_ades.cmd</em> of each model.</li>
<li>Different models may require different parameters. Refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> for specific usage.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li><p class="startli">For the <b>CVflow chip</b>:</p>
<p class="startli">i. Dummy mode; only for CVflow performance test </p><pre class="fragment">    board # test_eazyai -m 2 --model_path &lt;cavalry_gen_generated.bin&gt;
</pre><p class="startli">ii. The real image is used as an input with the correct preprocess and postprocess </p><pre class="fragment">    board # test_eazyai -m 1 -d 1 -r -n &lt;model_name&gt; \
        --model_path &lt;cavalry_gen_generated.bin&gt; \
        --lua_file &lt;model_name.lua&gt; --queue_size 1  --thread_num 1 \
        --isrc &lt;"i:&lt;input node name&gt;=&lt;image path&gt;|t:&lt;image type:jpeg, etc&gt;"&gt; \
        --output_dir &lt;usr_path&gt;
</pre><p class="startli">iii. The <code>raw.bin</code> is used as an input without the preprocess or postprocess </p><pre class="fragment">    board # test_eazyai -m 1 -d 1 -n &lt;model_name&gt; \
        --model_path &lt;cavalry_gen_generated.bin&gt;  \
        --isrc &lt;"i:&lt;input node name&gt;=&lt;image path&gt;|t:&lt;image type:jpeg, etc&gt;"&gt; \
        --output_dir &lt;usr_path&gt;
</pre></li>
</ol>
<p>For more details, refer to <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html">EazyAI Library API</a>. Content related to Cavalry and neural network control (NNCtrl) can be found in the <em>Linux SDK</em> Doxygen document.</p>
<hr  />
<h1><a class="anchor" id="sec_eazyai_live_application"></a>
7 EazyAI Live Application</h1>
<p>EazyAI has provided lots of different live demos for users to refer to, some can co-work with DSO video features.</p>
<p>For detailed usage, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html">Caffe Demos</a>, <a class="el" href="../../d7/d0a/fs_cnngen_tf_demos.html">Tensorflow Demos</a>, and <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html">ONNX Demos</a>.</p>
<div class="fragment"><div class="line">├── bodypix (Body Segmentation And Pose)</div>
<div class="line">│   └── test_bodypix.cpp</div>
<div class="line">├── bytetrack (Tracking)</div>
<div class="line">│   └── test_bytetrack.cpp</div>
<div class="line">├── face (Face Related)</div>
<div class="line">│   ├── face_alignment</div>
<div class="line">│   │   └── test_face_align.c</div>
<div class="line">│   ├── fdfr</div>
<div class="line">│   │   ├── test_fdfr_v1.c</div>
<div class="line">│   │   └── test_fdfr_v2.c</div>
<div class="line">│   ├── mtcnn</div>
<div class="line">│   │   └── test_mtcnn.c</div>
<div class="line">├── hand_landmark (Hand Detection)</div>
<div class="line">│   └── test_hand_landmark.cpp</div>
<div class="line">├── ssd</div>
<div class="line">│   ├── ssd_efm (Encode From Detected Result)</div>
<div class="line">│   │   └── test_ssd_live_efm.c</div>
<div class="line">│   ├── ssd_fgfd (Fast Header Detection)</div>
<div class="line">│   │   └── test_fgfd_ssd_live.c</div>
<div class="line">│   └── ssd_lpr (License Plate Detection And Recognition)</div>
<div class="line">│       └── test_ssd_lpr.cpp</div>
<div class="line">├── video_matting ()</div>
<div class="line">│   └── test_video_matting.cpp</div>
<div class="line">└── yolo</div>
<div class="line">    ├── test_yolo_live_smartrc.c (ROI Bitrate Control)</div>
<div class="line">    ├── test_yolov3_live_efm.c (Encode From Detected Resul)</div>
<div class="line">    └── test_yolov3_live_mono_depth.cpp (Mond Depth, Needs Special MONO Library)</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaigen-layercompare_html_ga4d5bb0c360b13429f65cd327c8d0aa12"><div class="ttname"><a href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a></div><div class="ttdeci">model_path</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-cavalry_html_ga9ca4d9b16dfe3a25699a7e93120a8962"><div class="ttname"><a href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga9ca4d9b16dfe3a25699a7e93120a8962">cavalry_option</a></div><div class="ttdeci">cavalry_option</div></div>
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_gac69e20380615374be0baa46ed46295b7"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a></div><div class="ttdeci">tuple run(self)</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-cvb_html_gae0c3a46d1be4129322235ced5c4f1b43"><div class="ttname"><a href="../../../library/d9/d64/group__cflite-eazyaigen-cvb.html#gae0c3a46d1be4129322235ced5c4f1b43">vas_option</a></div><div class="ttdeci">vas_option</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga81f22c9cd9a33cc05e5a1657974438bd"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a></div><div class="ttdeci">work_dir</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga8179f95715172cfcd3a44cd038a81a9f"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a></div><div class="ttdeci">transforms</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
