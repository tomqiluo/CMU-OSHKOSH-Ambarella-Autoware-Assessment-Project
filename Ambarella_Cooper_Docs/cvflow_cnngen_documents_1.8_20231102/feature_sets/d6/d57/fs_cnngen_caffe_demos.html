<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: Caffe Demos</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d6/d57/fs_cnngen_caffe_demos.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Caffe Demos </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the Caffe CNNGen demos.</p>
<dl class="section note"><dt>Note</dt><dd>Since CNNGen toolchain with Ubuntu2004, Caffe networks will not be maintained in future release, also there will be no maintenance for AmbaCaffe since Ubuntu2004.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_cgpp_deployment_for_yolov3"></a>
CGPP Deployment for Yolov3</h1>
<p>Toolchain allows users to combine a series of pre-processing operations before running on the neural networks with variable input. This tool gains performance benefits by connecting to VP and DSP directly, reducing DRAM bandwidth.</p>
<p>A few extra steps are required to enable this feature. The example below uses JSON for pre-processing and the CGPP library for deployment. For details, refer to the CNNGen tool guide <em>Ambarella CV UG Json Preprocessing Format</em>.</p>
<h2><a class="anchor" id="sub_sec_input_data"></a>
1 Input Data</h2>
<p>With this function, the network input is converted to a luma and chroma image (NV12) separately instead of a fixed RGB image. This is because color conversion can be run as an operation in the pre-processing phase, and the luma/chroma images can be obtained from IDSP in deployment.</p>
<p>For resizing, there is a limit that a resized node can only support up to 2x down sampling ratio in each dimension. If users run the pyramid buffer in IDSP, it can generate an input resolution with a different scale size of six layers, so the 2x ratio is acceptable for close resolution in the pyramid buffer. Users can always set this to the 2x real network input resolution, then it can accept any real input as long as the size is smaller than 2x real network input resolution.</p>
<p>With YOLOv3 as an example, the input shape is (1, 3, 416, 416) for luma and (1, 2, 208, 208) for chroma. Users can set one input luma image of the shape (1, 1, 832, 832), and one input chroma image of the shape (1, 2, 416, 416).</p>
<h2><a class="anchor" id="sub_sec_prepare_the_json_file"></a>
2 Prepare the JSON File</h2>
<p>Once users have the right input data, the next step is preparing a JSON file for the pre-processing operations.</p><ul>
<li><b>resize + color space conversion</b>: There are two inputs required: one luma image of shape (1, 1, 832, 832) and one chroma image of shape (1, 1, 416, 416) with fix-point 8. Use <em>gen_image_list.py</em> to generate a text file that contains a list of the image path for DRA. Then, the two variable resample operators followed that is used for resizing the Luma and Chroma to a fixed size that neural network requires, such as (1, 1, 416, 416) and (1, 1, 208 ,208) for YOLO v3. The last operator is a CSC node that performs a conversion from YUV to RGB. The name needs to match the name of the network's input layer, such as the example below. <pre class="fragment">  {
  "inputs":
  [
      {
      "name": "luma",
      "filepath": "word_dir/dra/dra_luma_list.txt",
      "shape": [1, 1, 832, 832],
      "quantized": true,
      "dataformat": "0,0,8,0"
      },
      {
      "name": "chroma",
      "filepath": "&lt;work_dir&gt;/dra/dra_chroma_list.txt",
      "shape": [1, 2, 416, 416],
      "quantized": true,
      "dataformat": "0,0,8,0",
      "type": "UV"
      }
  ],
  "operators":
  [
      {
      "type": "VARIABLE_RESAMP",
      "name": "luma_resize",
      "attr":
      {
          "out_w": 416,
          "out_h": 416
      },
      "inputs": ["luma"],
      "dataformat": "0,0,8,0"
      },
      {
      "type": "VARIABLE_RESAMP",
      "name": "chroma_resize",
      "attr":
      {
          "out_w": 208,
          "out_h": 208
      },
      "inputs": ["chroma"],
      "dataformat": "0,0,8,0"
      },
      {
      "type": "COLOR_CONVERT",
      "name": "data",
      "attr":
      {
          "code": "YUV2RGB_NV12"
      },
      "inputs": ["luma_resize", "chroma_resize"],
      "dataformat": "0,0,8,0"
      }
  ]
  }
</pre></li>
<li><b>Warp + color space conversion</b>: The warp affine transformation requires four inputs:<ul>
<li>The luma image of shape (1, 1, 720, 1280)</li>
<li>The chroma image of shape (1, 1, 360, 640)</li>
<li>The luma warp table of shape (2, 1, 2, 2)</li>
<li>The chroma warp table of shape (2, 1, 2, 2)</li>
</ul>
</li>
</ul>
<p>The warp data is used for a transformation to affine a region in a 720p image to a fixed size that YOLOv3 requires. Each input has to be configured with a file path that points to the directory of the prepared data.</p>
<p>The data format of these warp tables are always set to "1, 1, 4, 0" which represents a signed fixed 16-bit point number with 4-bit fraction. Two warp operators follow, which warp the luma and chroma image to a desired size according to the warp tables.</p>
<p>The numbers in the fields "horizontal_grid_spacing_log2" and "vertical_grid_spacing_log2" are obtained by the formulations ceil(<em>log⁡2 out_w</em>) and ceil(<em>log2⁡ out_h</em>).</p>
<p>The last operator is a CSC node that performs a conversion from YUV to RGB. The name needs to match the name of network's input layer, seen in the example below. </p><pre class="fragment">{
"inputs":
[
    {
    "name": "luma",
    "filepath": "&lt;work_dir&gt;/dra/luma_list.txt",
    "shape": [1, 1, 720, 1280],
    "quantized": true,
    "dataformat": "0,0,8,0"
    },
    {
    "name": "chroma",
    "filepath": "&lt;work_dir&gt;/dra/chroma_list.txt",
    "shape": [1, 2, 360, 640],
    "quantized": true,
    "dataformat": "0,0,8,0",
    "type": "UV"
    },
    {
    "name": "luma_warp_field",
    "filepath": "&lt;work_dir&gt;/warp_field/y_warp.txt",
    "shape": [2,1,2,2],
    "quantized": true,
    "dataformat": "1,1,4,0"
    },
    {
    "name": "chroma_warp_field",
    "filepath": "&lt;work_dir&gt;/warp_field/uv_warp.txt",
    "shape": [2,1,2,2],
    "quantized": true,
    "dataformat": "1,1,4,0"
    }
],
"operators":
[
    {
    "type": "WARP",
    "name": "luma_warp",
    "attr":
    {
        "out_w": 416,
        "out_h": 416,
        "horizontal_grid_spacing_log2": 9,
        "vertical_grid_spacing_log2": 9,
        "zr": 0
    },
    "inputs": ["luma", "luma_warp_field"]
    },
    {
    "type": "WARP",
    "name": "chroma_warp",
    "attr":
    {
        "out_w": 208,
        "out_h": 208,
        "horizontal_grid_spacing_log2": 8,
        "vertical_grid_spacing_log2": 8,
        "zr": 0
    },
    "inputs": ["chroma", "chroma_warp_field"]
    },
    {
    "type": "COLOR_CONVERT",
    "name": "data",
    "attr":
    {
        "code": "YUV2RGB_NV12"
    },
    "inputs": ["luma_warp", "chroma_warp"],
    "dataformat": "0,0,8,0"
    }
]
}
</pre><h2><a class="anchor" id="sub_sec_convert"></a>
3 Convert</h2>
<p>This section explains how users can deploy the CGPP pre-processing. For easier reproduction on the customer side, the CNNGen samples package includes the example below.</p>
<p>In these examples, the pre-process JSON files are automatically generated by the script, and the input image is an 832x832 image named <em>dog.jpg</em> in <em>cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/dra_img</em>, which will be converted to NV12 by script automatically as well.</p>
<ol type="1">
<li><p class="startli"><b>Convert the YOLOv3 resize with the JSON pre-process</b> </p><pre class="fragment"> build $ make menuconfig
 [*] Ambarella Caffe Networks  ---&gt;
     [*]   Build Caffe Demo Networks  ---&gt;
         [*]   Build Caffe YOLOV3 Network  ---&gt;
             Sparse_Ratio  ---&gt;
                 [*] SP50_BASIC_QUANT  ---&gt;
                     [*] Convert YOLOV3 resize with CGPP Pre-Process
 build $ make caffe_yolov3_cgpp_resize run_mode=cavalry
</pre><p class="startli">Then, <em>yolov3_cgpp_resize.bin</em> is generated in <em>cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/out_cavalry/</em>.</p>
</li>
<li><p class="startli"><b>Convert the YOLOv3 warp with the JSON pre-process</b> </p><pre class="fragment"> build $ make menuconfig
 [*] Ambarella Caffe Networks  ---&gt;
     [*]   Build Caffe Demo Networks  ---&gt;
         [*]   Build Caffe YOLOV3 Network  ---&gt;
             Sparse_Ratio  ---&gt;
                 [*] SP50_BASIC_QUANT  ---&gt;
                     [*] Convert YOLOV3 warp with CGPP Pre-Process
 build $ make caffe_yolov3_cgpp_warp run_mode=cavalry
</pre><p class="startli">Then, <em>yolov3_cgpp_warp.bin</em> is generated in <em>cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_warp/out_cavalry/</em>.</p>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>In this conversion, the input images are prepared beforehand. To use a specific input image, replace <em>dog.jpg</em> in <em>cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/dra_img</em> with a new image. The size should be 832 x 832 or less for the 2x ratio limitation.</li>
<li>In this convert, pre-processing json file is generated automatically and used by <em>-pp</em> option in <em>caffeparser.py</em>. To use specified json file, users can modify it based on the json file of this example.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_ades"></a>
4 ADES</h2>
<p>Run <em>ADES</em> below with the CNNGen samples package.</p>
<ol type="1">
<li><b>Run ADES of YOLOv3 resize</b>: <pre class="fragment"> build $ make menuconfig
 [*] Ambarella Caffe Networks  ---&gt;
     [*]   Build Caffe Demo Networks  ---&gt;
         [*]   Build Caffe YOLOV3 Network  ---&gt;
             Sparse_Ratio  ---&gt;
                 [*] SP50_BASIC_QUANT  ---&gt;
                     [*] Convert YOLOV3 resize CGPP Pre-Process
 build $ make caffe_yolov3_cgpp_resize run_mode=ades
</pre></li>
<li><b>Run ADES of YOLOv3 warp</b>: <pre class="fragment"> build $ make menuconfig
 [*] Ambarella Caffe Networks  ---&gt;
     [*]   Build Caffe Demo Networks  ---&gt;
         [*]   Build Caffe YOLOV3 Network  ---&gt;
             Sparse_Ratio  ---&gt;
                 [*] SP50_BASIC_QUANT  ---&gt;
                     [*] Convert YOLOV3 warp CGPP Pre-Process
 build $ make caffe_yolov3_cgpp_warp run_mode=ades
</pre></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>For this example, user should pay attention to the different inputs between parser and ADES. For the parser, it only needs two inputs as luma and chroma, but ADES needs two more inputs, <em>luma_resize_params.bin</em> and <em>chroma_resize_params.bin</em>, in <em>cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/out_caffe_parser/</em> which is generated by the parser, such as below. <pre class="fragment">    -ib
    luma=$(cat cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/dra/dra_luma_list.txt | head -1) \
    -ib
    chroma=$(cat cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/dra/dra_chroma_list.txt | head -1) \
    -ib
    luma_resize_params= cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/out_caffe_parser/luma_resize_params.bin \
    -ib
    chroma_resize_params= cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_cgpp_resize/out_caffe_parser/chroma_resize_params.bin
</pre></dd></dl>
<h2><a class="anchor" id="sub_sec_build_evk_binary"></a>
5 Build EVK Binary</h2>
<p>To build the EVK binary: </p><pre class="fragment">build $ make menuconfig
[*] Ambarella Unit Test Configuration  ---&gt;
    [*]   Ambarella Private Linux Unit test configs  ---&gt;
        [*]   Build CV unit tests  ---&gt;
            [*]   Build test_cgpp unit tests
build $ make test_cgpp

build $ make menuconfig
[*] Ambarella Application Configuration  ---&gt;
    [*]   Build AICAM  ---&gt;
        [*]   Build AICAM CVflow  ---&gt;
            [*]   Build CGPP live unit tests
build $ make test_cgpp_live
</pre><h2><a class="anchor" id="sub_sec_run_file_mode_with_cgpp"></a>
6 Run File Mode with CGPP</h2>
<p>Users can use <em>test_cgpp</em> to run these two examples resize and warp. This application will do variable resample to crop out the ROI area and resize the area to required input resolution by network, then do network inference.</p>
<ol type="1">
<li><b>Run resize example</b>: The test_cgpp application could be run at resize mode, which does variable resample to crop out the ROI area and resize the area to the network required input resolution, and then do the network inference. <pre class="fragment">board # cavalry_load -f /lib/firmware/cavalry.bin -r
board # test_cgpp -b cgpp_yolov3.bin --dump out_resize --pp 1 --yuv 1920x1080.nv12 --ires 1920x1080 --roi 200,100,960,540 --scale-type 0 --nnres 416x416
=======================
Cavalry Model Bin: [cgpp_yolov3.bin]
Network [0] [cgpp_yolov3.bin] total memory size: 52394368, dvi: 41076812, blob: 11164032. Bandwidth size: 83361344
[LIBNN WARN] Warning: Not specify net_io pointer, should get net_io by call nnctrl_get_net_io_cfg() later
From cgpp, set input: 0 luma phys: 0xd083ad0d, update_pitch: 1408, rotate_flip: 0x0, virt: 0x7fa9b43d0d
From cgpp, set input: 1 chroma phys: 0xd093720c, update_pitch: 1408, rotate_flip: 0x0, virt: 0x7fa9c4020c
[LIBNN INFO] Set port [luma], sub [luma], update_pitch: 1408
[LIBNN INFO] Set port [chroma], sub [chroma], update_pitch: 1408
Net_id: 0, Dags: 39 / 39, vp_ticks: 366045, vp_time: 29788 us, arm_time: 29839 us
</pre></li>
<li><b>Running in warp mode</b>: The test_cgpp application could be run at warpAffine mode, which does an affine transform from a specified pyramid layer to the target that the network requires. <pre class="fragment"> board # cavalry_load -f /lib/firmware/cavalry.bin -r
 board # test_cgpp -b cgpp_yolov3.bin --dump out_warp --pp 2 --yuv 1280x720.nv12 --ires 1280x720 --scale-type 0 --nnres 416x416 --pyr-lvl 0 --tran-mat 0.325,0,0,0,0.577,0
 =======================
 Cavalry Model Bin: [cgpp_yolov3.bin]
 Network [0] [cgpp_yolov3.bin] total memory size: 52815488, dvi: 41005240, blob: 11699840. Bandwidth size: 94537280
 [LIBNN WARN] Warning: Not specify net_io pointer, should get net_io by call nnctrl_get_net_io_cfg() later
 [LIBNN INFO] Set port [luma], sub [luma], update_pitch: 1280
 [LIBNN INFO] Set port [chroma], sub [chroma], update_pitch: 1280
 Net_id: 0, Dags: 38 / 38, vp_ticks: 2203430, vp_time: 179315 us, arm_time: 179489 us
</pre></li>
</ol>
<h2><a class="anchor" id="sub_sec_run_live_mode_with_cgpp"></a>
7 Run Live Mode with CGPP</h2>
<p>Users can use <em>test_cgpp_live</em> to take advantage of CGPP library to handle CNNGen Pre-processing nodes with live stream. Customers can take it as a sample code to write their own application. It will query NV12 frame data from IDSP pyramid buffer.</p>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><pre class="fragment">/sdcard/cgpp_yolov3
|--model
│    yolov3_cgpp_resize.bin
│    yolov3_cgpp_warp.bin
|
|--labels
│    coco_class_names.txt
|
|--in
|    bus.jpg
|    bus.bin
|
|--out
</pre><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"coco_class_names.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li><p class="startli"><b>Run resize with CGPP</b>:</p><ol type="1">
<li>Setup IAV and enable preview with imx274_mipi. <pre class="fragment"> board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --pyramid_manual_map 0x3F --pyramid_item_num 5 --reallocate_mem overlay,0x01000000
</pre> <dl class="section note"><dt>Note</dt><dd>Please use <code>--pyramid_manual_map 0x7F</code> for CV5x and CV7x as they support seve layers.</dd></dl>
</li>
<li><p class="startli">Run resize with CGPP:</p>
<p class="startli">The inputs have to follow the order: luma -&gt; chroma -&gt; luma resize parameters -&gt; chroma resize parameters.</p><ol type="a">
<li>Streams live mode (draw on stream without frame sync machine <code>rtsp://10.0.0.2/stream1</code>) <pre class="fragment"> board # osd_server_yolov3 -p 27182 -f /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua -r 180,80,360,320 -s 0 &amp;
 board # test_cgpp_live -p 27182 -b yolov3_cgpp_resize.bin --nnres 416x416 --pp 1 --roi 480,180,960,720 -n 1 -c 0
</pre></li>
<li>VOUT live mode (draw on VOUT HDMI) <pre class="fragment"> board # osd_server_yolov3 -p 27182 -f /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua -r 180,80,360,320 &amp;
 board # test_cgpp_live -p 27182 -b yolov3_cgpp_resize.bin --nnres 416x416 --pp 1 --roi 480,180,960,720 -n 1 -c 0
</pre></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>The option "-r" in <code>osd_server_yolov3</code> and "--roi" in <code>test_cgpp_live</code> sepcify the ROI range, defined as "[x_offset],[y_offset],[width],[height]".</li>
<li>The resolution of source buffer is 1920x1080, and the ROI area(480,180,960,720) of source buffer is scaled to 416x416 as the input of the network.</li>
<li>The resolution of framebuffer is 720x480 in default, which can be modified by option "--fb_res" in <code>eazyai_video.sh</code>, the ROI of framebuffer is 180,80,360,320, which needs to be a proportional correspondence with the ROI of source buffer.</li>
<li>The calculation process for the ROI of framebuffer is "x_offset = 480 * scale_1, y_offset = 180 * scale_2, width = 960 * scale_1, height = 720 * scale_2", and "scale_1 = 720 / 1920, scale_2 = 480 / 1080", the ROI of framebuffer will be 180,80,360,320 in this case.</li>
</ol>
</dd></dl>
</li>
<li><b>Run in warpAffine with CGPP</b>:<ol type="1">
<li>Setup IAV and enable preview with imx274_mipi. <pre class="fragment"> board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --fb_res 1920x1080 --fifth_source_buf_res 832x832 --pyramid_input_buf_id 4 --pyramid_manual_map 0x3F  --pyramid_item_num 5 --reallocate_mem overlay,0x01000000
</pre> <dl class="section note"><dt>Note</dt><dd>Please use <code>--pyramid_manual_map 0x7F</code> for CV5x and CV7x as they support seven layers.</dd></dl>
</li>
<li><p class="startli">Run in warpAffine with CGPP:</p>
<p class="startli">The inputs have to follow the order: luma -&gt; chroma -&gt; luma warp field -&gt; chroma warp field.</p><ol type="a">
<li>Streams live mode (draw on stream without frame sync machine <code>rtsp://10.0.0.2/stream1</code>) <pre class="fragment"> board # osd_server_yolov3 -p 27182 -f /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua -s 0 &amp;
 board # test_cgpp_live -p 27182 -b yolov3_cgpp_warp.bin --nnres 416x416 --pp 2  --pyr-lvl 0 --tran-mat 0.5,0,0,0,0.5,0 -n 1 -c 0
</pre></li>
<li>VOUT live mode (draw on VOUT HDMI) <pre class="fragment"> board # osd_server_yolov3 -p 27182 -f /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua &amp;
 board # test_cgpp_live -p 27182 -b yolov3_cgpp_warp.bin --nnres 416x416 --pp 2  --pyr-lvl 0 --tran-mat 0.5,0,0,0,0.5,0 -n 1 -c 0
</pre></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>The resolution of source buffer is 832x832, which is transformed to 416x416 by "transform matrix(0.5,0,0,0,0.5,0)" as the input of the network.</dd></dl>
<hr  />
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_hyperlpr"></a>
Car License Plate Detection and Recognition</h1>
<p>This live demo is for Chinese car license plate detection and recognition. The following sections will explain how to convert the models and run the live demo.</p>
<p>This demo includes three Caffe models, which need to be converted and run on CVflow:</p><ul>
<li><em>lpr.caffemodel</em> is used for license plate detection. It is a MobileNet SSD based model which can be downloaded from: <em><a href="https://github.com/zeusees/Mobilenet-SSD-License-Plate-Detection;">https://github.com/zeusees/Mobilenet-SSD-License-Plate-Detection;</a></em> </li>
<li><em>HorizonalFinemapping.caffemodel</em> is used for license plate cropping. It can be downloaded from: <em><a href="https://github.com/zeusees/HyperLPR;">https://github.com/zeusees/HyperLPR;</a></em> </li>
<li><em>SegmenationFree-Inception.caffemodel</em> is used for license plate character recognition. It could be downloaded from: <em><a href="https://github.com/zeusees/HyperLPR">https://github.com/zeusees/HyperLPR</a></em>.</li>
</ul>
<h2><a class="anchor" id="subsec_hyperlpr_convert_model"></a>
1 Convert Model File</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For mobilenetv1_ssd, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_cnngen_conversion">3 CNNGen Conversion</a>. In this demo, the model used is different from mobilenetv1_ssd: <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_cnngen_conversion">3 CNNGen Conversion</a>, and <code>ea_cvt_mobilenetv1_ssd.yaml</code> needs to be changed before conversion. <dl class="section note"><dt>Note</dt><dd>In this demo, if using the mobilenetv1_ssd compilation method, it is necessary to change the model_path, caffe_prototxt_path and out_shape in <code>ea_cvt_mobilenetv1_ssd.yaml</code>, as follows: <div class="fragment"><div class="line"><span class="preprocessor"># This yaml is used to quantify the model by tool &quot;eazyai_cvt&quot;.</span></div>
<div class="line"><a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: mobilenetv1_ssd</div>
<div class="line"><a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a>: out/caffe/demo_networks</div>
<div class="line"><a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>: caffe/demo_networks/mobilenetv1_ssd/models/lpr.caffemodel</div>
<div class="line">caffe_prototxt_path: caffe/demo_networks/mobilenetv1_ssd/models/lpr.prototxt</div>
<div class="line"> </div>
<div class="line">data_prepare:</div>
<div class="line">  dra_data_1:</div>
<div class="line">    in_path: caffe/demo_networks/mobilenetv1_ssd/dra_img</div>
<div class="line">    in_file_ext: jpg</div>
<div class="line">    out_shape: 1,3,480,640</div>
<div class="line">    out_data_format: uint8</div>
<div class="line">    <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>:</div>
<div class="line">      - <span class="keyword">class</span>: GenImageList</div>
<div class="line">        arguments:</div>
<div class="line">          color: BGR</div>
<div class="line"> </div>
<div class="line">input_nodes:</div>
<div class="line">  data:</div>
<div class="line">   data_prepare: dra_data_1</div>
<div class="line">   mean: 127.5,127.5,127.5</div>
<div class="line">   std: 128</div>
<div class="line"> </div>
<div class="line">output_nodes:</div>
<div class="line">  mbox_loc:</div>
<div class="line">    data_format: fp32</div>
<div class="line">  mbox_conf_flatten:</div>
<div class="line">    data_format: fp32</div>
<div class="line"> </div>
<div class="line">cnngen_convert:</div>
<div class="line">  dra_option:</div>
<div class="line">    version: 2</div>
<div class="line">    device: -2</div>
<div class="line">    strategy: <span class="keyword">auto</span></div>
</div><!-- fragment --></dd></dl>
</li>
<li>For segfree_inception, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/HyperLPR/segfree_inception/config/ea_cvt_segfree_inception.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/segfree_inception/</code>.</li>
<li>For LPHM, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/HyperLPR/LPHM/config/ea_cvt_LPHM.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/LPHM/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/segfree_inception(LPHM)</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_segfree_inception(LPHM).bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/segfree_inception(LPHM)/&lt;chip&gt;/&lt;chip&gt;_cavalry_segfree_inception(LPHM)</code>.</li>
<li>For X86 simulator, model desc json file <b>segfree_inception(LPHM).json</b> is in the cnngen output folder <code>out/caffe/demo_networks/segfree_inception(LPHM)/&lt;chip&gt;/out_segfree_inception(LPHM)_parser/</code>. ades command <b>segfree_inception(LPHM)_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/segfree_inception(LPHM)/&lt;chip&gt;/&lt;chip&gt;_ades_segfree_inception(LPHM)</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="subsec_hyperlpr_build_evk_bin"></a>
2 Build EVK Binary</h2>
<p>Choose SSD_LPR unit test in the SDK package to build the EVK binary: </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">      [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">          [*] Build EazyAI applications  ---&gt;</div>
<div class="line">              [*] Build SSD EazyAI apps  ---&gt;</div>
<div class="line">                  [*] Build SSD LPR EazyAI apps</div>
<div class="line">  build $ make test_ssd_lpr</div>
</div><!-- fragment --><h2><a class="anchor" id="subsec_hyperlpr_run_python_inference"></a>
3 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/segfree_inception/&lt;chip&gt;/&lt;chip&gt;_cavalry_segfree_inception/&lt;chip&gt;_cavalry&lt;version&gt;_segfree_inception.bin</div>
<div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/LPHM/&lt;chip&gt;/&lt;chip&gt;_cavalry_LPHM/&lt;chip&gt;_cavalry&lt;version&gt;_LPHM.bin</div>
</div><!-- fragment --></li>
<li>File Mode (Without Postprocess) <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/segfree_inception/segfree_inception_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/segfree_inception</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/LPHM/LPHM_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/LPHM</div>
</div><!-- fragment --></li>
<li>Accuracy Mode Not supported. <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#subsec_hyperlpr_run_c_inference">4 Run C inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="subsec_hyperlpr_run_c_inference"></a>
4 Run C inference</h2>
<p>Place the generated binary models <em>mobilenetv1_ssd_cavalry.bin</em>, <em>lpr_priorbox_fp32.bin</em>, <em>segfree_inception(LPHM)_cavalry.bin</em>, <em>LPHM_cavalry.bin</em> to the SD card, which can be under <em>/sdcard/ssd_lpr/</em> in the following example.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>In the manual feed mode, the image could be hold in the memory until the algorithm finishes the related calculation and the application side could set this memory part free.</li>
<li>The item_num should be equal to the application side necessary buffer number plus 3, since DSP also need 3 buffers by default. In this application, the state buffer by default needs at most 3 buffers, LPR need at most 1 and SSD need at most 1, so the total buffer number should be set as 8.</li>
</ul>
</dd></dl>
<p>This example uses the LT6911 module and CV22 board. </p><div class="fragment"><div class="line">board # eazyai_video.sh --vin lt6911 --hdmi 1080p --stream_A 1080p --reallocate_mem overlay,0x04009000 --enc_dummy_latency 5 \</div>
<div class="line">        --pyramid_input_buf_id 0 --pyramid_manual_map 0x03  --pyramid_scale_type 2 --pyramid_item_num 8 \</div>
<div class="line">        --pyramid_layer_1_rescale_size 640x480</div>
<div class="line">board # test_ssd_lpr -b mobilenetv1_ssd_cavalry.bin --in data --out mbox_loc \</div>
<div class="line">        --out mbox_conf_flatten --<span class="keyword">class </span>2 --background_id 0 --top_k 50 --nms_top_k 100 \</div>
<div class="line">        --nms_threshold 0.45 --pri_threshold 0.3 --priorbox lpr_priorbox_fp32.bin -i 0 \</div>
<div class="line">        -b segfree_inception(LPHM)_cavalry.bin --in data --out prob -b LPHM_cavalry.bin \</div>
<div class="line">        --in data --out dense</div>
</div><!-- fragment --><p> The result will be shown on the overlay of stream A with the license plate bounding box, the license plate picture and the license plate text. </p><dl class="section note"><dt>Note</dt><dd><ul>
<li>The overlay buffer size <b>should be enlarged</b> before running the LPR demo, so that enough memory could be allocated to draw the license plates on overlay. In the previous command line, the <em>test_mempart</em> sets the overlay buffer size to 67 MB. This size can be applied to a case where there are two streams with the resolution 1080p+480p. If users need to show car plate detection result on more stream overlay, or the stream resolution is very high, such as 4K, continuously enlarge the size of this buffer if necessary. Note that large overall size will <b>impact</b> the DSP's encode performance.</li>
<li>Users can adjust the license plate detection threshold using the options <em>"--pri_threshold"</em> and <em>"--nms_threshold"</em>.</li>
<li>Users can adjust the license plate recognition threshold by using the option "--recg_threshold".</li>
<li><em>"-N"</em> or <em>"--plate_num"</em> option can be used to adjust the number of car license plates that will be drawn on the overlay. Users need to adjust this number while overlay is being displayed on small resolution stream, or there is not enough space for drawing too much licenses.</li>
<li><em>"--overlay_offset"</em> can set the overlay x axis offset, the default is zero. The overlay can be on the left by default.</li>
<li><em>"--highlight_frames"</em> can set the new license highlight duration frames. The default value is 30, which can mean one second.</li>
<li><em>"--clear_frames"</em> can set the license to show duration frames. The default is 30*60*5, which means 5 minutes on 1080p.</li>
<li><em>"--text_ratio"</em> can set the overlay text background length ratio, the default value is 1.0. This parameter adjusts the text box background length. This ratio multiplies the default length.</li>
<li>For other options, refer to the the application information.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="subsec_hyperlpr_optimization"></a>
5 Performance Optimization</h2>
<p>The performance optimization includes the following two parts:</p><ul>
<li>This application uses EazyAI library and enabled <b>VP priority</b> which improves <b>25%</b> performance.</li>
<li>From SDK 3.0.6, the <b>manual resume</b> mechanism is added to this application.</li>
</ul>
<p>Context:</p><ul>
<li>For SSD thread, there is only one vproc (quick) + one network (slow about 20 ms on CV22)</li>
<li>For LPR thread, there are 2 + N vprocs (quick) plus 2 * N networks (quick). (N is license number)</li>
<li>VP tasks in the LPR thread has been set to the highest priority</li>
</ul>
<p>In the auto resume mode, after the SSD network is interrupted by the LPR VP tasks, the SSD network tries to occupy the VP as soon as the VP is in idle state. As there are small intervals between the LPR VP tasks, then the SSD network usually succeeds in regain the VP. However, these intervals are very short, the SSD network is then quickly be interrupted again by the next LPR VP task.</p>
<p>Although the SSD network needs to quit the VP again, it still has to finish the current DAG before quitting the VP. (Because the SSD networks is split into multiple DAGs and the NN can only quit VP in the interval of the DAGs) After the SSD DAG quits the VP, then the LPR VP task could enter into the VP. This is a waste of performance, because loading DAG could be time consuming and waiting for the SSD DAG could be more time consuming.</p>
<p>Now, in the manual resume mechanism, when the SSD network is interrupted, it needs to be "waked-up" manually before regaining the VP resource. In this case, the interrupted SSD network needs to wait for the program call, so that it is only able to run after the complete LPR VP task group is finished.</p>
<p>Below is the performance test result for the auto resume mode and manual resume mode: </p><a class="anchor" id="hyperlpr_optimization"></a>
<table class="doxtable">
<caption></caption>
<tr align="center">
<th></th><th colspan="3">Manual Resume </th><th colspan="3">Auto Resume </th></tr>
<tr align="center">
<td></td><td>SSD (ms/loop) </td><td>LPR (ms/loop) </td><td>VP Idle percentage </td><td>SSD (ms/loop) </td><td>LPR (ms/loop) </td><td>VP Idle percentage </td></tr>
<tr align="center">
<td>1 license </td><td>23.5 </td><td>20.2 </td><td>33.12% </td><td>22.9 </td><td>24.8 </td><td>31.35% </td></tr>
<tr align="center">
<td>5 licenses </td><td>25.0 </td><td>15.6 </td><td>30.35% </td><td>23.1 </td><td>16.5 </td><td>30.65% </td></tr>
</table>
<p>In the 1-license case, the LPR time has been saved (24.8 - 20.2) = 4.6ms, while it only causes minor performance drop for SSD (0.6ms).</p>
<p>In the 5-license case, the LPR time has been saved (16.5 - 15.6) * 5 = 4.5ms, which matches the "1-license" case.</p>
<dl class="section note"><dt>Note</dt><dd>This manual resume mode could be controlled via the <em>test_ssd_lpr</em> command line parameter. <em>"-A --abort_if_preempted"</em>: Abort the network if interrupted. The default value for this option is 1, which means to use manual resume.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_custom_node_example"></a>
Custom Node Example</h1>
<p>In 2015, Liu et al. proposed a new method for detecting objects called the "Single Shot MultiBox Detector" (SSD). They released a Caffe implementation that contained several new layers such as '<b>permute</b>' and '<b>priorbox</b>'.</p>
<p>This section provides an example of implementing a custom node, '<b>permute</b>'. Then, it describes how to develop a simple Caffe model containing a permute layer. Finally, it provides the steps for converting the Caffe model through CNNGen.</p>
<h2><a class="anchor" id="sub_sec_custom_example_implement_permute"></a>
1 Implement Permute</h2>
<p>To complete a custom node, users must create four files: <em>example_node.cc</em>, <em>example_node.h</em>, <em>export.cc</em>, and <em>export.h</em>. Additionally, users must create a Python script. The following provides examples of implementing the four files, as well as the Python script.</p>
<h3><a class="anchor" id="subb_sec_example_node_cc"></a>
1.1 example_node.cc</h3>
<p>As introduced in <a class="el" href="../../d2/d67/fs_cnngen.html#sec_custom_spec_apis">20.2 Special APIs</a>, users must implement three functions: <b>init()</b>, <b>expand()</b>, and <b>release()</b>. These functions should be completed in the <em>example_node.cc</em> file. Note that users must construct a primitive sub-graph that corresponds to the functionality of the custom node in <b>expand()</b>. For example, permute rearranges the dimensions of a tensor so that they are in the order specified by users. Ambarella CNNGen provides the following function:</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> void (*amba_cnn_ext_c_transpose_factory_t)</div>
<div class="line">(<span class="keyword">const</span> <span class="keywordtype">char</span>* id, <span class="keyword">const</span> <span class="keywordtype">char</span>* src0, <span class="keywordtype">int</span> port0,</div>
<div class="line"><span class="keyword">const</span> amba_cnn_ext_c_transpose_attr_t* attr,</div>
<div class="line"><span class="keyword">const</span> amba_cnn_c_data_format_t* dfs,</div>
<div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span>* intlvs);</div>
</div><!-- fragment --><p>Because the function of the <b>transpose_factory()</b> is the same as permute, users can call this function inside <b>expand()</b> to enable permute. The following code provides an example.</p>
<p><b>nd_permute.cc</b> </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;nd_permute.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdio&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> cid_ctr = 0;</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> permute_node_init(<span class="keywordtype">int</span>* cid, amba_cnn_c_vcoord_t* osz,</div>
<div class="line">                <span class="keywordtype">int</span> num, <span class="keyword">const</span> amba_cnn_c_vcoord_t* isz, <span class="keyword">const</span> <span class="keywordtype">char</span>* attr,</div>
<div class="line">                <span class="keyword">const</span> amba_cnn_c_data_format_t* df)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordflow">if</span> (num != 1) {</div>
<div class="line">        <span class="keywordflow">return</span> 1;</div>
<div class="line">    }</div>
<div class="line">    *cid = cid_ctr ++;</div>
<div class="line">    osz-&gt;w = isz[0].w;</div>
<div class="line">    osz-&gt;h = isz[0].h;</div>
<div class="line">    osz-&gt;d = isz[0].d;</div>
<div class="line">    osz-&gt;p = isz[0].p;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> permute_node_expand(<span class="keywordtype">int</span> cid, <span class="keyword">const</span> amba_cnn_ext_c_prim_factories_t* funcs)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordflow">if</span> (cid &gt;= cid_ctr) {</div>
<div class="line">        <span class="keywordflow">return</span> 1;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    amba_cnn_c_data_format_t df; <span class="comment">//uint8_t</span></div>
<div class="line">    df.undef = 1;</div>
<div class="line"> </div>
<div class="line">    amba_cnn_ext_c_transpose_attr_t attr;</div>
<div class="line">    attr.to_w = 0;</div>
<div class="line">    attr.to_h = 1;</div>
<div class="line">    attr.to_d = 2;</div>
<div class="line">    attr.to_p = 3;</div>
<div class="line"> </div>
<div class="line">    funcs-&gt;transpose_factory(<span class="stringliteral">&quot;output&quot;</span>, <span class="stringliteral">&quot;*in*0&quot;</span>, 0, &amp;attr, &amp;df, NULL);</div>
<div class="line">    funcs-&gt;select_output(0, <span class="stringliteral">&quot;output&quot;</span>, 0);</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> permute_node_release(<span class="keywordtype">int</span> cid)</div>
<div class="line">{<span class="keywordflow">return</span> 0;}</div>
</div><!-- fragment --><h3><a class="anchor" id="subb_sec_example_node_h"></a>
1.2 example_node.h</h3>
<p>This is the head file corresponding to the <em>example_node.cc</em> file shown in the previous example.</p>
<p><b>nd_permute.h</b> </p>
<div class="fragment"><div class="line"><span class="preprocessor">#ifndef ND_PERMUTE_H</span></div>
<div class="line"><span class="preprocessor">#define ND_PERMUTE_H</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &quot;api/amba-cnn-ext-c-types.h&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="stringliteral">&quot;C&quot;</span> {</div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="keywordtype">int</span></div>
<div class="line">permute_node_init(<span class="keywordtype">int</span>* cid, amba_cnn_c_vcoord_t* sz,</div>
<div class="line">                <span class="keywordtype">int</span> num, <span class="keyword">const</span> amba_cnn_c_vcoord_t* isz, <span class="keyword">const</span> <span class="keywordtype">char</span>* attr,</div>
<div class="line">                <span class="keyword">const</span> amba_cnn_c_data_format_t*);</div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="keywordtype">int</span></div>
<div class="line">permute_node_expand(<span class="keywordtype">int</span> cid, <span class="keyword">const</span> amba_cnn_ext_c_prim_factories_t* funcs);</div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="keywordtype">int</span></div>
<div class="line">permute_node_release(<span class="keywordtype">int</span> cid);</div>
<div class="line"> </div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#endif</span></div>
</div><!-- fragment --><h3><a class="anchor" id="subb_sec_export_cc"></a>
1.3 export.cc</h3>
<p>As introduced in <a class="el" href="../../d2/d67/fs_cnngen.html#sub_sec_custom_api_query">20.2.4 query() Function</a>, users must implement a <b>query()</b> function. This function can be implemented in the <em>export.cc</em> file (see the following code).</p>
<p><b>export.cc</b> </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;export.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;nd_permute.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdio&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#define NUM_CUSTOM_NODE (1)</span></div>
<div class="line"><span class="keyword">static</span> amba_cnn_ext_c_node_type_t _types[NUM_CUSTOM_NODE];</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">bool</span> _init = <span class="keyword">false</span>;</div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">int</span> query_node_types(<span class="keywordtype">int</span>* num_types, <span class="keyword">const</span> amba_cnn_ext_c_node_type_t** types)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordflow">if</span> (!_init) {</div>
<div class="line">        _types[0].type_name = <span class="stringliteral">&quot;Permute&quot;</span>;</div>
<div class="line">        _types[0].var_src = <span class="keyword">false</span>;</div>
<div class="line">        _types[0].num_out = 1;</div>
<div class="line">        _types[0].init = &amp;permute_node_init;</div>
<div class="line">        _types[0].expand = &amp;permute_node_expand;</div>
<div class="line">        _types[0].release = &amp;permute_node_release;</div>
<div class="line"> </div>
<div class="line">        _init = <span class="keyword">true</span>;</div>
<div class="line">    }</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> (num_types == NULL) {</div>
<div class="line">        <span class="keywordflow">return</span> 1;</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">if</span> (types == NULL) {</div>
<div class="line">        <span class="keywordflow">return</span> 1;</div>
<div class="line">    }</div>
<div class="line">    *num_types = NUM_CUSTOM_NODE;</div>
<div class="line">    *types = _types;</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">return</span> 0;</div>
<div class="line">}</div>
</div><!-- fragment --><h3><a class="anchor" id="subb_sec_export_h"></a>
1.4 export.h</h3>
<p>This is the head file that corresponds to the <em>export.cc</em> file shown in the previous section.</p>
<p><b>export.h</b> </p>
<div class="fragment"><div class="line"><span class="preprocessor">#ifndef EXPORT_H</span></div>
<div class="line"><span class="preprocessor">#define EXPORT_H</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &quot;api/amba-cnn-ext-c-types.h&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="stringliteral">&quot;C&quot;</span> {</div>
<div class="line"> </div>
<div class="line"><span class="keyword">extern</span> <span class="keywordtype">int</span></div>
<div class="line">query_node_types(<span class="keywordtype">int</span>* num_types, <span class="keyword">const</span> amba_cnn_ext_c_node_type_t** types);</div>
<div class="line"> </div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#endif</span></div>
</div><!-- fragment --><h3><a class="anchor" id="subb_sec_custom_example_makefile"></a>
1.5 Makefile</h3>
<p>To compile the source code listed above, Ambarella provides a Makefile for implementation.</p>
<p><b>Makefile</b> </p>
<div class="fragment"><div class="line">SRC :=$(wildcard *.cc)</div>
<div class="line">OBJS := $(patsubst %.cc,%.o,$(wildcard *.cc))</div>
<div class="line">CC :=g++</div>
<div class="line">CFLAGS :=-I. -I`tv2 -incpath AmbaCnn` -std=c++0x -pthread -fpic -O0  -g -Wall -Werror -Wall -Werror -Wno-deprecated -fno-strict-aliasing</div>
<div class="line">LDFLAGS :=-shared -fpic -std=c++0x -pthread -ldl -O0  -g</div>
<div class="line">LIB_CUSTOM :=custom_node.so</div>
<div class="line">all: $(LIB_CUSTOM)</div>
<div class="line">EXCLUDE_OBJECTS :=</div>
<div class="line">$(OBJS):%.o:%.cc</div>
<div class="line">    $(CC)  $(CFLAGS) -c $&lt; -o $@</div>
<div class="line">OUTPUT_OBJECTS := $(filter-out $(EXCLUDE_OBJECTS),$(OBJS))</div>
<div class="line">$(LIB_CUSTOM):$(OBJS)</div>
<div class="line">    $(CC) $(OUTPUT_OBJECTS) $(LDFLAGS) -o $@</div>
<div class="line">clean:</div>
<div class="line">    rm -rf *.so *.o</div>
</div><!-- fragment --><h3><a class="anchor" id="subb_sec_custom_nodes_py"></a>
1.6 Custom_nodes.py</h3>
<p>As mentioned in sub_sec_example_node_h, users are expected to create a Python script <em>custom_nodes.py</em> which helps CNNGen use the custom node (see the following code).</p>
<p><b>custom_nodes.py</b> </p>
<div class="fragment"><div class="line"><span class="preprocessor">#!/usr/bin/python3</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> sys, os, subprocess</div>
<div class="line"> </div>
<div class="line">lib_path = subprocess.check_output([<span class="stringliteral">&#39;tv2&#39;</span>, <span class="stringliteral">&#39;-basepath&#39;</span>, <span class="stringliteral">&#39;AmbaCnnUtils&#39;</span>])</div>
<div class="line">sys.path.append(lib_path.decode().rstrip(<span class="charliteral">&#39;\n&#39;</span>) + <span class="stringliteral">&#39;/parser/caffe/&#39;</span>)</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> caffe_common as cfc</div>
<div class="line"> </div>
<div class="line">def parse_permute_node(layer_obj):</div>
<div class="line"> </div>
<div class="line">    node = {}</div>
<div class="line">    src_list = []</div>
<div class="line">    permute_attr = [<span class="stringliteral">&#39;&#39;</span>] * 5</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">    # initialize node - id, type, description</span></div>
<div class="line">    node = cfc.init_node(layer_obj.name, <span class="stringliteral">&#39;Permute&#39;</span>, layer_obj.name)</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">    #Find the correct parent node for a given node</span></div>
<div class="line">    src_list.append(cfc.get_parent_node(layer_obj.bottom[0]))</div>
<div class="line">    node[<span class="stringliteral">&#39;src&#39;</span>] = src_list.copy()</div>
<div class="line"><span class="preprocessor">    #Update blob-layer dictionary for your custom node</span></div>
<div class="line">    cfc.add_to_blob_layer_dict(layer_obj.top[0], layer_obj.name)</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> len(layer_obj.permute_param.order):</div>
<div class="line">        permute_attr[0] = layer_obj.permute_param.order[0]</div>
<div class="line">        permute_attr[1] = layer_obj.permute_param.order[1]</div>
<div class="line">        permute_attr[2] = layer_obj.permute_param.order[2]</div>
<div class="line">        permute_attr[3] = layer_obj.permute_param.order[3]</div>
<div class="line"> </div>
<div class="line">    node[<span class="stringliteral">&#39;attr_str&#39;</span>] = <span class="stringliteral">&quot; &quot;</span>.join(str(x) for x in permute_attr)</div>
<div class="line"> </div>
<div class="line">    cfc.node_list.append(node)</div>
<div class="line"> </div>
<div class="line">    return node[<span class="stringliteral">&#39;id&#39;</span>]</div>
<div class="line"> </div>
<div class="line"># Entry function for custom node parser</div>
<div class="line"># User is expected to implement <span class="stringliteral">&quot;parse_custom_node&quot;</span> function</div>
<div class="line">def parse_custom_node(layer, caffe_net=None, coeff_folder=None):</div>
<div class="line">    node_id = None</div>
<div class="line"> </div>
<div class="line">    layer_type = layer.type</div>
<div class="line"> </div>
<div class="line">    if layer_type == <span class="stringliteral">&#39;Permute&#39;</span>:</div>
<div class="line">        node_id = parse_permute_node(layer)</div>
<div class="line"> </div>
<div class="line">    else:</div>
<div class="line">        node_id = <span class="stringliteral">&#39;CUSTOM_OP_NO_MATCH&#39;</span></div>
<div class="line"> </div>
<div class="line">    return node_id</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_custom_example_caffe_model"></a>
2 Caffe model</h2>
<p>To test the functionality of the custom node, users need to prepare a Caffe model that contains at least one permute layer. Then, CNNGen can be used to convert the model and verify the custom node.</p>
<h3><a class="anchor" id="subb_sec_custom_example_recompile_caffe"></a>
2.1 Re-compile Caffe</h3>
<p>Because the standard Caffe (BVLC) does not contain a permute layer, users can choose to either (A) change the currently used Caffe to a custom Caffe or (B) add the custom layers to the standard Caffe. Both methods are described in this section.</p>
<ul>
<li><b>Change to Custom Caffe</b></li>
</ul>
<p>If users would like to change their version of Caffe to a custom Caffe released by Wei Liu, it can be downloaded from <a href="https://github.com/weiliu89/caffe/tree/ssd">https://github.com/weiliu89/caffe/tree/ssd</a></p>
<p>After downloading, users can compile it and set it as the default Caffe.</p>
<ul>
<li><b>Add Custom Layers</b></li>
</ul>
<p>If users do not want to change their current version of Caffe, they can add the permute layers, and then re-compile Caffe. Download the Caffe released by Wei Liu from <a href="https://github.com/weiliu89/caffe/tree/ssd">https://github.com/weiliu89/caffe/tree/ssd</a></p>
<p>This version is named <b>Caffe-SSD</b>.</p>
<p>After downloading, users must copy the <em>permute_layer.hpp</em> file under <em>&lt;Caffe-SSD_root&gt;/include/caffe/layers</em> to <em>&lt;Standard_Caffe_root&gt;/include/caffe/layers</em></p>
<p>Next, users must copy the <em>permute_layer.cpp</em> file (and <em>permute_layer.cu</em> if they use GPU) under <em>&lt;Caffe-SSD root&gt;/src/caffe/layers</em> to <em>&lt;Standard Caffe root&gt;/src/caffe/layers</em></p>
<p>Finally, the permute layer needs to be registered in <em>caffe.proto</em> under <em>&lt;Standard_Caffe_root&gt;/src/caffe/proto</em></p>
<p>Add <em>optional PermuteParameter permute_param = 202</em>;</p>
<p>to <em>message LayerParameter</em> (around lines 326 to 424)</p>
<p>Also, add </p><pre class="fragment">message PermuteParameter {
    repeated uint32 order = 1;
}
</pre><p>at approximately line 918.</p>
<p>After finishing the procedures above, Caffe must be recompiled using the "build-caffe" script in CV toolchain package.</p>
<h3><a class="anchor" id="subb_sec_custom_example_prepare_prototxt_files"></a>
2.2 Prepare Prototxt Files</h3>
<p>Because the CNNGen uses <em>deploy.prototxt</em> and <em>*.caffemodel</em> as inputs and then converts the corresponding networks, users must create a <em>train.prototxt</em> containing <b>permute</b> to obtain the Caffe model for testing. For example, the following <em>train.prototxt</em> contains only an input layer and a permute layer.</p>
<p><b>train.prototxt</b> </p><pre class="fragment">layer {
    name: "data"
    type: "Input"
    top: "my_data"
    input_param {
    shape {
        dim:1
        dim:3
        dim:613
        dim:696
        }
    }
}
layer {
    name: "permute"
    type: "Permute"
    bottom: "my_data"
    top: "my_output"
    permute_param {
        order:0
        order:2
        order:3
        order:1
        }
}
</pre><dl class="section note"><dt>Note</dt><dd>In the example above, because the content of <em>deploy.prototxt</em> file is the same as the content in <em>train.prototxt</em>, only <em>train.prototxt</em> is required.</dd></dl>
<p>To train a network under Caffe framework, provide a <em>solver.prototxt</em> file as shown below.</p>
<p><b>solver.prototxt</b> </p><pre class="fragment">train_net: "train.prototxt"
base_lr: 0.1
display: 0
max_iter: 1
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 1
snapshot: 1
snapshot_prefix: "test_permute"
solver_mode: CPU
device_id: 0
</pre><h3><a class="anchor" id="subb_sec_custom_example_get_caffemodel_file"></a>
2.3 Get caffemodel File</h3>
<p>The following example provides a Python script to get the <em>*.caffemodel</em> file.</p>
<p><b>train.py</b> </p>
<div class="fragment"><div class="line"><span class="keyword">import</span> sys</div>
<div class="line"><span class="keyword">import</span> caffe</div>
<div class="line">from skimage <span class="keyword">import</span> io</div>
<div class="line">from skimage <span class="keyword">import</span> transform</div>
<div class="line"> </div>
<div class="line">img = io.imread(‘dra_img/test.jpg<span class="stringliteral">&#39;) #RGB</span></div>
<div class="line"><span class="stringliteral">img = transform.resize(img, (613, 696))</span></div>
<div class="line"><span class="stringliteral">img = img*0.00390625</span></div>
<div class="line"><span class="stringliteral">img = img.transpose((2, 0, 1))</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">caffe.set_mode_cpu()</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">solver = caffe.SGDSolver(&#39;</span>solver.prototxt<span class="stringliteral">&#39;)</span></div>
<div class="line"><span class="stringliteral">solver.net.blobs[&#39;</span>my_data<span class="stringliteral">&#39;].data[...] = img</span></div>
<div class="line"><span class="stringliteral">solver.solve()</span></div>
</div><!-- fragment --><p>The following command is used to get the <em>*.caffemodel</em> file. </p><pre class="fragment">build $ export PYTHONPATH=/&lt;users’ working space&gt;/ssd-caffe-install/usr/local/lib/python3.5/dist-packages:$PYTHONPATH
build $ export LD_LIBRARY_PATH=/&lt;users’ working space&gt;/ssd-caffe-install/usr/local/lib:$LD_LIBRARY_PATH
build $ python3 train.py (prepare the jpg file first as above path in above train.py)
</pre><p>After executing the commands above, users receive a file called <em>test_permute_iter_1.caffemodel</em>.</p>
<h2><a class="anchor" id="sub_sec_custom_example_convert"></a>
3 Convert</h2>
<p>At this point in the workflow, users should have three folders: <em>custom_nodes</em>, <em>test_img</em>, and <em>models</em>. The contents of the folders are as follows:</p>
<p><b>custom_nodes:</b> </p>
<ul>
<li>Makefile</li>
<li><em>custon_nodes.py</em></li>
<li><em>export.cc and export.h</em></li>
<li><em>nd_permute.cc and nd_permute.h</em></li>
</ul>
<p><b>dra_img:</b> </p>
<ul>
<li><em>test.jpg</em></li>
</ul>
<p><b>models:</b> </p>
<ul>
<li><em>train.prototxt</em></li>
<li><em>test_permute_iter_1.caffemodel</em></li>
</ul>
<h3><a class="anchor" id="subb_sec_custom_example_compile_custom_so"></a>
3.1 Compile custom.so</h3>
<p>Compile as follows: </p><pre class="fragment">build $ cd custom_nodes
build $ make clean
build $ make (It will generate custom_node.so)
</pre><h3><a class="anchor" id="subb_sec_custom_example_prepare_dra_image"></a>
3.2 Prepare DRA Image</h3>
<p>The following command shows an example of using one <b>jpg</b> file in the <em>dra_img</em> folder to generate a default DRA file, <em>dra_bin_list.txt</em>.</p>
<dl class="section note"><dt>Note</dt><dd>The <em>test_img</em> folder is the same as that introduced in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_gen_image_list_py">1 gen_image_list.py</a>.</dd></dl>
<h3><a class="anchor" id="subb_sec_custom_generate_the_vas_code"></a>
3.3 Generate the VAS Code</h3>
<p>The following commands show how to convert the Caffe model to primitive and VAS codes (the dynamic linked library <em>custom_node.so</em> and Python script <em>custom_nodes.py</em> are used). </p><pre class="fragment">build $ export PYTHONPATH=/&lt;users’ working space&gt;/ssd-caffe-install/usr/local/lib/python3.5/dist-packages:$PYTHONPATH
build $ export LD_LIBRARY_PATH=/&lt;users’ working space&gt;/ssd-caffe-install/usr/local/lib:$LD_LIBRARY_PATH
build $ caffeparser.py -p models/train.prototxt -m models/test_permute_iter_1.caffemodel -i dra_bin_list.txt -o test_permute -of out_fix8_full
    -iq -idf 0,0,0,0 -ic 256 -c coeff-force-fx8,act-force-fx8 -cp custom_nodes/custom_nodes.py -cd custom_nodes/custom_node.so
</pre><h3><a class="anchor" id="subb_sec_custom_compile_vas_code"></a>
3.4 Compile VAS Code</h3>
<p>The following commands show how to compile the VAS code described in sub_sec_custom_example_get_caffemodel_file. </p><pre class="fragment">build $ cd out_fix8_full/
build $ vas -auto -dvi test_permute.vas
</pre><h3><a class="anchor" id="subb_sec_custom_generate_the_ades_command"></a>
3.5 Generate the ADES Command</h3>
<p>The following commands are used to generate the ADES command and can be run on a PC. </p><pre class="fragment">build $ cd ../
build $ ades_autogen.py -v test_permute -p out_fix8_full -l ades_fix8_full
    -ib data=&lt;absolute path to test_img&gt;/dra_img/test.bin
</pre><h3><a class="anchor" id="subb_sec_custom_run_ades_command"></a>
3.6 Run ADES Command</h3>
<p>The following commands are used to run the ADES command described above. </p><pre class="fragment">build $ cd ades_fix8_full/
build $ ades test_permute_ades.cmd
</pre><p>If users have successfully run the custom node '<b>permute</b>', then the following information appears. </p><pre class="fragment">Copyright 2015-2018 Ambarella Inc.
... ... ... ...
... ... ... ...
[scripting] processing line: sb permute permute.bin
[scripting] completed.
</pre><hr  />
<h1><a class="anchor" id="sec_yolov3_quant_deploy"></a>
Deployment for Quantization Retrained Yolov3</h1>
<p>For instructions on how to use AmbaCaffe to retrain Yolov3 with pruning and quantization, refer to the documents under Ambarella CV UG AmbaCaffe.</p>
<dl class="section note"><dt>Note</dt><dd>This demo will be only valid in old CNNGen toolchain in Ubuntu 1604 and Ubuntu1804, since Ubuntu2005, Caffe is not maintained any more.</dd></dl>
<h2><a class="anchor" id="subsec_yolov3_quant_deployment"></a>
1 Deployment</h2>
<p>The section below introduces how to deploy a retrained model with CNNGen tools. For easy reproduction on the customer side, the CNNGen samples package includes the example below. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">     [*] Build Caffe Demo Networks  ---&gt;</div>
<div class="line">        [*] Build Caffe YOLOV3 Network  ---&gt;</div>
<div class="line">          Sparse_Ratio  ---&gt;</div>
<div class="line">            [*] SP50_AMB_QUANT  ---&gt;</div>
</div><!-- fragment --> <div class="fragment"><div class="line">build $ make yolo_v3_sp50_amb_quant run_mode=cavalry</div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin</div>
<div class="line">mkdir cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin</div>
<div class="line">gen_image_list.py -f cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/dra_img/ -o cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/img_list.txt -ns \</div>
<div class="line">  -e jpg -c 0 -d 0,0 -r 416,416 \</div>
<div class="line">  -bf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin \</div>
<div class="line">  -bo cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt</div>
<div class="line"> </div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser</div>
<div class="line">caffeparser.py -p cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_amb_quant.prototxt \</div>
<div class="line">  -m cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_13800_amb_quant.caffemodel \</div>
<div class="line">  -isrc <span class="stringliteral">&quot;is:1,3,416,416|iq|idf:0,0,8,0|i:data=cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt&quot;</span> \</div>
<div class="line">  -sb cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_sb.json \</div>
<div class="line">  -o yolo_v3_sp50_amb_quant \</div>
<div class="line">  -of cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser \</div>
<div class="line">  -c coeff-force-fx8,act-force-fx8,no-pre-dr-ccr,no-post-dr-ccr,no-cf,no-c2d,no-lpe,no-rcdf \</div>
<div class="line">  -dra allow_scaling=False -odst <span class="stringliteral">&quot;o:conv59|odf:fp32&quot;</span> \</div>
<div class="line">  -odst <span class="stringliteral">&quot;o:conv67|odf:fp32&quot;</span> -odst <span class="stringliteral">&quot;o:conv75|odf:fp32&quot;</span></div>
<div class="line"> </div>
<div class="line">echo <span class="stringliteral">&quot;VAS output node name is conv59,conv67,conv75&quot;</span></div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser/vas_output</div>
<div class="line">cd cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser;vas -<span class="keyword">auto</span> -show-progress yolo_v3_sp50_amb_quant.vas</div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/cavalry_yolo_v3_sp50_amb_quant</div>
<div class="line">mkdir cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/cavalry_yolo_v3_sp50_amb_quant</div>
<div class="line">cp -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/COCO_val2014_000000000042.bin cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/cavalry_yolo_v3_sp50_amb_quant</div>
<div class="line">cavalry_gen -d cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser/vas_output/ \</div>
<div class="line">  -f cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/cavalry_yolo_v3_sp50_amb_quant/yolo_v3_sp50_amb_quant_cavalry.bin \</div>
<div class="line">  -p cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/ \</div>
<div class="line">  -v &gt; cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/cavalry_yolo_v3_sp50_amb_quant/<a class="codeRef" href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a>.txt</div>
</div><!-- fragment --><p>Refer to the code below for the most important step in this deployment.</p>
<div class="fragment"><div class="line">build $ caffeparser.py -p cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_amb_quant.prototxt \</div>
<div class="line">      -m cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_13800_amb_quant.caffemodel \</div>
<div class="line">      -isrc <span class="stringliteral">&quot;is:1,3,416,416|iq|idf:0,0,8,0|i:data=cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt&quot;</span> \</div>
<div class="line">      -sb cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_sb.json \</div>
<div class="line">      -o yolo_v3_sp50_amb_quant \</div>
<div class="line">      -of cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser \</div>
<div class="line">      -c coeff-force-fx8,act-force-fx8,no-pre-dr-ccr,no-post-dr-ccr,no-cf,no-c2d,no-lpe,no-rcdf \</div>
<div class="line">      -dra allow_scaling=False -odst <span class="stringliteral">&quot;o:conv59|odf:fp32&quot;</span> \</div>
<div class="line">      -odst <span class="stringliteral">&quot;o:conv67|odf:fp32&quot;</span> -odst <span class="stringliteral">&quot;o:conv75|odf:fp32&quot;</span></div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li><em>"i:***"</em> and <em>"-sb ***"</em>, in this deployment, enable both DRA and JSON. With this setting, the data formats are determined by the JSON sideband file and DRA together. This is because the old sideband file cannot be guaranteed to work with the latest CNNGen tool, unless the new version has same primitive IDs as old version. Unfortunately, primitive IDs always change due to optimization steps or orders. So during conversion with old sideband file and new CNNGen tool, some primitive IDs in the sideband file will match the compiled graph, and some will not. Then, the sideband file will only be applied to the primitives that have matches, and the rest will be determined by DRA. With this method, the final result is slightly different from the quantization retraining result in PC. The user can ignore these differences in the accuracy.</li>
<li><p class="startli">If accuracy problems are important to the user, it is possible to only enable the JSON file (as shown below). The right CNNGen tool version is needed to generate the retrained JSON file, but this cannot be the latest CNNGen version.</p>
<p class="startli">build $ caffeparser.py -p cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_amb_quant.prototxt \ -m cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_13800_amb_quant.caffemodel \ -isrc "is:1,3,416,416|iq|idf:0,0,8,0|i:data" -sb cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_sb.json \ -o yolo_v3_sp50_amb_quant \ -of cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser \ -c coeff-force-fx8,act-force-fx8,no-pre-dr-ccr,no-post-dr-ccr,no-cf,no-c2d,no-lpe,no-rcdf \ -dra allow_scaling=False -odst "o:conv59|odf:fp32" \ -odst "o:conv67|odf:fp32" -odst "o:conv75|odf:fp32"</p>
</li>
<li>With step 2, ensure that the final deployment command is identical to the previous command used to generate the JSON files. For example, when generating the files before retraining, do not use <em>"-im"</em> and <em>"-ic"</em> as the value is 0 and 1, but when deployed, use <em>"-im 0"</em> and <em>"-ic 1"</em>. Although 0 and 1 signify nothing, they will affect the primitive IDs and result in problems matching the JSON file and the new primitive code.</li>
<li><em>"no-pre-dr-ccr,no-post-dr-ccr,no-cf,no-c2d,no-lpe,no-rcdf"</em>, some CNNGen optimization tricks do network surgery to reduce the quantization errors for the retraining case. For quantization cases, disable these tricks.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="subsec_yolov3_quant_layer_compare"></a>
2 Layer_compare</h2>
<p>Run "layer_compare" below with the CNNGen samples package. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">  [*] Build Caffe Demo Networks  ---&gt;</div>
<div class="line">    [*] Build Caffe YOLOV3 Network  ---&gt;</div>
<div class="line">      Sparse_Ratio  ---&gt;</div>
<div class="line">        [*] SP50_AMB_QUANT  ---&gt;</div>
<div class="line"> </div>
<div class="line">build $ make yolo_v3_sp50_amb_quant run_mode=layer_compare</div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin</div>
<div class="line">mkdir cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin</div>
<div class="line">gen_image_list.py -f cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/dra_img/ -o cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/img_list.txt -ns \</div>
<div class="line">    -e jpg -c 0 -d 0,0 -r 416,416 \</div>
<div class="line">    -bf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin \</div>
<div class="line">    -bo cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt</div>
<div class="line"> </div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser</div>
<div class="line">caffeparser.py -p cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_amb_quant.prototxt \</div>
<div class="line">    -m cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_13800_amb_quant.caffemodel \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;is:1,3,416,416|iq|idf:0,0,8,0|i:data=cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt&quot;</span> \</div>
<div class="line">    -sb cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_sb.json \</div>
<div class="line">    -o yolo_v3_sp50_amb_quant \</div>
<div class="line">    -of cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser \</div>
<div class="line">    -c coeff-force-fx8,act-force-fx8,no-pre-dr-ccr,no-post-dr-ccr,no-cf,no-c2d,no-lpe,no-rcdf \</div>
<div class="line">    -dra allow_scaling=False -odst <span class="stringliteral">&quot;o:conv59|odf:fp32&quot;</span> -odst <span class="stringliteral">&quot;o:conv67|odf:fp32&quot;</span> -odst <span class="stringliteral">&quot;o:conv75|odf:fp32&quot;</span></div>
<div class="line"> </div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser/vas_output</div>
<div class="line">cd cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser;vas -<span class="keyword">auto</span> -show-progress yolo_v3_sp50_amb_quant.vas</div>
<div class="line">rm -rf cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/layer_compare_yolo_v3_sp50_amb_quant; mkdir -p cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/layer_compare_yolo_v3_sp50_amb_quant</div>
<div class="line">layer_compare.py caffe -p cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_amb_quant_with_imme.prototxt \</div>
<div class="line">    -m cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v3/models/yolov3_0.5_13800_amb_quant.caffemodel \</div>
<div class="line">    -v cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/out_yolo_v3_sp50_amb_quant_parser \</div>
<div class="line">    -o cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/layer_compare_yolo_v3_sp50_amb_quant/layer_compare_yolo_v3_sp50_amb_quant \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;is:1,3,416,416|iq|idf:0,0,8,0|i:data=cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/dra_image_bin/dra_bin_list.txt&quot;</span></div>
<div class="line">mv ades lc_caffe_output lc_cnn_output preproc yolov3_0.5_amb_quant_with_imme_modified.prototxt -t cvflow_cnngen_samples_&lt;version&gt;/out/caffe/demo_networks/yolo_v3_sp50_amb_quant/layer_compare_yolo_v3_sp50_amb_quant</div>
<div class="line">make[1]: Leaving directory <span class="stringliteral">&#39;cvflow_cnngen_samples_&lt;version&gt;/caffe&#39;</span></div>
</div><!-- fragment --><p>As shown in the command above, the caffeparser uses a different prototxt with <em>layer_compare</em>. The difference between these two prototxts is provided below.</p><ul>
<li>The prototxt for the parser is an original FP32 prototxt that can work with any version of Caffe. The prototxt for <em>layer_compare</em> is a quantization prototxt, which only runs with AmbaCaffe because it contains the quantization layers.</li>
<li>Furthermore, the quantization prototxt uses <em>"*imm_e.bin"</em> in the <em>"out"</em> folder that the parser generates. With this method, when the final report is generated by layer_compare, users can find the <b>ADES</b> result is identical to the result run by AmbaCaffe** with the quantization prototxt, so the user will know the convert process is accurate.</li>
<li>Another possible problem arises when the quantization prototxt does not use the old <em>"*imm_e.bin"</em> in "caffe/demo_networks/yolo_v3/models/infos_before_retrain/imme_bin" which is generated before retraining. This happens because the user enabled the JSON file without the DRA. The new <em>"*imm_e.bin"</em> generated by the parser has only slight differences from using the old <em>"*imm_e.bin"</em>. There will only be a minor gap between final <b>ADES</b> result and <b>AmbaCaffe</b>. If users need to ensure that the ADES result is identical to the retraining result,perform the following:<ul>
<li>Use JSON with no DRA to convert the retrained model;</li>
<li>Replace all of the new <em>"*imm_e.bin"</em> with the old <em>"*imm_e.bin"</em>.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="subsec_yolov3_quant_accuracy"></a>
3 Accuracy</h2>
<p>For accuracy, run the test below with the CNNGen samples package. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">    [*] Build Caffe Demo Networks  ---&gt;</div>
<div class="line">      [*] Build Caffe YOLOV3 Network  ---&gt;</div>
<div class="line">        Sparse_Ratio  ---&gt;</div>
<div class="line">          [*] SP50_AMB_QUANT  ---&gt;</div>
<div class="line">build $ make yolo_v3_sp50_amb_quant run_mode=accuracy</div>
</div><!-- fragment --><p> The <b>mAP</b> takes <b>0.5075</b> with 2500 images in the COCO_2014 dataset. The original YOLOv3 model accuracy is approximately <b>0.5873</b>. To mitigate accuracy loss, users can do more retraining on the pruning and quantization model.</p>
<hr  />
<h1><a class="anchor" id="sec_caffe_face_landmarks_68"></a>
Face Alignment</h1>
<p>Face detection and face alignment work in a complementary manner. A CNN network locates the sub-image of each face in the entire picture for face detection; Another CNN model locates 68 landmark points on a face for face alignment. This section explains how to deploy the face alignment demo. In this demo, the MTCNN network is used to detect the faces; a custom Caffe network is used to locate 68 landmark points on a face.</p>
<p>The process steps in the demo can be summarized as shown below.</p>
<ol type="1">
<li>Detect bounding boxes of faces and 5 landmark points on each face by running MTCNN.</li>
<li>Generate input images for the custom Caffe network by adjusting the position of each bounding box.</li>
<li>Generate 68 landmark points on each face by using the custom Caffe model.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>The custom network is trained by the bounding boxes detected by dlib. The position of the bounding box around each face is slightly different from the detection result of MTCNN. Therefore, in this demo, the position of each bounding box detected by MTCNN is adjusted by moving the nose more central.</dd></dl>
<h2><a class="anchor" id="caffe_face_landmarks_68_download_model"></a>
1 Download Model</h2>
<p>The original model which detects 68 landmark points can be downloaded from <a href="https://github.com/lsy17096535/face-landmark/tree/master/model">here</a>. </p><div class="fragment"><div class="line">commit 34c8a893fee03eb3b6e5b90576fb335bbfc99571</div>
</div><!-- fragment --><p> The pre-trained model files <em>VanFace.caffemodel</em> and <em>landmark_deploy.prototxt</em> are used to generate Cavalry binary.</p>
<h2><a class="anchor" id="caffe_face_landmarks_68_generate_cavalry_binary"></a>
2 Generate Cavalry Binary</h2>
<p>Generate the Cavalry binary as below. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/face_landmarks_68/config/ea_cvt_face_landmarks_68.yaml</div>
</div><!-- fragment --><p> The output is in <code>out/caffe/demo_networks/face_landmarks_68/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/face_landmarks_68</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_face_landmarks_68.bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/face_landmarks_68/&lt;chip&gt;/&lt;chip&gt;_face_landmarks_68</code>.</li>
<li>For X86 simulator, model desc json file <b>face_landmarks_68.json</b> is in the cnngen output folder <code>out/caffe/demo_networks/face_landmarks_68/out_face_landmarks_68_parser/</code>. ades command <b>face_landmarks_68_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/face_landmarks_68/&lt;chip&gt;/&lt;chip&gt;_ades_face_landmarks_68</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="caffe_face_landmarks_68_build_evk_binary"></a>
3 Build EVK Binary</h2>
<p>Build the EVK binary as follows: </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">      [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">          [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">              [*] Build EazyAI applications  ---&gt;</div>
<div class="line">                  [*] Build FACE EazyAI apps  ---&gt;</div>
<div class="line">                      [*] Build FACE ALIGNMENT EazyAI apps</div>
<div class="line">build $ make test_face_align</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Select "[ ] Build eazyai library with OpenCV support" if the file mode will be run.</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_face_landmarks_68_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/face_landmarks_68/&lt;chip&gt;/&lt;chip&gt;_cavalry_face_landmarks_68/&lt;chip&gt;_cavalry&lt;version&gt;_face_landmarks_68.bin</div>
</div><!-- fragment --></li>
<li>File Mode (Without Postprocess) <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/face_landmarks_68/face_landmarks_68_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/face_landmarks_68</div>
</div><!-- fragment --></li>
<li>Accuracy Mode (Not Supported) <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_face_landmarks_68_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_caffe_face_landmarks_68_run_c_inference"></a>
5 Run C Inference</h2>
<ol type="1">
<li><p class="startli">Copy Cavalry binary models to SD card on the board.</p>
<p class="startli">The pnet model name must be <em>pnet*.bin</em>, the rnet model name must be <em>rnet*.bin</em>, the onet model name must be <em>onet*.bin</em> and the face_landmarks_68 model name must be <em>face_landmarks_68_cavalry.bin</em> for read by the <em>test_face_align</em> demo.</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/face_align</div>
<div class="line">|--in</div>
<div class="line">|       faces01.jpg</div>
<div class="line">|       faces02.jpg</div>
<div class="line">|       pnet_216x384_cavalry.bin</div>
<div class="line">|       pnet_154x273_cavalry.bin</div>
<div class="line">|       pnet_109x194_cavalry.bin</div>
<div class="line">|       pnet_77x137_cavalry.bin</div>
<div class="line">|       pnet_55x98_cavalry.bin</div>
<div class="line">|       pnet_39x69_cavalry.bin</div>
<div class="line">|       pnet_28x49_cavalry.bin</div>
<div class="line">|       pnet_20x35_cavalry.bin</div>
<div class="line">|       pnet_14x25_cavalry.bin</div>
<div class="line">|       rnet_cavalry.bin</div>
<div class="line">|       onet_cavalry.bin</div>
<div class="line">|       face_landmarks_68_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --></li>
<li>Initialize the environment on the CV board. Take CV22 Walnut and imx274_mipi for example. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 720p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, please use bigger value in <em>--enc_dummy_latency 4</em> such as 7. Then if the overlay buffer size is not enough, it can be added by changing the size in <em>--reallocate_mem overlay,0x01400000</em>.</dd></dl>
</li>
<li>Run the demonstration.<ol type="a">
<li>Stream live mode (draw on stream rtsp://10.0.0.2/stream1) <div class="fragment"><div class="line">board # test_face_align -m 0 -i /sdcard/face_align/in -o /sdcard/face_align/out -b 0 -a 1 -s 0</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_face_align -m 0 -i /sdcard/face_align/in -o /sdcard/face_align/out -b 0</div>
</div><!-- fragment --></li>
<li>File mode (the input of JPEG image files are placed under <em>/sdcard/face_align/in</em>, result of JPEG image files are generated under <em>/sdcard/face_align/out</em>) <div class="fragment"><div class="line">board # test_face_align -m 1 -i /sdcard/face_align/in -o /sdcard/face_align/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_caffe_mobilefacenets"></a>
Face Recognition</h1>
<p>Face detection and face recognition (FDFR) function works in a complementary manner. A CNN model locates the sub-image of each face in the entire picture for face detection; Another CNN model converts the image of each face into a feature vector for face recognition. This section explains how to deploy the face recognition demo. In this demo, the MTCNN network or the RetinaFace network is used to detect the faces; the MobileFaceNets network is used to generate feature vectors.</p>
<p>The steps performed in the demo is as below.</p>
<ol type="1">
<li>Detect five landmark points on each face by running MTCNN or RetinaFace.</li>
<li>Generate feature vectors of each face by running MobileFaceNets.</li>
<li>Recognize each detected face by calculating cosine similarity between the generated feature vectors and the recorded feature vectors with label in the database.</li>
</ol>
<h2><a class="anchor" id="caffe_mobilefacenets_download_model"></a>
1 Download Model</h2>
<p>For MTCNN model, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_mtcnn">MTCNN</a>.</p>
<p>For RetinaFace model, please refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sec_onnx_retinaface">RetinaFace</a>.</p>
<p>For MobileFaceNets, the original model can be downloaded from <a href="https://github.com/zhanglaplace/MobileFaceNet">https://github.com/zhanglaplace/MobileFaceNet</a>.</p>
<div class="fragment"><div class="line">commit d919a84550a98e59a593122db072be857812373a</div>
</div><!-- fragment --><p>The pre-trained model files <b>face_snapshot/MobileFaceNet.caffemodel</b> and <b>MobileFaceNet_deploy.prototxt</b> are used to generate the Cavalry binary.</p>
<h2><a class="anchor" id="caffe_mobilefacenets_generate_cavalry_binary"></a>
2 Generate Cavalry Binary</h2>
<p>For MTCNN, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_mtcnn">MTCNN</a>.</p>
<p>For MobileFaceNets, generate the Cavalry binary as below. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/mobilefacenets/config/ea_cvt_mobilefacenets.yaml</div>
</div><!-- fragment --><p> The output is in <code>out/caffe/demo_networks/mobilefacenets/cavalry_mobilefacenets</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/mobilefacenets</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_mobilefacenets.bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/mobilefacenets/&lt;chip&gt;/&lt;chip&gt;_mobilefacenets</code>.</li>
<li>For X86 simulator, model desc json file <b>mobilefacenets.json</b> is in the cnngen output folder <code>out/caffe/demo_networks/mobilefacenets/out_mobilefacenets_parser/</code>. ades command <b>mobilefacenets_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/mobilefacenets/&lt;chip&gt;/&lt;chip&gt;_ades_mobilefacenets</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="caffe_mobilefacenets_build_evk_binary"></a>
3 Build EVK Binary</h2>
<p>There are two versions of FDFR, one is MTCNN and MobileFaceNets, and another is RetinaFace and MobileFaceNets.</p>
<ol type="1">
<li>Build face recognition v1 EVK binary. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">      [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">          [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">              [*] Build EazyAI applications  ---&gt;</div>
<div class="line">                  [*] Build FACE EazyAI apps  ---&gt;</div>
<div class="line">                      [*] Build FDFR V1 EazyAI apps</div>
<div class="line"> build $ make test_fdfr_v1</div>
</div><!-- fragment --></li>
<li>Build face recognition v2 EVK binary. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">     [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">         [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">             [*] Build EazyAI applications  ---&gt;</div>
<div class="line">                 [*] Build FACE EazyAI apps  ---&gt;</div>
<div class="line">                     [*] Build FDFR V2 EazyAI apps</div>
<div class="line"> build $ make test_fdfr_v2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Select "[ ] Build eazyai library with OpenCV support" if the file mode will be run.</li>
</ul>
</dd></dl>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_mobilefacenets_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/mobilenetfacenets/&lt;chip&gt;/&lt;chip&gt;_cavalry_mobilenetfacenets/&lt;chip&gt;_cavalry&lt;version&gt;_mobilenetfacenets.bin</div>
</div><!-- fragment --></li>
<li>File Mode (Without Postprocess) <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/mobilenetfacenets/mobilenetfacenets_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/mobilenetfacenets</div>
</div><!-- fragment --></li>
<li>Accuracy Mode (Not Supported) <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#caffe_mobilefacenets_run_c_inference_with_v1">5 Run C Inference With V1</a> and <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#caffe_mobilefacenets_run_c_inference_with_v2">6 Run C Inference With V2</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="caffe_mobilefacenets_run_c_inference_with_v1"></a>
5 Run C Inference With V1</h2>
<ol type="1">
<li><p class="startli">Copy Cavalry bin models to board.</p>
<p class="startli">The PNet model name must be <b>pnet*.bin</b>, the RNet model name must be <b>rnet*.bin</b>, the ONet model name must be <b>onet*.bin</b>, the MobileFaceNets model name must be <b>mobilefacenets_cavalry.bin</b> for read by the <b>test_fdfr_v1</b> demo.</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/fdfr</div>
<div class="line">|--in</div>
<div class="line">|       faces01.jpg</div>
<div class="line">|       faces02.jpg</div>
<div class="line">|       pnet_216x384_cavalry.bin</div>
<div class="line">|       pnet_154x273_cavalry.bin</div>
<div class="line">|       pnet_109x194_cavalry.bin</div>
<div class="line">|       pnet_77x137_cavalry.bin</div>
<div class="line">|       pnet_55x98_cavalry.bin</div>
<div class="line">|       pnet_39x69_cavalry.bin</div>
<div class="line">|       pnet_28x49_cavalry.bin</div>
<div class="line">|       pnet_20x35_cavalry.bin</div>
<div class="line">|       pnet_14x25_cavalry.bin</div>
<div class="line">|       rnet_cavalry.bin</div>
<div class="line">|       onet_cavalry.bin</div>
<div class="line">|       mobilefacenets_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --></li>
<li><p class="startli">Initialize the environment on the CV board.</p>
<p class="startli">Take CV22 Walnut and imx274_mipi for example. </p><div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 720p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, please use bigger value in <b>--enc_dummy_latency 4</b> such as 7. Then if the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x01400000</b>.</dd></dl>
</li>
<li>Run the demo.<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_fdfr_v1 -m 0 -i /sdcard/fdfr/in -b 0 -a 1 -s 0 -f 0.8,0.7</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_fdfr_v1 -m 0 -i /sdcard/fdfr/in -b 0 -a 0 -f 0.8,0.7</div>
</div><!-- fragment --></li>
<li>File mode (the input of JPEG image files are placed under <code>/sdcard/fdfr/in</code>, result of JPEG image files are generated under <code>/sdcard/fdfr/out</code>) <div class="fragment"><div class="line">board # test_fdfr_v1 -m 1 -i /sdcard/fdfr/in -o /sdcard/fdfr/out -f 0.8,0.7</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="caffe_mobilefacenets_run_c_inference_with_v2"></a>
6 Run C Inference With V2</h2>
<ol type="1">
<li><p class="startli">Copy Cavalry bin models to board.</p>
<p class="startli">The RetinaFace model name must be <b>onnx_retinaface_cavalry.bin</b>, the MobileFaceNets model name must be <b>mobilefacenets_cavalry.bin</b> for read by the <b>test_fdfr_v2</b> demo.</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/fdfr</div>
<div class="line">|--in</div>
<div class="line">|   faces01.jpg</div>
<div class="line">|   faces02.jpg</div>
<div class="line">|   onnx_retinaface_cavalry.bin</div>
<div class="line">|   mobilefacenets_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --></li>
<li><p class="startli">Initialize environment on the CV board.</p>
<p class="startli">Take CV22 Walnut and imx274_mipi for example. </p><div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 720p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, please use bigger value in <b>--enc_dummy_latency 4</b> such as 7. Then if the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x01400000</b>.</dd></dl>
</li>
<li>Run the demo.<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_fdfr_v2 -m 0 -i /sdcard/fdfr/in -b 0 -a 1 -s 0 -f 0.8,0.7</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_fdfr_v2 -m 0 -i /sdcard/fdfr/in -b 0 -a 0 -f 0.8,0.7</div>
</div><!-- fragment --></li>
<li>File mode (the input of JPEG image files are placed under <code>/sdcard/fdfr/in</code>, result of JPEG image files are generated under <code>/sdcard/fdfr/out</code>) <div class="fragment"><div class="line">board # test_fdfr_v2 -m 1 -i /sdcard/fdfr/in -o /sdcard/fdfr/out -f 0.8,0.7</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="caffe_mobilefacenets_record_face_data"></a>
7 Record Face Data</h2>
<p>When the demo is in live mode, press the 'f' button on the keyboard to begin the procedure. Follow the prompt on the terminal to add one item of face data into the database. The database file is saved in the same folder with the Cavalry model files, for example, <code>/sdcard/fdfr/in</code>.</p>
<p>The primary steps to record an item of face data are as below:</p>
<ol type="1">
<li>Press 'f' on the keyboard to start the procedure.</li>
<li>Enter '2' and then press Enter to choose "2. Record a new face."</li>
<li>Enter a name and then press Enter. If the name already exists in the database, reenter another name. After entering the name, the recording procedure begins. On the HDMI VOUT, the detected faces are drawn in boxes. Ensure that only one face is detected during the recording procedure.</li>
<li>Record up to 9 views. Take the straight direction first, then move your face slightly from the straight direction at 8 different angles (left, right, up, down, up-left, up-right, down-left, down-right), pressing 'r' one by one to record each view. After recording 9 views, the recording procedure exits. Users can press 'f' to begin again, or press 'f' then choose "1. Show database information" to check the database.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>On the HDMI VOUT or stream, if a face is recognized, the bounding box is drawn in green, if a face is not recognized, the bounding box is drawn in yellow.</li>
<li>The name and score of a recognized individual is shown on the terminal; for example: “++++++++++ Someone 0.743951”.</li>
<li>The number of faces drawn on the screen is configured by the <b>-e</b> (--max_faces) option.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="caffe_mobilefacenets_run_with_pyramid_buffer"></a>
8 Run with Pyramid Buffer</h2>
<p>In this example, face recognition demo does detection and recognition on pyramid buffers.</p>
<ol type="1">
<li>Run the following commands on the board. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --enc_dummy_latency 4 --pyramid_scale_type 2 --pyramid_layer_1_rescale_size 720x480 --pyramid_manual_map 0x03 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, please use bigger value in <b>--enc_dummy_latency 4</b> such as 7. Then if the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x01400000</b>.</dd></dl>
</li>
<li>Run live mode (draw on stream <code>rtsp://10.0.0.2/stream2</code>) <div class="fragment"><div class="line">oard # test_fdfr_v1 -m 0 -i /sdcard/fdfr/in -a 1 -s 0 -f 0.8,0.7 -k 0,1,0</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>The similarity transform in the source code assumes the shape of the face on the source image is normal. There is no width / height ratio correction based on 5 landmark points. Therefore, when performing face recording and recognition, it's better to maintain the same width / height ratio on the source images.</li>
<li>The similarity transform in the source code may generate noise when downscaling from a large size. So in this demo, the bigger face may not be better in the result.</li>
<li>The option <b>-k 0,1,0</b> specifies the IAV pyramid configuration in the following order: channel ID of the VIN, pyramid layer index for detection, and pyramid layer index for recognition.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
<h1><a class="anchor" id="sec_caffe_googlenet"></a>
GoogLeNet</h1>
<p>This section uses GoogLeNet as an example with <b>cavalry_gen</b> and <b>ADES</b>. It uses FX8 for the data format. Refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_env_set">3 Environment Setting</a> for more information on how to configure the environments.</p>
<h2><a class="anchor" id="sub_sec_googlenet_gen_image_list_py"></a>
1 gen_image_list.py</h2>
<p>Dynamic range analysis (DRA) is a quantization scheme in AmbaCNN which is used to generate the <em>gen_image_list.py</em> file in the default configuration. The following section describes how to modify the quantization.</p>
<p>The following command shows an example of using four .jpg files in the <em>dra_img</em> folder to generate a default DRA file, <em>dra_bin_list.txt</em>. </p><div class="fragment"><div class="line">build $ gen_image_list.py -f dra_img/ -o img_list.txt -ns -e jpg -c 1 -d 0,0 -r 224,224 -bf dra_image_bin -bo dra_bin_list.txt -N 4</div>
</div><!-- fragment --><p> In an actual scenario, more than four images are required.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>"-N": chooses random pictures that are in the defined folder "-f". If the folder has more pictures than "-N", every result will be different. If the user removes "-N", it will use all of the images with the extension "jpg" in this folder. "-ns" is used to disable the random selection setting.</li>
<li>The final DRA binaries are in the folder "dra_image_bin", and needs to be resized to " -r 224x224".</li>
<li>"-c 1" refers to BGR.</li>
<li>"-d 0,0" means unsigned fixed8, which also specifies that the option below "-iq -idf" should be defined as "0,0,0,0".</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_googlenet_caffeparser_py"></a>
2 caffeparser.py</h2>
<p>Use the <b>caffeparser.py</b> tool to convert it to primitive and VAS code. </p><div class="fragment"><div class="line">build $ caffeparser.py -p deploy.prototxt -m bvlc_googlenet.caffemodel -i dra_bin_list.txt -o bvlc_googlenet -of out_bvlc_googlenet_parser -im 104,117,123</div>
<div class="line">    -ic 1 -iq -idf 0,0,0,0 -c act-force-fx8,coeff-force-fx8  -odst <span class="stringliteral">&quot;o:prob|odf:fp32&quot;</span></div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>"-i dra_bin_list.txt" is generated by <em>gen_image_list.py</em>.</li>
<li>"-o" is the output folder.</li>
<li>"-iq -idf", if the user used "-iq", "-idf" must be defined for the input data format. Typically, "0,0,0,0" for <b>fix8 BGR</b> is acceptable if the user includes the meaning and scale in the network. For data format detail, please refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_data_format">4 Data Format</a>.</li>
<li>"-im 104,117,123" refers to the meaning value.</li>
<li>"-ic 1" refers to the scale ratio.</li>
<li>"-c": "act-force-fx8" indicates that the output is 8-bit; "coeff-force-fx8" indicates that the weight is 8-bit; and if it is not used, the system determines the value.</li>
<li>"-odst "o:<b>prob</b>|odf:<b>fp32</b>"" indicates that the output should convert "prob" to <b>float-32</b>.</li>
</ul>
</dd></dl>
<p>By default, the parser tool generates the sideband file which enables users to determine and manually modify the quantization of each stage.</p>
<p>Use the following code: </p><div class="fragment"><div class="line">build $ caffeparser.py -p deploy.prototxt -m bvlc_googlenet.caffemodel -o bvlc_googlenet -of out_bvlc_googlenet_parser -im 104,117,123 -ic 1 -iq -idf 0,0,0,0 -c act-force-fx8,coeff-force-fx8</div>
<div class="line">    -odst <span class="stringliteral">&quot;o:prob|odf:fp32&quot;</span> -sb bvlc_googlenet_sb.jason</div>
</div><!-- fragment --> <h2><a class="anchor" id="sub_sec_googlenet_vas_compile"></a>
3 VAS Compile</h2>
<p>Compile the VAS code as shown below: </p><div class="fragment"><div class="line">build $ cd out_bvlc_googlenet_parser;vas -<span class="keyword">auto</span> -summary -show-progress -dvi bvlc_googlenet.vas</div>
</div><!-- fragment --> <h2><a class="anchor" id="sub_sec_googlenet_ades_autogen_py"></a>
4 ades_autogen.py</h2>
<p>Use <b>ades_autogen.py</b> to generate the ADES command to run on the PC. </p><div class="fragment"><div class="line">build $ ades_autogen.py -v bvlc_googlenet -p out_bvlc_googlenet_parser -l ades_bvlc_googlenet -ib data=/&lt;absolute patch&gt;/dra_image_bin/13.bin</div>
<div class="line">build $ cd ades_bvlc_googlenet; ades bvlc_googlenet_ades.cmd</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>First Command: "-ib" refers to the input files mapped to the input layer names. For example, "-ib data=/&lt;absolute path&gt;/input.bin". For this input, the test binary can be found in <em>dra_image_bin</em> in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_gen_image_list_py">1 gen_image_list.py</a> or <em>gen_image_list.py</em> to generate new images. Ensure that the data format is correct with the definition in "-iq -idf 0,0,0,0", RGB, or BGR, and the resolution is the same as defined in the prototxt.</li>
<li>Second Command: <em>ades</em> generates <em>prob.bin</em> in <em>ades_bvlc_googlenet</em> which can be compared to the output in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_test_nnctrl">7 test_nnctrl</a>.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_googlenet_cavalry_gen"></a>
5 cavalry_gen</h2>
<p><b>cavalry_gen</b> compiles the CV code and generates an output that can run on CV. </p><div class="fragment"><div class="line">build $ cavalry_gen -d out_bvlc_googlenet_parser/vas_output/ -f bvlc_googlenet_cavalry.bin -p ./ -v</div>
</div><!-- fragment --><p>Save to file bvlc_googlenet_cavalry.bin with file size 7076336. </p><div class="fragment"><div class="line">version 5 ( HASH 237d38e ) dvi_num: 5</div>
<div class="line"> </div>
<div class="line">dvi_id: 0  dvi_img_vaddr: 1384696  dvi_img_size: 695716    dvi_dag_vaddr: 2076280  input_num: 1    output_num: 2   dvi_pkg_size: 698896    vproc_id: 0     dag_name: bvlc_googlenet_prim_split_0</div>
<div class="line">dvi_id: 1  dvi_img_vaddr: 733056   dvi_img_size: 1346496   dvi_dag_vaddr: 2076316  input_num: 2    output_num: 2   dvi_pkg_size: 1350736   vproc_id: 1     dag_name: bvlc_googlenet_prim_split_1</div>
<div class="line">dvi_id: 2  dvi_img_vaddr: 595456   dvi_img_size: 1483420   dvi_dag_vaddr: 2076096  input_num: 2    output_num: 2   dvi_pkg_size: 1487660   vproc_id: 2     dag_name: bvlc_googlenet_prim_split_2</div>
<div class="line">dvi_id: 3  dvi_img_vaddr: 350336   dvi_img_size: 1678656   dvi_dag_vaddr: 2027312  input_num: 2    output_num: 3   dvi_pkg_size: 1683956   vproc_id: 3     dag_name: bvlc_googlenet_prim_split_3</div>
<div class="line">dvi_id: 4  dvi_img_vaddr: 184196   dvi_img_size: 1850036   dvi_dag_vaddr: 2032932  input_num: 3    output_num: 1   dvi_pkg_size: 1854276   vproc_id: 4     dag_name: bvlc_googlenet_prim_split_4</div>
<div class="line"> </div>
<div class="line">dvi_id: 0 dag_name: bvlc_googlenet_prim_split_0 input_num: 1 output_num: 2</div>
<div class="line">input_id: 0    port_size: 150528   port_byte_offset: 1192  dim: (P, D, H, W) = ( 1, 3, 224, 224)  pitch: 224  dram_format: 0  data_format: (0, 0, 0, 0)   port_name: data (layer_name: data)</div>
<div class="line">output_id: 0   port_size: 107520   port_byte_offset: 3720  dim: (P, D, H, W) = ( 1, 480, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: pool3__3x3_s2    (layer_name: pool3__3x3_s2)</div>
<div class="line">output_id: 1   port_size: 107520   port_byte_offset: 3896  dim: (P, D, H, W) = ( 1, 480, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4a__pool   (layer_name: inception_4a__pool)</div>
<div class="line"> </div>
<div class="line">dvi_id: 1 dag_name: bvlc_googlenet_prim_split_1 input_num: 2 output_num: 2</div>
<div class="line">input_id: 0    port_size: 107520   port_byte_offset: 28    dim: (P, D, H, W) = ( 1, 480, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: pool3__3x3_s2    (layer_name: pool3__3x3_s2)</div>
<div class="line">input_id: 1    port_size: 107520   port_byte_offset: 68    dim: (P, D, H, W) = ( 1, 480, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4a__pool   (layer_name: inception_4a__pool)</div>
<div class="line">output_id: 0   port_size: 114688   port_byte_offset: 2908  dim: (P, D, H, W) = ( 1, 512, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4c__output (layer_name: inception_4c__output)</div>
<div class="line">output_id: 1   port_size: 114688   port_byte_offset: 3080  dim: (P, D, H, W) = ( 1, 512, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4d__pool   (layer_name: inception_4d__pool)</div>
<div class="line"> </div>
<div class="line">dvi_id: 2 dag_name: bvlc_googlenet_prim_split_2 input_num: 2 output_num: 2</div>
<div class="line">input_id: 0    port_size: 114688   port_byte_offset: 28    dim: (P, D, H, W) = ( 1, 512, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4c__output (layer_name: inception_4c__output)</div>
<div class="line">input_id: 1    port_size: 114688   port_byte_offset: 68    dim: (P, D, H, W) = ( 1, 512, 14, 14)  pitch: 32   dram_format: 3  data_format: (0, 0, 0, 0)   port_name: inception_4d__pool   (layer_name: inception_4d__pool)</div>
<div class="line">output_id: 0   port_size: 46592    port_byte_offset: 2228  dim: (P, D, H, W) = ( 1, 832, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 1, 0)   port_name: pool4__3x3_s2    (layer_name: pool4__3x3_s2)</div>
<div class="line">output_id: 1   port_size: 46592    port_byte_offset: 2472  dim: (P, D, H, W) = ( 1, 832, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 1, 0)   port_name: inception_5a__pool   (layer_name: inception_5a__pool)</div>
<div class="line"> </div>
<div class="line">dvi_id: 3 dag_name: bvlc_googlenet_prim_split_3 input_num: 2 output_num: 3</div>
<div class="line">input_id: 0    port_size: 46592    port_byte_offset: 28    dim: (P, D, H, W) = ( 1, 832, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 1, 0)   port_name: pool4__3x3_s2    (layer_name: pool4__3x3_s2)</div>
<div class="line">input_id: 1    port_size: 46592    port_byte_offset: 68    dim: (P, D, H, W) = ( 1, 832, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 1, 0)   port_name: inception_5a__pool   (layer_name: inception_5a__pool)</div>
<div class="line">output_id: 0   port_size: 7168     port_byte_offset: 1440  dim: (P, D, H, W) = ( 1, 128, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 3, 0)   port_name: inception_5b__pool_proj___relu_pool_proj (layer_name: inception_5b__pool_proj___relu_pool_proj)</div>
<div class="line">output_id: 1   port_size: 34944    port_byte_offset: 1568  dim: (P, D, H, W) = ( 1, 624, 7, 7)    pitch: 32   dram_format: 3  data_format: (1, 0, 2, 0)   port_name: inception_5b__1x1___3x3_reduce___5x5_reduce  (layer_name: inception_5b__1x1___3x3_reduce___5x5_reduce)</div>
<div class="line">output_id: 2   port_size: 21504    port_byte_offset: 1620  dim: (P, D, H, W) = ( 1, 384, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 3, 0)   port_name: inception_5b__relu_1x1   (layer_name: inception_5b__relu_1x1)</div>
<div class="line"> </div>
<div class="line">dvi_id: 4 dag_name: bvlc_googlenet_prim_split_4 input_num: 3 output_num: 1</div>
<div class="line">input_id: 0    port_size: 7168     port_byte_offset: 28    dim: (P, D, H, W) = ( 1, 128, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 3, 0)   port_name: inception_5b__pool_proj___relu_pool_proj (layer_name: inception_5b__pool_proj___relu_pool_proj)</div>
<div class="line">input_id: 1    port_size: 34944    port_byte_offset: 68    dim: (P, D, H, W) = ( 1, 624, 7, 7)    pitch: 32   dram_format: 3  data_format: (1, 0, 2, 0)   port_name: inception_5b__1x1___3x3_reduce___5x5_reduce  (layer_name: inception_5b__1x1___3x3_reduce___5x5_reduce)</div>
<div class="line">input_id: 2    port_size: 21504    port_byte_offset: 108   dim: (P, D, H, W) = ( 1, 384, 7, 7)    pitch: 32   dram_format: 3  data_format: (0, 0, 3, 0)   port_name: inception_5b__relu_1x1   (layer_name: inception_5b__relu_1x1)</div>
<div class="line">output_id: 0   port_size: 4000     port_byte_offset: 1240  dim: (P, D, H, W) = ( 1, 1, 1, 1000)   pitch: 4000 dram_format: 0  data_format: (1, 2, 0, 7)   port_name: prob (layer_name: prob)</div>
<div class="line"> </div>
<div class="line">.dvi files are parsed to folder ./parse</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>"-d" refers to the directory path for VAS outputs.</li>
<li>Similar to <b>out_bvlc_googlenet_parser/vas_output</b>, it is generated in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_vas_compile">3 VAS Compile</a>.</li>
<li>"-f" is for the output file used to deploy on CV, for example, <b>bvlc_googlenet_cavalry.bin</b>.</li>
<li>"-v" prints a debug message.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_googlenet_prepare_the_input"></a>
6 Prepare the input</h2>
<p>Use the test binary in the folder <em>dra_image_bin</em> in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_gen_image_list_py">1 gen_image_list.py</a> or <em>gen_image_list.py</em> to generate new images. Ensure that the data format is in alignment with the definition in "-iq -idf 0,0,0,0" RGB, or BGR, and the resolution is the same as defined in <b>prototxt</b>.</p>
<h2><a class="anchor" id="sub_sec_googlenet_test_nnctrl"></a>
7 test_nnctrl</h2>
<p>Flash the binary to the EVK board, and then run the following commands. </p><div class="fragment"><div class="line">board # modprobe cavalry; cavalry_load -f /lib/firmware/cavalry.bin -r (Only CV2x and CV5x need to <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> <span class="keyword">this</span>)</div>
<div class="line">board # test_nnctrl -b bvlc_googlenet_cavalry.bin --in data=13.bin --out prob=prob.bin -e</div>
</div><!-- fragment --><p> For more information about "--in" and "--out", refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_cavalry_gen">5 cavalry_gen</a>. The definition is the name of first layer input and last layer output.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can generate a new <em>prob.bin</em> and compare it to the prob.bin in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_googlenet_ades_autogen_py">4 ades_autogen.py</a>.</li>
<li>Using the same input, the results will be identical, except for minor differences that occur as a result of the 32-bytes alignment.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_lstm_warpctc"></a>
LSTM Warp-CTC</h1>
<p>This chapter explains how to port and run the Warp-CTC LSTM net. The network takes in two inputs: one CAPTCHA image and one continuation indicator. The indicator resets the LSTM state, because running LSTM must include looping over a time factor "T". Loops cannot run on VP, so the given prototxt should be split into LSTM and non-LSTM parts. This allows the LSTM prototxt to run through the CV tools and be converted into DAGs, where the loop control is realized in the Arm part.</p>
<h2><a class="anchor" id="subsec_download_warpctc_caffe"></a>
1 Download Warp-CTC Caffe</h2>
<ul>
<li>To download the Warp-CTC Caffe, use the following commands. <pre class="fragment">  Build $ export GIT_SSL_NO_VERIFY=1
  Build $ git clone https://github.com/xmfbit/warpctc-caffe.git
  Build $ cd warpctc-caffe/
  Build $ git reset --hard 2375b2170
</pre></li>
<li>Modify the file <em>caffe/src/caffe/layers/ctc_loss_layer.cpp</em> to avoid a build error('accumulate' is not a member of 'std'). <pre class="fragment">    Build $ vim caffe/src/caffe/layers/ctc_loss_layer.cpp  (add header file #include &lt;numeric&gt;)
    Build $ git diff
    --- a/src/caffe/layers/ctc_loss_layer.cpp
    +++ b/src/caffe/layers/ctc_loss_layer.cpp
    @@ -1,4 +1,5 @@
     #include &lt;caffe/layers/ctc_loss_layer.hpp&gt;
    +#include &lt;numeric&gt;
     namespace caffe {
</pre></li>
<li>User could use the build script in <em>&lt;user_path&gt;/Ambarella_Toolchain_CNNGen_&lt;version&gt;/AmbaCaffe_&lt;version&gt;/build_script</em> to build the warpctc Caffe. (This file is included in the CNNGen Toolchain package). The following command is an example for building the Caffe in <em>Ubuntu18.04</em> CPU environment. There will be a <em>caffe-install-cpu</em> folder generated under the current path: <pre class="fragment">    Build $ cd ..
    Build $ ./build_script/build-caffe-ubuntu18.04-cpu caffe
</pre></li>
<li>Next, source the CNNGen toolchain environment file and then export the Caffe environment. Refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_env_set">3 Environment Setting</a>.</li>
</ul>
<h2><a class="anchor" id="subsec_warpctc_modify_prototxt"></a>
2 Modify Prototxt</h2>
<ul>
<li>Replace the custom layers in <em>deploy.prototxt</em> with layers in Stock Caffe. Because the layer <em>"ContinuationIndicator"</em> is not supported in the Stock Caffe, it can be replaced by an <em>"Input"</em> layer. <pre class="fragment">    $ graph_surgery.py caffe -p deploy.prototxt \
    -m lstm_ctc_iter_50000.caffemodel -t "replace_cont_ind"
</pre> This step generates a new prototxt called "deploy_modified.prototxt".</li>
<li>Split the <em>deploy_modified.prototxt</em> into different prototxts using <em>graph_surgery</em>. This network is structured as A-&gt;LSTM1-&gt;LSTM2-&gt;B. The user needs to read the prototxt and decide the split points. Split points correspond to layer names in the graph. <pre class="fragment">    $ graph_surgery.py caffe -p deploy_modified.prototxt \
    -m lstm_ctc_iter_50000.caffemodel -isrc "i:input" -on permuted_data \
    -t "cut_prototxt" -o deploy_modified_split1.prototxt
    $ graph_surgery.py caffe -p deploy_modified.prototxt \
    -m lstm_ctc_iter_50000.caffemodel -isrc "i:lstm1" -on lstm1 \
    -t "cut_prototxt" -o deploy_modified_split2.prototxt
    $ graph_surgery.py caffe -p deploy_modified.prototxt \
    -m lstm_ctc_iter_50000.caffemodel -isrc "i:lstm2" -on lstm2 \
    -t "cut_prototxt" -o deploy_modified_split3.prototxt
    $ graph_surgery.py caffe -p deploy_modified.prototxt \
    -m lstm_ctc_iter_50000.caffemodel -isrc "i:fc1" -on permute_fc \
    -t "cut_prototxt" -o deploy_modified_split4.prototxt
</pre></li>
<li>Remove or comment the indicator layer in <em>deploy_modified_split1.prototxt</em>.</li>
<li>In <em>deploy_modified_split2.prototxt</em> and <em>deploy_modified_split3.prototxt</em>, change the plane number of <em>"permuted_data_input"</em>, <em>"indicator_input"</em> and <em>"lstm1_input"</em> from 80 to 1.</li>
</ul>
<h2><a class="anchor" id="subsec_warpctc_convert_compile"></a>
3 Convert and Compile</h2>
<p>Use the following command to convert and compile. </p><pre class="fragment">$ make warpctc
</pre><p>The LSTM layers require two inputs, the output from the previous layer and the continuation indicator input. LSTM parsing needs DRA files for the c0 and h0 inputs, which correspond to the cell state (c0) and hidden state (h0). The make-workflow uses the parser option <em>"-dinf cerr"</em> to generate the c0 and h0 middle results for the LSTM layer. As a result, the parser needs to run twice. The first time will generate the c0 and h0 middle inputs for DRA, and the second time will take the DRA from the first execution and parses the next.</p>
<p>The CNNGen forward pass results are generated as a binary file in the <em>"inf_results"</em> folder. The h0 and c0 for the next steps will be generated as: <em>&lt;lstm-layername&gt;_000000_unquant.bin</em> and <em>&lt;lstm-layername&gt;_c_000000_unquant.bin</em>.</p>
<p>The middle binaries are in FP32 format, so a data format conversion may be necessary depending on the next layer's input data format. If necessary, it is possible to hack the custom Caffe code to dump the middle results from the PC Caffe results instead of using the <em>"cerr"</em> parser option.</p>
<h2><a class="anchor" id="subsec_warpctc_SDK_preparation"></a>
4 SDK Preparation</h2>
<p>To quickly run LSTM on the board, install the application <em>"test_lstm"</em> before compiling the SDK. </p><pre class="fragment">build $ cd boards/cv*_&lt;platform&gt;
build $ make sync_build_mkcfg
build $ make cv*_ipcam_config
build $ make menuconfig
[*] Ambarella Unit Test Configuration
    [*] Ambarella Private Linux Unit test configs
        [*] Build CV unit tests
            [*] Build test_lstm unit tests
</pre><h2><a class="anchor" id="subsec_warpctc_run_on_board"></a>
5 Run on Board</h2>
<p>The application <em>"test_lstm"</em> helps run the LSTM in the file mode: </p><pre class="fragment">board # modprobe cavalry; cavalry_load -f /lib/firmware/cavalry.bin -r (Only CV2x and CV5x need to run this)
board # test_lstm -b warpctc_cavalry_split1.bin \
--in input=00002-7688.bin --out permuted_data \
-b warpctc_cavalry_split2.bin --in permuted_data_input=permuted_data -f \
--in indicator_input=indicator_fix8_1x1x1x1_000.bin --in lstm1_h0=lstm1 -f \
--in lstm1_c0=lstm1_c -f --out lstm1 --out lstm1_c -t lstm -l 80 \
-b warpctc_cavalry_split3.bin --in lstm1_input=lstm1 -f \
--in lstm2_h0=lstm2 -f --in lstm2_c0=lstm2_c -f \
--in indicator_input=indicator_fix8_1x1x1x1_000.bin \
--out lstm2_c --out lstm2 -t lstm -l 80 \
-b warpctc_cavalry_split4.bin --in lstm2_input=lstm2 -f --out fc1=fc1.bin
</pre><p>Options for <em>test_lstm</em> are provided below:</p>
<p><b>-f</b> option: This option tells the app that the port data is not coming from the local binary file, but from another port. It can connect to different networks by memory if it is specified after each input port. Example: </p><pre class="fragment">--in permuted_data_input=permuted_data -f
</pre><p><b>-t</b> option: This option specifies the network type. If '-t' is specified, the current network (or the current cavalry.bin) should be regarded as a LSTM layer.</p>
<p><b>-l</b> option: This option is placed at the end of the current network to set the loop number. Example: </p><pre class="fragment">-b warpctc_cavalry_split3.bin --in lstm1_input=lstm1 -f \
--in lstm2_h0=lstm2 -f --in lstm2_c0=lstm2_c -f \
--in indicator_input=indicator_fix8_1x1x1x1_000.bin \
--out lstm2_c --out lstm2=lstm2.bin -t lstm -l 80
</pre><p>When each port is connected by memory, the LSTM layer input port name with <em>"_c0"</em> is the cell state, whereas <em>"_h0"</em> is the hidden state. The output port with <em>"_c"</em> is the cell state output, which will feed the next cell state input. The other output without <em>"_c"</em> is the output of the hidden state, which will feed to next hidden state input.*</p>
<hr  />
<h1><a class="anchor" id="sec_lstm_warpctc_with_one_shot"></a>
LSTM Warp-CTC with One Shot</h1>
<p>As descriptions in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_lstm_warpctc">LSTM Warp-CTC</a>, loops cannot run on VP while running LSTM net. With old covert flow, the user needs to split LSTM model into different parts, and realize the loop control in the Arm part. It's hard for user to prepare DRA files for every stage and deploy on chip, so this chapter provides a new approach to compile LSTM model in one shot.</p>
<dl class="section note"><dt>Note</dt><dd>For chineseocr lite which is in onnx/test_networks/chineseocr_lite/, the conversion process is similar to the network in this section. The only difference is that the warpctc_one_shot requires an indicator DRA file as an input of some stages, and chineseocr lite does not need. So users can refer to this section for Onnx demo chineseocr lite conversion.</dd></dl>
<h2><a class="anchor" id="sec_prepare_environment"></a>
1 Prepare Environment</h2>
<p>To prepare the environment, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#subsec_download_warpctc_caffe">1 Download Warp-CTC Caffe</a>.</p>
<h2><a class="anchor" id="subsec_file_preparation"></a>
2 File Preparation</h2>
<p>The original LSTM model should be compiled with the Ambarella CNNGen ToolKit and then the compiled files can be deployed on board.</p>
<p>The necessary files are listed as shown below. </p><pre class="fragment">├─warpctc
│  ├─dra_img
│  │  └─00000-50337.png
│  ├─dra_matrix
│  │  └─indicator.txt
│  ├─models
│  │  ├─deploy_modified.prototxt
│  │  └─lstm_ctc_iter_50000_retrain.caffemodel
│  └─warpctc_one_shot
│      ├─lstm_graph.json
│      └─Makefile
</pre><h3><a class="anchor" id="subb_sec_prepare_pretrained_model"></a>
2.1 Pretrained Model</h3>
<ol type="1">
<li>Pretrained model should be frozen before being fed into the CNNGen ToolKit. In this example, a Caffe framework pretrained LSTM model is used. <pre class="fragment">├─models
│  ├─deploy_modified.prototxt
│  └─lstm_ctc_iter_50000_retrain.caffemodel
</pre></li>
<li>Among them, deploy_modified.prototxt is generated by the following command. <pre class="fragment"> build $ graph_surgery.py caffe -p deploy.prototxt \
 -m lstm_ctc_iter_50000.caffemodel -t "replace_cont_ind"
</pre></li>
</ol>
<h3><a class="anchor" id="subb_sec_prepare_dra_files"></a>
2.2 DRA Files</h3>
<p>The CNNGen ToolKit needs one or more binary files to do the dynamic range analysis (DRA) for each input node. There are two input nodes in this LSTM model, “dra_image_bin” and “indicator”. Take input node “dra_image_bin” for example, there is a text file “dra_bin_list.txt” created to list all DRA files. The file path in “dra_bin_list.txt” must be listed as an absolute filepath.</p>
<ol type="1">
<li>To generate “dra_image_bin” files, using the following command. <pre class="fragment"> build $ gen_image_list.py \
 -f &lt;user_path&gt;/caffe/demo_networks/warpctc/dra_img/ \
 -o &lt;user_path&gt;/out/caffe/demo_networks/warpctc_one_shot/dra_image_bin/img_list.txt \
 -ns -e png -c 1 -d 0,0 -r 30,80 \
 -bf &lt;user_path&gt;/out/caffe/demo_networks/warpctc_one_shot/dra_image_bin \
 -bo &lt;user_path&gt;/out/caffe/demo_networks/warpctc_one_shot/dra_image_bin/dra_bin_list.txt
</pre></li>
<li>“indicator.bin” is the DRA file for “indicator” node, to generate it, using the following command. <pre class="fragment"> build $ cv_f2vp_convert \
 -it -fmt 0,0,0,0 \
 -i &lt;user_path&gt;/caffe/demo_networks/warpctc/dra_matrix/indicator.txt \
 -o &lt;user_path&gt;/out/caffe/demo_networks/warpctc_one_shot/indicator/indicator.bin
</pre></li>
</ol>
<h3><a class="anchor" id="subb_sec_prepare_mode_graph_description_file"></a>
2.3 Model Graph Description File</h3>
<p>To compile the LSTM model in one shot, the CNNGen ToolKit should read in one model graph description file to check the compilation options. This description file is in JSON format. It provides information on CNNGen DRA configuration and parser options for IO nodes. Configuration information is like data shape, data format, using FX8, FX16, or FP16.</p>
<p>The original LSTM model will be split into four blocks in one shot after CNNGen ToolKit compilation. User can specify different DRA configurations and parser configurations for each split block in graph description file. Adjacent split blocks which share the same CNNGen configurations can be merged together into this description file. For LSTM model, there are split1, split2, split3, and split4 blocks after CNNGen compilation, split1’s CNNGen configurations are different from those of split2-split4, while split2-split4, which share the same CNNGen configurations, can be configured together in one description block. Therefore this model graph description file includes two parts and would be shown as below after the substitution of configuration options.</p>
<p><b>lstm_graph.json</b> </p><pre class="fragment">{
"warpctc_split1": {
        "type": "VP",
        "begin": {
                "input" : {
                        "shape" : [1,3,30,80],
                        "dtype" : "0,0,8,0",
                        "file" : "&lt;user_path&gt;/out/../../../dra_image_bin/dra_bin_list.txt",
                        "extn" : "bin"
                }
        },
        "end": {
                "permuted_data" : {}
        },
        "attr": {
                "cnngen_flags": "",
                "vas_flags" : "-auto",
                "graph_surgery_transforms": ""
                }
        },

"warpctc_split2": {
        "type": "VP",
        "begin": {
                "permuted_data" : {},
                "indicator" : {
                        "shape" : [80,1],
                        "dtype" : "0,0,0,0",
                        "init" : "&lt;user_path&gt;/out/../../../indicator/indicator.bin"
                }
        },
        "end": {
                "fc1" : {
                        "dtype": "1,2,0,7"
                }
        },
        "attr": {
                "cnngen_flags": "-c act-force-fp16,coeff-allow-fp16 ",
                "vas_flags" : "-auto",
                "graph_surgery_transforms": ""
                }
        }
}
</pre><h2><a class="anchor" id="subsec_build_lstm_in_one_shot"></a>
3 Build LSTM in One Shot</h2>
<ol type="1">
<li><p class="startli">Make menuconfig for warpctc one shot. </p><div class="fragment"><div class="line">build $ make sync_build_mkcfg</div>
<div class="line">build $ make cvflow_&lt;v2 or v3&gt;_config</div>
<div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">    [*] Build Caffe Demo Networks  ---&gt;</div>
<div class="line">        [*] Build Caffe WARPCTC Network  ---&gt;</div>
<div class="line">            [*] Build Caffe WARPCTC Network in one shot</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>For chineseocr lite, the user can refer to the following steps. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella Onnx Networks  ---&gt;</div>
<div class="line">    [*] Build ONNX Test Networks  ---&gt;</div>
<div class="line">        [*] Build ONNX CHINESE OCR LITE Network  ---&gt;</div>
</div><!-- fragment --></dd></dl>
</li>
<li>Generate the Cavalry binary for EVK. <div class="fragment"><div class="line">build $ make warpctc_one_shot run_mode=cavalry dra=<span class="stringliteral">&quot;-dra mode=3&quot;</span></div>
</div><!-- fragment --> Then, "lstm_net.bin" is generated in out/caffe/demo_networks/warpctc_one_shot/ cavalry_warpctc_one_shot. Next, the user should copy this binary file into the sdcard of the EVK board to generate the final result as the steps in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#subsec_run_lstm_on_board">4 Run LSTM on Board</a>.</li>
<li>Generate the simulator result and calculate the MD5 value. <div class="fragment"><div class="line">build $ make warpctc_one_shot run_mode=ades dra=<span class="stringliteral">&quot;-dra mode=3&quot;</span></div>
<div class="line">build $ md5sum &lt;out_dir&gt;/ades_warpctc_one_shot/fc1_iter0.bin</div>
</div><!-- fragment --> Then, "fc1_iter0.bin" is generated as the simulator result in out/caffe/demo_networks/ warpctc_one_shot/ades_warpctc_one_shot. The MD5 value of this result should be exactly the same as the MD5 value of the EVK result.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>The step2 and step3 above have integrated everything which is mentioned in <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#subb_sec_prepare_dra_files">2.2 DRA Files</a> and <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#subb_sec_prepare_mode_graph_description_file">2.3 Model Graph Description File</a>. However, this example does not cover all the cases, the user must modify the scripts if needed. All output files are in cv2x_cnngen_samples/out/caffe/ demo_networks/warpctc_one_shot/.</dd></dl>
<p>The main structure of the output files is as follows. </p><div class="fragment"><div class="line">out/caffe/demo_networks/warpctc_one_shot</div>
<div class="line">├─lstm_graph.json                                    configuration file <span class="keywordflow">for</span> prepare stage</div>
<div class="line">├─cavalry_warpctc_one_shot                           cavalry out</div>
<div class="line">│  ├─warpctc_one_shot_cavalrygen_task.json           input of the cavalry_gen</div>
<div class="line">│  ├─lstm_net.bin                                    cavalry binary</div>
<div class="line">│  └─cnngen_outputs                                  context objects</div>
<div class="line">│      ├─warpctc_split2__lstm2</div>
<div class="line">│      │  └─vas_output</div>
<div class="line">│      ├─warpctc_split2__lstm1</div>
<div class="line">│      │  └─vas_output</div>
<div class="line">│      ├─warpctc_split2__expanded_2</div>
<div class="line">│      │  └─vas_output</div>
<div class="line">│      └─warpctc_split1</div>
<div class="line">│          └─vas_output</div>
<div class="line">├─indicator</div>
<div class="line">├─dra_image_bin</div>
<div class="line">├─prepare_stage_warpctc_one_shot                     outputs of prepare stage</div>
<div class="line">│  ├─warpctc_one_shot_ctx.npy                        summary file</div>
<div class="line">│  ├─warpctc_one_shot.ambapb.ckpt.onnx               ambapb <span class="stringliteral">&#39;checkpoint&#39;</span></div>
<div class="line">│  ├─dra_files                                       spilt dra files</div>
<div class="line">│  ├─outputs</div>
<div class="line">│  │  ├─full_graph_outputs</div>
<div class="line">│  │  └─split_graph_outputs</div>
<div class="line">│  ├─ambacnn_out                                     graphproto objects</div>
<div class="line">│  │  ├─warpctc_split1</div>
<div class="line">│  │  │  ├─vas_output</div>
<div class="line">│  │  │  └─weights</div>
<div class="line">│  │  ├─warpctc_split2__lstm1</div>
<div class="line">│  │  │  └─vas_output</div>
<div class="line">│  │  ├─warpctc_split2__lstm2</div>
<div class="line">│  │  │  └─vas_output</div>
<div class="line">│  │  └─warpctc_split2__expanded_2</div>
<div class="line">│  │      ├─weights</div>
<div class="line">│  │      └─vas_output</div>
<div class="line">│  ├─logs</div>
<div class="line">│  └─split_graphs                                    split graphs</div>
<div class="line">└─ades_warpctc_one_shot                              simulator result</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>LSTM networks are not ready on CV72, please try the other chips first.</dd></dl>
<h2><a class="anchor" id="subsec_run_lstm_on_board"></a>
4 Run LSTM on Board</h2>
<ol type="1">
<li>Run the Cavalry network binary on board by test_nnctrl. <pre class="fragment"> board # modprobe cavalry; cavalry_load -f /lib/firmware/cavalry.bin -r (Only CV2x and CV5x need to run this)
 board # test_nnctrl -b lstm_net.bin \
 --in input=00000-50337.bin \
 --in indicator \
 --out fc1=/tmp/fc1.bin -e
</pre></li>
<li>Parse the result by warpctc_captcha_parse_result. <pre class="fragment"> board # warpctc_captcha_parse_result /tmp/fc1.bin
 Predict result:/tmp/fc1.bin:50337
</pre></li>
<li>Calculate the MD5 value of the result by md5sum. <pre class="fragment"> board # md5sum /tmp/fc1.bin
</pre></li>
</ol>
<dl class="section note"><dt>Note</dt><dd>On CV22_Walnut, the performance of Caffe LSTM is "2.8 ms", Onnx LSTM is "5.4 ms", with DRA strategy "act-force-fx8, coeff-force-fx8", density 100%.</dd></dl>
<h2><a class="anchor" id="subsec_compare_result"></a>
5 Compare Result</h2>
<ol type="1">
<li><p class="startli">Refer to the following script to get PC Caffe result. </p><pre class="fragment"> build $ python3 gen_warpctc_caffe_result.py \
 --olayer fc1 --prototxt deploy_modified.prototxt \
 --model lstm_ctc_iter_50000_retrain.caffemodel \
 --ilayer data --iblob indicator --test_bin input_bin_list.txt \
 --test_img input_list.txt --out ./out/ --mean [0,0,0] --scale 0.00392156863 \
 --img_type .png --is_bgr 1 --imload opencv
</pre><dl class="section note"><dt>Note</dt><dd>For this step, the user can find the script gen_warpctc_caffe_result.py in cv2x_cnngen_samples/caffe/ demo_networks/warpctc/pc_caffe_script. The file paths in "input_list.txt" and "input_bin_list.txt" are the absolute paths of the test image and indicator.bin respectively. Or the user can run the script run_pc.sh in cv2x_cnngen_samples/caffe/demo_networks/warpctc/warpctc_one_shot/pc_caffe_script/ directly, a bin file will be generated as a result.</dd></dl>
</li>
<li><p class="startli">Compare the PC Caffe result and EVK result. </p><pre class="fragment"> build $ ./bincmp user_path/00000-50337#fc1#80-1-11#fp32.bin \
 user_path/fc1.bin 80 1 11 32 -1 1 32 -1 1
</pre><p class="startli">Then the result will be given as below:</p><ul>
<li>Datasize=880,the cosine similarity is 0.999724, norm1 = 246.738113, norm2 = 245.874847</li>
<li>euclidean_distance/norm1 = 0.023730, euclidean_distance/norm2 = 0.023814</li>
</ul>
</li>
<li><p class="startli">Compare the simulator result (ADES) and the EVK result.</p>
<p class="startli">The MD5 values of the ADES and EVK results should be exactly the same.</p>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_caffe_light_yolo"></a>
Light Yolo</h1>
<p>This version of YOLO is a public release from dog-qiuqiu on <a href="https://github.com/dog-qiuqiu">https://github.com/dog-qiuqiu</a> as below.</p>
<ul>
<li><a href="https://github.com/dog-qiuqiu/Yolo-Fastest">https://github.com/dog-qiuqiu/Yolo-Fastest</a>, Yolo-Fastest is based on Tiny Yolov3 and using EfficientNet-lite as backbone, there are two models.</li>
<li><a href="https://github.com/dog-qiuqiu/MobileNet-Yolo">https://github.com/dog-qiuqiu/MobileNet-Yolo</a>, Mobilenetv2-yolov3 is based on Tiny Yolov3 and using mobilenetv2 as backbone, there are two models.</li>
</ul>
<p>The detail mAP is as below table. </p><a class="anchor" id="CV22 Performance Table for DRAv2"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Network </th><th>VOC mAP(0.5) </th><th>COCO mAP(0.5) </th><th>Resolution </th><th>Weight size </th><th>CV22 EVK Inference time with Default DRA (ms) </th></tr>
<tr align="middle">
<td>MobileNetV2-YOLOv3-Lite </td><td>73.26 </td><td>37.44 </td><td>320 </td><td>8.0MB </td><td>7.3 </td></tr>
<tr align="middle">
<td>MobileNetV2-YOLOv3-Nano </td><td>65.27 </td><td>30.13 </td><td>320 </td><td>3.0MB </td><td>1.9 </td></tr>
<tr align="middle">
<td>Yolo-Fastest </td><td>61.02 </td><td>23.65 </td><td>320 </td><td>1.3MB </td><td>1.5 </td></tr>
<tr align="middle">
<td>Yolo-Fastest-XL </td><td>69.43 </td><td>32.45 </td><td>320 </td><td>3.5MB </td><td>3.1 </td></tr>
</table>
<p>Below used <b>VOC</b> models as an example, if to use <b>COCO</b> model, the step are the same.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="caffe_light_yolo_export_caffe_model"></a>
1 Export Caffe model</h2>
<p>The Caffe model is exported from Darknet with the tool in <code>caffe/demo_networks/light_yolo/script/darknet2caffe</code> which is based on <a href="https://github.com/dog-qiuqiu/MobileNet-Yolo/tree/master/darknet2caffe">https://github.com/dog-qiuqiu/MobileNet-Yolo/tree/master/darknet2caffe</a> with yolo license.</p>
<p>Please refer "readme.txt" in above folder for how to convert.</p>
<h2><a class="anchor" id="sec_caffe_light_yolo_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For yolov3_fastest, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/light_yolo/config/ea_cvt_yolov3_fastest.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolov3_fastest/</code>.</li>
<li>For yolov3_fastest_xl, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/light_yolo/config/ea_cvt_yolov3_fastest_xl.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolov3_fastest_xl/</code>.</li>
<li>For yolov3_mnetv2_nano, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/light_yolo/config/ea_cvt_yolov3_mnetv2_nano.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolov3_mnetv2_nano/</code>.</li>
<li>For yolov3_mnetv2_lite, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/light_yolo/config/ea_cvt_yolov3_mnetv2_lite.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolov3_mnetv2_lite/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/yolov3_(*)</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_yolov3_(*).bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/yolov3_(*)/&lt;chip&gt;/&lt;chip&gt;_yolov3_(*)</code>.</li>
<li>For X86 simulator, model desc json file <b>yolov3_(*).json</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolov3_(*)/out_yolov3_(*)_parser/</code>. ades command <b>yolov3_(*)_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolov3_(*)/&lt;chip&gt;/&lt;chip&gt;_ades_yolov3_(*)</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sec_caffe_light_yolo_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build the EVK binary as shown below. <pre class="fragment">  build $ make menuconfig
  [*] Ambarella package Configuration ---&gt;
      -*- Build Ambarella EAZYAI library ---&gt;
          -*- Build eazyai library with OpenCV support
          -*- Build Ambarella custom postprocess library ---&gt;
              [*] Build Ambarella custom postprocess library with yolov3
          [*] Build EazyAi unit tests
  build $ make test_eazyai
</pre></li>
<li><p class="startli">Build X86 simulator binary with make.</p>
<p class="startli">Refer to the CNNGen Doxgen library EazyAI 3 EazyAI Simulator to build the x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</p>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_light_yolo_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/yolov3*/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolov3*/&lt;chip&gt;_cavalry&lt;version&gt;_yolov3*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolov3_fastest/yolov3_fastest_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/light_yolo/config/ea_inf_yolov3_fastest.yaml -pwd ./out/caffe/demo_networks/yolov3_fastest</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolov3_fastest_xl/yolov3_fastest_xl_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/light_yolo/config/ea_inf_yolov3_fastest_xl.yaml -pwd ./out/caffe/demo_networks/yolov3_fastest_xl</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolov3_mnetv2_nano/yolov3_mnetv2_nano_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/light_yolo/config/ea_inf_yolov3_mnetv2_nano.yaml -pwd ./out/caffe/demo_networks/yolov3_mnetv2_nano</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolov3_mnetv2_lite/yolov3_mnetv2_lite_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/light_yolo/config/ea_inf_yolov3_mnetv2_lite.yaml -pwd ./out/caffe/demo_networks/yolov3_mnetv2_lite</div>
</div><!-- fragment --></li>
<li>Accuracy Mode (Not Supported). <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption `-p orig</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/yolov3_fastest/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolov3_fastest/&lt;chip&gt;_cavalry&lt;version&gt;_yolov3_fastest.bin \</div>
<div class="line">        -pn yolov3 -pl caffe/demo_networks/light_yolo/config/yolov3_fastest.lua -dm 0 \</div>
<div class="line">        -lp caffe/demo_networks/light_yolo/config/label_voc.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/yolov3_fastest_xl/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolov3_fastest_xl/&lt;chip&gt;_cavalry&lt;version&gt;_yolov3_fastest_xl.bin \</div>
<div class="line">        -pn yolov3 -pl caffe/demo_networks/light_yolo/config/yolov3_fastest.lua -dm 0 \</div>
<div class="line">        -lp caffe/demo_networks/light_yolo/config/label_voc.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">       -cb out/caffe/demo_networks/yolov3_mnetv2_nano/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolov3_mnetv2_nano/&lt;chip&gt;_cavalry&lt;version&gt;_yolov3_mnetv2_nano.bin \</div>
<div class="line">       -pn yolov3 -pl caffe/demo_networks/light_yolo/config/yolov3_mnetv2_nano.lua -dm 0 \</div>
<div class="line">       -lp caffe/demo_networks/light_yolo/config/label_voc.txt \</div>
<div class="line">       --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">       -cb out/caffe/demo_networks/yolov3_mnetv2_lite/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolov3_mnetv2_lite/&lt;chip&gt;_cavalry&lt;version&gt;_yolov3_mnetv2_lite.bin \</div>
<div class="line">       -pn yolov3 -pl caffe/demo_networks/light_yolo/config/yolov3_mnetv2_lite.lua -dm 0 \</div>
<div class="line">       -lp caffe/demo_networks/light_yolo/config/label_voc.txt \</div>
<div class="line">       --fsync_off -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_light_yolo_run_c_inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example, refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For EVK board:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_light_yolo_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;network_name&gt;_cavalry.bin</b>.</li>
<li>The <b>yolov3_*.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ul>
</li>
<li>For X86:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sec_caffe_light_yolo_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;network_name&gt;.json</b> and <b>&lt;network_name&gt;_ades.cmd</b>.</li>
</ul>
</li>
</ul>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/light_yolo/</div>
<div class="line">|--model</div>
<div class="line">|    yolov3_fastest_cavalry.bin</div>
<div class="line">|    yolov3_fastest_xl_cavalry.bin</div>
<div class="line">|    yolov3_mnetv2_nano_cavalry.bin</div>
<div class="line">|    yolov3_mnetv2_lite_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|    label_voc.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    dog.bin</div>
<div class="line">|    dog.jpg</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li><b>live_tiny_anchors.txt</b> has the following content inside, <code>26,48,67,84,72,175,189,126,137,236,265,259</code>.</li>
<li>Users can find <b>"label_voc.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users must keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 simulator:<ol type="a">
<li>Run ADES mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">         --model_path &lt;usr_path&gt;/out_yolov3_fastest_parser/yolov3_fastest.json \</div>
<div class="line">         --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_fastest/yolov3_fastest_ades.cmd \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">         --output_dir &lt;usr_path&gt;/yolov3_fastest/out</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_xl_parser/yolov3_fastest_xl.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_fastest_xl/yolov3_fastest_xl_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_fastest_xl/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_lite_parser/yolov3_mnetv2_lite.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_mnetv2_lite/yolov3_mnetv2_lite_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_mnetv2_lite/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_nano_parser/yolov3_mnetv2_nano.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_mnetv2_nano/yolov3_mnetv2_nano_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_mnetv2_nano/out</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The image is used as an input with the correct preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_parser/yolov3_fastest.json \</div>
<div class="line">        --lua_file yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_fastest/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_fastest/yolov3_fastest_ades.cmd</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_xl_parser/yolov3_fastest_xl.json \</div>
<div class="line">        --lua_file yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_fastest_xl/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_fastest_xl/yolov3_fastest_xl_ades.cmd</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_lite_parser/yolov3_mnetv2_lite.json \</div>
<div class="line">        --lua_file yolov3_mnetv2_lite.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_mnetv2_lite/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_mnetv2_lite/yolov3_mnetv2_lite_ades.cmd</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_nano_parser/yolov3_mnetv2_nano.json \</div>
<div class="line">        --lua_file yolov3_mnetv2_nano.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov3_mnetv2_nano/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_yolov3_mnetv2_nano/yolov3_mnetv2_nano_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>Run Acinference mode.<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_parser/yolov3_fastest.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">         --model_path &lt;usr_path&gt;/out_yolov3_fastest_xl_parser/yolov3_fastest_xl.json \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">         --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_lite_parser/yolov3_mnetv2_lite.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_nano_parser/yolov3_mnetv2_nano.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The image is used as an input with the correct preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_parser/yolov3_fastest.json \</div>
<div class="line">        --lua_file yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_fastest_xl_parser/yolov3_fastest_xl.json \</div>
<div class="line">        --lua_file yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_lite_parser/yolov3_mnetv2_lite.json \</div>
<div class="line">        --lua_file yolov3_mnetv2_lite.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolov3_mnetv2_nano_parser/yolov3_mnetv2_nano.json \</div>
<div class="line">        --lua_file yolov3_mnetv2_nano.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_voc.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/light_yolo/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>For EVK board:<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV7x, cavalry has been already boot up, users do not need to run this command.</li>
<li>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</li>
</ul>
</dd></dl>
</li>
<li>Run the following.<ol type="i">
<li>Dummy mode, to test CVflow performance:<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_cavalry.bin</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_xl_cavalry.bin</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_lite_cavalry.bin</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_nano_cavalry.bin</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The real image is used as an input with the correct preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_cavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/light_yolo/labels/label_voc.txt \</div>
<div class="line">        --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_xl_cavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_fastest.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/light_yolo/labels/label_voc.txt \</div>
<div class="line">        --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_litecavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_lite.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/light_yolo/labels/label_voc.txt \</div>
<div class="line">        --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_nanocavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_nano.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/light_yolo/labels/label_voc.txt \</div>
<div class="line">        --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The raw.bin is used as an input without the correct preprocess and postprocess.<ol type="A">
<li>yolov3_fastest: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_xl_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_lite_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_nano_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=/sdcard/light_yolo/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/light_yolo/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/caffe/demo_networks/light_yolo/dra_img/dog.jpg</code>) in <em>/sdcard/light_yolo/in</em>, and create <code>/sdcard/light_yolo/out</code> as the output directory.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --fb_num 8 --reallocate_mem overlay,0x01000000</div>
</div><!-- fragment --></li>
<li>Run the following.<ol type="a">
<li>Streams live mode (draw on stream without frame sync machine <code>rtsp://10.0.0.2/stream1</code>)<ol type="i">
<li>yolov3_fastest: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_fastest.lua \</div>
<div class="line">     --label_path /sdcard/light_yolo/labels/label_voc.txt --fsync_off</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_xl_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_fastest.lua \</div>
<div class="line">     --label_path /sdcard/light_yolo/labels/label_voc.txt --fsync_off</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_lite_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_lite.lua \</div>
<div class="line">     --label_path /sdcard/light_yolo/labels/label_voc.txt --fsync_off</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_nano_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_nano.lua \</div>
<div class="line">     --label_path /sdcard/light_yolo/labels/label_voc.txt --fsync_off</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For stream live mode, option <b>&ndash;fsync_off</b> disables frame sync. If need to enable frame sync, users should enable encode dummy in eazyai_video.sh.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li>Video output (VOUT) live mode (draw on VOUT high definition multimedia interface (HDMI®))<ol type="i">
<li>yolov3_fastest: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_cavalry.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/yolov3_fastest.lua \</div>
<div class="line">      --label_path /sdcard/light_yolo/labels/label_voc.txt</div>
</div><!-- fragment --></li>
<li>yolov3_fastest_xl: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_fastest_xl_cavalry.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/yolov3_spp.lua \</div>
<div class="line">      --label_path /sdcard/light_yolo/labels/label_voc.txt</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_lite: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_lite_cavalry.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_lite.lua \</div>
<div class="line">      --label_path /sdcard/light_yolo/labels/label_voc.txt</div>
</div><!-- fragment --></li>
<li>yolov3_mnetv2_nano: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">      --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/light_yolo/model/yolov3_mnetv2_nano_cavalry.bin \</div>
<div class="line">      --lua_file /usr/share/ambarella/eazyai/lua/yolov3_mnetv2_nano.lua \</div>
<div class="line">      --label_path /sdcard/light_yolo/labels/label_voc.txt</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<hr  />
<h1><a class="anchor" id="sec_caffe_mtcnn"></a>
MTCNN</h1>
<h2><a class="anchor" id="caffe_mtcnn_download_model"></a>
1 Download Model</h2>
<p>The pre-trained MTCNN model files can be downloaded from <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment/tree/master/code/codes/MTCNNv1/model">here</a>. </p><div class="fragment"><div class="line">commit bace6de9fab6ddf41f1bdf1c2207c50f7039c877</div>
</div><!-- fragment --><p> The pre-trained model files include the following files.</p>
<ul>
<li>PNet: det1.caffemodel, det1.prototxt</li>
<li>RNet: det2.caffemodel, det2.prototxt</li>
<li>ONet: det3.caffemodel, det3.prototxt</li>
</ul>
<h2><a class="anchor" id="caffe_mtcnn_generate_cavalry_binary"></a>
2 Generate Cavalry Binary</h2>
<p>Because the default dimension order in Caffe (N, C, Height, Width) differs from the dimension order of each input data layer in the downloaded model (0, 3, width, height), transpose pre-processing for input is added in <em>caffeparser.py</em>.</p>
<p>Since the PNet model does not include a fully connected layer, it can be used with different input resolutions. However, because the VP only supports a fixed input size, users cannot use one Cavalry binary for various input resolutions. Therefore, users should generate several Cavalry binaries of PNet with different input sizes.</p>
<p>Use the following steps to calculate the input sizes in the image pyramid of PNet.</p><ol type="1">
<li>Determine the size of the original FoV, and the minimum squared face size on the FoV. For example, an FoV with resolution 1902x1080 is used, the minimum squared face size is 60x60.</li>
<li>Calculate sizes in the pyramid. Use <code>mtcnn_get_pnet_scales</code> in <a class="elRef" href="../../../library/d5/d9f/page_lib_cflite_doc.html#cflite_tools">3 CFlite Python Tools</a> to calculate sizes in the pyramid. The factor option is used to configure the scale-down factor between two neighbor sizes in the pyramid. (Typically, it is 0.709). <div class="fragment"><div class="line">build $ mtcnn_get_pnet_scales --height 1080 --width 1920 --min 60 --factor 0.709</div>
<div class="line">height=1080, width=1920, min=60, factor=0.709</div>
<div class="line">Scale from</div>
<div class="line">(H,W)</div>
<div class="line">(1080,1920)</div>
<div class="line">to</div>
<div class="line">(H,W)</div>
<div class="line">(216,384)</div>
<div class="line">(154,273)</div>
<div class="line">(109,194)</div>
<div class="line">(77,137)</div>
<div class="line">(55,98)</div>
<div class="line">(39,69)</div>
<div class="line">(28,49)</div>
<div class="line">(20,35)</div>
<div class="line">(14,25)</div>
<div class="line">Done</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>The factor takes effect only from the second layer.</li>
<li>The first layer’s resolution is not determined by the <em>--factor</em> option; it is determined by the <em>--min</em> option and the sliding windown size 12 of PNet. For further details, refer to the source code in <em>mtcnn_get_pnet_scales.py</em>.</li>
</ul>
</dd></dl>
</li>
<li>Generate a PNet Cavalry binary for each size in the above calculated pyramid sizes.<ol type="a">
<li>Using (216,384) as an example, modify the input shape in <em>cnngen/caffe/demo_networks/mtcnn/pnet/models/det1.protxt</em>. <div class="fragment"><div class="line">input: <span class="stringliteral">&quot;data&quot;</span></div>
<div class="line">layer {</div>
<div class="line">    <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>: <span class="stringliteral">&quot;data&quot;</span></div>
<div class="line">    type: <span class="stringliteral">&quot;Input&quot;</span></div>
<div class="line">    top: <span class="stringliteral">&quot;data&quot;</span></div>
<div class="line">    input_param {</div>
<div class="line">    shape: { dim: 1 dim: 3 dim: 384 dim: 216 }</div>
<div class="line">    }</div>
<div class="line">}</div>
</div><!-- fragment --></li>
<li>Generate the Cavalry binary. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/mtcnn/pnet/config/ea_cvt_pnet.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/pnet/</code>. <dl class="section note"><dt>Note</dt><dd><ul>
<li>The default configuration uses default parser option for the balance between accuracy and performance.</li>
<li>As users seeing, the shapes are different between prototxt and the cfg_yaml file <code>ea_cvt_pnet.yaml</code>. In prototxt, it is (384, 216), but in <code>ea_cvt_pnet.yaml</code>, it should be (216, 384), which will be used by parser. The reason is that the network was trained in Matlab. The input of the caffe model is an image which is transposed between width and height. So for parser, the input image is (216, 384) which is defined in <code>ea_cvt_pnet.yaml</code>, it will use <em>tranpose="0,1,3,2"</em> to transpose to the real network input (384, 216) which is defined in the prototxt file.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li>CNNGen sample has included a full flow to convert all the different input sizes of PNet. <div class="fragment"><div class="line">build $ make sync_build_mkcfg</div>
<div class="line">build $ make cvflow_&lt;v2 or v3&gt;_config</div>
<div class="line">build $ make menuconfig</div>
<div class="line">         [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">           [*] Ambarella Caffe Demo Networks  ---&gt;</div>
<div class="line">             [*]   Build Caffe PNET Networks  ---&gt;</div>
<div class="line">                () Parser Option</div>
<div class="line">                [*]   Convert Multiple Pnet Model  ---&gt;</div>
<div class="line">                   (1080,1920) Height and Width of Original Image--&gt;</div>
<div class="line">                   (60)  Minimum Squared Face Size on the Image</div>
<div class="line">                   (0.709) Scale Factor Between Two Scales</div>
<div class="line">build $ make pnet_multi run_mode=cavalry</div>
</div><!-- fragment --> The following Cavalry binary files are output under the following folder <em>out/caffe/demo_networks/pnet_multi/cavalry</em>. <div class="fragment"><div class="line">build $ ls -l out/caffe/demo_networks/pnet_multi/cavalry/</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:29 cavalry_pnet_109x194</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:31 cavalry_pnet_14x25</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:29 cavalry_pnet_154x273</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:30 cavalry_pnet_20x35</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:29 cavalry_pnet_216x384</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:30 cavalry_pnet_28x49</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:30 cavalry_pnet_39x69</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:30 cavalry_pnet_55x98</div>
<div class="line">     drwxr-xr-x 2 4096 May 25 14:30 cavalry_pnet_77x137</div>
</div><!-- fragment --></li>
<li>Generate the RNet Cavalry binary. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/mtcnn/rnet/config/ea_cvt_rnet.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/rnet/</code>.</li>
<li>Generate the ONET Cavalry binary. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/mtcnn/rnet/config/ea_cvt_onet.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/onet/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/rnet(pnet/onet)</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_rnet(pnet/onet).bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/rnet(pnet/onet)/&lt;chip&gt;/&lt;chip&gt;_rnet(pnet/onet)</code>.</li>
<li>For X86 simulator, model desc json file <b>rnet(pnet/onet).json</b> is in the cnngen output folder <code>out/caffe/demo_networks/rnet(pnet/onet)/out_rnet(pnet/onet)_parser/</code>. ades command <b>rnet(pnet/onet)_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/rnet(pnet/onet)/&lt;chip&gt;/&lt;chip&gt;_ades_rnet(pnet/onet)</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="caffe_mtcnn_build_evk_binary"></a>
3 Build EVK Binary</h2>
<p>Build the EVK binary as below. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">    [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">        [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">            [*] Build EazyAI applications  ---&gt;</div>
<div class="line">                [*] Build FACE EazyAI apps  ---&gt;</div>
<div class="line">                    [*] Build MTCNN EazyAI apps</div>
<div class="line">build $ make test_mtcnn</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Select "[ ] Build eazyai library with OpenCV support" if the file mode will run.</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_mtcnn_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/onet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onet/&lt;chip&gt;_cavalry&lt;version&gt;_onet.bin</div>
</div><!-- fragment --></li>
<li>File Mode (Without Postprocess) <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/onet/onet_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/onet</div>
</div><!-- fragment --></li>
<li>Accuracy Mode (Not supported) <dl class="section note"><dt>Note</dt><dd><ul>
<li>onet can be replaced with pnet or rnet.</li>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#caffe_mtcnn_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="caffe_mtcnn_run_c_inference"></a>
5 Run C Inference</h2>
<ol type="1">
<li><p class="startli">Copy files to SD card on the CV board.</p>
<p class="startli">The demo application reads the Cavalry binary files of PNet, RNet, and ONet from a single folder specified in the parameters. The folder includes several PNet Cavalry binary files, one RNet file, and one ONet Cavalry file. The demo application identifies the PNet binary files using the key words <em>pnet</em> and <em>bin</em>, identifies the RNet binary file by the key words <em>rnet</em> and <em>bin</em>, and identifies the ONet binary file by the key words <em>onet</em> and <em>bin</em>. So the PNet model name must be <em>pnet*.bin</em>, the rnet model name must be <em>rnet*.bin</em>, and the ONet model name must be <em>onet*.bin</em> for reading by test_mtcnn demo.</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/mtcnn</div>
<div class="line">|--in</div>
<div class="line">|       faces01.jpg</div>
<div class="line">|       faces02.jpg</div>
<div class="line">|       pnet_216x384_cavalry.bin</div>
<div class="line">|       pnet_154x273_cavalry.bin</div>
<div class="line">|       pnet_109x194_cavalry.bin</div>
<div class="line">|       pnet_77x137_cavalry.bin</div>
<div class="line">|       pnet_55x98_cavalry.bin</div>
<div class="line">|       pnet_39x69_cavalry.bin</div>
<div class="line">|       pnet_28x49_cavalry.bin</div>
<div class="line">|       pnet_20x35_cavalry.bin</div>
<div class="line">|       pnet_14x25_cavalry.bin</div>
<div class="line">|       rnet_cavalry.bin</div>
<div class="line">|       onet_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Because the demo application does not identify the input resolution from the names of the PNet Cavalry binaries, users do not need to specify the size in the file name.</li>
<li>The demo application can load a maximum of 16 PNet binary files in the folder by default. If more than 16 PNet Cavalry binary files are required, change <em>MAX_PNET_NUM</em> as defined in <em>[SDK]/ambarella/unit_test/private/cv_test/face/mtcnn/mtcnn.h</em>.</li>
<li>If detection of faraway faces is not required, users can remove the larger-sized PNet binary files from the folder. For example, if <em>pnet_288x384_cavalry.bin</em> and <em>pnet_205x273_cavalry.bin</em> are removed, then some small faces will no longer be detected.</li>
<li>If detection of large faces is not required, users can remove the smaller-sized PNet binary files from the folder. For example, if <em>pnet_14x18_cavalry.bin</em> and <em>pnet_19x25_cavalry.bin</em> are removed, some large faces will no longer be detected.</li>
</ul>
</dd></dl>
</li>
<li><p class="startli">Initialize the environment on the CV board.</p>
<p class="startli">Take CV22 Walnut and imx274_mipi for example. </p><div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 720p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, please use bigger value in <em>--enc_dummy_latency 4</em> such as 7. Then if the overlay buffer size is not enough, it can be added by changing the size in <em>--reallocate_mem overlay,0x01400000</em>.</dd></dl>
</li>
<li>Run the demo.<ol type="a">
<li>Live mode (draw on stream <em>rtsp://10.0.0.2/stream1</em>) <div class="fragment"><div class="line">board # test_mtcnn -m 0 -i /sdcard/mtcnn/in -b 0 -a 1 -s 0</div>
</div><!-- fragment --></li>
<li>Live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_mtcnn -m 0 -i /sdcard/mtcnn/in -b 0 -a 0</div>
</div><!-- fragment --></li>
<li>File mode (the input of JPEG image files are placed under <em>/sdcard/mtcnn/in</em>, the result of JPEG image files are generated under <em>/sdcard/mtcnn/out</em>) <div class="fragment"><div class="line">board # test_mtcnn -m 1 -i /sdcard/mtcnn/in -o /sdcard/mtcnn/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_caffe_mobilenetv1"></a>
Mobilenet Runtime Rotation</h1>
<p>Runtime rotation / flip helps the camera adapt to variance deployment posture. The following section provides an example of performing the mobilenet 180 degree rotation.</p><ul>
<li>Create a json preprocessing file: <div class="fragment"><div class="line">{</div>
<div class="line">    <span class="stringliteral">&quot;inputs&quot;</span>: [</div>
<div class="line">       {</div>
<div class="line">            <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;filepath&quot;</span>: <span class="stringliteral">&quot;&lt;user_path&gt;/out/caffe/demo_networks/mobilenetv1_ssd/dra_image_bin/dra_bin_list.txt&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;shape&quot;</span>: [1,3,224,224],</div>
<div class="line">            <span class="stringliteral">&quot;quantized&quot;</span>: <span class="keyword">true</span>,</div>
<div class="line">            <span class="stringliteral">&quot;dataformat&quot;</span>: <span class="stringliteral">&quot;0,0,0,0&quot;</span>,</div>
<div class="line">            &lt;em&gt;<span class="stringliteral">&quot;rt_config&quot;</span>: 1&lt;/em&gt;</div>
<div class="line">        }</div>
<div class="line">    ],</div>
<div class="line">    <span class="stringliteral">&quot;constants&quot;</span>: [</div>
<div class="line">        {</div>
<div class="line">            <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;mean_const&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;values&quot;</span>: [104,117,124]</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;scale_const&quot;</span></div>
<div class="line">            <span class="stringliteral">&quot;values&quot;</span>: [58.8]</div>
<div class="line">        }</div>
<div class="line">    ],</div>
<div class="line">    <span class="stringliteral">&quot;operators&quot;</span>: [</div>
<div class="line">        {</div>
<div class="line">            <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;SUBTRACT&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;input_mean&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;inputs&quot;</span>: [<span class="stringliteral">&quot;input&quot;</span>, <span class="stringliteral">&quot;mean_const&quot;</span>]</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            <span class="stringliteral">&quot;type&quot;</span>: <span class="stringliteral">&quot;SCALE&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;name&quot;</span>: <span class="stringliteral">&quot;data&quot;</span>,</div>
<div class="line">            <span class="stringliteral">&quot;inputs&quot;</span>: [<span class="stringliteral">&quot;input_mean&quot;</span>, <span class="stringliteral">&quot;scale_const&quot;</span>]</div>
<div class="line">        }</div>
<div class="line">    ]</div>
<div class="line">}</div>
</div><!-- fragment --> On one side, the CNNGen network input name changes because of the definition of the <em>"name"</em> value in the <em>"inputs"</em> field. On the other side, the <em>"name"</em> value in the last <em>"operators"</em> field should be the same as the original input layer name in the prototxt/pb file.</li>
<li>Use the json pre-processing file in the parser option. <div class="fragment"><div class="line">build $ caffeparser.py -p prototxt_file -m model_file \</div>
<div class="line">      -o net_name -of parser_output_folder -pp preprocessing_file.json ……</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Do not use the <em>"-i"</em> option, <em>"-idf"</em> option, <em>"-im"</em> option, <em>"-is"</em> option simultaneously with the <em>"-pp"</em> option. In future releases, Ambarella will consider using the json file to unify the pre-processing workflow.</dd></dl>
</li>
<li><p class="startli">Use rotate-flip flag in "test_nnctrl". </p><div class="fragment"><div class="line">board # test_nnctrl -b mobilenet_v1_cavalry.bin \</div>
<div class="line">      --in data=13_rot.bin --rotate-flip 0x06 \</div>
<div class="line">      --out fc7=rot_with_flag.bin</div>
</div><!-- fragment --><p class="startli">The rotation-flip value follows bitmap order: </p><a class="anchor" id="Rotation_flip_value"></a>
<table class="doxtable">
<caption></caption>
<tr>
<td>Plan_flip </td><td>Depth_flip </td><td>Vertical_flip </td><td>Horizontal_flip </td><td>Rotate_clockwise_90 </td></tr>
</table>
<p class="startli">A 180 degree rotation could be divided into a vertical flip 0x04 and a horizontal flip 0x02, resulting in a 180 degree rotation of 0x06. Similarly, a 270 degree rotation would result in 0x07. Note that because rotate-flip is a port character, in this input rotation case, the <em>"--rotate-flip"</em> parameter should follow the port <em>"--in"</em> instead of the port <em>"--out"</em>.</p>
</li>
<li>Check the result. Use the tool binrotate.py to rotate an original RGB file to 180 degree. <div class="fragment"><div class="line">build $ python3 binrotate.py -i 13_org.bin -r 180 \</div>
<div class="line">      -o 13_rot.bin -width 224 -height 224 -chan 3</div>
</div><!-- fragment --></li>
<li>Copy the origin and the rotated binary input file to board. Run the following: <div class="fragment"><div class="line">board # test_nnctrl -b mobilenet_v1_cavalry.bin \</div>
<div class="line">      --in input=13_rot.bin --rotate-flip 0x06 --out fc7=rot_with_flag.bin</div>
<div class="line">board # test_nnctrl -b mobilenet_v1_cavalry.bin \</div>
<div class="line">      --in input=13_org.bin --out fc7=org_no_flag.bin</div>
<div class="line">board # test_nnctrl -b mobilenet_v1_cavalry.bin \</div>
<div class="line">      --in input=13_rot.bin --out fc7=rot_no_flag.bin</div>
</div><!-- fragment --></li>
<li>Use <em>md5sum</em> or other tools to check the output binaries. The <em>org_no_flag.bin</em> is the same as the <em>rot_with_flag.bin</em>, while <em>rot_no_flag.bin</em> is different. Also note that if the input image is rotated clockwise 90 degrees, then test_nnctrl should use 0x07 to rotate the input anti-clockwise 90 degrees instead of using 0x01.</li>
</ul>
<hr  />
<h1><a class="anchor" id="fs_caffe_pvanet"></a>
PVANET Demo</h1>
<p>With the development of deep neural networks (DNN), some networks use a two-step structure to perform object detection, which reduces computational costs and improves the detection accuracy for most practical usages. Due to the efficiency, networks such as the Fast-RCNN [1] and Faster-RCNN [2] are becoming popular in practical scenarios.</p>
<p>This section uses PVANet [3] as an example for deploying multiple-step structure networks on to the Ambarella CV platform, using the toolchain provided by Ambarella. The following section describes the PVANet network structure and the tools required for conversion, explains procedures for performing individual network tasks, and then provides the performance of the PVANet on the Ambarella CV platform.</p>
<p>Some references [1], [2], [3], [4] and [5] are as below.</p>
<ol type="1">
<li>Ross Girshick. Fast R-CNN. Proceedings of the International Conference on Computer Vision (ICCV), 2015.</li>
<li>Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Advances in Neural Information Processing Systems, 2015.</li>
<li>Sanghoon Hong, Byungseok Roh, Kye-Hyeon Kim, Yeongjae Cheon, and Minje Park. PVANet: Lightweight Deep Neural 4. Networks for Real-time Object Detection. arXiv preprint arXiv: 1611.08588, 2016.</li>
<li>Sean Bell, C.Lawrence Zitnick, Kavita Bala, and Ross Girshick. Inside-outside net: Detection objects in context with skip pooling and recurrent neural networks. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</li>
<li>Tao Kong, Anbang Yao, Yurong Chen, and Fuchun Sun. HyperNet: Towards accurate region proposal generation and joint object detection. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016.</li>
</ol>
<hr  />
<h2><a class="anchor" id="sec_pvanet_download"></a>
1 Download</h2>
<p>Download PVANet and its fork Caffe from the GitHub repository. </p><pre class="fragment">Build $ git clone --recursive https://github.com/sanghoon/pva-faster-rcnn.git
</pre><p>Download the PVANet detection model for VOC2012.</p>
<ul>
<li>Use the Google Drive link provided below to download the full Caffe model for VOC2012: <a href="https://drive.google.com/open?id=0Bw_6VpHzQoMVa3M0Zm5zNnEtQUE">https://drive.google.com/open?id=0Bw_6VpHzQoMVa3M0Zm5zNnEtQUE</a></li>
<li>Use the Google Drive link provided below to download the compressed version: <a href="https://drive.google.com/open?id=0Bw_6VpHzQoMVZU1BdEJDZG5MVXM">https://drive.google.com/open?id=0Bw_6VpHzQoMVZU1BdEJDZG5MVXM</a></li>
</ul>
<p>Follow the instructions on the PVANet GitHub page to setup Caffe or use AmbaCaffe.</p>
<hr  />
<h2><a class="anchor" id="sec_pvanet_network_split"></a>
2 Network Split</h2>
<p>Basically, the PVANet is a fork that comes from Faster-RCNN [2], which utilizes a two-step structure to perform object detection. It follows the same method as Faster-RCNN [2] with some modifications specialized for object detection [3]. The entire structure of PVANet detection network is shown in the figure below.</p>
<div class="image">
<img src="../../pvanet_structure.jpg" alt=""/>
<div class="caption">
The Structure of PVANet Detection Network</div></div>
   <p>The structure shows how the PVANet uses a convolution neural network (CNN) to extract features. Similar to the ION [4] and HyperNet [5], PVANet takes advantage of the structure of Hyper-feature concatenation that combines the features from the last layer and two intermediate layers. Next, the region proposal network (RPN) takes first 128 channels from ‘convf’ as an input to reduce the computational costs without decreasing accuracy. The RPN has two predication layers for 84 scores (2 x 42 anchors) and 168 bounding box regressions (4 x 42 anchors) following a 384x3x3 convolutional layer. All 512 channels from ‘convf’ are used for the classification network. Therefore, a 512x6x6 tensor is generated from each ROI pooling. Afterward, the tensors are fed into two fully-connected layers to generated 21 scores and 4 x 21 predicated values of 21 bounding box.</p>
<dl class="section note"><dt>Note</dt><dd>For 20-class object detection, RCNN produces 21 predicated scores (20 classes + 1 background).</dd></dl>
<p>Because of the constraints of the Ambarella CVflow Vector Processor, some logistical operations such as ROI pooling and non-maximum suppression (NMS) are not performed in the current toolchain; instead, they must be performed on the Arm side. As a result, manual network splitting is required before feeding into tools for auto-conversion. Based upon this analysis of the PVANet detection network structure, Ambarella splits it into four parts: 1) FEN (feature extraction network) and RPN, 2) ROI pooling, 3) fully-connected classifier, and 4) post-processing. This proposed splitting is shown in the figure below.</p>
<div class="image">
<img src="../../pvanet_split_structure.jpg" alt=""/>
<div class="caption">
The Split Structure of PVANet Detection Network</div></div>
   <h3><a class="anchor" id="sub_pvanetfen_rpn"></a>
2.1 FEN and RPN</h3>
<p>Run the FEN and RPN on the CVflow Vector Processor using the following steps:</p>
<p>Modify the original prototxt file, such as <em>faster_rcnn_train_test_21cls.pt</em>.</p>
<ul>
<li>Remove the layers that come after proposal layers, such as the ROI pooling and FC layers.</li>
<li>Remove layers that can only be used in training phase, such as SoftmaxWithLoss layer.</li>
<li>Replace the Dummy / <b>Python</b> data layer with a simple data layer that only provides the data dimensions.</li>
<li>To utilize NEON technology to boost ROI pooling stage, it requires a reshape layer and a permute layer to convert the shape of the output feature map from 512xh/16xw/16 to 32xh/16xw/16x16.</li>
</ul>
<p>For the prototxt file that was modified for FEN and RPN, refer to the file name: <em>faster_rcnn_train_test_21cls_rpn_mod.pt</em>.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The original PVANet utilizes OpenCV to take the input image as a BGR order.</li>
<li>For easier integration on the Arm side, the output “rpn_bbox_pred” and “rpn_cls_prob_reshape” of the FEN and RPN model use the 16-bit data format in floating point, while the output “concat_convf_permute” still takes the 16-bit data format in fixed point.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="sub_pvanet_roi_pooling"></a>
2.2 ROI Pooling</h3>
<p>The ROI pooling block is performed on the Arm side, which implements the “proposal” Python layer along with the “roi_pool_conv5” ROI pooling layer in the original Caffe model. It takes the output of FEN and RPN as its input, and then outputs the final results of the ROI pooling to the FC classifier. This module leverages the multithreading technique in order to achieve a better performance.</p>
<p>The following figure depicts the ROI pooling workflow. The green blocks represent the data buffers and the blue blocks represent the processing functions.</p>
<div class="image">
<img src="../../roi_pooling_flow.jpg" alt=""/>
<div class="caption">
Flowchart for ROI Pooling</div></div>
   <p>As illustrated in the flowchart above:</p>
<ul>
<li><b>resize_anchor</b> uses the original 42 anchors, and generates new anchors at each feature map point.</li>
<li><b>bbox_trans_inv</b> uses these generated anchors and the RPN-generated bbox deltas, adjusts each bbox, and filters out the bboxes that are not large enough.</li>
<li><b>pre_nms</b> sorts the proposal bboxes according to the foreground probability supplied by RPN for each box, and then keeps only a small portion for the later NMS.</li>
<li><b>nms</b> calculates the IOU over the remaining bboxes, and keeps only one bbox among several bboxes that have an IOU greater than the predefined threshold.</li>
<li><b>ROI pooling</b> performs pooling using the ROI ouput of NMS and the feature map of FEN. It generates pooled feature maps for the FC classifier to obtain more accurate bbox coordinates and scores over each object class.</li>
</ul>
<h3><a class="anchor" id="sub_pvanet_fc_classifier"></a>
2.3 Split:  FC Classifier</h3>
<p>Run the FC classifier on the CVflow Vector Processor by following these steps:</p>
<p>Modify the original prototxt file, such as <em>faster_rcnn_train_test_21cls.pt</em>.</p>
<ul>
<li>Keep only the layers that start from the fully-connected layers and remove the rest.</li>
<li>Add the data input layer with the data dimensions.</li>
<li>Add a permute layer and a reshape layer to revert the ROI tensors from 32x6x6x16 to 512x6x6.</li>
</ul>
<p>For the prototxt file modified for the FC classifier, refer to the file <em>faster_rcnn_train_test_21cls_fc_mod.pt</em>.</p>
<dl class="section note"><dt>Note</dt><dd>Here, the input data format should be set the same as that of the feature maps coming from the FEN and RPN in the fixed 16-bit data format. For easier integration on the Arm side, the outputs take the 16-bit data format in floating point format.</dd></dl>
<h3><a class="anchor" id="sub_pvanet_post_process"></a>
2.4 Split:  Post-Processing</h3>
<p>After obtaining the RCNN output, rcnn_bbox_delta and rcnn_cls_prob, the post-processing module adjusts the final bbox and performs another NMS for each object class. Then, it gets the final result, which runs on the Arm side.</p>
<p>The following flowchart demonstrates the workflow of the post-processing module.</p>
<div class="image">
<img src="../../pvanet_post_process_flow.jpg" alt=""/>
<div class="caption">
Flowchart of Post-Processing</div></div>
   <p>As illustrated in the flowchart:</p>
<ul>
<li><b>bbox_trans_inv_prcnn</b> uses the output bboxes of the previous NMS and the bbox delta of RCNN to perform a final adjustment on the bboxes.</li>
<li><b>nms_all_class</b> fetches the classification probability for each bbox from RCNN. Then, for each class, it sorts all of the remaining bboxes according to their probability, performing NMS afterwards to prevent repeating a bbox for the same object.</li>
</ul>
<hr  />
<h2><a class="anchor" id="sec_pvanet_integration"></a>
3 Integration</h2>
<p>FEN, RPN, and FC classifier run on the CVflow Vector Processor. Alternatively, the ROI pooling and post-processing operations require Arm. To link these together, Ambarella uses the Cavalry framework to chain these blocks together. The following figure provides an overview of this structure.</p>
<div class="image">
<img src="../../pvanet_implement_overview.jpg" alt=""/>
<div class="caption">
Overview of PVANet Implementation on Ambarella CV</div></div>
   <p>For easier integration, the data flow between the CVflow Vector Processor and Arm uses 16-bit floating point format. Additionally, by default, the ROI pooling number (N) is set to 50 to improve the performance without damaging the final detection results.</p>
<hr  />
<h2><a class="anchor" id="sec_pvanet_deployment"></a>
4 Deployment</h2>
<h3><a class="anchor" id="sub_pvanet_pipeline_flow"></a>
4.1 Pipeline Flow</h3>
<p>For the illustration below, assume the average time to process one frame is 156 ms (30 + 75 + 50 + 1) in serial. Arm and VP are independent hardwares, so they can run simultaneously if the pipeline technique is applied. For example, when VP processes the N frames, Arm can process the N-1 frames. The following figure shows the pipeline flow when it is fed five frames. The more frames that are fed, the sooner that the average time reaches 80 ms.</p>
<div class="image">
<img src="../../pvanet_imple_pipiline.jpg" alt=""/>
<div class="caption">
Pipeline Flow</div></div>
   <h3><a class="anchor" id="sub_pvanet_pipeline_dependency"></a>
4.2 Pipeline Dependency</h3>
<p>The following figure demonstrates stage dependency using all of the stages that require the semaphore to communicate. <b>ARM_1</b> and <b>ARM_2</b> cannot run at the same time. When the Arm loading is full, the mutex lock (<b>arm_locker</b>) protects them.</p>
<ul>
<li>Data dependency: <b>vp_stage_1_done</b>, <b>arm_stage_1_done</b> and <b>vp_stage_2_done</b>.</li>
<li>Hardware dependency: <b>vp_stage_1_read</b> and <b>arm_stage_1_ready</b>.</li>
</ul>
<div class="image">
<img src="../../pvanet_imple_pipiline_dependency.jpg" alt=""/>
<div class="caption">
Pipeline Dependency</div></div>
   <h3><a class="anchor" id="sub_pvanet_pipeline_ring_buf"></a>
4.3 Pipeline Ring Buffer Number</h3>
<p>In order to use the pipeline technique, VP and Arm process different frames in a single flow. For example, if VP is processing the N frame, Arm will simultaneously process the N-1 frame. As a result of this process, it is important to use the ring buffer to cache the data history. The following explains the required amount of ring buffers.</p>
<ul>
<li>For P3, users only need to cache one frame of the 2.VP_1 output after 1.VP_2 is done. The VP_1 output and the ARM_1 input use the dual buffer.</li>
<li>For P3, users only need to cache one frame of the 2.ARM_1 output after 1.VP_2 is done. The ARM_1 output uses the dual buffer.</li>
<li>For P3 ~ P7, the VP_2 and ARM_2 is the final output. This means that the VP_2 and ARM_2 output can be one buffer. In order to keep the VP_2 capability for future changes, use the dual buffer for the VP_2 output―it will still function if the VP_2 output gets changed to a single buffer.</li>
</ul>
<hr  />
<h2><a class="anchor" id="sec_pvanet_cavalry_binary"></a>
5 Generate Cavalry Binary</h2>
<p>The Cavalry binary files converted from the split Caffe models can be generated with the CNNGen sample package in one command. </p><pre class="fragment">build $ make sync_build_mkcfg
build $ make cvflow_&lt;v2 or v3&gt;_config
build $ make menuconfig
    [*] Ambarella Caffe Networks  ---&gt;
      [*]   Build Caffe Demo Networks  ---&gt;
        [*]   Build Caffe PVANet Network  ---&gt;
            (faster_rcnn_train_test_21cls_rpn_mod.pt) RPN Prototxt File Name
            (faster_rcnn_train_test_21cls_fc_mod.pt) FC Prototxt File Name
            (PVA9.1_ImgNet_COCO_VOC0712plus.caffemodel) Model File Name
            (-c act-force-fx16) RPN Parser Option
            ()    FC Parser Option
            (data) Input Layer Name
            (480,640) Input Image Height, Width
            (102.9801,115.9465,122.7717) Input Data Mean Vector
            (1)   RGB or BGR (0 RGB, 1 BGR)
build $ make pvanet run_mode=cavalry
</pre><p>The output is in <em>out/caffe/demo_networks/pvanet/cavalry_pvanet/</em>.</p>
<hr  />
<h2><a class="anchor" id="sec_pvanet_evk_binary"></a>
6 Build EVK Binary</h2>
<p>Build the EVK binary as follows: </p><pre class="fragment">build $ make menuconfig
    [*] Ambarella Application Configuration  ---&gt;
        [*] Build AICAM  ---&gt;
            [*] Build AICAM CVflow  ---&gt;
                [*] Build PVANet unit tests
</pre><hr  />
<h2><a class="anchor" id="sec_pvanet_run_on_board"></a>
7 Run on Board</h2>
<p><em>test_pvanet</em> uses one binary file generated for the FEN and RPN phase and one binary file for the FC classifier. Additionally, it requires the I/O port name. Assuming the two binary files are named <b>pvanet_rpn_cavalry.bin</b> and <b>pvanet_fc_cavalry.bin</b>, which are generated by <em>cavalry_gen</em> with command <em>cavalry_gen –d vas_output/ -f xxx.bin</em>, use the following to run PVANet.</p>
<p>In this application, input.bin was preprocessed by the submean and RGB to BGR since the RPN network excluded it. </p><pre class="fragment">board # eazyai_video.sh --idle
board # test_pvanet -c \
        -b arch_cv22/pvanet_rpn_cavalry.bin \
        --in data=input.bin \
        --out rpn_cls_prob_reshape --no \
        --out rpn_bbox_pred --no \
        --out concat_convf_permute  --no \
        -b arch_cv22/pvanet_fc_cavalry.bin \
        --in roi_pool_conv5 --no \
        --out cls_prob --no \
        --out bbox_pred --no
</pre><p>It supports the draw bbox result on the original image file (JPG/PNG/BMP), with the “&ndash;out-img xxx.jpg” added as an option in <em>test_pvanet</em>. It generates a new output file, including the bbox result with the prefix “out_”.</p>
<p><em>test_pvanet_live</em> takes the live stream into account and provides an end-to-end application for detection and classification purposes. Use the following commands to enable the livestream and run the PVANet for content analysis. </p><pre class="fragment">board # eazyai_video.sh --hdmi 1080p
board # test_pvanet_live -c \
        -b pvanet_rpn_cavalry.bin \
        --in data \
        --out rpn_cls_prob_reshape --no \
        --out rpn_bbox_pred --no \
        --out concat_convf_permute --no \
        -b pvanet_fc_cavalry.bin \
        --in roi_pool_conv5 --no \
        --out cls_prob --no \
        --out bbox_pred --no \
        -m pvanet_mean_fx8_bgr_103_116_123.bin \
        -s 0 -i 0
</pre><dl class="section note"><dt>Note</dt><dd>Users can find "pvanet_mean_fx8_bgr_103_116_123.bin" in "SDK/ambarella/app/ai_cam/cvflow/pvanet/".</dd></dl>
<hr  />
<h2><a class="anchor" id="sec_pvanet_pverall_perf"></a>
8 Overall Performance</h2>
<p>The following shows the performance of the PVANet detection network using the model for VOC2012 on the Ambarella CV when Arm, VP pipeline, and Arm NEON technology is enabled. The following performance results are based on CV22.</p>
<a class="anchor" id="PVANet for VOC2012 Ambarella CV Performance with Arm, VP Pipeline, and NEON Technology Enabled"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Model </th><th>RPN </th><th>ROI Pooling </th><th>FC Classifier </th><th>Post-Processing </th><th>Average time per frame </th></tr>
<tr align="middle">
<td>Full Model </td><td>33.5 ms </td><td>9.5 ms </td><td><b>258.9 ms</b> </td><td>0.5 ms </td><td>292.5 ms </td></tr>
<tr align="middle">
<td>Compressed Model </td><td>33.5 ms </td><td>9.5 ms </td><td><b>50.4 ms</b> </td><td>0.5 ms </td><td>85 ms </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>The above results are based on the two public models on public web with ROI number 50, one is original model, the other is compress model which is similar to a pruned model.</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_caffe_qrcode"></a>
QR Code</h1>
<p>This is the live demo for quick response (QR) code detection. There are three parts in the original project: QR code detection, clarity enhancement, and QR code decoding. The detect network is used to detect the location of multiple QR codes. The super-resolution network is used for clarity enhancement of the detected QR codes. The QRCodeDetector() in OpenCV is used for decoding. In this demo, the super-resolution module is not included.</p>
<p>The address of this public project is as follows: </p><div class="fragment"><div class="line">https:<span class="comment">//github.com/WeChatCV/opencv_3rdparty</span></div>
</div><!-- fragment --><p> Users can directly use the following to download the complete opencv_3rdparty: </p><div class="fragment"><div class="line">git clone https:<span class="comment">//github.com/WeChatCV/opencv_3rdparty.git</span></div>
</div><!-- fragment --><p> Then, the detector Caffe model files <code>detect.caffemodel</code> and <code>detect.prototxt</code> can be found.</p>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_download_ssd_caffe"></a>
1 Download SSD Caffe</h2>
<ol type="1">
<li>To download the SSD Caffe, use the following commands. <div class="fragment"><div class="line">build $ export GIT_SSL_NO_VERIFY=1</div>
<div class="line">build $ git clone https:<span class="comment">//github.com/weiliu89/caffe.git</span></div>
<div class="line">build $ cd caffe</div>
<div class="line">build $ git checkout ssd</div>
</div><!-- fragment --></li>
<li>Next, compile and export the Caffe environment. Refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_env_set">3 Environment Setting</a>.</li>
<li>Use <em>buildcaffe</em> to build Caffe (the script is included in the toolchain package).</li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_extract_params"></a>
2 Extract Parameters of PriorBox Layer</h2>
<p>The SSD framework includes two distinctive layers: <b>"PriorLayer"</b> and <b>"DetectionOutputLayer"</b>.</p>
<ul>
<li>"PriorLayer" generates prior bounding boxes on the last few feature maps. It uses the input image size and the feature map size as inputs and does not include back propagation calculation. Because the input image size and the feature map size are fixed, the results of "PriorLayer" remain unchanged.</li>
<li>"DetectionOutputLayer" takes the output of each "PriorLayer" and additional data as inputs. Users can consider the output of "PriorLayer" as parameters, such as the weights of convolution layer, and hence, dump the output of "PriorLayer" from the Caffe model file directly.</li>
</ul>
<p>Ambarella provides the tool <code>ssd_prior_box_handling.py</code> which generates the offline binary of "PriorLayer" and removes "PriorLayer" and "DetectionOutputLayer" in the prototxt file.</p>
<p>Because the computer vision (CV) chip cannot accelerate the calculation speed of the DetectionOutputLayer (as it is a large network management system (NMS) that includes judgment logic), the code exists in <code>ambarella/packages/data_process/</code>.</p>
<p>Therefore, <code>ssd_prior_box_handling.py</code> removes these two layers and implements them on Arm®. The following commands show users how to use <code>ssd_prior_box_handling.py</code>.</p>
<ol type="1">
<li>Source the toolchain, following this command: <div class="fragment"><div class="line">build $ source build/env/cv*.env</div>
</div><!-- fragment --></li>
<li>Change the Caffe version to Caffe-SSD. <div class="fragment"><div class="line">build $ export PYTHONPATH=/&lt;user’s working space&gt;/ssd-caffe-install/usr/local/lib/python3.5/dist-packages:$PYTHONPATH</div>
<div class="line">build $ export LD_LIBRARY_PATH=/&lt;user’s working space&gt;/ssd-caffe-install/usr/local/lib:$LD_LIBRARY_PATH</div>
</div><!-- fragment --></li>
<li>Enter the qrcode folder. <div class="fragment"><div class="line">build $ cd caffe/demo_networks/qrcode/models/</div>
</div><!-- fragment --></li>
<li>Use "ssd_prior_box_handling.py" to remove "PriorLayer" and "DetectionOutputLayer", and dump the results of "PriorLayer". <div class="fragment"><div class="line">build $ ssd_prior_box_handling.py -p detect.prototxt \</div>
<div class="line">        -c detect.caffemodel \</div>
<div class="line">        -op qrcode_deploy.prototxt \</div>
<div class="line">        -opb qrcode_priorbox_fp32.bin</div>
</div><!-- fragment --> </li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_cnngen_conversion"></a>
3 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/qrcode/config/ea_cvt_qrcode.yaml</div>
</div><!-- fragment --><p> The output is in <code>out/caffe/demo_networks/qrcode/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/qrcode</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_qrcode.bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/qrcode/&lt;chip&gt;/&lt;chip&gt;_qrcode</code>.</li>
<li>For X86 simulator, model desc json file <b>qrcode.json</b> is in the cnngen output folder <code>out/caffe/demo_networks/qrcode/out_qrcode_parser/</code>. ades command <b>qrcode_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/qrcode/&lt;chip&gt;/&lt;chip&gt;_ades_qrcode</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_build_unit_test"></a>
4 Build Unit Test</h2>
<ul>
<li>Build unit test for the EVK: <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">      [*]   Ambarella Package Configuration  ---&gt;</div>
<div class="line">      -*-   Build Amabrella EAZYAI library  ---&gt;</div>
<div class="line">          -*-   Build eazyai library with openCV support</div>
<div class="line">          -*-   Build Ambarella custom postprocess library  ---&gt;</div>
<div class="line">              -*-   Build Ambarella custom postprocess library with ssd</div>
<div class="line">              [*]   Build Ambarella custom postprocess library with qrcode</div>
<div class="line">          [*]   Build EazyAI unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li><p class="startli">Build unit test for the X86 simulator:</p>
<p class="startli">Refer to the CNNGen Doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build the X86 unit test. The executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build</code>.</p>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_run_python_inference"></a>
5 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/qrcode/&lt;chip&gt;/&lt;chip&gt;_cavalry_qrcode/&lt;chip&gt;_cavalry&lt;version&gt;_qrcode.bin</div>
</div><!-- fragment --></li>
<li>File Mode (Without Postprocess) <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/qrcode/qrcode_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/caffe/demo_networks/qrcode</div>
</div><!-- fragment --></li>
<li>Accuracy Mode (Not Supported) <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/qrcode/&lt;chip&gt;/&lt;chip&gt;_cavalry_qrcode/&lt;chip&gt;_cavalry&lt;version&gt;_qrcode.bin \</div>
<div class="line">        -pn qrcode -pl caffe/demo_networks/qrcode/config/qrcode.lua -dm 1 \</div>
<div class="line">        -ei caffe/demo_networks/qrcode/models/qrcode_priorbox_fp32.bin \</div>
<div class="line">        --fsync_off -dd STREAM1 -hi</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_caffe_qrcode_run_c_inference"></a>
6 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the EVK:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_qrcode_cnngen_conversion">3 CNNGen Conversion</a> for information on how to generate <code>qrcode_cavalry.bin</code>.</li>
<li>The <code>qrcode.lua</code> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of the EVK. If it does not exist, users can find it in <code>&lt;cvflow_cnngen_samples&gt;/library/eazyai/unit_test/resource/lua</code>.</li>
</ul>
</li>
<li>For X86:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_qrcode_cnngen_conversion">3 CNNGen Conversion</a> for information on how to generate <code>qrcode.json</code> and <code>qrcode_ades.cmd</code>.</li>
</ul>
</li>
</ul>
</dd></dl>
<ul>
<li><p class="startli">Copy files to an SD card for the EVK test.</p>
<p class="startli">Place files on the SD card with the following example structure: </p><div class="fragment"><div class="line">/sdcard/qrcode</div>
<div class="line">|--model</div>
<div class="line">|       qrcode_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|--anchor_boxes</div>
<div class="line">|       qrcode_priorbox_fp32.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|       label_qrcode.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|       qrcode.jpg</div>
<div class="line">|       qrcode.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can find <code>label_qrcode.txt</code> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>Users can find <b>"qrcode_priorbox_fp32.bin"</b> in <code>cvflow_cnngen_samples/caffe/demo_networks/qrcode/models/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's requirements. Users must keep the file path consistent during use.</li>
</ul>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as an input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/qrcode/out_qrcode_parser/qrcode.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/ades_qrcode/qrcode_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/qrcode/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/qrcode/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 --model_path &lt;usr_path&gt;/qrcode/out_qrcode_parser/qrcode.json \</div>
<div class="line">        -n qrcode --lua_file qrcode.lua --queue_size 1 --thread_num 1 --label_path label_qrcode.txt \</div>
<div class="line">        --extra_input qrcode_priorbox_fp32.bin --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/qrcode/out --ades_cmd_file &lt;usr_path&gt;/qrcode/ades_qrcode/qrcode_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as an input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 --model_path &lt;usr_path&gt;/qrcode/out_qrcode_parser/qrcode.json  \</div>
<div class="line">        -n to_file --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/qrcode/dra_img|t:raw&quot;</span> --output_dir &lt;usr_path&gt;/qrcode/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 --model_path &lt;usr_path&gt;/qrcode/out_qrcode_parser/qrcode.json \</div>
<div class="line">        -n qrcode --lua_file qrcode.lua --queue_size 1 --thread_num 1 --label_path label_qrcode.txt \</div>
<div class="line">        --extra_input qrcode_priorbox_fp32.bin --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/qrcode/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/qrcode/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV7x, cavalry has been already boot up, users do not need to run this command.</li>
<li>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</li>
</ul>
</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode for CVflow® performance test: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/qrcode/model/qrcode_cavalry.bin</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input, without preprocess or postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/qrcode/model/qrcode_cavalry.bin \</div>
<div class="line">        -n to_file --isrc <span class="stringliteral">&quot;i:data=/sdcard/qrcode/in|t:raw&quot;</span> --output /sdcard/qrcode/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input, with correct preprocess and postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 1 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/qrcode/model/qrcode_cavalry.bin \</div>
<div class="line">        -n qrcode --lua_file /usr/share/ambarella/eazyai/lua/qrcode.lua \</div>
<div class="line">        --label_path /sdcard/qrcode/labels/label_qrcode.txt \</div>
<div class="line">        --extra_input sdcard/qrcode/anchor_boxes/qrcode_priorbox_fp32.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/qrcode/in|t:jpg|d:vp|c:rgb&quot;</span> --output_dir /sdcard/qrcode/out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode using an image as input, place the test image (such as <code>&lt;cvflow_cnngen_samples&gt;/caffe/demo_networks/qrcode/dra_img/qrcode.jpg</code>) in <code>/sdcard/ssd/in</code>, then create <code>/sdcard/qrcode/out</code> as the output directory.</li>
<li>Default preprocess is based on OpenCV; users can enable VProc if required with the option <b>"d:vp"</b>. The default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b> and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li><p class="startli">Performance on CV22_Walnut</p>
<p class="startli">This demo included two parts.</p><ol type="i">
<li>Detect the QR code with SSD, which include VP part (1 ms for Default DRA) ARM part (1~7 ms), these time will not be effected by QR code number, just the Arm part will wave a little.</li>
<li>QR code recognition with OepnCV <b>detectAndDecode()</b>, based on SSD result, it needs to do the second edge point positioning, which will be much more accurate than the SSD result. It will take 375 ms with 25% CPU for one QR code, increasing linearly with the QR code number, which means running in parallel may get 4 x performance.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode For EVK<ol type="1">
<li>Initialize the environment on the CV board. CV22 Walnut and imx274_mipi are used as examples. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run the following:<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 1 -r -s 0 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/qrcode/model/qrcode_cavalry.bin -n qrcode \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/qrcode.lua \</div>
<div class="line">        --label_path label_qrcode.txt --extra_input qrcode_priorbox_fp32.bin --hold_img</div>
</div><!-- fragment --></li>
<li>Video output (VOUT) live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 1 -r --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/qrcode/model/qrcode_cavalry.bin \</div>
<div class="line">        -n qrcode --lua_file /usr/share/ambarella/eazyai/lua/qrcode.lua \</div>
<div class="line">        --label_path label_qrcode.txt --extra_input qrcode_priorbox_fp32.bin --hold_img</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluent, check the following two points:<ul>
<li>If the display is not fluent, use a greater value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be increased by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (1024 + resolution * (enc-dummy-latency + 5)). For details, please refer to "EazyAI Library API related content in Linux SDK Doxygen documents".</li>
<li>This demo runs inference and postprocess in parallel, and needs the same image for postprocess which is fed in inference stage. For live mode, this demo adds option <b>&ndash;hold_img</b> to convert and save image for postprocess every frame in case of later frames overwrite current frame buffer in postprocess stage.</li>
<li>In postprocess stage, this demo crops an image into some images including QR code according to the detection results, and then resizes the size of these images to 1x1x244x244 to do QR code recognition. It will affect the accuracy of QR code recognition. To get a better accuracy, please query canvas images with the big resolution as input images or get QR code close to the sensor.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_caffe_vgg16_ssd"></a>
Regression for VGG16-SSD</h1>
<p>This section provides the steps for evaluating the mAP accuracy for detection network, including steps for resource preparation, running the regression tool, and obtaining results. The VGG16-SSD is used as an example; other CNN detection networks should use this example as a reference for accuracy regression.</p>
<h2><a class="anchor" id="sub_sec_caffe_vgg16_ssd_context"></a>
1 Context</h2>
<p>The original model of VGG-SSD can be downloaded from the following address: <a href="https://github.com/FreeApe/VGG-or-MobileNet-SSD">https://github.com/FreeApe/VGG-or-MobileNet-SSD</a>.</p>
<p>The original mAP accuracy is 77.2% (according to the author’s statement with IOU=0.5) after training. To reproduce this mAP value, follow the instructions on the GitHub page to set up Caffe and download the VOC2007 and VOC2012 dataset.</p>
<dl class="section note"><dt>Note</dt><dd>The <b>"Build-Caffe"</b> script can also be used to compile the Caffe source code in the CV toolchain package.</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_vgg16_ssd_model_optimization"></a>
2 VGG-SSD Model Optimization</h2>
<p>Users can utilize the sparse accelerator of the Ambarella CVflow engine to perform coefficient pruning. This enables a consistent performance improvement and accuracy. The deeper the network is pruned, the higher the level of performance that can be achieved on the CVflow engine.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Because Ambarella is unable to support all training methods, customers are encouraged to perform the sparse training independently.</li>
<li>Additionally, to best utilize the fast convolution/inner-productor engine, the original model should be quantized to the 8 or 16-bit fixed data type. If this leads to quantization loss from the fractions value, users may need to perform quantization training before deploying the Caffe/TensorFlow model to CVflow engine.</li>
<li>For further details, refer to <a class="el" href="../../d1/d82/fs_workflow.html#sec_model_opt">2 Network Optimization</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_vgg16_ssd_accuracy_regression_workflow"></a>
3 VGG-SSD Accuracy Regression Workflow</h2>
<p>The VGG-SSD is a combined CNN that uses VGG as a front-end feature-map extractor and SSD as a back-end object detector. The accuracy regression test can be divided into two sub-tasks: one to obtain an accuracy number from <b>Caffe/TensorFlow</b>, which is treated as the baseline accuracy; the second sub-task obtains the accuracy number from the CVflow deployment, determining the accuracy gap (by comparing it to the baseline accuracy) after deployment.</p>
<p>Because CNNGen is limited in parsing certain special CNN layers, the entire network is separated into two parts: the parser-enabled network, and the offline process layers. In VGG-SSD, because the output data from the <b>prior_box</b> layer requires only input dimensions, it can be separated from VGG-SSD and the result can be generated offline by Caffe.</p>
<p>The <b>detect_out</b> layer is the back-end layer that implements box decoding and NMS processing. It can also be separated from VGG-SSD and processed offline.</p>
<p>The VGG-SSD CVflow deployment includes two steps: (1) run CVflow using the makefile work-flow to obtain the results of <b>mbox_loc</b> and <b>mbox_conf</b>; (2) run it offline to obtain the results of <b>prior_box</b> and <b>detect_out</b>.</p>
<p>To increase the accuracy between Caffe and CVflow, the Caffe deployment uses the two-step method as well.</p>
<div class="image">
<img src="../../vgg_ssd_workflow.jpg" alt=""/>
<div class="caption">
VGG-SSD Workflow</div></div>
   <div class="image">
<img src="../../vgg_ssd_network_deployment_structure.jpg" alt=""/>
<div class="caption">
VGG-SSD Network Deployment Structure</div></div>
   <p>The dedicated tool <b>"draw_ssd"</b> configures the box decoder and NMS processing, enabling the same mAP calculation method used by the source version of Caffe. Additionally, the Python tool <b>"mAP_evaluation.py"</b> is provided for mAP evaluation.</p>
<p>The following diagram shows the workflow of the VGG-SSD accuracy regression test.</p>
<div class="image">
<img src="../../vgg_ssd_regression_workflow.jpg" alt=""/>
<div class="caption">
VGG-SSD Regression Workflow</div></div>
   <h2><a class="anchor" id="sub_sec_caffe_vgg16_ssd_ground_truth_file"></a>
4 Ground Truth File</h2>
<p>In this implementation, test images are extracted from VOC2007 dataset as recommended by the user. The ground truth annotations in <b>.xml</b> file are reformatted to a <b>csv</b> file for maintenance purposes. The detection result from <b>"draw_ssd"</b> tool is stored in the same manner.</p>
<p>The ground truth file format in the <b>.CSV</b> file includes one line for each image with several ground truth object boxes.</p>
<h4><a class="anchor" id="autotoc_md104"></a>
Ground Truth File Format:</h4>
<a class="anchor" id="Ground Truth File Format"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Image Name </th><th>Box Quantity </th><th>Interval </th><th>Object label </th><th>X_min </th><th>Y_min </th><th>X_max </th><th>Y_max </th><th>Confidence </th><th>Difficult Tag </th></tr>
<tr align="middle">
<td>009888.jpg </td><td>1 </td><td>7 (total of 7 elements from object label to difficult tag) </td><td>20 </td><td>202 </td><td>113 </td><td>316 </td><td>271 </td><td>1.0 (always keep 1.0 in ground truth) </td><td>0 (0=easy to detect, 1= difficult to detect, always keep "0" in detection result) </td></tr>
</table>
<h2><a class="anchor" id="sub_sec_caffe_vgg16_ssd_configuration"></a>
5 Run Regression Test</h2>
<p>The following provides the steps for implementing the classification network evaluation and the detection network evaluation.</p>
<ol type="1">
<li>Download the SSD version of Caffe. For more information about downloading Caffe SSD, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_download_ssd_caffe">1 Download SSD Caffe</a>.</li>
<li>Generate the prior box binary and <b>prototxt</b> file. For detailed steps on generating these files, refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_extract_params">2 Extract Parameters of PriorBox Layer</a>. (The primary steps are the same; only the file path must be changed.) Remember to export the <b>PYTHONPATH</b> and <b>LD_LIBRARY_PATH</b> per the previous section. <div class="fragment"><div class="line">build $ ssd_prior_box_handling.py -p deploy_300_300.prototxt \</div>
<div class="line">        -c VGG_VOC0712_SSD_300x300.caffemodel \</div>
<div class="line">        -op vgg_ssd_out_remove_priobox_300_300.prototxt \</div>
<div class="line">        -opb priorbox_1-2-34928_fp32_300_300.bin</div>
</div><!-- fragment --></li>
<li>Get the test image and the remaining DRA images. After executing the following command, two folders are generated: <b>"dra_img_100"</b> and <b>"test_img_4952"</b>. If the folders are not generated, users manually generate them by copying the corresponding number of images from the user dataset to the two folders. <div class="fragment"><div class="line">build $ sh &lt;user_path&gt;/cnngen_sample/data/VOC_07/get_voc07.sh</div>
</div><!-- fragment --></li>
<li>Convert the network if necessary. If users need to do inference with mode <b>cvflow</b>, <b>acinf</b>, <b>ades</b>, use the previous command to convert original network to CVFlow format. Otherwise, for <b>orig</b> mode, user could skip the this section and directly do the forward inference with the network original platform. The following command starts the convert process: <div class="fragment"><div class="line">build $ python3 eazyai_cvt.py -cy &lt;cnngen_sample_user_path&gt;/caffe/demo_networks/vgg16_ssd/config/ea_cvt_vgg16_ssd.yaml \</div>
<div class="line">        -ey &lt;cnngen_sample_user_path&gt;/caffe/demo_networks/vgg16_ssd/config/ea_cvt_ext_vgg16_ssd.yaml</div>
</div><!-- fragment --> The output is generated under <em>&lt;user_work_dir&gt;/vgg16_ssd</em>. There is a convert summary YAML file, which is named as <em>vgg16_ssd_cvt_summary.yaml</em> generated under the same folder. This summary file is an input configuration for the inference in the next step.</li>
<li>Do the forward inference. The SSD post-processing and mAP evaluation is integrated in the inference workflow. <div class="fragment"><div class="line">build $ python3 eazyai_inf.py -ip 10.0.0.2 -cy &lt;user_work_dir&gt;/vgg16_ssd_cvt_summary.yaml \</div>
<div class="line">        -iy &lt;cnngen_sample_user_path&gt;/caffe/demo_networks/vgg16_ssd/config/ea_inf_acc_vgg16_ssd.yaml --platform cvflow</div>
<div class="line">build $ python3 eazyai_inf.py -cy &lt;user_work_dir&gt;/vgg16_ssd_cvt_summary.yaml \</div>
<div class="line">        -iy &lt;cnngen_sample_user_path&gt;/caffe/demo_networks/vgg16_ssd/config/ea_inf_acc_vgg16_ssd.yaml --platform orig</div>
</div><!-- fragment --> Valid platform choices include <b>cvflow</b>, <b>acinf</b>, <b>ades</b> and <b>orig</b>, IP address is needed on platform <b>cvflow</b>.</li>
</ol>
<p>Some inference results are as follows.</p><ol type="1">
<li>The inference result files can be found in <em>&lt;user_work_dir&gt;/vgg16_ssd/cvflow/inference</em>.</li>
<li>The post-processing result files can be found in <em>&lt;user_work_dir&gt;/vgg16_ssd/cvflow/post_process</em>.</li>
<li>The evaluation report file can be found in <em>&lt;user_work_dir&gt;/vgg16_ssd/cvflow/eval/</em>.</li>
</ol>
<p>For detailed introduction, please refer to fs_accuracy_tool.</p>
<hr  />
<h1><a class="anchor" id="sec_caffe_resnet50_with_drav3"></a>
ResNet50 with DRAv3</h1>
<p>DRAv3 can work with either a CPU or GPU, but it is suggested to use GPU as DRAv3 is a little complex—which can result in slower speeds with a CPU.</p>
<h2><a class="anchor" id="sub_sec_caffe_resnet50_with_drav3_prepare_gpu_env"></a>
1 Prepare GPU Environment</h2>
<p>For DRAv3 GPU, it only can support <b>Cuda9.2</b>, so if users need to use some other version in their GPU server, they need install Cuda9.2 by themselves.</p>
<p>And it is more suggested to use Docker with special Cuda9.2 version which won’t break the user’s original environment.</p>
<p>Steps to install GPU docker image on the host Ubuntu1804-Cuda10.1: </p><div class="fragment"><div class="line">build $ docker pull nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04</div>
<div class="line">build $ docker images</div>
<div class="line">    REPOSITORY   TAG</div>
<div class="line">    nvidia/cuda  9.2-cudnn7-devel-ubuntu18.04</div>
<div class="line">build $ nvidia-docker <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> -v &lt;User Folder&gt;/share_host:/share_docker -itd --<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> ubunt1804_cuda9.2 nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04</div>
<div class="line">build $ docker exec -it ubunt1804_cuda9.2 /bin/bash</div>
<div class="line">build $ docker exec --user &lt;User_Name&gt; -it ubunt1804_cuda9.2 /bin/bash</div>
</div><!-- fragment --><p>Then users need to run <b>"nvidia-smi"</b> to check if the docker image works well and install CNNGen toolchain in this system.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find right docker image in <a href="https://hub.docker.com/r/nvidia/cuda/tags/?page=1">https://hub.docker.com/r/nvidia/cuda/tags/?page=1</a>. For Cuda in host and docker, they will share the same Cuda driver, so users should make sure this docker image’s Cuda library should be compatible with host’s Cuda driver, for details, please refer to <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver">https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver</a>.</li>
<li>For more details on how to set up the docker image and install the toolchain, please refer to Section_3.5 of <em>Ambarella CV* UG Flexible Linux SDK *.* Code Building and Debug Environment</em>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_resnet50_with_drav3_run_drav3"></a>
2 Run DRAv3 with CNNGen Samples</h2>
<p>The example below uses ResNet50 as an example. Other programs are similar: </p><div class="fragment"><div class="line">build $ source build/env/cv*_gpu.env</div>
<div class="line">build $ make sync_build_mkcfg</div>
<div class="line">build $ make cvflow_&lt;v2 or v3&gt;_config</div>
<div class="line">build $ make resnet50 run_mode=accuracy dra=<span class="stringliteral">&quot;-dra mode=3 -gpu -gpu_id 3&quot;</span></div>
</div><!-- fragment --><p>The accuracy output is in <code>out/caffe/test_networks/resnet50/accuracy/</code>. Users can run <b>"run_accuracy.sh"</b> to get the final accuracy. For information on how to use the accuracy tool in the CNNGen Samples, refer to fs_accuracy_tool.</p>
<p>Then, users will get the results as <b>Top1 0.6908</b> and <b>Top5 0.8941</b>, for DRAv2, <b>Top1 is 0.69</b> and <b>Top5 is 0.86</b>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li><b>"-gpu_id"</b> is used to choose which GPU to be used, current parser can only use one GPU, if the user has not set it, the parser will choose a free one.</li>
<li><b>"-gpu"</b> is used to enable GPU, but if user’s environment has no GPU support, it will use CPU.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_resnet50_with_drav3_final_tuning"></a>
3 Final Tuning with DRAv3</h2>
<p>Users could tune strategy of <b>Mode 3</b> by adding command line options after the <em>"dra"</em> flag.</p>
<p>If the user is not satisfied with the default Mode 3 accuracy or speed, these steps could be followed to find a good operating point:</p>
<ol type="1">
<li>Find a tuning baseline by trying: <div class="fragment"><div class="line">build $ make resnet50 run_mode=accuracy dra=<span class="stringliteral">&quot;-dra mode=3,error_measure=1,allow_equalization=true -gpu&quot;</span></div>
</div><!-- fragment --> This setting will have the best performance.</li>
<li>if the baseline is fast enough but not accurate enough: <div class="fragment"><div class="line">build $ make resnet50 run_mode=accuracy dra=<span class="stringliteral">&quot;-dra mode=3,error_measure=1,slope_pcc_end=1,allow_equalization=true -gpu&quot;</span></div>
</div><!-- fragment --> This setting will have the best accuracy. Then, modify “slope_pcc_end” values between 0.95 (default) and 1 (max) to find a good combination.</li>
<li>If the baseline is accurate but not fast enough, adjust the parameters as follows: <div class="fragment"><div class="line">build $ make resnet50 run_mode=accuracy dra=<span class="stringliteral">&quot;-dra mode=3, error_measure=1,slope_pcc_end=0.94,allow_equalization=true -gpu&quot;</span></div>
</div><!-- fragment --></li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_resnet50_with_json_preprocessing"></a>
ResNet50 with Json Pre-Processing</h1>
<p>CNNGen tool allows users to combine a series of pre-processing operations before running on the neural networks with variable input. This can improve the performance by connecting to VP and DSP directly and reducing DRAM bandwidth. It is an advanced feature compared to the normal input pre-process.</p>
<p>This section provides users with steps to convert and deploy ResNet50 network with using Json for pre-processing, which subtracts mean values from a file**. For details on Json, refer to the CNNGen tool guide <em>Ambarella CV UG Json Preprocessing Format.pdf</em>.</p>
<h2><a class="anchor" id="sub_sec_resnet50_json_prepare_mean_file"></a>
1 Prepare Mean file</h2>
<p>In this sample, the pre-processing performs mean subtraction operation of input data. The mean values should be packaged into a binary file, which can be converted from binaryproto file by <em>get_mean</em> tool. Using Resnet50 as an example, the shape of input data and mean binary file is (1, 3, 224, 224).</p>
<p>The <em>get_mean</em> tool and the binaryproto file <em>resnet50.binaryproto</em> are located in <em>cvflow_cnngen_samples_&lt;version&gt;/tools/get_mean/</em>.</p>
<p>Steps to get the correct mean file with <em>get_mean</em> tool are shown below: </p><div class="fragment"><div class="line">build $ cd cvflow_cnngen_samples_&lt;version&gt;/tools/get_mean/</div>
<div class="line">build $ ./run_getmean.sh</div>
</div><!-- fragment --><p>The output is <em>cvflow_cnngen_samples_&lt;version&gt;/tools/get_mean/output_Resnet_mean.bin</em>.</p>
<h2><a class="anchor" id="sub_sec_resnet50_json_prepare_json_file"></a>
2 Prepare the Json File</h2>
<p>Once users have the right input data and mean file, use the following steps to prepare a Json file for the pre-processing operations.</p>
<p>In this case, the Json file should include below arrays:</p>
<ol type="1">
<li>"inputs" There is only one input for the shape (1, 3, 224, 224) required in this case. Users can use <em>gen_image_list.py</em> to generate the text file that contains a list of images path for DRA.</li>
<li>"constants" This array has the constant tensors needed for subtraction. In this case, the mean file is <em>output_Resnet_mean.bin</em> which is generated by get_mean tool, and the shape is (1, 3, 224, 224).</li>
<li><p class="startli">"operators" The operator is a SUBTRACT node that performs a subtraction of the input data. The name needs to match the name of the network's input layer.</p>
<p class="startli">An example Json file is shown below: </p><pre class="fragment">{
    "inputs":
    [
        {
        "name": "input",
        "filepath": "dra_bin/dra_list.txt",
        "shape": [1, 3, 224, 224],
        "quantized": true,
        "dataformat": "0,0,0,0"
        }
    ],
    "constants":
    [
        {
        "name": "mean_const",
        "filepath": "mean_file/get_mean/output_Resnet_mean.bin",
        "shape": [1, 3, 224, 224],
        }
    ],
    "operators":
    [
        {
        "type": "SUBTRACT",
        "name": "data",
        "inputs": ["input", "mean_const"]
        }
    ]
}
</pre></li>
</ol>
<h2><a class="anchor" id="sub_sec_resnet50_json_Convert"></a>
3 Convert</h2>
<p>The section below introduces how to convert the Resnet50 with using Json for pre-processing. For easier reproduction on the customer side, the CNNGen samples package includes the example below.</p>
<p>In this sample, the Json file is automatically generated by the script, and the input images are in <em>cvflow_cnngen_samples_&lt;version&gt;/caffe/test_networks/resnet50/dra_img</em>, which will be converted to NV12 by script automatically as well.</p>
<p>Convert the Resnet50 with Json pre-processing by the following commands. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">    [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">        [*] Build Caffe Test Networks  ---&gt;</div>
<div class="line">            [*] Build Caffe RESNET50 Network  ---&gt;</div>
<div class="line">                [*] Build caffe RESNET50 network with Json pre-process ---&gt;</div>
<div class="line">build $ make resnet50_json_preprocess run_mode=cavalry</div>
</div><!-- fragment --><p>Then, <em>resnet50_json_preprocess_cavalry.bin</em> will be generated in <em>cvflow_cnngen_samples_&lt;version&gt;/out/caffe/test_networks/resnet50_json_preprocess/cavalry_resnet50_json_preprocess/</em>.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>In this convert, pre-processing json file is generated automatically and used by "-pp" option in caffeparser.py. To use specified json file, users can modify it based on the json file of the example Json file, which is located in <em>cvflow_cnngen_samples_&lt;version&gt;/caffe/test_networks/resnet50/example_json</em>.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_resnet50_json_ades"></a>
4 ADES</h2>
<p>Run <b>ADES</b> below with the CNNGen samples package. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">    [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">        [*] Build Caffe Test Networks  ---&gt;</div>
<div class="line">            [*] Build Caffe RESNET50 Network  ---&gt;</div>
<div class="line">                [*] Build caffe RESNET50 network with Json pre-process ---&gt;</div>
<div class="line">build $ make resnet50_json_preprocess run_mode=ades</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ul>
<li>Users should pay attention to the option "-ib" of <em>ades_autogen.py</em> which should match the first input name in Json file, as shown below. <div class="fragment"><div class="line">build $ ades_autogen.py</div>
<div class="line">      ...</div>
<div class="line">      -ib</div>
<div class="line">      input=/CNNGen_sample/out/caffe/test_networks/resnet50_json_preprocess/dra/bike.bin</div>
<div class="line">      ...</div>
</div><!-- fragment --></li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_resnet50_json_accuracy"></a>
5 Accuracy</h2>
<p>Steps to run accuracy test are as below.</p>
<ol type="1">
<li>Compile and generate the command file by the following commands. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">     [*] Ambarella Caffe Networks  ---&gt;</div>
<div class="line">         [*] Build Caffe Test Networks  ---&gt;</div>
<div class="line">             [*] Build Caffe RESNET50 Network  ---&gt;</div>
<div class="line">                 [*] Build caffe RESNET50 network with Json pre-process ---&gt;</div>
<div class="line">build $ make resnet50_json_preprocess run_mode=accuracy</div>
</div><!-- fragment --></li>
<li>Run accuracy test by the following commands. <div class="fragment"><div class="line">build $ sh &lt;user_path&gt;/cnngen_sample/out/caffe/test_networks/resnet50_json_preprocess/accuracy/run_accuracy.sh</div>
</div><!-- fragment --></li>
</ol>
<p>Locate the accuracy report file from the following folder path: <em>&lt;user_path&gt;/cnngen_sample/out/test_networks/resnet50_json_preprocess/accuracy/report/cv_topN.csv</em></p>
<p>The result binary is stored in the following folder path: <em>&lt;user_path&gt;/cnngen_sample/out/test_networks/resnet50_json_preprocess/accuracy/io_bin/cv_out/</em></p>
<p>The test result for the converted network based on ILSVRC_2012 dataset is: </p><pre class="fragment">Test_num[1284]  Top1: 0.6892523364485982, Top5: 0.8660436137071651
</pre><p>For more information about this accuracy test tool, please refer to fs_accuracy_tool.</p>
<h2><a class="anchor" id="sub_sec_resnet50_json_build_evk_binary"></a>
6 Build EVK Binary</h2>
<p>Build the EVK binary as follows: </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">    [*] Ambarella Application Configuration  ---&gt;</div>
<div class="line">        [*] Build AICAM  ---&gt;</div>
<div class="line">            [*] Build AICAM CVflow  ---&gt;</div>
<div class="line">                -*- Build test_nnctrl_live unit tests</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_resnet50_json_run_on_board"></a>
7 Run on Board</h2>
<p>In this example, the camera module imx274 and CV22 board are used.</p>
<ol type="1">
<li><p class="startli">Copy files to SD card for EVK test For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/resnet50</div>
<div class="line">|--model</div>
<div class="line">|        resnet50_json_preprocess_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|--imagenet</div>
<div class="line">|        imagenet_1000.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        bike.jpg</div>
<div class="line">|        cat.jpg</div>
<div class="line">|        dog.jpg</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="a">
<li>The label name file <em>imagenet_1000.txt</em> is located in <em>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</em> of cvflow_cnngen_samples package.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>Run by the following commands. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p</div>
<div class="line">board # osd_server_imagenet -p 27182 -n /sdcard/resnet50/imagenet/imagenet_1000.txt &amp;</div>
<div class="line">board # test_nnctrl_live -p 27182 -b /sdcard/resnet50/model/resnet50_json_preprocess_cavalry.bin --in data --out prob</div>
<div class="line">     -s 0 -i 0 -t 1</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The option "--in" specifies the input of network, which should match the name of first input in Json file.</li>
<li>The option "-s" specifies the input data source, 0=canvas_buffer, 1=pyramid_buffer.</li>
<li>The option "-i" specifies the input data canvas/pyramid id.</li>
<li>The option "-t" specifies output data color space type, 0=RGB, 1=BGR.</li>
</ul>
</dd></dl>
<hr  />
<h1><a class="anchor" id="sec_caffe_mobilenetv1_ssd"></a>
SSD MobileNetV1</h1>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_download_ssd_caffe"></a>
1 Download SSD Caffe</h2>
<ol type="1">
<li>To download the SSD Caffe, use the following commands. <div class="fragment"><div class="line">build $ export GIT_SSL_NO_VERIFY=1</div>
<div class="line">build $ git clone https:<span class="comment">//github.com/weiliu89/caffe.git</span></div>
<div class="line">build $ cd caffe</div>
<div class="line">build $ git checkout ssd</div>
</div><!-- fragment --></li>
<li>Next, compile and export the Caffe environment. Refer to <a class="el" href="../../d2/d67/fs_cnngen.html#sec_cnngen_env_set">3 Environment Setting</a>.</li>
<li>Use <b>buildcaffe</b> to build Caffe (the script is included in the toolchain package).</li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_extract_params"></a>
2 Extract Parameters of PriorBox Layer</h2>
<p>The SSD framework includes two distinctive layers: <b>"PriorLayer"</b> and <b>"DetectionOutputLayer"</b>.</p>
<ul>
<li>"PriorLayer" generates prior bounding boxes on the last few feature maps. It uses the input image size and the feature map size as inputs and does not include back propagation calculation. Because the input image size and the feature map size are fixed, the results of "PriorLayer" remain unchanged.</li>
<li>"DetectionOutputLayer" takes the output of each "PriorLayer" and additional data as inputs. Users can consider the output of "PriorLayer" as parameters, such as the weights of convolution layer, and hence, dump the output of "PriorLayer" from the Caffe model file directly.</li>
</ul>
<p>Ambarella provides the tool <b>"ssd_prior_box_handling.py"</b> which will generate the offline binary of "PriorLayer" and also remove "PriorLayer" and "DetectionOutputLayer" in the <b>prototxt file</b>.</p>
<p>Because the CV chip cannot accelerate the calculation speed of the DetectionOutputLayer (as it is a large NMS which includes judgment logic), the code exists in <code>ambarella/packages/data_process/</code>.</p>
<p>Therefore, <b>"ssd_prior_box_handling.py"</b> removes these two layers and implements them on Arm®. The following commands show users how to use <b>"ssd_prior_box_handling.py"</b>.</p>
<ol type="1">
<li>Source the toolchain following this command. <div class="fragment"><div class="line">build $ source build/env/cv*.env</div>
</div><!-- fragment --></li>
<li>Change the Caffe version to Caffe-SSD. <div class="fragment"><div class="line">build $ export PYTHONPATH=/&lt;user’s working space&gt;/ssd-caffe-install/usr/local/lib/python3.8/dist-packages:$PYTHONPATH</div>
<div class="line">build $ export LD_LIBRARY_PATH=/&lt;user’s working space&gt;/ssd-caffe-install/usr/local/lib:$LD_LIBRARY_PATH</div>
</div><!-- fragment --></li>
<li>Enter the mobilenetv1_ssd folder. <div class="fragment"><div class="line">build $ cd caffe/demo_networks/mobilenetv1_ssd/models/</div>
</div><!-- fragment --></li>
<li>Use "ssd_prior_box_handling.py" to remove "PriorLayer" and "DetectionOutputLayer", and dump the results of "PriorLayer". <div class="fragment"><div class="line">build $ ssd_prior_box_handling.py -p MobileNetSSD_deploy.prototxt \</div>
<div class="line">        -c MobileNetSSD_deploy.caffemodel \</div>
<div class="line">        -op MobileNetSSD_deploy_wo_do.prototxt \</div>
<div class="line">        -opb mobilenet_priorbox_fp32.bin</div>
</div><!-- fragment --> </li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_cnngen_conversion"></a>
3 CNNGen Conversion</h2>
<p>Generate the SSD Mobilenet as follows. </p><div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/mobilenetv1_ssd/config/ea_cvt_mobilenetv1_ssd.yaml</div>
</div><!-- fragment --><p> The output is in <code>out/caffe/demo_networks/mobilenetv1_ssd/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/mobilenetv1_ssd</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_mobilenetv1_ssd.bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/mobilenetv1_ssd/&lt;chip&gt;/&lt;chip&gt;_mobilenetv1_ssd</code>.</li>
<li>For X86 simulator, model desc json file <b>mobilenetv1_ssd.json</b> is in the cnngen output folder <code>out/caffe/demo_networks/mobilenetv1_ssd/out_mobilenetv1_ssd_parser/</code>. ades command <b>mobilenetv1_ssd_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/mobilenetv1_ssd/&lt;chip&gt;/&lt;chip&gt;_ades_mobilenetv1_ssd</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_build_unit_test"></a>
4 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">      [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">          -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">              -*- Build eazyai library with OpenCV support</div>
<div class="line">              -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">                  [*] Build Ambarella custom postprocess library with ssd</div>
<div class="line">              [*] Build EazyAi unit tests</div>
<div class="line"> build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator <div class="fragment"><div class="line">Refer to cnngen doxgen library EazyAI @ref eazyai_simulator to build x86 binary.</div>
<div class="line">Then, the executable file &lt;b&gt;test_eazyai&lt;/b&gt; can be found in `&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/`.</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_run_python_inference"></a>
5 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -cb out/caffe/demo_networks/mobilenetv1_ssd/&lt;chip&gt;/&lt;chip&gt;_cavalry_mobilenetv1_ssd/&lt;chip&gt;_cavalry&lt;version&gt;_mobilenetv1_ssd.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/mobilenetv1_ssd/mobilenetv1_ssd_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/mobilenetv1_ssd/config/ea_inf_mobilenetv1_ssd.yaml -pwd ./out/caffe/demo_networks/mobilenetv1_ssd</div>
</div><!-- fragment --></li>
<li>Accuracy Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/mobilenetv1_ssd/mobilenetv1_ssd_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/mobilenetv1_ssd/config/ea_inf_acc_mobilenetv1_ssd.yaml -pwd ./out/caffe/demo_networks/mobilenetv1_ssd</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/mobilenetv1_ssd/&lt;chip&gt;/&lt;chip&gt;_cavalry_mobilenetv1_ssd/&lt;chip&gt;_cavalry&lt;version&gt;_mobilenetv1_ssd.bin -ei caffe/demo_networks/mobilenetv1_ssd/models/mobilenet_priorbox_fp32.bin \</div>
<div class="line">        -pn ssd -pl caffe/demo_networks/mobilenetv1_ssd/config/ssd_caffe.lua -dm 0 \</div>
<div class="line">        -lp caffe/demo_networks/mobilenetv1_ssd/config/label_voc_with_bg.txt \</div>
<div class="line">        -ei caffe/demo_networks/mobilenetv1_ssd/models/mobilenet_priorbox_fp32.bin \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_caffe_mobilenetv1_ssd_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For EVK Board:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_cnngen_conversion">3 CNNGen Conversion</a> for how to generate <b>mobilenetv1_ssd_cavalry.bin</b>.</li>
<li>The <b>ssd_caffe.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ul>
</li>
<li>For X86 Simulator:<ul>
<li>Refer to <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_caffe_mobilenetv1_ssd_cnngen_conversion">3 CNNGen Conversion</a> for how to generate <b>mobilenetv1_ssd.json</b> and <b>mobilenetv1_ssd_ades.cmd</b>.</li>
</ul>
</li>
</ul>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/ssd</div>
<div class="line">|--model</div>
<div class="line">|       mobilenetv1_ssd_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|--anchor_boxes</div>
<div class="line">|       mobilenet_priorbox_fp32.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|       label_voc_with_bg.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|       dog.jpg</div>
<div class="line">|       dog.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_voc_with_bg.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>Users can find <b>"label_voc_with_bg.jpg"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resoutce/</code>. It displays the segmentation color map of label_voc_with_bg.txt.</li>
<li>Users can find <b>"mobilenet_priorbox_fp32.bin"</b> in <code>cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/models/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
<p>Before running model <b>mobilenetv1_ssd_cavalry.bin</b>, users need to verify whether the parameters <b>num_class, output_score_name, output_loc_name, tf_scale_factors</b> in the <b>ssd_caffe.lua</b> file are correct. If not, users need to modify them.</p>
<dl class="section note"><dt>Note</dt><dd><div class="fragment"><div class="line">_nn_arm_nms_config_ = {</div>
<div class="line">      conf_threshold = 0.2,    -- Confidence threshold</div>
<div class="line">      nms_threshold = 0.3,     -- NMS threshold</div>
<div class="line">      log_level = 2,           -- 0 none, 1 error, 2 notice, 3 debug, 4 verbose.</div>
<div class="line">      keep_top_k = 100,</div>
<div class="line">      nms_top_k = 50,</div>
<div class="line">      background_label_id = 0,</div>
<div class="line">      unnormalized = 0,</div>
<div class="line">      num_class = 21,          -- <span class="keyword">class </span>num</div>
<div class="line">      thread_num = 4,</div>
<div class="line">      output_score_name = <span class="stringliteral">&quot;mbox_conf_flatten&quot;</span>,</div>
<div class="line">      output_loc_name = <span class="stringliteral">&quot;mbox_loc&quot;</span>,</div>
<div class="line">      tf_scale_factors = {0, 0, 0, 0},</div>
<div class="line">      ssd_debug = 0,</div>
<div class="line">}</div>
</div><!-- fragment --> </dd>
<dd>
OpenMP is used in data process library to process the loops in parallel which will benefit from multiple cores for performance. Please specify the number of parallel threads for Arm tasks through "--thread_num", the default is the maximum core number of the chip.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_mobilenetv1_ssd_parser/mobilenetv1_ssd.json \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_mobilenetv1_ssd/mobilenetv1_ssd_ades.cmd \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:data=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/mobilenetv1_ssd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_mobilenetv1_ssd_parser/mobilenetv1_ssd.json \</div>
<div class="line">     --lua_file ssd_caffe.lua \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">     --extra_input &lt;usr_path&gt;mobilenet_priorbox_fp32.bin \</div>
<div class="line">     --label_path &lt;user path&gt;label_voc_with_bg.txt\</div>
<div class="line">     --output_dir &lt;usr_path&gt;/mobilenetv1_ssd/out \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_mobilenetv1_ssd/mobilenetv1_ssd_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_mobilenetv1_ssd_parser/mobilenetv1_ssd.json\</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/mobilenetv1_ssd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_mobilenetv1_ssd_parser/mobilenetv1_ssd.json \</div>
<div class="line">        --lua_file ssd_caffe.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">        --extra_input &lt;usr_path&gt;mobilenet_priorbox_fp32.bin \</div>
<div class="line">        --label_path &lt;user path&gt;label_voc_with_bg.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/mobilenetv1_ssd/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV7x, cavalry has been already boot up, users do not need to run this command.</li>
<li>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</li>
</ul>
</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n ssd \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/ssd_caffe.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=/sdcard/ssd/in|t:jpg|d:vp&quot;</span> \</div>
<div class="line">       --extra_input /sdcard/ssd/anchor_boxes/mobilenet_priorbox_fp32.bin \</div>
<div class="line">       --label_path /sdcard/ssd/labels/label_voc_with_bg.txt \</div>
<div class="line">       --output_dir /sdcard/ssd/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/ssd/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/ssd/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/caffe/demo_networks/mobilenetv1_ssd/dra_img/dog.jpg</code>) in <code>/sdcard/ssd/in</code>, and create <code>/sdcard/ssd/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n ssd \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/ssd_caffe.lua \</div>
<div class="line">     --extra_input /sdcard/ssd/anchor_boxes/mobilenet_priorbox_fp32.bin \</div>
<div class="line">     --label_path /sdcard/ssd/labels/label_voc_with_bg.txt</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -n ssd \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ssd/model/mobilenetv1_ssd_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/ssd_caffe.lua \</div>
<div class="line">     --extra_input /sdcard/ssd/anchor_boxes/mobilenet_priorbox_fp32.bin \</div>
<div class="line">     --label_path /sdcard/ssd/labels/label_voc_with_bg.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluency, check the following two points.<ul>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>For this Caffe network, it is BGR input, not RGB which is the case for the Tensorflow SSD model.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_caffe_yolo_v2"></a>
YOLO V2 and V3</h1>
<p>You only look once (YOLO) is a state-of-the-art, real-time object detection system. The following sections demonstrate how to convert the YOLO V2 using the Ambarella CNNGen Samples Package.</p>
<h2><a class="anchor" id="sub_sec_caffe_yolo_v2_recompile_caffe"></a>
1. Re-compile Caffe</h2>
<p>Using YOLO V2 as an example, users create a layer called <em>Reorg</em> that the standard Caffe (BVLC) cannot support. As a result, users must add the custom layers to the standard Caffe, using the same steps as the second method provided in the <a class="el" href="../../d6/d57/fs_cnngen_caffe_demos.html#sub_sec_custom_example_caffe_model">2 Caffe model</a>.</p>
<p><b>Add custom layers</b></p>
<ol type="1">
<li>In <em>cvflow_cnngen_samples_&lt;version&gt;/caffe/demo_networks/yolo_v2/code</em>, locate the folder <em>caffe_source</em>, which includes three files: <em>reorg_layer.hpp</em>, <em>reorg_layer.cpp</em> and <em>caffe.proto</em>.</li>
<li>Copy the <em>reorg_layer.hpp</em> file to: <em>&lt;Standard Caffe root&gt;/include/caffe/layers</em>.</li>
<li>Copy the <em>reorg_layer.cpp</em> file to: <em>&lt;Standard Caffe root&gt;/src/caffe/layers</em>.</li>
<li>Register the reorg layer in <em>caffe.proto</em> which is under: <em>&lt;Standard Caffe root&gt;/src/caffe/proto</em>.</li>
<li><p class="startli">Add <em>optional ReorgParameter reorg_param = 148;</em> to message LayerParameter (around lines 326 to 424).</p>
<p class="startli">Also add codes below to approximately line 1445. </p><div class="fragment"><div class="line">message ReorgParameter {</div>
<div class="line">    optional uint32 stride = 1;</div>
<div class="line">     optional <span class="keywordtype">bool</span> reverse = 2 [<span class="keywordflow">default</span> = <span class="keyword">false</span>];</div>
<div class="line">}</div>
</div><!-- fragment --></li>
<li>Re-compile Caffe.</li>
</ol>
<h2><a class="anchor" id="sub_sec_caffe_yolo_v2_prepare_caffe_model"></a>
2 Prepare Caffe Model</h2>
<p>Although the author of <em>YOLO V2</em> created the network under a deep learning framework called Darknet, Ambarella has converted it to a Caffe framework. Users can find the <em>*.caffemodel</em> and <em>deploy.prototxt</em> files under <em>/caffe/demo_networks/yolo_v2/models</em>. Users are also encouraged to convert the Darknet framework to Caffe framework.</p>
<h2><a class="anchor" id="sub_sec_caffe_yolo_v2_convert"></a>
3 Convert</h2>
<p>The following steps describe how to convert the <em>YOLO_v2</em>.</p>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>Please execute the following commands to generate DLL before running <code>eazyai_cvt</code>. <div class="fragment"><div class="line">build $ cd caffe/demo_networks/yolo_v2/custom_nodes/</div>
<div class="line">build $ make</div>
</div><!-- fragment --></li>
<li>For yolo_v2_sp60, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/yolo_v2/config/ea_cvt_yolo_v2_sp60.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolo_v2_sp60/</code>.</li>
<li>For yolo_v2_full, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/yolo_v2/config/ea_cvt_yolo_v2_full.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolo_v2_full/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/yolo_v2_(*)</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_yolo_v2_(*).bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v2_(*)/&lt;chip&gt;/&lt;chip&gt;_yolo_v2_(*)</code>.</li>
<li>For X86 simulator, model desc json file <b>yolo_v2_(*).json</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v2_(*)/out_yolo_v2_(*)_parser/</code>. ades command <b>yolo_v2_(*)_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v2_(*)/&lt;chip&gt;/&lt;chip&gt;_ades_yolo_v2_(*)</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_caffe_yolo_v2_deploy_with_live_mode"></a>
4 Deploy with Live Mode</h2>
<p>For details on the live mode, refer to sec_deploy_live_mode. Run the binary as follows.</p>
<p><b>YOLOv2 SP60:</b> </p><pre class="fragment">board # eazyai_video.sh --hdmi 720p
board # osd_server_yolov2 -p 27182 &amp;
board # test_nnctrl_live -p 27182 -b yolo_v2_sp60_cavalry.bin -s 0 -i 1 --in data --out conv23 -t 0 -r 0.00390625
</pre><dl class="section note"><dt>Note</dt><dd>As shown above, the input data format is <em>0,0,8,0</em> for YOLO. As a result, users need to add scale <em>"-r 0.00390625"</em>.</dd></dl>
<p><b>YOLOv2 Full:</b> </p><pre class="fragment">board # eazyai_video.sh --hdmi 720p
board # osd_server_yolov2 -p 27182 &amp;
board # test_nnctrl_live -p 27182 -b yolo_v2_full_cavalry.bin -s 0 -i 1 --in data --out conv23 -t 0 -r 0.00390625
</pre><p>Use the applications below to run YOLO in the Cavalry live mode. <b>"osd_server_yolov2"</b> receives the result from <b>test_nnctrl_live</b> and draws the OSD in VOUT. </p><pre class="fragment">osd_server_yolov2: Show yolov2_result on VOUT
        -p &lt;server port, 27182 by default&gt;
        -t &lt;threshold, default=0.20&gt;
        -v &lt;verbose&gt;
</pre><h2><a class="anchor" id="sub_sec_caffe_yolo_v2_yolo_v3_demo"></a>
5 YOLOV3 Demonstration</h2>
<h3><a class="anchor" id="sub_sec_caffe_yolov3_sp50_full_cnngen_conversion"></a>
5.1 CNNGen Conversion</h3>
<p>The following steps describe how to convert the <em>yolo_v3</em>.</p>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For yolo_v3_sp50, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/yolo_v3/config/ea_cvt_yolo_v3_sp50.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolo_v3_sp50/</code>.</li>
<li>For yolo_v3_full, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy caffe/demo_networks/yolo_v3/config/ea_cvt_yolo_v3_full.yaml</div>
</div><!-- fragment --> The output is in <code>out/caffe/demo_networks/yolo_v3_full/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use <code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/caffe/demo_networks/yolo_v3_(*)</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_yolo_v3_(*).bin</code> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v3_(*)/&lt;chip&gt;/&lt;chip&gt;_yolo_v3_(*)</code>.</li>
<li>For X86 simulator, model desc json file <b>yolo_v3_(*).json</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v3_(*)/out_yolo_v3_(*)_parser/</code>. ades command <b>yolo_v3_(*)_ades.cmd</b> is in the cnngen output folder <code>out/caffe/demo_networks/yolo_v3_(*)/&lt;chip&gt;/&lt;chip&gt;_ades_yolo_v3_(*)</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>. </li>
</ol>
</dd></dl>
<h3><a class="anchor" id="sub_sec_caffe_yolo_v3_sp50_full_build_unit_test"></a>
5.2 Build Unit Test</h3>
<ul>
<li>Build the EVK binary as shown below. <pre class="fragment">  build $ make menuconfig
  [*] Ambarella package Configuration ---&gt;
      -*- Build Ambarella EAZYAI library ---&gt;
          -*- Build eazyai library with OpenCV support
          -*- Build Ambarella custom postprocess library ---&gt;
              [*] Build Ambarella custom postprocess library with yolo_v3
          [*] Build EazyAi unit tests
  build $ make test_eazyai
</pre></li>
<li><p class="startli">Build X86 simulator binary with <em>make</em>.</p>
<p class="startli">Refer to the CNNGen Doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build the x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</p>
</li>
</ul>
<h3><a class="anchor" id="sub_sec_caffe_yolo_v3_sp50_full_run_python_inference"></a>
5.3 Run Python Inference</h3>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</p>
<ul>
<li>Start CVflow Engine For below Dummy and File Mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --></li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 -b out/caffe/demo_networks/yolo_v3*/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolo_v3*/&lt;chip&gt;_cavalry&lt;version&gt;_yolo_v3*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolo_v3_sp50/yolo_v3_sp50_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/yolo_v3/config/ea_inf_yolo_v3_sp50.yaml -pwd ./out/caffe/demo_networks/yolo_v3_sp50</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolo_v3_full/yolo_v3_full_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/yolo_v3/config/ea_inf_yolo_v3_full.yaml -pwd ./out/caffe/demo_networks/yolo_v3_full</div>
</div><!-- fragment --></li>
<li>Accuracy Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolo_v3_sp50/yolo_v3_sp50_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/yolo_v3/config/ea_inf_acc_yolo_v3_sp50.yaml -pwd ./out/caffe/demo_networks/yolo_v3_sp50</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/caffe/demo_networks/yolo_v3_full/yolo_v3_full_cvt_summary.yaml \</div>
<div class="line">        -iy caffe/demo_networks/yolo_v3/config/ea_inf_acc_yolo_v3_full.yaml -pwd ./out/caffe/demo_networks/yolo_v3_full</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please use option <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/yolo_v3_sp50/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolo_v3_sp50/&lt;chip&gt;_cavalry&lt;version&gt;_yolo_v3_sp50.bin \</div>
<div class="line">        -pn yolov3 -pl caffe/demo_networks/yolo_v3/config/yolov3_sp50_full.lua -dm 0 \</div>
<div class="line">        -lp caffe/demo_networks/yolo_v3/config/label_coco_80.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/caffe/demo_networks/yolo_v3_full/&lt;chip&gt;/&lt;chip&gt;_cavalry_yolo_v3_full/&lt;chip&gt;_cavalry&lt;version&gt;_yolo_v3_full.bin \</div>
<div class="line">        -pn yolov3 -pl caffe/demo_networks/yolo_v3/config/yolov3_sp50_full.lua -dm 0 \</div>
<div class="line">        -lp caffe/demo_networks/yolo_v3/config/label_coco_80.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h3><a class="anchor" id="sub_sec_caffe_yolov3_sp50_full_run_c_inference"></a>
5.4 Run C Inference</h3>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example; refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the EVK board:<ul>
<li>Refer to sub_sec_caffe_yolo_v3_sp50_full_cnngen_conversion for information on how to generate <b>yolo_v3_*_cavalry.bin</b>.</li>
<li>The <b>yolov3_sp50_full.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of the EVK. If it does not exist, it can be found in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ul>
</li>
<li>For X86:<ul>
<li>Refer to sub_sec_caffe_yolo_v3_sp50_full_cnngen_conversion for information on how to generate <b>yolo_v3_*.json</b> and <b>yolo_v3_*_ades.cmd</b>.</li>
</ul>
</li>
</ul>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/yolo_v3</div>
<div class="line">|--model</div>
<div class="line">│    yolo_v3_sp50_cavalry.bin</div>
<div class="line">│    yolo_v3_full_cavalry.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">│    label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    dog.jpg</div>
<div class="line">|    dog.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_coco_80.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users must keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode:<ol type="1">
<li>For the X86 simulator:<ol type="a">
<li>Run ADES mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_yolo_v3_sp50_parser/yolo_v3_sp50.json \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_yolo_v3_sp50/yolo_v3_sp50_ades.cmd \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/yolo_v3_sp50/out</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">     --model_path &lt;usr_path&gt;/out_yolo_v3_full_parser/yolo_v3_full.json \</div>
<div class="line">     --ades_cmd_file &lt;usr_path&gt;/ades_yolo_v3_full/yolo_v3_full_ades.cmd \</div>
<div class="line">     --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">     --output_dir &lt;usr_path&gt;/yolo_v3_full/out</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The image is used as an input with the correct preprocess and postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_sp50_parser/yolo_v3_sp50.json \</div>
<div class="line">        --lua_file yolov3_sp50_full.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolo_v3/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/ades_yolo_v3_sp50/yolo_v3_sp50_ades.cmd</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_full_parser/yolo_v3_full.json \</div>
<div class="line">        --lua_file yolov3_sp50_full.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolo_v3/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/ades_yolo_v3_full/yolo_v3_full_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>Run Acinference mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_sp50_parser/yolo_v3_sp50.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolo_v3_sp50/out</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_full_parser/yolo_v3_full.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolo_v3_full/out</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The image is used as an input with the correct preprocess and postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_sp50_parser/yolo_v3_sp50.json \</div>
<div class="line">        --lua_file yolov3_sp50_full.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_yolo_v3_full_parser/yolo_v3_full.json \</div>
<div class="line">        --lua_file yolov3_sp50_full.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>For the EVK board:<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV7x, cavalry has been already boot up, users do not need to run this command.</li>
<li>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</li>
</ul>
</dd></dl>
</li>
<li>Run the following.<ol type="i">
<li>Dummy mode, to test CVflow performance:<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_sp50_cavalry.bin</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_full_cavalry.bin</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>The image is used as an input with preprocess and postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">         --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_sp50_cavalry.bin \</div>
<div class="line">         --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=/sdcard/yolo_v3/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">         --label_path /sdcard/yolo_v3/labels/label_coco_80.txt \</div>
<div class="line">         --output_dir /sdcard/yolo_v3/out/</div>
</div><!-- fragment --><ol type="1">
<li>yolo_v3_full: <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">         --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_full_cavalry.bin \</div>
<div class="line">         --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=/sdcard/yolo_v3/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">         --label_path /sdcard/yolo_v3/labels/label_coco_80.txt \</div>
<div class="line">         --output_dir /sdcard/yolo_v3/out/</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>The raw.bin is used as an input without the corrcet preprocess or postprocess.<ol type="A">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">         --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_sp50_cavalry.bin \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=/sdcard/yolo_v3/in|t:raw&quot;</span> \</div>
<div class="line">         --output_dir /sdcard/yolo_v3/out/</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">         --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_full_cavalry.bin \</div>
<div class="line">         --isrc <span class="stringliteral">&quot;i:data=/sdcard/yolo_v3/in|t:raw&quot;</span> \</div>
<div class="line">         --output_dir /sdcard/yolo_v3/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode using image as input, place the test image (such as <code>cvflow_cnngen_samples/caffe/demo_networks/yolo_v3/dra_img/dog.jpg</code>) in <code>/sdcard/yolo_v3/in</code>, and create <code>/sdcard/yolo_v3/out</code> as the output directory.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode:<ol type="1">
<li>Initialize the environment on the CV board. CV22 Walnut and imx274_mipi are used as examples. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --reallocate_mem overlay,0x01000000</div>
</div><!-- fragment --></li>
<li>Run the following.<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>):<ol type="i">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_sp50_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">     --label_path /sdcard/yolo_v3/labels/label_coco_80.txt --fsync_off</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_full_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">     --label_path /sdcard/yolo_v3/labels/label_coco_80.txt --fsync_off</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For stream live mode, option <b>&ndash;fsync_off</b> disables frame sync. If need to enable frame sync, users should enable encode dummy in eazyai_video.sh.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li>Video output (VOUT) live mode (draw on VOUT high definition multimedia interface (HDMI®)):<ol type="i">
<li>yolo_v3_sp50: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_sp50_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">     --label_path /sdcard/yolo_v3/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>yolo_v3_full: <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">     --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolo_v3/model/yolo_v3_full_cavalry.bin \</div>
<div class="line">     --lua_file /usr/share/ambarella/eazyai/lua/yolov3_sp50_full.lua \</div>
<div class="line">     --label_path /sdcard/yolo_v3/labels/label_coco_80.txt</div>
</div><!-- fragment --> </li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaigen-layercompare_html_ga4d5bb0c360b13429f65cd327c8d0aa12"><div class="ttname"><a href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a></div><div class="ttdeci">model_path</div></div>
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_gac69e20380615374be0baa46ed46295b7"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a></div><div class="ttdeci">tuple run(self)</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga81f22c9cd9a33cc05e5a1657974438bd"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga81f22c9cd9a33cc05e5a1657974438bd">work_dir</a></div><div class="ttdeci">work_dir</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga8179f95715172cfcd3a44cd038a81a9f"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a></div><div class="ttdeci">transforms</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-cavalry_html_ga37331a8707327be2ea6a7ae00e8322ae"><div class="ttname"><a href="../../../library/d8/d8d/group__cflite-eazyaigen-cavalry.html#ga37331a8707327be2ea6a7ae00e8322ae">cavalry_info</a></div><div class="ttdeci">cavalry_info</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
