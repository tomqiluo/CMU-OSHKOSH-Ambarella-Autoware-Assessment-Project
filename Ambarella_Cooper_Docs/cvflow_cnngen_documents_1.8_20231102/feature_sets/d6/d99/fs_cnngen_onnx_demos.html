<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>CNNGen Development: ONNX Demos</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="CNNGen Development"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">CNNGen Development
   </div>
   <div id="projectbrief">CVflow_1.8 @ 2023.11.02</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('d6/d99/fs_cnngen_onnx_demos.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">ONNX Demos </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This chapter describes the ONNX CNNGen demos.</p>
<hr  />
<h1><a class="anchor" id="sec_onnx_bytetrack"></a>
ByteTrack</h1>
<p>The <b>ByteTrack</b> is for Multi-object tracking(MOT), which is similar to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sec_onnx_fairmot">FairMOT</a>. Unlike <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sec_onnx_fairmot">FairMOT</a>, the output of the backbone network of ByteTrack does not include re-ID features. For more details, please refer to <a href="https://github.com/ifzhang/ByteTrack">GitHub</a>.</p>
<p>The following contents demonstrate how to export ByteTrack open neural network exchange (ONNX) models from the public source project and how to run the ONNX model with the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_bytetrack_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a source project in GitHub with MIT License. The steps are below.</p>
<ol type="1">
<li>Download the source code project from <a href="https://github.com/ifzhang/ByteTrack">here</a>. <div class="fragment"><div class="line">build $ git clone https:<span class="comment">//github.com/ifzhang/ByteTrack.git</span></div>
<div class="line">build $ cd ByteTrack/</div>
<div class="line">build $ git reset --hard d1bf0191adff59bc8fcfeaa0b33d3d1642552a99</div>
</div><!-- fragment --></li>
<li>Go to the source project directory and install the required packages. <div class="fragment"><div class="line">build $ cd ByteTrack/</div>
<div class="line">build $ pip3 install -r requirements.txt</div>
<div class="line">build $ python3 setup.py develop</div>
<div class="line">build $ pip3 install cython; pip3 install <span class="stringliteral">&#39;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#39;</span></div>
<div class="line">build $ pip3 install cython_bbox</div>
</div><!-- fragment --></li>
<li><p class="startli">Download pretrained model. </p><div class="fragment"><div class="line">build $ cd ByteTrack/</div>
<div class="line">build $ mkdir pretrained</div>
</div><!-- fragment --><p class="startli">Download the pretrained models from the link below and place them in <em>pretrained/</em>.</p><ul>
<li><a href="https://drive.google.com/file/d/1uSmhXzyV1Zvb4TJJCzpsZOIcw7CCJLxj/view?usp=sharing">bytetrack_s_mot17.pth.tar</a></li>
<li><a href="https://drive.google.com/file/d/11Zb0NN_Uu7JwUd9e6Nk8o2_EUfxWqsun/view?usp=sharing">bytetrack_m_mot17.pth.tar</a></li>
<li><a href="https://drive.google.com/file/d/1XwfUuCBF4IgWBWK2H7oOhQgEj9Mrb3rz/view?usp=sharing">bytetrack_l_mot17.pth.tar</a></li>
<li><a href="https://drive.google.com/file/d/1P4mY0Yyd3PPTybgZkjMYhFri88nTmJX5/view?usp=sharing">bytetrack_x_mot17.pth.tar</a></li>
</ul>
</li>
<li>Refer to the following steps to modify the input size of network. Take bytetrack_s as an example. <div class="fragment"><div class="line">--- a/exps/example/mot/yolox_s_mix_det.py</div>
<div class="line">+++ b/exps/example/mot/yolox_s_mix_det.py</div>
<div class="line">@@ -17,8 +17,8 @@ <span class="keyword">class </span>Exp(MyExp):</div>
<div class="line">      self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(&quot;.&quot;)[0]</div>
<div class="line">      self.train_ann = &quot;train.json&quot;</div>
<div class="line">      self.val_ann = &quot;train.json&quot;</div>
<div class="line">-     self.input_size = (608, 1088)</div>
<div class="line">-     self.test_size = (608, 1088)</div>
<div class="line">+     self.input_size = (384, 640)  #(608, 1088)</div>
<div class="line">+     self.test_size =  (384, 640)  #(608, 1088)</div>
</div><!-- fragment --></li>
<li>Export the ONNX model.<ol type="a">
<li>bytetrack_s <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> bytetrack_s.onnx -f exps/example/mot/yolox_s_mix_det.py -c pretrained/bytetrack_s_mot17.pth.tar</div>
</div><!-- fragment --></li>
<li>bytetrack_m <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> bytetrack_m.onnx -f exps/example/mot/yolox_m_mix_det.py -c pretrained/bytetrack_m_mot17.pth.tar</div>
</div><!-- fragment --></li>
<li>bytetrack_l <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> bytetrack_l.onnx -f exps/example/mot/yolox_l_mix_det.py -c pretrained/bytetrack_l_mot17.pth.tar</div>
</div><!-- fragment --></li>
<li>bytetrack_x <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> bytetrack_x.onnx -f exps/example/mot/yolox_x_mix_det.py -c pretrained/bytetrack_x_mot17.pth.tar</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The input size of the exported model needs to be 32 aligned.</li>
<li>This model only detects person.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_bytetrack_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package. Take bytetrack_s as an example.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/bytetrack/config/ea_cvt_onnx_bytetrack_s.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_bytetrack_*</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_bytetrack_s.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_bytetrack_(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_bytetrack_*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_bytetrack_*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_bytetrack_(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_bytetrack_*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_bytetrack_(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_bytetrack_*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_bytetrack_build_evk_binary"></a>
3 Build EVK Binary</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">   [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">     [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">       [*] Build EazyAI applications  ---&gt;</div>
<div class="line">         [*] Build ByteTrack EazyAI apps</div>
<div class="line">build $ make test_bytetrack</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_bytetrack_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_bytetrack_*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_bytetrack_*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_bytetrack_*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_bytetrack_*/onnx_bytetrack_*_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_bytetrack_*</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_bytetrack_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_bytetrack_run_c_inference"></a>
5 Run C Inference</h2>
<ol type="1">
<li><p class="startli">Copy files to the secure digital (SD) card for the EVK test.</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/bytetrack</div>
<div class="line">|--in</div>
<div class="line">|    palace_frame.jpg</div>
<div class="line">|    bytetrack_cavalry*.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Please rename the <code>&lt;chip&gt;_cavalry&lt;version&gt;_bytetrack.bin</code> generated by CFlite to <code>bytetrack_cavalry*.bin</code> and place it in the directory above.</dd></dl>
</li>
<li><p class="startli">File mode:</p><ul>
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run the following commads. <div class="fragment"><div class="line">board # test_bytetrack -m 1 -u 1 -i in/ -o out/</div>
</div><!-- fragment --></li>
</ul>
<dl class="section note"><dt>Note</dt><dd>For the file mode using image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/bytetrack/dra_img/palace_frame.jpg</code>) in <code>/sdcard/bytetrack/in</code>, and create <code>/sdcard/bytetrack/out</code> as the output directory.</dd></dl>
</li>
<li><p class="startli">Live mode:</p>
<p class="startli">a. Initialize the environment on the CV board. The cv5_timn and imx274_mipi are used as examples. </p><div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --enc_dummy_latency 4</div>
</div><!-- fragment --><p class="startli">b. Run the following commands, take bytetrack_s as an example.</p><ul>
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>): <div class="fragment"><div class="line">board # test_bytetrack -b 1 -d 1 -s 0 -u 1 -i in/ -o out/</div>
</div><!-- fragment --></li>
<li>Video output (VOUT) live mode (draw on VOUT high definition multimedia interface (HDMI®)): <div class="fragment"><div class="line">board # test_bytetrack -b 1 -d 0 -u 1 -i in/ -o out/</div>
</div><!-- fragment --></li>
</ul>
<dl class="section note"><dt>Note</dt><dd>If there is no display on the stream or the display is not fluent, check the following points.<ul>
<li>If the display is not fluency, use a larger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be increased by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<b>1024 + resolution * (enc-dummy-latency + 5)</b>). For more details, refer to the <b>EazyAI Library application programming interface (API)-related content in the Linux software development kit (SDK) Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
<h1><a class="anchor" id="sec_onnx_centernet"></a>
CenterNet</h1>
<p>CenterNet models an object as a single point - the center point of its bounding box. The CenterNet detectors use the key point estimation to find center points and regress to all of the other object properties, such as size, three-dimensional location, orientation, and pose. It is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors.</p>
<p>The following sections demonstrate how to export CenterNet ONNX model from PyTorch and run the ONNX model in the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_centernet_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a project in GitHub with MIT license. The steps are shown as below.</p>
<ol type="1">
<li>Download the source code project from <a href="https://github.com/xingyizhou/CenterNet">here</a>. <div class="fragment"><div class="line">commit 1085662179604dd4c2667e3159db5445a5f4ac76</div>
</div><!-- fragment --></li>
<li>Install the required Python packages and build libraries following the steps in the following link. <div class="fragment"><div class="line">https:<span class="comment">//github.com/xingyizhou/CenterNet/blob/master/readme/INSTALL.md</span></div>
</div><!-- fragment --></li>
<li><p class="startli">Download <em>ctdet_coco_dlav0_1x.pth</em> from <a href="https://drive.google.com/drive/folders/1px-Xg7jXSC79QqgsD1AAGJQkuf5m0zh_">here</a> and put it under the models folder of the source project.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>ctdet_coco_dlav0_1x.pth is the pre-trained weight file of the DLA backbone.</li>
<li>There are several implementations of the CenterNet backbone: hourglass, DLA (Deep Layer Aggregation), DLA+DCN (Deformable Convolutional Network), ResNet, and ResNet+DCN. DLA is chosen as the sample for the following reasons.</li>
<li>Hourglass is too heavy, and the performance is low on CV chips.</li>
<li>DCN is not supported by the CV toolchain.</li>
<li>The pre-trained ResNet model is not provided by the source project.</li>
</ul>
</dd></dl>
</li>
<li>Create a Python3 source file (named <b>export_ctdet_onnx_model.py</b> for example) with the following code. <div class="fragment"><div class="line">from __future__ <span class="keyword">import</span> absolute_import</div>
<div class="line">from __future__ <span class="keyword">import</span> division</div>
<div class="line">from __future__ <span class="keyword">import</span> print_function</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> _init_paths</div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> onnx</div>
<div class="line">from opts <span class="keyword">import</span> opts</div>
<div class="line"><span class="keyword">import</span> torch</div>
<div class="line">from torch <span class="keyword">import</span> nn</div>
<div class="line"><span class="keyword">import</span> os</div>
<div class="line"> </div>
<div class="line">from models.model <span class="keyword">import</span> create_model, load_model</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>CtdetDetectorExport(nn.Module):</div>
<div class="line">  def <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga36536ba84124361916240b28721fe384">__init__</a>(self, opt):</div>
<div class="line">    super(CtdetDetectorExport, self).<a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga36536ba84124361916240b28721fe384">__init__</a>()</div>
<div class="line"> </div>
<div class="line">    opt.device = torch.device(&#39;cpu&#39;)</div>
<div class="line"> </div>
<div class="line">    print(&#39;Creating model...&#39;)</div>
<div class="line">    self.model = create_model(opt.arch, opt.heads, opt.head_conv)</div>
<div class="line">    print(&#39;Loading model...&#39;)</div>
<div class="line">    self.model = load_model(self.model, opt.load_model)</div>
<div class="line">    self.model = self.model.to(opt.device)</div>
<div class="line">    self.model.eval()</div>
<div class="line"> </div>
<div class="line">    self.mean = torch.tensor(opt.mean, device=opt.device).reshape(1, 3, 1, 1)</div>
<div class="line">    self.std = torch.tensor(opt.std, device=opt.device).reshape(1, 3, 1, 1)</div>
<div class="line">    self.max_per_image = 100</div>
<div class="line">    self.num_classes = opt.num_classes</div>
<div class="line">    self.opt = opt</div>
<div class="line"> </div>
<div class="line">  def pre_process(self, image):</div>
<div class="line"> </div>
<div class="line">    inp_image = (image / 255. - self.mean) / self.std</div>
<div class="line">    return inp_image</div>
<div class="line"> </div>
<div class="line">  def process(self, image):</div>
<div class="line">    output = self.model(image)[-1]</div>
<div class="line">    hm = output[&#39;hm&#39;].sigmoid_()</div>
<div class="line">    wh = output[&#39;wh&#39;]</div>
<div class="line">    reg = output[&#39;reg&#39;] if self.opt.reg_offset else None</div>
<div class="line">    hmax = nn.functional.max_pool2d(hm, (3, 3), stride=1, padding=1)</div>
<div class="line"> </div>
<div class="line">    return hm, hmax, wh, reg</div>
<div class="line"> </div>
<div class="line">  def forward(self, x):</div>
<div class="line">    with torch.no_grad():</div>
<div class="line">      image = x</div>
<div class="line">      image = self.pre_process(image)</div>
<div class="line">      hm, hmax, wh, reg = self.process(image)</div>
<div class="line">    return hm, hmax, wh, reg</div>
<div class="line"> </div>
<div class="line">if __name__ == &#39;__main__&#39;:</div>
<div class="line">  opt = opts().init()</div>
<div class="line"> </div>
<div class="line">  model = CtdetDetectorExport(opt)</div>
<div class="line"> </div>
<div class="line">  print(&#39;Exporting model...&#39;)</div>
<div class="line">  onnx_model_name = os.path.basename(opt.load_model).replace(&#39;pth&#39;, &#39;onnx&#39;)</div>
<div class="line">  x = torch.randn(1, 3, opt.input_h, opt.input_w, requires_grad=True, device=opt.device).to(opt.device)</div>
<div class="line">  torch.onnx.export(model, x, onnx_model_name,</div>
<div class="line">       verbose=False, opset_version=11,</div>
<div class="line">       export_params=True, do_constant_folding=True)</div>
<div class="line"> </div>
<div class="line">  print(&#39;Checking model...&#39;)</div>
<div class="line">  onnx_model = onnx.load(onnx_model_name)</div>
<div class="line">  onnx.checker.check_model(onnx_model)</div>
<div class="line"> </div>
<div class="line">  print(&quot;ONNX model is saved at {}&quot;.format(onnx_model_name))</div>
<div class="line">  print(&#39;Done.&#39;)</div>
</div><!-- fragment --></li>
<li>Copy <b>export_ctdet_onnx_model.py</b> to the <b>src</b> folder of the source project and run the following command. <div class="fragment"><div class="line">build $ python3 export_ctdet_onnx_model.py ctdet --load_model ../models/ctdet_coco_dlav0_1x.pth --arch dlav0_34 --input_h 384 --input_w 384</div>
</div><!-- fragment --> The ONNX model file <b>ctdet_coco_dlav0_1x.onnx</b> will be generated under the <b>src</b> folder.</li>
<li>Run the following command to do the graph surgery. <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m ctdet_coco_dlav0_1x.onnx -o ctdet_coco_dlav0_1x_384x384.onnx -isrc=<span class="stringliteral">&quot;i:image|is:1,3,384,384&quot;</span> -on <span class="stringliteral">&quot;517,520,521,522&quot;</span> -t ConstantifyShapes,FoldConstants</div>
</div><!-- fragment --> The final model file <em>ctdet_coco_dlav0_1x_384x384.onnx</em> is generated for converting by the CV toolchain.</li>
<li>Run the following command to verify that there are no unsupported operators. <div class="fragment"><div class="line">build $ onnx_print_graph_summary.py -p ctdet_coco_dlav0_1x_384x384.onnx</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_centernet_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/centernet/config/ea_cvt_onnx_centernet.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_centernet</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_centernet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_centernet</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_centernet.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_centernet/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_centernet_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_centernet/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_centernet</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_centernet_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">  -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">    -*- Build eazyai library with OpenCV support</div>
<div class="line">    -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">      [*] Build Ambarella custom postprocess library with centernet</div>
<div class="line">    [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_centernet_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_centernet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_centernet/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_centernet/onnx_centernet_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/centernet/config/ea_inf_onnx_centernet.yaml -pwd ./out/onnx/demo_networks/onnx_centernet</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_centernet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_centernet/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin \</div>
<div class="line">        -pn centernet -pl &lt;usr_path&gt;/centernet/config/centernet.lua -dm 0 -lp &lt;usr_path&gt;/centernet/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_centernet_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_centernet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin</b>.</li>
<li>The <b>centernet.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_centernet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_centernet.json</b> and <b>onnx_centernet_ades.cmd</b>.</li>
</ol>
</dd></dl>
<ul>
<li>Create a text file named <b>label_coco_80.txt</b>, and fill it with the following content. <div class="fragment"><div class="line"><span class="stringliteral">&#39;person&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bicycle&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;car&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;motorcycle&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;airplane&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bus&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;train&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;truck&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;boat&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;traffic light&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;fire hydrant&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;stop sign&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;parking meter&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bench&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bird&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;cat&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;dog&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;horse&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;sheep&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;cow&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;elephant&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bear&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;zebra&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;giraffe&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;backpack&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;umbrella&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;handbag&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;tie&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;suitcase&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;frisbee&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;skis&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;snowboard&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;sports ball&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;kite&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;baseball bat&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;baseball glove&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;skateboard&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;surfboard&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;tennis racket&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bottle&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;wine glass&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;cup&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;fork&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;knife&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;spoon&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bowl&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;banana&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;apple&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;sandwich&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;orange&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;broccoli&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;carrot&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;hot dog&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;pizza&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;donut&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;cake&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;chair&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;couch&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;potted plant&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;bed&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;dining table&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;toilet&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;tv&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;laptop&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;mouse&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;remote&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;keyboard&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;cell phone&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;microwave&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;oven&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;toaster&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;sink&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;refrigerator&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;book&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;clock&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;vase&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;scissors&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;teddy bear&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;hair drier&#39;</span>,</div>
<div class="line"><span class="stringliteral">&#39;toothbrush&#39;</span></div>
</div><!-- fragment --></li>
<li><p class="startli">Copy files to SD card on the EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/centernet</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        33887522274_eebd074106_k.jpg</div>
<div class="line">|        33887522274_eebd074106_k.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|        label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_coco_80.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_centernet_parser/onnx_centernet.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_centernet/onnx_centernet_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/centernet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n centernet \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_centernet_parser/onnx_centernet.json \</div>
<div class="line">        --lua_file centernet.lua --queue_size 1 --thread_num 1 \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">        --label_path &lt;user path&gt;label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/centernet/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_centernet/onnx_centernet_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_centernet_parser/onnx_centernet.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/centernet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n centernet \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_centernet_parser/onnx_centernet.json \</div>
<div class="line">        --lua_file centernet.lua --queue_size 1 --thread_num 1 \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">        --label_path &lt;user path&gt;label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/centernet/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/centernet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n centernet \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/centernet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/centernet.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=/sdcard/centernet/in|t:jpg&quot;</span> \</div>
<div class="line">        --label_path /sdcard/centernet/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/centernet/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/centernet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=/sdcard/centernet/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/centernet/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/centernet/dra_img/33887522274_eebd074106_k.jpg</code>) in <code>/sdcard/centernet/in</code>, and create <code>/sdcard/centernet/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n centernet \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/centernet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/centernet.lua \</div>
<div class="line">        --label_path /sdcard/centernet/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -n centernet \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/centernet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_centernet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/centernet.lua \</div>
<div class="line">        --label_path /sdcard/centernet/labels/label_coco_80.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_ddrnet"></a>
DDRNet</h1>
<p>DDRNet is a street scene segmentation network with the high performance and accuracy.</p>
<h2><a class="anchor" id="onnx_ddrnet_download_model"></a>
1 Download and Export Model</h2>
<p>Please follow the steps below to download and export the PYTorch model to ONNX model.</p><ol type="1">
<li><p class="startli">Download the PYTorch model file.</p>
<p class="startli">Download the model file from: <a href="https://github.com/ydhongHIT/DDRNet">https://github.com/ydhongHIT/DDRNet</a>. For example, download the DDRNet_23_slim on Cityscapes(val mIoU:77.8) URL: <a href="https://drive.google.com/file/d/1d_K3Af5fKHYwxSo8HkxpnhiekhwovmiP/view?usp=sharing">https://drive.google.com/file/d/1d_K3Af5fKHYwxSo8HkxpnhiekhwovmiP/view?usp=sharing</a></p>
</li>
<li>Download and modify the source code. <div class="fragment"><div class="line">build $ git clone https:<span class="comment">//github.com/chenjun2hao/DDRNet.pytorch.git</span></div>
<div class="line">build $ cd DDRNet.pytorch</div>
<div class="line">build $ git reset --hard bc0e193e87ead839dbc715c48e6bfb059cf21b27</div>
<div class="line">build $ git apply ../git.diff</div>
</div><!-- fragment --></li>
<li>Convert to ONNX. Please ensure that the user current environment follows the requirment described in <a href="https://github.com/chenjun2hao/DDRNet.pytorch.git">https://github.com/chenjun2hao/DDRNet.pytorch.git</a>. <div class="fragment"><div class="line">build $ python3 tools/to_onnx.py --cfg experiments/cityscapes/ddrnet23_slim.yaml</div>
</div><!-- fragment --> The model file will be generated under <code>&lt;user_path&gt;/cnngen_sample_package/onnx/demo_networks/DDRNet/models/</code>. It also reads the example image in dra_img and output a gray-scale result torch_out.jpg in DDRNet.pytorch.</li>
<li>Use the graph surgery tool to optimize the model. <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m ../../models/ddrnet23.onnx -o ../../models/ddrnet23_modify_640_480.onnx -t Default,InferShapes</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_ddrnet_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/DDRNet/config/ea_cvt_onnx_ddrnet.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_ddrnet</code>. 3 For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_ddrnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_ddrnet</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_ddrnet.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_ddrnet/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_ddrnet_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_ddrnet/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_ddrnet</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_ddrnet_build_unit_test"></a>
3 Build Unit Test</h2>
<p>As this DDRNet has one simple input port and one simple output port, it could reuse the Deeplab_v3's board application. Please refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_ddrnet_build_unit_test">3 Build Unit Test</a></p>
<h2><a class="anchor" id="sub_sec_onnx_ddrnet_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_ddrnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_ddrnet/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_ddrnet/onnx_ddrnet_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_ddrnet</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_ddrnet/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_ddrnet/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin \</div>
<div class="line">        -pn deeplabv3 -pl &lt;usr_path&gt;/ddrnet/config/ddrnet.lua -dm 1 \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_ddrnet_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_ddrnet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin</b>.</li>
<li>The <b>ddrnet.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
</ol>
</dd></dl>
<ol type="1">
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_ddrnet_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_ddrnet.json</b> and <b>onnx_ddrnet_ades.cmd</b>.</li>
</ol>
<p>Copy files to SD card for EVK test</p>
<p>For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/ddrnet</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        frankfurt_000000_000294_leftImg8bit.png</div>
<div class="line">|        frankfurt_000000_000294_leftImg8bit.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
<ul>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_ddrnet_parser/onnx_ddrnet.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_ddrnet/onnx_ddrnet_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/ddrnet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_ddrnet_parser/onnx_ddrnet.json \</div>
<div class="line">        --lua_file ddrnet.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=&lt;usr_path&gt;/dra_img|t:png&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/ddrnet/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_ddrnet/onnx_ddrnet_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_ddrnet_parser/onnx_ddrnet.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/ddrnet/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_ddrnet_parser/onnx_ddrnet.json \</div>
<div class="line">        --lua_file ddrnet.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=&lt;usr_path&gt;/dra_img|t:png&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/ddrnet/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ddrnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 1 -n deeplabv3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ddrnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/ddrnet.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=/sdcard/ddrnet/in|t:png|d:vp&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/ddrnet/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ddrnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:inputx=/sdcard/ddrnet/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/ddrnet/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/ddrnet/dra_img/frankfurt_000000_000294_leftImg8bit.png</code>) in <code>/sdcard/ddrnet/in</code>, and create <code>/sdcard/ddrnet/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000 --fb_num 4</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 1 -n deeplabv3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ddrnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/ddrnet.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 1 -n deeplabv3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/ddrnet/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_ddrnet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/ddrnet.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
<li><b>&ndash;fb_num</b>: Specify the cycle buffer number of frame buffer, default is 2.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_face_detection_and_blur_function"></a>
Face Detection and Blur Function</h1>
<p>Certain situations might require users to blur an individual's face in order to maintain their privacy and avoid potential legal consequences.</p>
<p>This chapter explains how to deploy the face detection and blur function demo. In this demo, the ONNX Fast Generic Face Detector (FGFD) network is used to detect the face using VP; DSP then performs the blur function.</p>
<h2><a class="anchor" id="sub_sec_fd_blur_down_model_file"></a>
1 Download Model File</h2>
<p>Users can download the original model of FGFD from the following address:</p>
<p><a href="https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/models/onnx/version-RFB-320.onnx">https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/models/onnx/version-RFB-320.onnx</a></p>
<p>The pre-trained model file version-RFB-320.onnx is used to generate the Cavalry binary for FGFD.</p>
<h2><a class="anchor" id="sub_sec_fdfd_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/FGFD/config/ea_cvt_onnx_fgfd.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_fgfd</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fgfd/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fgfd</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_fgfd.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fgfd/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_fgfd_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fgfd/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_fgfd</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_fd_blur_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Uint Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">  -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">    -*- Build eazyai library with OpenCV support</div>
<div class="line">    -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">      [*] Build Ambarella custom postprocess library with fgfd</div>
<div class="line">   [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_fgfd_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_fgfd/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fgfd/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_fgfd/onnx_fgfd_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_fgfd</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_fgfd/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fgfd/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin \</div>
<div class="line">        -pn fgfd -pl &lt;usr_path&gt;/fgfd/config/fgfd.lua -dm 0 \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_fd_blur_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_fdfd_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin</b>.</li>
<li>The <b>fgfd.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_fdfd_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_fgfd.json</b> and <b>onnx_fgfd_ades.cmd</b>.</li>
</ol>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/fgfd</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        2.jpg</div>
<div class="line">|        2.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</dd></dl>
</li>
<li>File mode</li>
</ul>
<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fgfd_parser/onnx_fgfd.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_fgfd/onnx_fgfd_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/out_onnx_fgfd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n fgfd \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fgfd_parser/onnx_fgfd.json \</div>
<div class="line">        --lua_file fgfd.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/out_onnx_fgfd/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_fgfd/onnx_fgfd_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fgfd_parser/onnx_fgfd.json\</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/out_onnx_fgfd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n fgfd \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fgfd_parser/onnx_fgfd.json \</div>
<div class="line">        --lua_file fgfd.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/out_onnx_fgfd/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<ol type="a">
<li><p class="startli">For EVK Board:</p><ol type="i">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="A">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/fgfd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n fgfd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>  /sdcard/fgfd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fgfd.lua \</div>
<div class="line">        --output_dir /sdcard/fgfd/out \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=/sdcard/fgfd/in|t:jpg|c:rgb&quot;</span></div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a>  /sdcard/fgfd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin \</div>
<div class="line">        --output_dir /sdcard/fgfd/out \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:input=/sdcard/fgfd/in|t:raw&quot;</span></div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/FGFD/dra_img/2.jpg</code>) in <code>/sdcard/fgfd/in</code>, and create <code>/sdcard/fgfd/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
<ul>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples<ol type="a">
<li>Normal demo on stream <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Blur demo on stream <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --blur --enc_dummy_latency 3 --reallocate_mem blur,0x01000000:overlay,0x01000000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>In Blur mode, the lua file <b>blur_strength</b> needs to be changed to <b>0</b>, and the default value of <b>blur_strength</b> is <b>0</b>.</li>
<li>In Normal mode, the lua file <b>blur_strength</b> needs to be changed to <b>-1</b>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n fgfd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/fgfd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fgfd.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n fgfd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/fgfd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fgfd.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fgfd.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024(color_table) + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_fd_blur_dsp_blur_function"></a>
6 DSP Blur Function</h2>
<p>The DSP supports inserting blurred areas onto streams.</p>
<p>In this demo, the result of FGFD is sent to the DSP as a structure including x, y, width, and height using the ioctl command. Because the blur function supports up to 16 blurred areas per stream, the FGFD result will be filtered by sizes of detected face boxes when more than 16 faces are detected.</p>
<p>Users can specify the color of a blurred area before starting the encoder using the following command: </p><div class="fragment"><div class="line">board # test_blur -d 0 -U 128 -V 128</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>The option "-U 128 -V 128" specifies the U and V values of color, and "-d 0" specifies the color table of the blur function.</li>
</ul>
</dd></dl>
<p>For more details, please refer to the document in "Doxygen", included in SDK. For details on using "Doxygen", refer to Chapter 9 of <em>Ambarella_CV*_UG_Flexible_Linux_SDK*.*_EVK_Getting_Started_Guide.pdf</em>.</p>
<hr  />
<h1><a class="anchor" id="sec_onnx_fairmot"></a>
FairMOT</h1>
<p>To do multiple object tracking, FairMOT consists of two homogeneous branches to predict pixel-wise object scores and re-ID features with an anchor-free and single-shot deep network.</p>
<p>The following sections demonstrate how to export FairMOT ONNX model (with dlav0 backbone which doesn't use DCNv2) from PyTorch and run the ONNX model in the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, please contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_fairmot_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a project in GitHub with MIT license. Due to the pretrained model with dlav0 is not provided by the author, the following steps start with how to train the network with the dlav0_34 backbone on the dataset CrowdHuman and MOT17. The whole steps are shown as below.</p>
<ol type="1">
<li>Clone the source project from <a href="https://github.com/ifzhang/FairMOT">here</a>. <div class="fragment"><div class="line">commit c281132da5358c5aa925d9f295793171f8dc2038</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Clone DCNv2 if the network with dla_34 will be evaluated on PC. (tested with commit c7f778f28b84c66d3af2bf16f19148a07051dac1)</dd></dl>
<div class="fragment"><div class="line">build $ cd ${FAIRMOT_ROOT}/src/lib/models/networks/</div>
<div class="line">build $ git clone https:<span class="comment">//github.com/CharlesShang/DCNv2</span></div>
</div><!-- fragment --></li>
<li>Install requirements. <div class="fragment"><div class="line">build $ conda create -n FairMOT python=3.7.9</div>
<div class="line">build $ conda activate FairMOT</div>
<div class="line"><span class="preprocessor"># If CUDA will be used</span></div>
<div class="line">(FairMOT) build $ pip install torch==1.4.0 torchvision==0.5.0</div>
<div class="line"># If CPU will be used</div>
<div class="line">(FairMOT) build $ pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https:<span class="comment">//download.pytorch.org/whl/torch_stable.html</span></div>
<div class="line">(FairMOT) build $ pip install onnx==1.6.0</div>
<div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}</div>
<div class="line">(FairMOT) build $ pip install -r requirements.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Compile DCNv2 if network with dla_34 will be evaluated on PC.</dd></dl>
<div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}/src/lib/models/networks/DCNv2</div>
<div class="line">(FairMOT) build $ git clean -fx -d</div>
<div class="line">(FairMOT) build $ ./make.sh</div>
</div><!-- fragment --></li>
<li>Download dataset.<ol type="a">
<li>Download MOT17 dataset from <a href="https://motchallenge.net/data/MOT17.zip">here</a>.</li>
<li>Download CrowdHuman dataset from <a href="https://www.crowdhuman.org/download.html">here</a>.</li>
</ol>
<ul>
<li>CrowdHuman_train01.zip</li>
<li>CrowdHuman_train02.zip</li>
<li>CrowdHuman_train03.zip</li>
<li>CrowdHuman_val.zip</li>
<li>annotation_train.odgt</li>
<li>annotation_val.odgt</li>
</ul>
</li>
<li>Prepare the dataset in the following structure. <div class="fragment"><div class="line">${DATASET_ROOT}/</div>
<div class="line">MOT17</div>
<div class="line">|--images</div>
<div class="line">|    |--train</div>
<div class="line">|    |__test</div>
<div class="line">crowdhuman</div>
<div class="line">|--images</div>
<div class="line">|    |--train</div>
<div class="line">|    |__val</div>
<div class="line">|--annotation_train.odgt</div>
<div class="line">|__annotation_val.odgt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Unzip MOT17.zip, then move all folders under unzipped "MOT17" to <code>${DATASET_ROOT}/MOT17/images</code>.</li>
<li>Unzip CrowdHuman_train01.zip, CrowdHuman_train02.zip and CrowdHuman_train03.zip, then move all image files under unzipped "Images" folder to <code>/crowdhuman/images/train</code>.</li>
<li>Unzip CrowdHuman_val.zip, then move all image files under unzipped "Images" folder to <code>/crowdhuman/images/val</code>.</li>
</ul>
</dd></dl>
</li>
<li>Generate dataset labels.<ol type="a">
<li>Copy <code>${CNNGEN_SAMPLES_PACKAGE_ROOT}/onnx/demo_networks/fairmot/script/gen_labels_mot17.py</code> to <code>${FAIRMOT_ROOT}/src</code>, then run the following commands. <div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}/src</div>
<div class="line">(FairMOT) build $ python gen_labels_mot17.py --dataset_root ${DATASET_ROOT}</div>
</div><!-- fragment --></li>
<li>Copy <code>${CNNGEN_SAMPLES_PACKAGE_ROOT}/onnx/demo_networks/fairmot/script/gen_labels_crowdhuman.py</code> to <code>${FAIRMOT_ROOT}/src</code>, then run the following commands. <div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}/src</div>
<div class="line">(FairMOT) build $ python gen_labels_crowdhuman.py --dataset_root ${DATASET_ROOT}</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Download dlav0_34 COCO pretrained model <b>ctdet_coco_dlav0_1x.pth</b> from <a href="https://drive.google.com/drive/folders/1px-Xg7jXSC79QqgsD1AAGJQkuf5m0zh_">here</a>, and copy it to <em>${FAIRMOT_ROOT}/models</em>. <div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}</div>
<div class="line">(FairMOT) build $ mkdir models</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The pretrained model is from the CenterNet project at <a href="https://github.com/xingyizhou/CenterNet">https://github.com/xingyizhou/CenterNet</a>.</dd></dl>
</li>
<li>Change dataset configurations.<ol type="a">
<li>Change the following item in <code>${FAIRMOT_ROOT}/src/lib/cfg/mot17.json</code> to the actual one. <div class="fragment"><div class="line"><span class="stringliteral">&quot;root&quot;</span>:<span class="stringliteral">&quot;${DATASET_ROOT}&quot;</span>,</div>
</div><!-- fragment --></li>
<li>Change the following item in <code>${FAIRMOT_ROOT}/src/lib/cfg/crowdhuman.json</code> to the actual one. <div class="fragment"><div class="line"><span class="stringliteral">&quot;root&quot;</span>:<span class="stringliteral">&quot;${DATASET_ROOT}&quot;</span>,</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Modify <code>${FAIRMOT_ROOT}/src/train.py</code>. <div class="fragment"><div class="line">line 33: dataset = Dataset(opt, dataset_root, trainset_paths, (288, 160), augment=True, <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>=<a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a>)</div>
</div><!-- fragment --></li>
<li><p class="startli">Launch training.</p>
<p class="startli">Copy <code>${CNNGEN_SAMPLES_PACKAGE_ROOT}/onnx/demo_networks/fairmot/script/crowdhuman_dlav0_34.sh</code> and <code>${CNNGEN_SAMPLES_PACKAGE_ROOT}/onnx/demo_networks/fairmot/script/mot17_on_crowdhuman_dlav0_34.sh</code> to <code>${FAIRMOT_ROOT}/experiments</code>, and launch the training process. </p><div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}</div>
<div class="line">(FairMOT) build $ sh experiments/crowdhuman_dlav0_34.sh</div>
<div class="line">(FairMOT) build $ sh experiments/mot17_on_crowdhuman_dlav0_34.sh</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Change "--gpus 0,1,2,3" in mot17_on_crowdhuman_dlav0_34.sh to the actual one. The training will take about 1 hour on 4 GeForce RTX 2080 Ti gpus.</li>
<li>Change "--gpus 0,1" in crowdhuman_dlav0_34.sh to the actual one. The training will take about 7 hours on 2 GeForce RTX 2080 Ti gpus. "--gpus 0,1,2,3" can't be used in crowdhuman_dlav0_34.sh, or else it will cause error "RuntimeError: invalid argument 2: non-empty vector or matrix expected at `/pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:31`".</li>
<li>The trained model will be generated at <code>${FAIRMOT_ROOT}/exp/mot/mot17_on_crowdhuman_dlav0_34_288x160/model_last.pth</code>.</li>
</ul>
</dd></dl>
</li>
<li>After the training is done, copy <code>${CNNGEN_SAMPLES_PACKAGE_ROOT}/onnx/demo_networks/fairmot/script/export_fairmot_onnx_model.py</code> to <code>${FAIRMOT_ROOT}/src</code>, and run the following commands. <div class="fragment"><div class="line">(FairMOT) build $ cd ${FAIRMOT_ROOT}/src</div>
<div class="line">(FairMOT) build $ python export_fairmot_onnx_model.py mot --load_model ../exp/mot/mot17_on_crowdhuman_dlav0_34_288x160/model_last.pth --arch dlav0_34 --input_w 288 --input_h 160</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The ONNX model file <b>model_last_288x160.onnx</b> will be generated under the <code>${FAIRMOT_ROOT}/src</code> folder.</dd></dl>
</li>
<li>Quit conda and initialize the CV toolchain environment. <div class="fragment"><div class="line">(FairMOT) build $ conda deactivate</div>
<div class="line">(base) build $ conda deactivate</div>
<div class="line">build $ source ${CNNGEN_SAMPLES_PACKAGE_ROOT}/build/env/cv22.env</div>
</div><!-- fragment --></li>
<li><p class="startli">Do graph surgery. </p><div class="fragment"><div class="line">build $ graph_surgery.py onnx -m model_last_288x160.onnx -o fairmot_dlav0_34_288x160_mot17_on_crowdhuman.onnx -isrc=<span class="stringliteral">&quot;i:image|is:1,3,160,288&quot;</span> -on hm,hmax,wh,reg,<span class="keywordtype">id</span> -t ConstantifyShapes,FoldConstants</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>The file <b>fairmot_dlav0_34_288x160_mot17_on_crowdhuman.onnx</b> will be generated. It can be put under <code>${CNNGEN_SAMPLE_PACKAGE_ROOT}/onnx/demo_networks/fairmot/models</code>. Use the following command to verify there are no unsupported operators.</dd></dl>
<div class="fragment"><div class="line">build $ onnx_print_graph_summary.py -p fairmot_dlav0_34_288x160_mot17_on_crowdhuman.onnx</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_fairmot_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/fairmot/config/ea_cvt_onnx_fairmot.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_fairmot</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fairmot/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fairmot</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_fairmot.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fairmot/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_fairmot_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_fairmot/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_fairmot</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_fairmot_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella Package Configuration  ---&gt;</div>
<div class="line">    [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library  ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with fairmot</div>
<div class="line">      [*] Build EazyAI unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_fairmot_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_fairmot/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fairmot/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_fairmot/onnx_fairmot_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_fairmot</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_fairmot/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_fairmot/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin \</div>
<div class="line">        -pn fairmot -pl &lt;usr_path&gt;/fairmot/config/fairmot.lua -dm 0 \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_fairmot_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_fairmot_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin</b>.</li>
<li>The <b>fairmot.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86 Simulator: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_fairmot_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_fairmot.json</b> and <b>onnx_fairmot_ades.cmd</b>.</li>
</ol>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/fairmot</div>
<div class="line">|--model</div>
<div class="line">|      &lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|      mot_frame.jpg</div>
<div class="line">|      mot_frame.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fairmot_parser/onnx_fairmot.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_fairmot/onnx_fairmot_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/fairmot/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n fairmot \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fairmot_parser/onnx_fairmot.json \</div>
<div class="line">        --lua_file fairmot.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/onnx_fairmot/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_fairmot_ssd/onnx_fairmot_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fairmot_parser/onnx_fairmot.json\</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/fairmot/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n fairmot \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_fairmot_parser/onnx_fairmot.json \</div>
<div class="line">        --lua_file fairmot.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/fairmot/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --model_path &lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/fairmot/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=/sdcard/fairmot/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/fairmot/out</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n fairmot \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/fairmot/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fairmot.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:image=/sdcard/fairmot/in|t:jpeg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/fairmot/out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/fairmot/dra_img/mot_frame.jpg</code>) in <code>/sdcard/fairmot/dra_img</code>, and create <code>/sdcard/fairmot/out</code> as the output directory.</li>
<li>For the file mode with raw.bin as input, place the raw.bin (such as <code>cvflow_cnngen_samples/out/onnx/demo_networks/onnx_fairmot/dra_image_bin/mot_frame.bin</code>) in <code>/sdcard/fairmot/bin</code>, and create <code>/sdcard/fairmot/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 5 --reallocate_mem overlay,0x01600000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -r -s 0 -r -n fairmot \</div>
<div class="line">        --model_path &lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin  \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fairmot.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -c 1 -r -n fairmot \</div>
<div class="line">        --model_path &lt;chip&gt;_cavalry&lt;version&gt;_onnx_fairmot.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/fairmot.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <em>--enc_dummy_latency 4</em>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <em>--reallocate_mem overlay,0x04000000</em>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (1024 + resolution * (enc-dummy-latency + 5)). For details, please refer to <em>EazyAI Library API related content in Linux SDK Doxygen documents</em>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_lffd"></a>
LFFD</h1>
<p>The LFFD network is a general detection framework that is applicable to one class detection, such as face detection, pedestrian detection, head detection, vehicle detection and so on. The following sections show how users can export the LFFD ONNX model from MXNet, and work with it in the Ambarella CNNGen samples package.</p>
<h2><a class="anchor" id="onnx_lffd_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a GitHub project with the MIT license. The steps are provided as below.</p>
<ol type="1">
<li>Download the source code project from <a href="https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices">here</a>. <div class="fragment"><div class="line">commit e00a7aecc3c48c3f022dee197fb7b365bd1cebcb</div>
</div><!-- fragment --></li>
<li>Install the following dependent Python packages. <div class="fragment"><div class="line">mxnet == 1.6.0</div>
<div class="line">onnx == 1.3.0</div>
<div class="line">onnxruntime == 1.0.0</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>ONNX 1.3.0 is required to export the ONNX model from MXNet, or else the error "Node (slice_axis16) has input size 1 not in range [min=3, max=5]" will appear.</li>
<li>ONNX runtime 1.0.0 is required to run the Ambarella toolchain on the exported ONNX model.</li>
<li>A newer version of ONNX is needed when MXNet upgrades relate to exporting the slice operator.</li>
<li>A future version of Ambarella toolchain may not support the older versions of ONNX.</li>
</ul>
</dd></dl>
</li>
<li><p class="startli">Use the following Python code to export the ONNX model for the head detection. </p><div class="fragment"><div class="line"><span class="keyword">import</span> numpy as np</div>
<div class="line">from onnx <span class="keyword">import</span> checker</div>
<div class="line"><span class="keyword">import</span> onnx</div>
<div class="line"><span class="keyword">import</span> mxnet as mx</div>
<div class="line">from mxnet.contrib <span class="keyword">import</span> onnx as onnx_mxnet</div>
<div class="line"><span class="keyword">import</span> logging</div>
<div class="line">logging.basicConfig(level=logging.INFO)</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># Configuration begin</span></div>
<div class="line"><span class="preprocessor"># Downloaded input symbol and params files</span></div>
<div class="line">sym = <span class="stringliteral">&#39;./symbol_10_160_17L_4scales_v1_deploy-symbol.json&#39;</span></div>
<div class="line">params = <span class="stringliteral">&#39;./symbol_10_160_17L_4scales_v1_deploy-800000.params&#39;</span></div>
<div class="line">input_shape = (1,3,480,720)</div>
<div class="line"># Path of the output file</div>
<div class="line">onnx_file = <span class="stringliteral">&#39;./mxnet_exported_lffd_head_720x480.onnx&#39;</span></div>
<div class="line"># Configuration end</div>
<div class="line"> </div>
<div class="line">converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file)</div>
<div class="line">model_proto = onnx.load_model(converted_model_path)</div>
<div class="line">checker.check_graph(model_proto.graph)</div>
</div><!-- fragment --><p> The following model file will be generated and can be converted with the Ambarella toolchain. It is in the CNNGen sample package.</p>
<p class="startli">mxnet_exported_lffd_head_720x480.onnx</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The file <b>symbol_10_160_17L_4scales_v1_deploy-symbol.json</b> in the code is renamed from <b>symbol_10_160_17L_4scales_v1_deploy.json</b> under <code>head_detection/symbol_farm</code>.</li>
<li>The file <b>symbol_10_160_17L_4scales_v1_deploy-800000.params</b> in the code is renamed from <b>train_10_160_17L_4scales_v1_iter_800000.params</b> under <code>head_detection/saved_model/configuration_10_160_17L_4scales_v1_2019-09-20-13-08-26</code>.</li>
<li><b>input_shape</b> can be adjusted to a different size.</li>
</ul>
</dd></dl>
</li>
<li>There is no need to do graph surgery on the exported ONNX model. However, users can try it by running the following command. <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m mxnet_exported_lffd_head_720x480.onnx -o mxnet_exported_lffd_head_720x480-surgery.onnx -isrc <span class="stringliteral">&quot;i:data|is:1,3,480,720&quot;</span> -on slice_axis16,conv8_3_bbox,slice_axis17,conv11_3_bbox,slice_axis18,conv14_3_bbox,slice_axis19,conv17_3_bbox -t ConstantifyShapes:15,FoldConstants,CutGraph</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_lffd_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/LFFD/config/ea_cvt_onnx_lffd.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_lffd</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_lffd/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_lffd</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_lffd.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_lffd/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_lffd_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_lffd/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_lffd</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_lffd_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">   -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">   -*- Build eazyai library with OpenCV support</div>
<div class="line">   -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">     [*] Build Ambarella custom postprocess library with lffd</div>
<div class="line">   [*] Build EazyAi unit tests</div>
<div class="line"> build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_lffd_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -b out/onnx/demo_networks/onnx_yolov3/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_lffd/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_lffd/onnx_lffd_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_lffd</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_lffd/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_lffd/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin \</div>
<div class="line">        -pn lffd -pl &lt;usr_path&gt;/lffd/config/lffd.lua -dm 0 \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_lffd_run_c_inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_lffd_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</b>.</li>
<li>The <b>lffd.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_lffd_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_lffd.json</b> and <b>onnx_lffd_ades.cmd</b>.</li>
</ol>
</dd></dl>
<ul>
<li>Copy files to SD card for EVK test For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/lffd</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        00034000_640x480.jpg</div>
<div class="line">|        00034000_640x480.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">      --model_path &lt;usr_path&gt;/out_onnx_lffd_parser/onnx_lffd.json \</div>
<div class="line">      --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_lffd/onnx_lffd_ades.cmd \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">      --output_dir &lt;usr_path&gt;/lffd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n lffd \</div>
<div class="line">      --model_path &lt;usr_path&gt;/out_onnx_lffd_parser/onnx_lffd.json \</div>
<div class="line">      --lua_file lffd.lua \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">      --output_dir &lt;usr_path&gt;/lffd/out \</div>
<div class="line">      --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_lffd/onnx_lffd_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">      --model_path &lt;usr_path&gt;/out_onnx_lffd_parser/onnx_lffd.json \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">      --output_dir &lt;usr_path&gt;/lffd/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n lffd \</div>
<div class="line">      --model_path &lt;usr_path&gt;/out_onnx_lffd_parser/onnx_lffd.json \</div>
<div class="line">      --lua_file lffd.lua --queue_size 1 --thread_num 1 \</div>
<div class="line">      --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg&quot;</span> \</div>
<div class="line">      --output_dir &lt;usr_path&gt;/lffd/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/lffd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n lffd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/lffd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/lffd.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/lffd/in|t:jpg|d:vp&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/lffd/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/lffd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/lffd/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/lffd/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/lffd/dra_img/00034000_640x480.jpg</code>) in <code>/sdcard/lffd/in</code>, and create <code>/sdcard/lffd/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n lffd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/lffd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/lffd.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -n lffd \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/lffd/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_lffd.bin</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/lffd.lua</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <em>--enc_dummy_latency 4</em>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <em>--reallocate_mem overlay,0x04000000</em>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (1024 + resolution * (enc-dummy-latency + 5)). For details, please refer to <em>EazyAI Library API related content in Linux SDK Doxygen documents</em>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_retinaface"></a>
RetinaFace</h1>
<p>RetinaFace is a practical single-stage SOTA face detector which is initially introduced in arXiv technical report and then accepted by CVPR 2020.</p>
<p>The following sections demonstrate how to export an RetinaFace ONNX model from the public source project implemented with MXNet and how to run the ONNX model in the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_retinaface_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a source project in GitHub with MIT license. The steps are shown as below.</p>
<ol type="1">
<li>The source code project is from <a href="https://github.com/deepinsight/insightface/tree/master/RetinaFace">here</a>. Only the pre-trained model files are used when exporting the model. <div class="fragment"><div class="line">commit 3866cd77a6896c934b51ed39e9651b791d78bb57</div>
</div><!-- fragment --></li>
<li>Install the required Python packages. <div class="fragment"><div class="line">mxnet&gt;=1.6.0</div>
<div class="line">onnx&gt;=1.6.0</div>
</div><!-- fragment --></li>
<li>Download pre-trained model <a href="http://insightface.ai/files/models/retinaface_mnet025_v1.zip">retinaface_mnet025_v1.zip</a>. <dl class="section note"><dt>Note</dt><dd>The download link is derived from the <a href="https://github.com/deepinsight/insightface/blob/master/python-package/insightface/model_zoo/model_store.py">source code</a>.</dd></dl>
</li>
<li>Write a Python3 source file to convert model from MXNet to ONNX. The detailed example can be found in <code>cnngen_samples_package/onnx/demo_networks/retinaface/script/export_retinaface_onnx_model.py</code>.</li>
<li>Extract the model files and put the model folder together with <b>export_retinaface_onnx_model.py</b>. <div class="fragment"><div class="line">|-- export_retinaface_onnx_model.py</div>
<div class="line">|__ retinaface_mnet025_v1</div>
<div class="line">  |-- mnet10-0000.params</div>
<div class="line">  |__ mnet10-symbol.json</div>
</div><!-- fragment --></li>
<li>Modify <b>_op_translations.py</b> in the installed MXNet package path. For example <code>~/.local/lib/python3.6/site-packages/mxnet/contrib/onnx/mx2onnx/_op_translations.py</code>.<ol type="a">
<li>The original <b>export_onnx</b> module in mxnet 1.6.0 does not support the <b>slice</b> operator. Add the following functions to support exporting the <b>slice</b> operator. <div class="fragment"><div class="line">def create_helper_tensor_node(input_vals, output_name, kwargs):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;</span><span class="stringliteral">&quot;create extra tensor node from numpy values&quot;</span><span class="stringliteral">&quot;&quot;</span></div>
<div class="line">    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[input_vals.dtype]</div>
<div class="line"> </div>
<div class="line">    tensor_node = onnx.helper.make_tensor_value_info(</div>
<div class="line">        <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=output_name,</div>
<div class="line">        elem_type=data_type,</div>
<div class="line">        shape=input_vals.shape</div>
<div class="line">    )</div>
<div class="line">    kwargs[<span class="stringliteral">&quot;initializer&quot;</span>].append(</div>
<div class="line">        onnx.helper.make_tensor(</div>
<div class="line">            <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=output_name,</div>
<div class="line">            data_type=data_type,</div>
<div class="line">            dims=input_vals.shape,</div>
<div class="line">            vals=input_vals.flatten(),</div>
<div class="line">            raw=False,</div>
<div class="line">        )</div>
<div class="line">    )</div>
<div class="line"> </div>
<div class="line">    return [tensor_node]</div>
<div class="line"> </div>
<div class="line">@mx_op.register(<span class="stringliteral">&quot;slice&quot;</span>)</div>
<div class="line">def convert_slice(node, **kwargs):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;</span><span class="stringliteral">&quot;Map MXNet&#39;s slice_axis operator attributes to onnx&#39;s Slice operator</span></div>
<div class="line"><span class="stringliteral">    and return the created node.</span></div>
<div class="line"><span class="stringliteral">    &quot;</span><span class="stringliteral">&quot;&quot;</span></div>
<div class="line">    <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>, input_nodes, attrs = get_inputs(node, kwargs)</div>
<div class="line"> </div>
<div class="line">    axes = [2,3]</div>
<div class="line">    starts = convert_string_to_list(attrs.get(<span class="stringliteral">&quot;begin&quot;</span>))</div>
<div class="line">    ends = convert_string_to_list(attrs.get(<span class="stringliteral">&quot;end&quot;</span>))</div>
<div class="line"> </div>
<div class="line">    export_nodes = []</div>
<div class="line"> </div>
<div class="line">    starts = np.atleast_1d(np.asarray(starts, dtype=np.int))</div>
<div class="line">    ends = np.atleast_1d(np.asarray(ends, dtype=np.int))</div>
<div class="line">    axes = np.atleast_1d(np.asarray(axes, dtype=np.int))</div>
<div class="line"> </div>
<div class="line">    starts_node = create_helper_tensor_node(starts, <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> + <span class="stringliteral">&#39;__starts&#39;</span>, kwargs)</div>
<div class="line">    export_nodes.extend(starts_node)</div>
<div class="line">    starts_node = starts_node[-1].<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div>
<div class="line"> </div>
<div class="line">    ends_node = create_helper_tensor_node(ends, <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> + <span class="stringliteral">&#39;__ends&#39;</span>, kwargs)</div>
<div class="line">    export_nodes.extend(ends_node)</div>
<div class="line">    ends_node = ends_node[-1].<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div>
<div class="line"> </div>
<div class="line">    axes_node = create_helper_tensor_node(axes, <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> + <span class="stringliteral">&#39;__axes&#39;</span>, kwargs)</div>
<div class="line">    export_nodes.extend(axes_node)</div>
<div class="line">    axes_node = axes_node[-1].<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div>
<div class="line"> </div>
<div class="line">    input_node = input_nodes[0]</div>
<div class="line">    node = onnx.helper.make_node(</div>
<div class="line">        <span class="stringliteral">&quot;Slice&quot;</span>,</div>
<div class="line">        [input_node, starts_node, ends_node, axes_node],</div>
<div class="line">        [<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>],</div>
<div class="line">        <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>,</div>
<div class="line">    )</div>
<div class="line">    export_nodes.extend([node])</div>
<div class="line"> </div>
<div class="line">    return export_nodes</div>
</div><!-- fragment --></li>
<li>The original <b>export_onnx</b> module in mxnet 1.6.0 has problem to export the <b>BatchNorma</b> operator to ONNX in opset 9 and above. The new function in mxnet 1.7.0 has been improved, but it is not released in pip yet. Use the following content from mxnet 1.7.0 to replace the original <b>convert_batchnorm</b> function. <div class="fragment"><div class="line">@mx_op.register(<span class="stringliteral">&quot;BatchNorm&quot;</span>)</div>
<div class="line">def convert_batchnorm(node, **kwargs):</div>
<div class="line">    <span class="stringliteral">&quot;&quot;</span><span class="stringliteral">&quot;Map MXNet&#39;s BatchNorm operator attributes to onnx&#39;s BatchNormalization operator</span></div>
<div class="line"><span class="stringliteral">    and return the created node.</span></div>
<div class="line"><span class="stringliteral">    &quot;</span><span class="stringliteral">&quot;&quot;</span></div>
<div class="line">    <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>, input_nodes, attrs = get_inputs(node, kwargs)</div>
<div class="line"> </div>
<div class="line">    momentum = float(attrs.get(<span class="stringliteral">&quot;momentum&quot;</span>, 0.9))</div>
<div class="line">    eps = float(attrs.get(<span class="stringliteral">&quot;eps&quot;</span>, 0.001))</div>
<div class="line"> </div>
<div class="line">    bn_node = onnx.helper.make_node(</div>
<div class="line">        <span class="stringliteral">&quot;BatchNormalization&quot;</span>,</div>
<div class="line">        input_nodes,</div>
<div class="line">        [<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>],</div>
<div class="line">        <a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>=<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a>,</div>
<div class="line">        epsilon=eps,</div>
<div class="line">        momentum=momentum</div>
<div class="line">        # MXNet computes mean and variance per channel for batchnorm.</div>
<div class="line">        # Default for onnx is across all spatial features. Relying on default</div>
<div class="line">        # ONNX behavior of spatial=1 for ONNX opset 8 and below. As the spatial</div>
<div class="line">        # attribute is deprecated in opset 9 and above, not explicitly encoding it.</div>
<div class="line">    )</div>
<div class="line">    return [bn_node]</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run the following command to generate the ONNX model by replacing some unsupported operators without retrain. The ONNX model file <b>mxnet_exported_retina_face_640x640.onnx</b> will be generated under the folder <em>out_dir</em>. <div class="fragment"><div class="line">build $ python3 export_retinaface_onnx_model.py --prefix ./retinaface_mnet025_v1/mnet10 --epoch 0 --input_h 640 --input_w 640 --out_dir ./</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>UpSampling is replaced with Deconvolution.</li>
<li>Crop is replaced with slice.</li>
<li>SoftmaxActivation is replaced with softmax.</li>
</ul>
</dd></dl>
</li>
<li>(Optional) Run the following command to do graph surgery. <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m mxnet_exported_retina_face_640x640.onnx -o mxnet_exported_retina_face_640x640_surgery.onnx -isrc=<span class="stringliteral">&quot;i:data|is:1,3,640,640&quot;</span> –on <span class="stringliteral">&quot;face_rpn_cls_prob_reshape_stride32,face_rpn_bbox_pred_stride32,face_rpn_landmark_pred_stride32,face_rpn_cls_prob_reshape_stride16,face_rpn_bbox_pred_stride16,face_rpn_landmark_pred_stride16,face_rpn_cls_prob_reshape_stride8,face_rpn_bbox_pred_stride8,face_rpn_landmark_pred_stride8&quot;</span> -t ConstantifyShapes,FoldConstants</div>
</div><!-- fragment --></li>
<li>Run the following command to verify that there are no unsupported operators. <div class="fragment"><div class="line">build $ onnx_print_graph_summary.py -p mxnet_exported_retina_face_640x640.onnx</div>
<div class="line">build $ onnx_print_graph_summary.py -p mxnet_exported_retina_face_640x640_surgery.onnx</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_retinaface_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/retinaface/config/ea_cvt_onnx_retinaface.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_retinaface</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_retinaface.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_retinaface/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_retinaface</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_retinaface.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_retinaface/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_retinaface_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_retinaface/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_retinaface</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_retinaface_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with retinaface</div>
<div class="line">      [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_retinaface_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_retinaface/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_retinaface/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_retinaface.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_retinaface/onnx_retinaface_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_retinaface</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_retinaface/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_retinaface/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_retinaface.bin \</div>
<div class="line">        -pn retinaface -pl &lt;usr_path&gt;/retinaface/config/retinaface.lua -dm 0 \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_retinaface_run_c_Inference"></a>
5 Run C Inference</h2>
<p>The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_retinaface_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin</b>.</li>
<li>The <b>retinaface.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_retinaface_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_retinaface.json</b> and <b>onnx_retinaface_ades.cmd</b>.</li>
</ol>
</dd></dl>
<ul>
<li><p class="startli">Copy files to SD card for EVK test</p>
<p class="startli">For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/retinaface</div>
<div class="line">|--model</div>
<div class="line">|        &lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|        faces01.jpg</div>
<div class="line">|        faces02.jpg</div>
<div class="line">|        faces02.bin</div>
<div class="line">|        faces01.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file\</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_retinaface_parser/onnx_retinaface.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_retinaface/onnx_retinaface_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/retinaface/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n retinaface \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_retinaface_parser/onnx_retinaface.json \</div>
<div class="line">        --lua_file retinaface.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/retinaface/out \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_retinaface/onnx_retinaface_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_retinaface_parser/onnx_retinaface.json</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/retinaface/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n retinaface \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_retinaface_parser/onnx_retinaface.json \</div>
<div class="line">        --lua_file retinaface.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n retinaface \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/retinaface/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/retinaface/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:data=/sdcard/retinaface/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/retinaface/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/retinaface/dra_img/</code>) in <code>/sdcard/retinaface/in</code>, and create <code>/sdcard/retinaface/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples<ol type="a">
<li>Normal demo <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Blur demo <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --blur --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000:blur,0x00100000</div>
</div><!-- fragment --></li>
<li>Query YUV with pyramid manual feed <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000 --pyramid_manual_map 0x03 --pyramid_scale_type 2 --pyramid_layer_1_rescale_size 720x480</div>
</div><!-- fragment --></li>
<li>Query YUV with canvas feed <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000 --canvas_manual_feed_frame_num 1</div>
</div><!-- fragment --></li>
<li>EFM demo <div class="fragment"><div class="line">eazyai_video.sh doesn<span class="stringliteral">&#39;t support initializing environment for EFM yet. For initializing, please refer to the description(2. Run/e. EFM demo) about how to run the EFM demo.</span></div>
</div><!-- fragment --></li>
</ol>
</li>
<li><p class="startli">Run</p><ol type="a">
<li>Normal demo<ol type="i">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n retinaface \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n retinaface \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Blur and Overlay (draw on stream <code>rtsp://10.0.0.2/stream1</code> with DSP blur (high score faces) and stream overlay (low score faces)) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n retinaface \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua \</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>The Blur mode includes <b>Blur</b> and <b>Blur and Overlay</b> modes.</li>
<li>The lua file <b>enable_blur</b> needs to be changed to <b>1</b>, and the default value of <b>enable_blur</b> is <b>0</b>. And set the value of <b>conf_threshold</b> and <b>blur_conf_threshold</b> according to note 2, 3. Their values range from 0 to 1.</li>
<li>In Blur mode, user needs to set the value of <b>blur_conf_threshold</b> less than <b>conf_threshold</b> under lua file.</li>
<li>In Blur and Overlay mode, user needs to set the value of <b>blur_conf_threshold</b> greater than <b>conf_threshold</b> under lua file.</li>
<li>In this case, if the <b>score</b> is between <b>conf_threshold</b> and <b>blur_conf_threshold</b>, it is the Overlay mode. If <b>score</b> is greater than <b>blur_conf_threshold</b>, it is the Blur mode.</li>
</ul>
</dd></dl>
</li>
<li>Query YUV with pyramid manual feed <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -p 0,1 -n retinaface \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --></li>
<li>Query YUV with canvas manual feed <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n retinaface \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --></li>
<li>EFM demo (<em>Take CV22 Walnut and imx274_mipi for example</em>).<ol type="i">
<li>Overlay (draw on stream <code>rtsp://10.0.0.2/stream1</code> with overlay only) <div class="fragment"><div class="line">board # init.sh --imx274_mipi</div>
<div class="line">board # test_aaa_service -a &amp;</div>
<div class="line">board # test_encode --resource-cfg /usr/local/bin/scripts/cv22_multi_efm_1080p_linear.lua</div>
<div class="line">board # test_encode -A -h1080p -b 16 -e</div>
<div class="line">board # rtsp_server &amp;</div>
<div class="line">board # modprobe cavalry;cavalry_load -f /lib/firmware/cavalry.bin -r (Only CV2x and CV5x need to <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> <span class="keyword">this</span>)</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -c 0 -n retinaface \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --></li>
<li><p class="startli">Overlay and Blur (draw on stream <code>rtsp://10.0.0.2/stream1</code> with DSP blur (high score faces) and stream overlay (low score faces)) </p><div class="fragment"><div class="line">board # init.sh --imx274_mipi</div>
<div class="line">board # test_aaa_service -a &amp;</div>
<div class="line">board # test_mempart -m 27 -s 0x100000</div>
<div class="line">board # test_encode --resource-cfg /usr/local/bin/scripts/cv22_multi_efm_1080p_linear.lua --hdmi 1080p --blur-enable 1 --blur-vout-mixer 0 --osd-mixer off --mixer 0 --yuv422 1</div>
<div class="line">board # test_encode -A -h1080p -b 16 -e</div>
<div class="line">board # rtsp_server &amp;</div>
<div class="line">board # modprobe cavalry;cavalry_load -f /lib/firmware/cavalry.bin -r (Only CV2x and CV5x need to <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a> <span class="keyword">this</span>)</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -c 0 -n retinaface \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/retinaface/model/&lt;chip&gt;_cavalry&lt;verion&gt;_onnx_retinaface.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/retinaface.lua</div>
</div><!-- fragment --><p class="startli">In Overlay and Blur mode, the lua file <b>enable_blur</b> needs to be changed to <b>1</b>, and the default value of <b>enable_blur</b> is <b>0</b>.</p>
</li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_robust_video_matting"></a>
Robust Video Matting</h1>
<p>The model is from the official repository for the paper <em>Robust High-Resolution Video Matting with Temporal Guidance</em>. Robust video matting (RVM) is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network (NN) to process videos with temporal memory. RVM can perform matting in real-time on any video without additional inputs.</p>
<p>The following sections demonstrate how to export an RVM Open Neural Network Exchange (ONNX) model from the public source project implemented with PyTorch, as well as how to convert the ONNX model in the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_robust_video_matting_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a source project in GitHub with GPL-3.0 license. The steps are as shown below:</p>
<ol type="1">
<li>Clone the source code project from <a href="https://github.com/PeterL1n/RobustVideoMatting">GitHub</a>. <div class="fragment"><div class="line">commit 81a10937c73f68eeddb863221c61fe6a60a1cca2</div>
</div><!-- fragment --></li>
<li>Install the required Python packages. <div class="fragment"><div class="line">av==8.0.3</div>
<div class="line">torch==1.9.0</div>
<div class="line">torchvision==0.10.0</div>
<div class="line">tqdm==4.61.1</div>
<div class="line">pims==0.5</div>
</div><!-- fragment --></li>
<li>Download the pre-trained model <a href="https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth">rvm_mobilenetv3.pth</a>.</li>
<li>Switch to the <em>onnx</em> branch to export the ONNX model.<ol type="a">
<li>Go to the <em>RobustVideoMatting</em> folder and checkout the <em>onnx</em> branch. <div class="fragment"><div class="line">build $ git checkout onnx</div>
</div><!-- fragment --></li>
<li>Modify <em>model/model.py</em> using the comments in the following code: <div class="fragment"><div class="line"><span class="keyword">class </span>MattingNetwork(nn.Module):</div>
<div class="line">  def <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga36536ba84124361916240b28721fe384">__init__</a>(self,</div>
<div class="line">      variant: str = &#39;mobilenetv3&#39;,</div>
<div class="line">      refiner: str = &#39;deep_guided_filter&#39;,</div>
<div class="line">      downsample_ratio : float = 0.25,                 # add this line</div>
<div class="line">...</div>
<div class="line">  if refiner == &#39;deep_guided_filter&#39;:</div>
<div class="line">   self.refiner = DeepGuidedFilterRefiner()</div>
<div class="line">  else:</div>
<div class="line">   self.refiner = FastGuidedFilterRefiner()</div>
<div class="line">  self.downsample_ratio = downsample_ratio                 # add this line</div>
<div class="line">...</div>
<div class="line">  if torch.onnx.is_in_onnx_export():</div>
<div class="line">   print(&quot;MattingNetwork forwards with downsample_ratio&quot;, self.downsample_ratio)  # add this line</div>
<div class="line">   downsample_ratio = torch.tensor([self.downsample_ratio])         # add this line</div>
<div class="line">   src_sm = CustomOnnxResizeByFactorOp.apply(src, downsample_ratio)</div>
<div class="line">...</div>
<div class="line">  if not segmentation_pass:</div>
<div class="line">   fgr_residual, pha = self.project_mat(hid).split([3, 1], dim=-3)</div>
<div class="line">   if torch.onnx.is_in_onnx_export() or downsample_ratio != 1:</div>
<div class="line">      fgr_residual, pha = self.refiner(src, src_sm, fgr_residual, pha, hid)</div>
<div class="line">   fgr = fgr_residual + src</div>
<div class="line">   fgr = fgr.clamp(0., 1.)</div>
<div class="line">   pha = pha.clamp(0., 1.)</div>
<div class="line">   return [fgr, pha, *rec]                       # remove this line</div>
<div class="line">   pha = pha * torch.tensor([255.0])                  # add this line</div>
<div class="line">   return [pha, *rec]                        # add this line</div>
</div><!-- fragment --></li>
<li>Change <em>export_onnx.py</em> to the following code. <div class="fragment"><div class="line"><span class="stringliteral">&quot;&quot;</span><span class="stringliteral">&quot;</span></div>
<div class="line"><span class="stringliteral">python export_onnx.py \</span></div>
<div class="line"><span class="stringliteral"> --model-variant mobilenetv3 \</span></div>
<div class="line"><span class="stringliteral"> --checkpoint rvm_mobilenetv3.pth \</span></div>
<div class="line"><span class="stringliteral"> --precision float16 \</span></div>
<div class="line"><span class="stringliteral"> --opset 12 \</span></div>
<div class="line"><span class="stringliteral"> --device cuda \</span></div>
<div class="line"><span class="stringliteral"> --output model.onnx</span></div>
<div class="line"><span class="stringliteral"> --downsample-ratio 0.25</span></div>
<div class="line"><span class="stringliteral"> --input-width 1920</span></div>
<div class="line"><span class="stringliteral"> --input-height 1080</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">Note:</span></div>
<div class="line"><span class="stringliteral"> The device is only used for exporting. It has nothing to do with the final model.</span></div>
<div class="line"><span class="stringliteral"> Float16 must be exported through cuda. Float32 can be exported through cpu.</span></div>
<div class="line"><span class="stringliteral">&quot;</span><span class="stringliteral">&quot;&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">import</span> argparse</div>
<div class="line"><span class="keyword">import</span> torch</div>
<div class="line"><span class="keyword">import</span> math</div>
<div class="line"> </div>
<div class="line">from model <span class="keyword">import</span> MattingNetwork</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>Exporter:</div>
<div class="line"> def <a class="codeRef" href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga36536ba84124361916240b28721fe384">__init__</a>(self):</div>
<div class="line">     self.parse_args()</div>
<div class="line">     self.init_model()</div>
<div class="line">     self.export()</div>
<div class="line"> </div>
<div class="line"> def parse_args(self):</div>
<div class="line">     parser = argparse.ArgumentParser()</div>
<div class="line">     parser.add_argument(&#39;--model-variant&#39;, type=str, required=True, choices=[&#39;mobilenetv3&#39;, &#39;resnet50&#39;])</div>
<div class="line">     parser.add_argument(&#39;--model-refiner&#39;, type=str, default=&#39;deep_guided_filter&#39;, choices=[&#39;deep_guided_filter&#39;, &#39;fast_guided_filter&#39;])</div>
<div class="line">     parser.add_argument(&#39;--precision&#39;, type=str, required=True, choices=[&#39;float16&#39;, &#39;float32&#39;])</div>
<div class="line">     parser.add_argument(&#39;--opset&#39;, type=int, required=True)</div>
<div class="line">     parser.add_argument(&#39;--device&#39;, type=str, required=True)</div>
<div class="line">     parser.add_argument(&#39;--checkpoint&#39;, type=str, required=False)</div>
<div class="line">     parser.add_argument(&#39;--output&#39;, type=str, required=True)</div>
<div class="line">     parser.add_argument(&#39;--downsample-ratio&#39;, type=float, default=0.25)</div>
<div class="line">     parser.add_argument(&#39;--input-size&#39;, type=int, default=(1920,1080), nargs=2)</div>
<div class="line">     self.args = parser.parse_args()</div>
<div class="line"> </div>
<div class="line"> def init_model(self):</div>
<div class="line">     self.precision = torch.float32 if self.args.precision == &#39;float32&#39; else torch.float16</div>
<div class="line">     self.model = MattingNetwork(self.args.model_variant, self.args.model_refiner, self.args.downsample_ratio).eval().to(self.args.device, self.precision)</div>
<div class="line">     if self.args.checkpoint is not None:</div>
<div class="line">     self.model.load_state_dict(torch.load(self.args.checkpoint, map_location=self.args.device), strict=False)</div>
<div class="line"> </div>
<div class="line"> def export(self):</div>
<div class="line">     size_w = self.args.input_size[0] * self.args.downsample_ratio</div>
<div class="line">     size_h = self.args.input_size[1] * self.args.downsample_ratio</div>
<div class="line">     src = torch.randn(1, 3, self.args.input_size[1], self.args.input_size[0]).to(self.args.device, self.precision)</div>
<div class="line">     r1i = torch.randn(1, 16, math.ceil(size_h/2), math.ceil(size_w/2)).to(self.args.device, self.precision)</div>
<div class="line">     r2i = torch.randn(1, 20, math.ceil(size_h/4), math.ceil(size_w/4)).to(self.args.device, self.precision)</div>
<div class="line">     r3i = torch.randn(1, 40, math.ceil(size_h/8), math.ceil(size_w/8)).to(self.args.device, self.precision)</div>
<div class="line">     r4i = torch.randn(1, 64, math.ceil(size_h/16), math.ceil(size_w/16)).to(self.args.device, self.precision)</div>
<div class="line"> </div>
<div class="line">     torch.onnx.export(</div>
<div class="line">     self.model,</div>
<div class="line">     (src, r1i, r2i, r3i, r4i),</div>
<div class="line">     self.args.output,</div>
<div class="line">     export_params=True,</div>
<div class="line">     opset_version=self.args.opset,</div>
<div class="line">     do_constant_folding=True,</div>
<div class="line">     input_names=[&#39;src&#39;, &#39;r1i&#39;, &#39;r2i&#39;, &#39;r3i&#39;, &#39;r4i&#39;],</div>
<div class="line">     output_names=[&#39;pha&#39;, &#39;r1o&#39;, &#39;r2o&#39;, &#39;r3o&#39;, &#39;r4o&#39;])</div>
<div class="line"> </div>
<div class="line"> if __name__ == &#39;__main__&#39;:</div>
<div class="line">     Exporter()</div>
</div><!-- fragment --></li>
<li>Run the following command to export the ONNX model. <div class="fragment"><div class="line">build $ python3 export_onnx.py --model-variant mobilenetv3 \</div>
<div class="line">     --checkpoint rvm_mobilenetv3.pth \</div>
<div class="line">     --precision float32 \</div>
<div class="line">     --opset 11 \</div>
<div class="line">     --device cpu \</div>
<div class="line">     --output rvm_mobilenetv3_1920x1080_ratio_0.25.onnx \</div>
<div class="line">     --downsample-ratio 0.25 \</div>
<div class="line">    --input-size 1920 1080</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This command is an example to set input with to 1920x1080 and downsample ratio with 0.25. The file <em>rvm_mobilenetv3_1920x1080_ratio_0.25.onnx</em> will be generated.</dd></dl>
</li>
<li>Perform graph surgery. <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m rvm_mobilenetv3_1920x1080_ratio_0.25.onnx \</div>
<div class="line">    -o rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;i:src|is:1,3,1080,1920&quot;</span> \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;i:r1i|is:1,16,135,240&quot;</span> \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;i:r2i|is:1,20,68,120&quot;</span> \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;i:r3i|is:1,40,34,60&quot;</span> \</div>
<div class="line">    -isrc <span class="stringliteral">&quot;i:r4i|is:1,64,17,30&quot;</span> \</div>
<div class="line">    -on <span class="stringliteral">&quot;pha,r1o,r2o,r3o,r4o&quot;</span> \</div>
<div class="line">    -t ConstantifyShapes,FoldConstants</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The file <em>rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx</em> will be generated. This file is used to generate the Cavalry binary model.</dd></dl>
</li>
<li>Run the following command to verify that there are no unsupported operators. <div class="fragment"><div class="line">build $ onnx_print_graph_summary.py -p rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Switch to the <em>master</em> branch to generate dynamic range analysis (DRA) data.<ol type="a">
<li>Go to the <em>RobustVideoMatting</em> folder and checkout the <em>master</em> branch. <div class="fragment"><div class="line">build $ git checkout master</div>
</div><!-- fragment --></li>
<li>Modify <em>inference.py</em> using the comments in the following code. <div class="fragment"><div class="line">with torch.no_grad():</div>
<div class="line">    bar = tqdm(total=len(source), disable=not progress, dynamic_ncols=True)</div>
<div class="line">    rec = [None] * 4</div>
<div class="line">    from torchvision.transforms.functional <span class="keyword">import</span> to_pil_image  # add <span class="keyword">this</span> line</div>
<div class="line">    <span class="keyword">import</span> shutil                  # add <span class="keyword">this</span> line</div>
<div class="line">    FRAME_STEP = 200                 # add <span class="keyword">this</span> line</div>
<div class="line">    FRAME_NUM = 3                  # add <span class="keyword">this</span> line</div>
<div class="line">    cur_index = 0                  # add <span class="keyword">this</span> line</div>
<div class="line">    shutil.rmtree(<span class="stringliteral">&quot;dra_data&quot;</span>, ignore_errors=True)      # add <span class="keyword">this</span> line</div>
<div class="line">    os.makedirs(<span class="stringliteral">&quot;dra_data/dra_src&quot;</span>)            # add <span class="keyword">this</span> line</div>
<div class="line">    os.makedirs(<span class="stringliteral">&quot;dra_data/dra_r1i&quot;</span>)            # add <span class="keyword">this</span> line</div>
<div class="line">    os.makedirs(<span class="stringliteral">&quot;dra_data/dra_r2i&quot;</span>)            # add <span class="keyword">this</span> line</div>
<div class="line">    os.makedirs(<span class="stringliteral">&quot;dra_data/dra_r3i&quot;</span>)            # add <span class="keyword">this</span> line</div>
<div class="line">    os.makedirs(<span class="stringliteral">&quot;dra_data/dra_r4i&quot;</span>)            # add <span class="keyword">this</span> line</div>
<div class="line">    <span class="keywordflow">for</span> src in reader:</div>
<div class="line"> </div>
<div class="line">    <span class="keywordflow">if</span> downsample_ratio is None:</div>
<div class="line">     downsample_ratio = auto_downsample_ratio(*src.shape[2:])</div>
<div class="line"> </div>
<div class="line">    src = src.to(device, dtype, non_blocking=True).unsqueeze(0) <span class="preprocessor"># [B, T, C, H, W]</span></div>
<div class="line">    <span class="keywordflow">if</span> rec[0] is not None and cur_index % FRAME_STEP == 1:            # add <span class="keyword">this</span> line</div>
<div class="line">     to_pil_image(src[0][0]).save(<span class="stringliteral">&quot;dra_data/dra_src/src_{}.jpg&quot;</span>.format(cur_index))  # add <span class="keyword">this</span> line</div>
<div class="line">     rec[0].numpy().tofile(<span class="stringliteral">&quot;dra_data/dra_r1i/r1i_{}.bin&quot;</span>.format(cur_index))     # add <span class="keyword">this</span> line</div>
<div class="line">     rec[1].numpy().tofile(<span class="stringliteral">&quot;dra_data/dra_r2i/r2i_{}.bin&quot;</span>.format(cur_index))     # add <span class="keyword">this</span> line</div>
<div class="line">     rec[2].numpy().tofile(<span class="stringliteral">&quot;dra_data/dra_r3i/r3i_{}.bin&quot;</span>.format(cur_index))     # add <span class="keyword">this</span> line</div>
<div class="line">     rec[3].numpy().tofile(<span class="stringliteral">&quot;dra_data/dra_r4i/r4i_{}.bin&quot;</span>.format(cur_index))     # add <span class="keyword">this</span> line</div>
<div class="line">     <span class="keywordflow">if</span> cur_index &gt;= FRAME_STEP * (FRAME_NUM - 1):              # add <span class="keyword">this</span> line</div>
<div class="line">       print(<span class="stringliteral">&quot;src shape&quot;</span>, src[0].shape)                 <span class="preprocessor"># add this line</span></div>
<div class="line">       print(<span class="stringliteral">&quot;r1i shape&quot;</span>, rec[0].shape)                 <span class="preprocessor"># add this line</span></div>
<div class="line">       print(<span class="stringliteral">&quot;r2i shape&quot;</span>, rec[1].shape)                 <span class="preprocessor"># add this line</span></div>
<div class="line">       print(<span class="stringliteral">&quot;r3i shape&quot;</span>, rec[2].shape)                 <span class="preprocessor"># add this line</span></div>
<div class="line">       print(<span class="stringliteral">&quot;r4i shape&quot;</span>, rec[3].shape)                 <span class="preprocessor"># add this line</span></div>
<div class="line">       quit()                           <span class="preprocessor"># add this line</span></div>
<div class="line">    cur_index = cur_index + 1                       # add <span class="keyword">this</span> line</div>
<div class="line">    fgr, pha, *rec = model(src, *rec, downsample_ratio)</div>
</div><!-- fragment --></li>
<li>Run the following command to generate DRA data for converting <em>rvm_mobilenetv3_1920x1080_ratio_0.25_surgery.onnx</em>. <div class="fragment"><div class="line">build $ python3 inference.py \</div>
<div class="line">    --variant mobilenetv3 \</div>
<div class="line">    --checkpoint <span class="stringliteral">&quot;rvm_mobilenetv3.pth&quot;</span> \</div>
<div class="line">    --device cpu \</div>
<div class="line">    --output-type video \</div>
<div class="line">    --output-composition <span class="stringliteral">&quot;composition.mp4&quot;</span> \</div>
<div class="line">    --output-alpha <span class="stringliteral">&quot;alpha.mp4&quot;</span> \</div>
<div class="line">    --output-foreground <span class="stringliteral">&quot;foreground.mp4&quot;</span> \</div>
<div class="line">    --output-video-mbps 4 \</div>
<div class="line">    --seq-chunk 1 \</div>
<div class="line">    --input-source <span class="stringliteral">&quot;google.mp4&quot;</span> \</div>
<div class="line">    --input-resize 1920 1080 \</div>
<div class="line">    --downsample-ratio 0.25</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>The DRA data will be generated in the <em>dra_data</em> folder.</li>
<li>This command is an example to generate DRA data for input 1920x1080, downsample ratio 0.25.</li>
<li>The sample video used as the input source can be downloaded from <a href="https://drive.google.com/drive/folders/1VFnWwuu-YXDKG-N6vcjK_nL7YZMFapMU">GoogleDrive</a>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="onnx_robust_video_matting_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/robust_video_matting/config/ea_cvt_onnx_robust_video_matting.yaml</div>
</div><!-- fragment --><dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_robust_video_matting</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_robust_video_matting.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_robust_video_matting/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_robust_video_matting</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_robust_video_matting.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_robust_video_matting/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_robust_video_matting_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_robust_video_matting/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_robust_video_matting</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="onnx_robust_video_matting_build_evk_binary"></a>
3 Build EVK Binary</h2>
<p>Build the evaluation kit (EVK) binary with <em>make</em>. </p><div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">[*] Ambarella Package Configuration  ---&gt;</div>
<div class="line"> [*] Build Ambarella EAZYAI library  ---&gt;</div>
<div class="line">   [*] Build EazyAI applications  ---&gt;</div>
<div class="line">    [*] Build video matting EazyAI apps</div>
<div class="line">build $ make test_video_matting</div>
</div><!-- fragment --><h2><a class="anchor" id="sub_sec_onnx_robust_video_matting_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_robust_video_matting/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_robust_video_matting/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_robust_video_matting.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_robust_video_matting/onnx_robust_video_matting_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_robust_video_matting</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_robust_video_matting_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_robust_video_matting_run_c_inference"></a>
5 Run C Inference</h2>
<ol type="1">
<li><p class="startli">Copy files to the SD card on the computer vision (CV) board.</p>
<p class="startli">The model name must be <em>cavalry_robust_video_matting.bin</em> for read by the test_video_matting demo by default. For example, place files on the SD card with the following structure. </p><div class="fragment"><div class="line">/sdcard/video_matting</div>
<div class="line">|--in</div>
<div class="line">|       src_0.jpg</div>
<div class="line">|       src_60.jpg</div>
<div class="line">|       background.png</div>
<div class="line">|       cavalry_robust_video_matting.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users should prepare the images by themselves.</li>
<li><code>background.png</code> is used for the background in below live mode demo.</li>
<li><code>src_n.jpg</code> is used for the input in below file mode demo.</li>
</ul>
</dd></dl>
</li>
<li>Initialize environment on the CV board. CV22 Walnut and imx274_mipi are used as an example. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 720p --stream_A 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluent, use a greater value in <em>--enc_dummy_latency 4</em>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be expanded by changing the size in <em>--reallocate_mem overlay,0x04000000</em>.</li>
</ul>
</dd></dl>
</li>
<li>Run the demo.<ol type="a">
<li>Stream live mode (draw on stream <em>rtsp://10.0.0.2/stream1</em>)<ol type="i">
<li>Use default parameters: <div class="fragment"><div class="line">board # test_video_matting -m 0 -i /sdcard/video_matting/in -b 0 -a 1 -s 0</div>
</div><!-- fragment --></li>
<li>With specific alpha thresh, background RGB color, and blend mode:<ul>
<li>Blend mode:<ul>
<li>0: green background</li>
<li>1: half green background</li>
<li>2: moving green background</li>
<li>3: green foreground</li>
<li>4: half green foreground</li>
<li>5: half green foreground and background</li>
<li>6: image background <div class="fragment"><div class="line">board # test_video_matting -m 0 -i /sdcard/video_matting/in -b 0 -a 1 -s 0 --alpha_thresh 32 --bg_rgb 120,255,155 --blend_mode 0</div>
<div class="line">board # test_video_matting -m 0 -i /sdcard/video_matting/in -b 0 -a 1 -s 0 --alpha_thresh 32 --blend_mode 6 --bg_path /sdcard/video_matting/in/background.png</div>
</div><!-- fragment --></li>
</ul>
</li>
</ul>
</li>
<li>With specific model parameters: <div class="fragment"><div class="line">board # test_video_matting -m 0 -b 0 -a 1 -s 0  \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/video_matting/in/cavalry_robust_video_matting.bin \</div>
<div class="line">       --input_src src --input_r1i r1i --input_r2i r2i --input_r3i r3i --input_r4i r4i \</div>
<div class="line">       --output_alpha pha --output_r1o r1o --output_r2o r2o --output_r3o r3o --output_r4o r4o</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Video output (VOUT) live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_video_matting -m 0 -i /sdcard/video_matting/in -b 0 -a 0</div>
</div><!-- fragment --></li>
<li>File mode (Only JPG image file inputs are placed under <code>/sdcard/video_matting/in</code>; the JPEG image file results are generated under <code>/sdcard/video_matting/out</code>) <div class="fragment"><div class="line">board # test_video_matting -m 1 -i /sdcard/video_matting/in -o /sdcard/video_matting/out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>This live demo is not ready on CV72, please try file mode first.</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
<h1><a class="anchor" id="sec_swin_tiny"></a>
SWIN Transformer</h1>
<p>This chapter gives a brief introduction about transformer and explains how to deploy the live demo for it.</p>
<h2><a class="anchor" id="sub_sec_swin_tiny_introduction"></a>
1 Introduction</h2>
<p>Transformer is a new approach for computer vision (CV) task comparing with general CNN solution. It was commonly used in the natural language processing (NLP) domain. The most significant feature of it is the usage of the “attention” module. Transformer has obtained great success in language processing domain and led researchers to adapt it to computer vision tasks. Related works includes the following:</p>
<ul>
<li>Vision Transformer (ViT) for image classification. This network divides the image into 2D patches. These patches are regarded as words and are input to the encoder of the transformer for analysis.</li>
<li>DETR for detection, which uses the CNN to extract feature. The points in the feature map are then regarded as a sequence of words.</li>
<li>SETR for segmentation: It uses the ViT as the encoder of the image and then uses CNN to decode and generate the segmentation result.</li>
</ul>
<p>However, as the computer vision task has larger scale and higher resolution comparing with word processing, the model’s adaptability to different vision task and computation complexity still remains a question. Based on these two points, SWIN transformer has proposed an hierarchical feature representation and a shifted window based multi-head self-attention (SWIN-MSA) mechanism. The hierarchical feature representation is alike to the general CNN down-sampling procedure, this allows the network to be better adapted to the variant scale. The SWIN-MSA mechanism could divide the image into non-overlapped windows so that self-attention is calculated in local windows for efficiency. The shifted window also introduces cross-window connections to enlarge the model power.</p>
<p>The SWIN transformer currently support different kinds of vison tasks, including:</p>
<ul>
<li>Image classification (<a href="https://github.com/microsoft/Swin-Transformer">https://github.com/microsoft/Swin-Transformer</a>)</li>
<li>Object detection (<a href="https://github.com/SwinTransformer/Swin-Transformer-Object-Detection">https://github.com/SwinTransformer/Swin-Transformer-Object-Detection</a>)</li>
<li>Semantic segmentation (<a href="https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation">https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation</a>), etc.</li>
</ul>
<p>For future development of transformer, model effectiveness and performance efﬁciency are two important points. In current CV field, CNN is still the main choice, while transformer becomes more and more popular. In the future, the transformer may replace CNN’s position or the transformer may co-exist with CNN and develop collaboratively with CNN.</p>
<p>For more information, please refer to the paper of SWIN transformer:</p><ul>
<li><a href="https://arxiv.org/pdf/2103.14030.pdf">https://arxiv.org/pdf/2103.14030.pdf</a></li>
<li><a href="https://arxiv.org/pdf/2012.12556.pdf">https://arxiv.org/pdf/2012.12556.pdf</a>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_swin_tiny_genertate_onnx_model"></a>
2 Generate ONNX Model</h2>
<ul>
<li>Prepare source code <div class="fragment"><div class="line">build $ cd onnx/demo_networks/transformer_swin_tiny/scripts</div>
<div class="line">build $ git clone https:<span class="comment">//github.com/microsoft/Swin-Transformer.git</span></div>
<div class="line">build $ cd Swin-Transformer</div>
<div class="line">build $ git reset --hard 5d2aede42b4b12cb0e7a2448b58820aeda604426</div>
<div class="line">build $ git apply ../git.patch</div>
<div class="line">build $ cp ../inference.py ./</div>
</div><!-- fragment --></li>
<li>Download pytorch weight <div class="fragment"><div class="line">build $ mkdir weights; cd weights</div>
<div class="line">build $ wget https:<span class="comment">//github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth</span></div>
<div class="line">build $ cd -</div>
</div><!-- fragment --></li>
<li>Generate ONNX model <div class="fragment"><div class="line">build $ python3 inference.py</div>
</div><!-- fragment --> An ONNX model swin_tiny_patch4_window7_224.onnx will be generated in the current folder.</li>
<li>Graph surgery <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m swin_tiny_patch4_window7_224.onnx \</div>
<div class="line">        -o swin_tiny_patch4_window7_224_modified.onnx \</div>
<div class="line">        -isrc <span class="stringliteral">&#39;i:input|is:1,3,224,224&#39;</span> -on output -t CVFlow</div>
</div><!-- fragment --> The pre-trained model file <em>swin_tiny_patch4_window7_224_modified.onnx</em> is used to generate the Cavalry binary.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Test environment is:<ul>
<li>Torch 1.8.0</li>
<li>Torchvision 0.9.0</li>
<li>ONNX 1.6.0</li>
<li>Onnxruntime 1.6.0</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_swin_tiny_model_convert"></a>
3 Model Convert</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/transformer_swin_tiny/config/ea_cvt_onnx_swin_tiny.yaml</div>
</div><!-- fragment --><p> The output is in <code>out/onnx/demo_networks/swin_tiny/</code>.</p>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/swin_tiny</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_swin_tiny.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_swin_tiny/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_swin_tiny</code>.</li>
<li>For X86 simulator, model desc json file <b>swin_tiny.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/swin_tiny/out_&lt;build_target&gt;_parser/</code>. ades command <b>swin_tiny_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/swin_tiny/&lt;chip&gt;/&lt;chip&gt;_ades_swin_tiny</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_swin_tiny_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/swin_tiny/&lt;chip&gt;/&lt;chip&gt;_cavalry_swin_tiny/&lt;chip&gt;_cavalry&lt;version&gt;_swin_tiny.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/swin_tiny/swin_tiny_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/swin_tiny</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>For live mode, currently as this network has not been supported in CFlite Python Inference library, please refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_swin_tiny_run_c_inference">5 Run C Inference</a>.</li>
</ul>
</dd></dl>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_swin_tiny_run_c_inference"></a>
5 Run C Inference</h2>
<p>In this example, the camera module imx274 and CV2 board are used.</p>
<ol type="1">
<li><p class="startli">Copy files to SD card on the CV board.</p>
<p class="startli">Copy the <em>imagenet_1000.txt</em> and <em>swin_tiny_cavalry.bin</em> to EVK, for example, <em>/sdcard</em> folder. The <em>imagenet_1000.txt</em> is located in the folder path <em>ambarella/unit_test/private/cv_test/imagenet</em>. The *&lt;chip&gt;_cavalry&lt;version&gt;_swin_tiny.bin* is in <em>out/onnx/demo_networks/swin_tiny/&lt;chip&gt;/&lt;chip&gt;_cavalry_swin_tiny/&lt;chip&gt;_cavalry&lt;version&gt;_swin_tiny.bin</em>.</p>
</li>
<li>Run the live demo on the CV board. <div class="fragment"><div class="line">board # eazyai_video.sh --hdmi 1080p</div>
<div class="line">board # cd /sdcard</div>
<div class="line">board # osd_server_imagenet -p 27182 -n imagenet_1000.txt &amp;</div>
<div class="line">board # test_nnctrl_live -p 27182 -b swin_tiny_cavalry.bin --in input --out output -s 0 -i 0 -t 0</div>
</div><!-- fragment --> The performance of this network is arround <b>59.4ms</b> on CV2 platform with default DRA setting. Test clock is gclk_ddr 1560000000 Hz, gclk_vision 912000000 Hz. The classification result will be shown on HDMI OSD.</li>
</ol>
<hr  />
<h1><a class="anchor" id="sec_onnx_yolov3"></a>
YOLOv3</h1>
<p>YOLOv3 is the third version of a popular object detection algorithm YOLO – You Only Look Once. The published model recognizes 80 different classes in images and videos, but most importantly it is super-fast and nearly as accurate as Single Shot MultiBox (SSD). The following sections demonstrate how to export YOLOv3, YOLOv3 SPP, and YOLOv3 Tiny ONNX models from PyTorch, and how to handle the ONNX models in the Ambarella CNNGen samples package.</p>
<h2><a class="anchor" id="onnx_yolov3_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a project in Github with GPL v3.0 license. The steps are as below.</p>
<ol type="1">
<li>Download source code project from <a href="https://github.com/ultralytics/yolov3">here</a>. <div class="fragment"><div class="line">commit dce753ead4a8378055fc07be54c3f54bcf55e2ed</div>
</div><!-- fragment --></li>
<li>Download pre-trained weight files <em>yolov3.pt</em>, <em>yolov3-spp.pt</em> and <em>yolov3-tiny.pt</em> from <a href="https://drive.google.com/open?id=1LezFG5g3BCW6iYaV89B2i64cqEUZD7e0">here</a>, and put it under the weights folder in the project directory.</li>
<li>Install the following dependent Python packages. <div class="fragment"><div class="line">numpy</div>
<div class="line">opencv-python &gt;= 4.1</div>
<div class="line">torch &gt;= 1.4</div>
<div class="line">torchvision</div>
<div class="line">onnx</div>
<div class="line">onnxruntime</div>
<div class="line">matplotlib</div>
<div class="line">tqdm</div>
<div class="line">pillow &lt; 7.0.0</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>torch and torchvision can be installed by the guidance in <a href="https://pytorch.org/get-started/locally">https://pytorch.org/get-started/locally</a>.</li>
<li>onnx 1.6.0 and onnxruntime 1.1.1 are installed in test.</li>
<li>Use pillow &lt; 7.0.0 to avoid package conflict.</li>
<li>torch==1.4.0 is used in the test. If newer version of torch is used, the output names may be different from 290,352,414(yolov3), 298,360,422(yolov3 spp), and 75,127(yolov3 tiny). The output names should be updated with the names of the final Conv nodes in the networks.</li>
</ul>
</dd></dl>
</li>
<li>Modify the code in the source project.<ol type="a">
<li>models.py <div class="fragment"><div class="line">ONNX_EXPORT = True</div>
</div><!-- fragment --></li>
<li>detect.py <div class="fragment"><div class="line"><span class="keyword">import</span> argparse</div>
<div class="line">from sys <span class="keyword">import</span> platform</div>
<div class="line"><span class="keyword">import</span> onnx # <span class="keyword">this</span> line is added</div>
<div class="line">...</div>
<div class="line">img_size = (416, 416) <span class="keywordflow">if</span> ONNX_EXPORT <span class="keywordflow">else</span> opt.img_size</div>
<div class="line">...</div>
<div class="line">    <span class="keywordflow">if</span> ONNX_EXPORT:</div>
<div class="line">        model.fuse()</div>
<div class="line">        img = torch.zeros((1, 3) + img_size)  # (1, 3, 320, 192)</div>
<div class="line">        f = opt.weights.replace(opt.weights.split(<span class="charliteral">&#39;.&#39;</span>)[-1], <span class="stringliteral">&#39;onnx&#39;</span>)  # *.onnx filename</div>
<div class="line">        torch.onnx.export(model, img, f, verbose=False, opset_version=11,</div>
<div class="line">        export_params=True, do_constant_folding=True)</div>
<div class="line">...</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run the following command under the project directory to export the ONNX models.<ol type="a">
<li>YOLOv3 <div class="fragment"><div class="line">build $ python3 detect.py --source data/samples/bus.jpg --cfg cfg/yolov3.cfg --weights weights/yolov3.pt</div>
</div><!-- fragment --></li>
<li>YOLOv3 SPP <div class="fragment"><div class="line">build $ python3 detect.py --source data/samples/bus.jpg --cfg cfg/yolov3-spp.cfg --weights weights/yolov3-spp.pt</div>
</div><!-- fragment --></li>
<li>YOLOv3 Tiny <div class="fragment"><div class="line">build $ python3 detect.py --source data/samples/bus.jpg --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.pt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The ONNX model files are generated under the <em>weights</em> folder.</dd></dl>
</li>
</ol>
</li>
<li>Run the following command to do graph surgery on the ONNX models.<ol type="a">
<li>YOLOv3 <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolov3.onnx -o yolov3-416x416-out-290-352-414-surgery.onnx -isrc <span class="stringliteral">&quot;i:input.1|is:1,3,416,416&quot;</span> -on 290,352,414 -t ConstantifyShapes,FoldConstants,CutGraph</div>
</div><!-- fragment --></li>
<li>YOLOv3 SPP <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolov3-spp.onnx -o yolov3-spp-416x416-out-298-360-422-surgery.onnx -isrc <span class="stringliteral">&quot;i:input.1|is:1,3,416,416&quot;</span> -on 298,360,422 -t ConstantifyShapes,FoldConstants,CutGraph</div>
</div><!-- fragment --></li>
<li>YOLOv3 Tiny <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolov3-tiny.onnx -o yolov3-tiny-416x416-out-75-127-surgery.onnx -isrc <span class="stringliteral">&quot;i:input.1|is:1,3,416,416&quot;</span> -on 75,127 -t ConstantifyShapes,FoldConstants,CutGraph</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<p>The following model files which can be converted with Ambarella toolchain are generated:</p><ul>
<li>yolov3-416x416-out-290-352-414-surgery.onnx</li>
<li>yolov3-spp-416x416-out-298-360-422-surgery.onnx</li>
<li>yolov3-tiny-416x416-out-75-127-surgery.onnx These files have been put in the CNNGen sample package.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The steps above are only for reference, users need to modify it if needed, which may be resulted by different environments, such as the input name in step 7.</li>
<li>To convert PyTorch to ONNX, the above installation may break the environment of CNNGen tool, such as <code>graph_surgery.py</code>. If users find some problems, please use the installation script in CNNGen Toolchain package to recover the environment.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolov3_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For YOLOv3, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov3/config/ea_cvt_onnx_yolov3.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov3/</code>.</li>
<li>For YOLOv3 SPP, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov3_spp/config/ea_cvt_onnx_yolov3_spp.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov3_spp/</code>.</li>
<li>For YOLOv3 Tiny, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov3_tiny/config/ea_cvt_onnx_yolov3_tiny.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov3_tiny/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_yolov*</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov*.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_yolov*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_yolov*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_yolov*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolov3_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with yolov3</div>
<div class="line">      [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>User should use Cooper Linux SDK package, not this CVflow CNNGen Samples package, and this application must be included in EVK firmware.</dd></dl>
</li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov3_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov3/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov3/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov3/onnx_yolov3_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov3/config/ea_inf_onnx_yolov3.yaml -pwd ./out/onnx/demo_networks/onnx_yolov3</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov3_spp/onnx_yolov3_spp_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov3_spp/config/ea_inf_onnx_yolov3_spp.yaml -pwd ./out/onnx/demo_networks/onnx_yolov3_spp</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -c out/onnx/demo_networks/onnx_yolov3_tiny/onnx_yolov3_tiny_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov3_tiny/config/ea_inf_onnx_yolov3_tiny.yaml -pwd ./out/onnx/demo_networks/onnx_yolov3_tiny</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov3/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov3/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin \</div>
<div class="line">        -pn yolov3 -pl &lt;usr_path&gt;/yolov3/config/yolov3.lua -dm 0 -lp &lt;usr_path&gt;/yolov3/config/label_coco_80.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov3_spp/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov3_spp/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp.bin \</div>
<div class="line">        -pn yolov3 -pl &lt;usr_path&gt;/yolov3_spp/config/yolov3_spp.lua -dm 0 -lp &lt;usr_path&gt;/yolov3_spp/config/label_coco_80.txt \</div>
<div class="line">        --fsync_off -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">       -cb out/onnx/demo_networks/onnx_yolov3_tiny/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov3_tiny/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny.bin \</div>
<div class="line">       -pn yolov3 -pl &lt;usr_path&gt;/yolov3_tiny/config/yolov3_tiny.lua -dm 0 -lp &lt;usr_path&gt;/yolov3_tiny/config/label_coco_80.txt \</div>
<div class="line">       --fsync_off -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using live streaming, option <code>--fsync_off</code> can disable frame sync, which means the result may not be applied to the right frame.</li>
<li>For Vout display , please use option <code>-dd HDMI</code>, and remove <code>--fsync_off</code> which is only for streaming.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov3_run_c_inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov3_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3*.bin</b>.</li>
<li>The <b>yolov3*.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov3_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_yolov3*.json</b> and <b>onnx_yolov3*_ades.cmd</b>.</li>
</ol>
</dd></dl>
<p>Please follow below steps to run inference.</p>
<ul>
<li>Copy files to SD card for EVK test For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/yolov3</div>
<div class="line">|--model</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp.bin</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">│    label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    bus.jpg</div>
<div class="line">|    bus.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li><b>tiny_anchors.txt</b> has the following content inside. 10,14,23,27,37,58,81,82,135,169,344,319</li>
<li>Users can find <b>"label_coco_80.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_parser/onnx_yolov3.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3/onnx_yolov3_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_spp_parser/onnx_yolov3_spp.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3/onnx_yolov3_spp_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_spp/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_tiny_parser/onnx_yolov3_tiny.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3/onnx_yolov3_tiny_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_tiny/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_parser/onnx_yolov3.json \</div>
<div class="line">       --lua_file yolov3.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3/out \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3/onnx_yolov3_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_spp_parser/onnx_yolov3_spp.json \</div>
<div class="line">       --lua_file yolov3_spp.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_spp/out \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3_spp/onnx_yolov3_spp_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_tiny_parser/onnx_yolov3_tiny.json \</div>
<div class="line">       --lua_file yolov3_tiny.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_tiny/out \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov3_tiny/onnx_yolov3_tiny_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_parser/onnx_yolov3.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_spp_parser/onnx_yolov3_spp.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_spp/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_tiny_parser/onnx_yolov3_tiny.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_tiny/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_parser/onnx_yolov3.json \</div>
<div class="line">       --lua_file yolov3.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_spp_parser/onnx_yolov3_spp.json \</div>
<div class="line">       --lua_file yolov3_spp.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_spp/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov3_tiny_parser/onnx_yolov3_tiny.json \</div>
<div class="line">       --lua_file yolov3_tiny.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:data=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov3_tiny/out</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li><p class="startli">Run</p><ol type="i">
<li>Dummy mode, only for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny.bin</div>
</div><!-- fragment --></li>
<li>The real image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/yolov3.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov3/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/yolov3_spp.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov3/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov3 \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny.bin \</div>
<div class="line">       --lua_file /usr/share/ambarella/eazyai/lua/yolov3_tiny.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov3/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">       --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny_cavalry.bin \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:input.1=/sdcard/yolov3/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir /sdcard/yolov3/out/</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/yolov3/dra_img/bus.jpg</code>) in <em>/sdcard/yolov3/in</em>, and create <code>/sdcard/yolov3/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, Users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --reallocate_mem overlay,0x01000000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream without frame sync machine <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt --fsync_off</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_spp.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_spp.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt --fsync_off</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov3_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_tiny.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt --fsync_off</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For stream live mode, option <b>&ndash;fsync_off</b> disables frame sync. If need to enable frame sync, users should enable encode dummy in eazyai_video.sh.</li>
</ul>
</dd></dl>
</li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/onnx_yolov3_cavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/onnx_yolov3_spp_cavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_spp.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov3 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov3/model/onnx_yolov3_tiny_cavalry.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov3_tiny.lua \</div>
<div class="line">        --label_path /sdcard/yolov3/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
</ul>
<hr  />
<h1><a class="anchor" id="sec_onnx_yolov5"></a>
YOLOv5</h1>
<p>This version of YOLOv5 is a public release from Ultralytics on <a href="https://github.com/ultralytics/yolov5">GitHub</a>.</p>
<p>The following contents demonstrate how to export YOLOv5 ONNX models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_yolov5_export_onnx_model"></a>
1 Export ONNX model</h2>
<p>The ONNX model is exported from a source project in GitHub with GNU General Public License v3.0 license. The steps are as below.</p>
<ol type="1">
<li>Download source code project from <a href="https://github.com/ultralytics/yolov5">here</a>. <div class="fragment"><div class="line">build $ git reset --hard 702c4fa53eeaecfa5563c1441cb4a0c4aa8e908e</div>
</div><!-- fragment --></li>
<li>Go to the source project directory and install the required packages. <div class="fragment"><div class="line">build $ pip install -r requirements.txt</div>
<div class="line">build $ pip install onnx==1.6.0 pillow==7.2.0 torch==1.6.0 torchvision==0.7.0</div>
</div><!-- fragment --></li>
<li>Export the ONNX model with the script under the source project directory with opset_version=11.<ol type="a">
<li>Edit <a class="elRef" href="../../../library/df/de4/export_8py.html">export.py</a> <div class="fragment"><div class="line">build $ vi ./models/export.py</div>
<div class="line">...</div>
<div class="line">59         torch.onnx.export(model, img, f, verbose=False, opset_version=11, input_names=[<span class="stringliteral">&#39;images&#39;</span>], # <span class="keyword">this</span> line is changed from opset_version=12 to opset_version=11</div>
<div class="line">...</div>
</div><!-- fragment --></li>
<li>Export YOLOv5s <div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python3 ./models/export.py --weights yolov5s.pt --img-size 416 --batch 1</div>
<div class="line">...</div>
<div class="line">ONNX export success, saved as yolov5s.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOv5m <div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python3 ./models/export.py --weights yolov5m.pt --img-size 416 --batch 1</div>
<div class="line">...</div>
<div class="line">ONNX export success, saved as yolov5m.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOv5l <div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python3 ./models/export.py --weights yolov5l.pt --img-size 416 --batch 1</div>
<div class="line">...</div>
<div class="line">ONNX export success, saved as yolov5l.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOv5x <div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python3 ./models/export.py --weights yolov5x.pt --img-size 416 --batch 1</div>
<div class="line">...</div>
<div class="line">ONNX export success, saved as yolov5x.onnx</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Do graph surgery on the ONNX model by cutting off the <em>Detect()</em> layers (YOLO layers).<ol type="a">
<li>YOLOv5s <div class="fragment"><div class="line">build $ export PYTHONPATH=</div>
<div class="line">build $ graph_surgery.py onnx -m yolov5s.onnx -o yolov5s-416x416-out-1037-1017-997-surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on 1037,1017,997 -t ConstantifyShapes,FoldConstants,CutGraph</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov5s-416x416-out-1037-1017-997-surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOv5m <div class="fragment"><div class="line">build $ export PYTHONPATH=</div>
<div class="line">build $ graph_surgery.py onnx -m yolov5m.onnx -o yolov5m-416x416-out-1428-1408-1388-surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on 1428,1408,1388 -t ConstantifyShapes,FoldConstants,CutGraph</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov5m-416x416-out-1428-1408-1388-surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOv5l <div class="fragment"><div class="line">build $ export PYTHONPATH=</div>
<div class="line">build $ graph_surgery.py onnx -m yolov5l.onnx -o yolov5l-416x416-out-1819-1799-1779-surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on 1819,1799,1779 -t ConstantifyShapes,FoldConstants,CutGraph</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov5l-416x416-out-1819-1799-1779-surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOv5x <div class="fragment"><div class="line">build $ export PYTHONPATH=</div>
<div class="line">build $ graph_surgery.py onnx -m yolov5x.onnx -o yolov5x-416x416-out-2210-2190-2170-surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on 2210,2190,2170 -t ConstantifyShapes,FoldConstants,CutGraph</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov5x-416x416-out-2210-2190-2170-surgery.onnx</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If <code>layer_compare.py</code> in the CV toolchain is used under the source project directory, the name of the folder <code>utils</code> should be changed because it conflicts with a package name in the CV toolchain.</dd></dl>
</li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_yolov5_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For YOLOv5s, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov5/config/ea_cvt_onnx_yolov5s.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov5s/</code>.</li>
<li>For YOLOv5m, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov5/config/ea_cvt_onnx_yolov5m.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov5m/</code>.</li>
<li>For YOLOv5l, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov5/config/ea_cvt_onnx_yolov5l.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov5l/</code>.</li>
<li>For YOLOv5x, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov5/config/ea_cvt_onnx_yolov5x.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov5x/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_yolov5*</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5*.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_yolov5*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov5(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_yolov*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov5(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_yolov5*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolov5_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build the EVK binary as below. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with yolov5</div>
<div class="line">      [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>User should use Cooper Linux SDK package, not this CVflow CNNGen Samples package, and this application must be included in EVK firmware.</dd></dl>
</li>
<li>Build the X86 Simulator Binary with <code>make</code> Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov5_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov5*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov5s/onnx_yolov5s_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov5/config/ea_inf_onnx_yolov5s.yaml -pwd ./out/onnx/demo_networks/onnx_yolov5s</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov5m/onnx_yolov5m_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov5/config/ea_inf_onnx_yolov5m.yaml -pwd ./out/onnx/demo_networks/onnx_yolov5m</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov5l/onnx_yolov5l_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov5/config/ea_inf_onnx_yolov5l.yaml -pwd ./out/onnx/demo_networks/onnx_yolov5l</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov5x/onnx_yolov5x_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov5/config/ea_inf_onnx_yolov5x.yaml -pwd ./out/onnx/demo_networks/onnx_yolov5x</div>
</div><!-- fragment --></li>
<li>Accuracy Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov5s/onnx_yolov5s_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolov5/config/ea_inf_acc_onnx_yolov5s.yaml -pwd ./out/onnx/demo_networks/onnx_yolov5s</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov5s/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5s/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin \</div>
<div class="line">        -pn yolov5 -pl &lt;usr_path&gt;/yolov5/config/yolov5s.lua -dm 0 -lp &lt;usr_path&gt;/yolov5/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov5m/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5m/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin \</div>
<div class="line">        -pn yolov5 -pl &lt;usr_path&gt;/yolov5/config/yolov5m.lua -dm 0 -lp &lt;usr_path&gt;/yolov5/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">       -cb out/onnx/demo_networks/onnx_yolov5l/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5l/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin \</div>
<div class="line">       -pn yolov5 -pl &lt;usr_path&gt;/yolov5/config/yolov5l.lua -dm 0 -lp &lt;usr_path&gt;/yolov5/config/label_coco_80.txt \</div>
<div class="line">       -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">       -cb out/onnx/demo_networks/onnx_yolov5x/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov5x/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin \</div>
<div class="line">       -pn yolov5 -pl &lt;usr_path&gt;/yolov5/config/yolov5x.lua -dm 0 -lp &lt;usr_path&gt;/yolov5/config/label_coco_80.txt \</div>
<div class="line">       -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov5_run_c_inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov5_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <code>onnx_yolov5*_cavalry.bin</code>.</li>
<li>The <code>yolov5*.lua</code> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov5_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <code>onnx_yolov5*.json</code> and <code>onnx_yolov5*_ades.cmd</code>.</li>
</ol>
</dd></dl>
<p>Please follow below steps to run inference.</p>
<ul>
<li>Copy files to SD card for EVK test For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/yolov5</div>
<div class="line">|--model</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin</div>
<div class="line">|    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">│    label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    33887522274_eebd074106_k.jpg</div>
<div class="line">|    bus.jpg</div>
<div class="line">|    bus.bin</div>
<div class="line">|    33887522274_eebd074106_k.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_coco_80.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5s_parser/onnx_yolov5s.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5s/onnx_yolov5s_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5s/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5m_parser/onnx_yolov5m.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5m/onnx_yolov5m_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5m/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file\</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5l_parser/onnx_yolov5l.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5l/onnx_yolov5l_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5l/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5x_parser/onnx_yolov5x.json \</div>
<div class="line">       --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5x/onnx_yolov5x_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5x/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5_parser/onnx_yolov5s.json \</div>
<div class="line">       --lua_file yolov5s.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5s/onnx_yolov5s_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5_parser/onnx_yolov5m.json \</div>
<div class="line">       --lua_file yolov5m.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5m/onnx_yolov5m_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5_parser/onnx_yolov5l.json \</div>
<div class="line">       --lua_file yolov5l.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5l/onnx_yolov5l_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5_parser/onnx_yolov5x.json \</div>
<div class="line">       --lua_file yolov5x.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov5x/onnx_yolov5x_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5s_parser/onnx_yolov5s.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5s/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5m_parser/onnx_yolov5m.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5m/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5l_parser/onnx_yolov5l.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5l/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5x_parser/onnx_yolov5x.json</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr_path&gt;/yolov5x/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5s_parser/onnx_yolov5s.json \</div>
<div class="line">       --lua_file yolov5s.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5m_parser/onnx_yolov5m.json \</div>
<div class="line">       --lua_file yolov5m.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5l_parser/onnx_yolov5l.json \</div>
<div class="line">       --lua_file yolov5l.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">       --model_path &lt;usr_path&gt;/out_onnx_yolov5x_parser/onnx_yolov5x.json \</div>
<div class="line">       --lua_file yolov5x.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">       --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li><p class="startli">Run</p><ol type="i">
<li>Dummy mode, for test CVflow® performance <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin</div>
</div><!-- fragment --></li>
<li>The image is used as input, with preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5s.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5m.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5l.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5x.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov5/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov5/out/</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/yolov5/dra_img/bus.jpg</code>) in <code>/sdcard/yolov5/in</code>, and create <code>/sdcard/yolov5/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>, default preprocess is based on OpenCV, users can enable Vproc if needed with option <b>"d:vp"</b>, default value is cpu.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
<li><p class="startli">Live mode</p><ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5s.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5m.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5l.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5x.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5s.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5m.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5l.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov5/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov5x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov5x.lua \</div>
<div class="line">        --label_path /sdcard/yolov5/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluency, check the following two points.</li>
<li>If the display is not fluency, use bigger value in <code>\-\-enc_dummy_latency 4</code>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <code>\-\-reallocate_mem overlay,0x04000000</code>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_yolov7"></a>
YOLOv7</h1>
<p>This version of YOLOv7 is a public release from Ultralytics on <a href="https://github.com/WongKinYiu/yolov7">GitHub</a>.</p>
<p>The following contents demonstrate how to export YOLOv7 open neural network exchange (ONNX) models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_yolov7_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a source project in GitHub with GNU General Public License version 3, 29 June 2007. The steps are below.</p>
<ol type="1">
<li>Download the source code project from <a href="https://github.com/WongKinYiu/yolov7">here</a>. <div class="fragment"><div class="line">build $ git clone https:<span class="comment">//github.com/WongKinYiu/yolov7.git</span></div>
</div><!-- fragment --></li>
<li>Go to the source project directory and install the required packages. <div class="fragment"><div class="line">build $ pip install -r requirements.txt</div>
<div class="line">build $ pip install onnx&gt;=1.9.0 pillow&gt;=7.1.2 torch&gt;=1.7.0,!=1.12.0 torchvision&gt;=0.8.1,!=0.13.0 onnx-simplifier&gt;=0.3.6 scikit-learn==0.19.2 tensorflow&gt;=2.4.1 tensorflowjs&gt;=3.9.0 openvino-dev</div>
</div><!-- fragment --></li>
<li>Download the weights of standard models and tiny models, then place them under the source project directory.<ol type="a">
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt">YOLOv7_tiny</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt">YOLOv7</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt">YOLOv7-X</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt">YOLOv7-W6</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt">YOLOv7-E6</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt">YOLOv7-D6</a></li>
<li><a href="https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt">YOLOv7-E6E</a></li>
</ol>
</li>
<li>Export the ONNX model with the script under the source project directory with opset_version=11.<ol type="a">
<li>Edit <a class="elRef" href="../../../library/df/de4/export_8py.html">export.py</a> <div class="fragment"><div class="line">build $ vi ./yolov7/export.py</div>
<div class="line">...</div>
<div class="line">159  torch.onnx.export(model, img, f, verbose=False, opset_version=11, input_names=[<span class="stringliteral">&#39;images&#39;</span>], output_names=output_names,</div>
<div class="line">                  dynamic_axes=dynamic_axes) # <span class="keyword">this</span> line is changed from opset_version=12 to opset_version=11</div>
<div class="line">...</div>
</div><!-- fragment --></li>
<li>Export YOLOv7_tiny <div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python export.py --weights yolov7-tiny.pt --grid --end2end --simplify \</div>
<div class="line">        --topk-all 200 --iou-thres 0.5 --conf-thres 0.3 --img-size 416 416 --max-wh 416</div>
<div class="line">        ...</div>
</div><!-- fragment --> ONNX export success, saved as yolov7_tiny.onnx</li>
<li><p class="startli">Export YOLOv7 </p><div class="fragment"><div class="line">build $ export PYTHONPATH=<span class="stringliteral">&quot;$PWD&quot;</span></div>
<div class="line">build $ python export.py --weights yolov7.pt --grid --end2end --simplify \</div>
<div class="line">        --topk-all 200 --iou-thres 0.5 --conf-thres 0.3 --img-size 416 416 --max-wh 416</div>
<div class="line">      ...</div>
</div><!-- fragment --><p> ONNX export success, saved as yolov7.onnx</p>
<dl class="section note"><dt>Note</dt><dd>If users require other types of YOLOv7 models, they can export them to the ONNX model according to the weight, such as YOLOv7-X, YOLOv7-W6, etc.</dd></dl>
</li>
</ol>
</li>
<li>Perform graph surgery on the ONNX model by cutting off the <em>Detect()</em> layers (YOLO layers).<ol type="a">
<li>YOLOv7_tiny <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolov7_tiny.onnx -o yolov7_tiny_416x416.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on 259,300,341 -t CutGraph,CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov7_tiny_416x416.onnx</div>
</div><!-- fragment --></li>
<li>YOLOv7 <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolov7.onnx -o yolov7_416x416.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -on onnx::Reshape_489,onnx::Reshape_524,onnx::Reshape_559 -t CutGraph,CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov7_416x416.onnx</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="i">
<li>When using the AMBA toolchain&ndash;<code>graph_surgery.py</code>, it is necessary to match the onnx version with the toolchain. Therefore, it is recommended that users use <code>onnx==1.6.0</code>.</li>
<li>If <code>layer_compare.py</code> in the CV toolchain is used under the source project directory, the name of the folder <code>utils</code> should be changed because it conflicts with the package name in the CV toolchain.</li>
</ol>
</dd></dl>
</li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_yolov7_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For YOLOv7_tiny, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov7/config/ea_cvt_onnx_yolov7_tiny.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov7_tiny/</code>.</li>
<li>For YOLOv7, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov7/config/ea_cvt_onnx_yolov7.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolov7/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_yolov*</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov*.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_yolov*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_yolov*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_yolov*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolov7_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build the EVK binary as shown below. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with yolov5</div>
<div class="line">      [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>User should use Cooper Linux SDK package, not this CVflow CNNGen Samples package, and this application must be included in EVK firmware.</dd></dl>
</li>
<li>Build X86 Simulator binary with <em>make</em>. Refer to the CNNGen Doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build the x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov7_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov7/onnx_yolov7_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_yolov7</div>
<div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov7_tiny/onnx_yolov7_tiny_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_yolov7_tiny</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov7/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov7/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin \</div>
<div class="line">        -pn yolov5 -pl &lt;usr_path&gt;/yolov7/config/yolov7.lua -dm 0 -lp &lt;usr_path&gt;/yolov7/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
<div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov7_tiny/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov7_tiny/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin \</div>
<div class="line">        -pn yolov5 -pl &lt;usr_path&gt;/yolov7/config/yolov7_tiny.lua -dm 0 -lp &lt;usr_path&gt;/yolov7/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov7_run_c_inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example; refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For the EVK board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov7_cnngen_conversion">2 CNNGen Conversion</a> for information on how to generate <code>onnx_yolov7*_cavalry.bin</code>.</li>
<li>The <b>yolov7*.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of the EVK. If it does not exist, it can be found in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov7_cnngen_conversion">2 CNNGen Conversion</a> for information on how to generate <b>onnx_yolov7*.json</b> and <b>onnx_yolov7*_ades.cmd</b>.</li>
</ol>
</dd></dl>
<p>Please follow below steps to run inference.</p>
<ul>
<li>Copy files to the secure digital (SD) card for the EVK test. For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/yolov5</div>
<div class="line">|--model</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin</div>
<div class="line">│    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">│    label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    bus.jpg</div>
<div class="line">|    bus.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can find <b>label_coco_80.txt</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users must keep the file path consistent during using.</li>
</ul>
</dd></dl>
</li>
<li>File mode:<ol type="1">
<li>For the X86 simulator:<ol type="a">
<li>Run ADES mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_tiny_parser/onnx_yolov7_tiny.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov7_tiny/onnx_yolov7_tiny_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov7_tiny/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_parser/onnx_yolov7.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov7/onnx_yolov7_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov7/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_tiny_parser/onnx_yolov7_tiny.json \</div>
<div class="line">        --lua_file yolov7_tiny.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov7_tiny/onnx_yolov7_tiny_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_parser/onnx_yolov7.json \</div>
<div class="line">        --lua_file yolov7.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov7/onnx_yolov7_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_tiny_parser/onnx_yolov7_tiny.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov7_tiny/out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_parser/onnx_yolov7.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov7/out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_tiny_parser/onnx_yolov7_tiny.json \</div>
<div class="line">        --lua_file yolov7_tiny.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov7_parser/onnx_yolov7.json \</div>
<div class="line">        --lua_file yolov7.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For the EVK board:<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run the following.<ol type="i">
<li>Dummy mode, to test CVflow® performance: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin</div>
</div><!-- fragment --></li>
<li>The image is used as an input with preprocess and postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7_tiny.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov7/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov7/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov7/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov7/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the corrcet preprocess or postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov7/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov7/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov7/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov7/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode using image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/yolov7/dra_img/bus.jpg</code>) in <code>/sdcard/yolov7/in</code>, and create <code>/sdcard/yolov7/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>: default preprocess is based on OpenCV; users can enable VProc if required with the option <b>"d:vp"</b>. The default value is CPU.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode:<ol type="1">
<li>Initialize the environment on the CV board. CV22 Walnut and imx274_mipi are used as examples. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run the following.<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>): <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7_tiny.lua \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7.lua \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>Video output (VOUT) live mode (draw on VOUT high definition multimedia interface (HDMI®)): <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7_tiny.lua \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov5 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov7/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov7.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov7.lua \</div>
<div class="line">        --label_path /sdcard/yolov7/labels/label_coco_80.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>If there is no display on the stream or the display is not fluent, check the following two points.</li>
<li>If the display is not fluency, use a larger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be increased by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<b>1024 + resolution * (enc-dummy-latency + 5)</b>). For more details, refer to the <b>EazyAI Library application programming interface (API)-related content in the Linux software development kit (SDK) Doxygen documents</b>.</li>
</ul>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_yolov8"></a>
YOLOv8</h1>
<p>This version of YOLOv8 is a public release from Ultralytics on <a href="https://github.com/ultralytics/ultralytics">GitHub</a>.</p>
<p>The following contents demonstrate how to export YOLOv8 open neural network exchange (ONNX) models from the public source project implemented with PyTorch and how to run the ONNX model with the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_yolov8_export_onnx_model"></a>
1 Export ONNX Model</h2>
<p>The ONNX model is exported from a source project in GitHub with GNU AFFERO GENERAL PUBLIC LICENSE Version 3, 19 November 2007. The steps are below.</p>
<ol type="1">
<li>Download the source code project from <a href="https://github.com/ultralytics/ultralytics">here</a>. <div class="fragment"><div class="line">build $ git clone https:<span class="comment">//github.com/ultralytics/ultralytics.git</span></div>
<div class="line">build $ cd ultralytics/</div>
<div class="line">build $ git reset --hard a38f22767254f2c2ee241490bd027987780d83ca</div>
</div><!-- fragment --></li>
<li>Set up conda environment to export model to avoid damaging the environment of CNNGen toolchain. <div class="fragment"><div class="line">build $ conda create -n yolov8 python=3.8 protobuf numpy=1.23.4</div>
<div class="line">build $ conda activate yolov8</div>
<div class="line">(yolov8)build $ pip3 install ultralytics</div>
</div><!-- fragment --></li>
<li><p class="startli">Export the ONNX model.</p><ol type="a">
<li>Detection<ol type="i">
<li>YOLOv8n <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8n.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8s <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8s.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8m <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8m.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8l <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8l.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8x <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8x.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Segmentation<ol type="i">
<li>YOLOv8n <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8n-seg.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8s <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8s-seg.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8m <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8m-seg.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8l <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8l-seg.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
<li>YOLOv8x <div class="fragment"><div class="line">(yolov8)build $ yolo export model=yolov8x-seg.pt format=onnx opset=12 imgsz=416</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>If conda is not used, please revert to the CNNGen toolchain related environment after exporting the model.</li>
<li>The exported model is yolov8[n|s|m|l|x].onnx or yolov8[n|s|m|l|x]-seg.onnx.</li>
</ul>
</dd></dl>
</li>
<li>Perform graph surgery on the ONNX model, taking yolov8s.onnx as an example <div class="fragment"><div class="line">(yolov8)build $ conda deactivate</div>
<div class="line">build $ graph_surgery.py onnx -m yolov8s.onnx -o yolov8s_surgery.onnx -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolov8s_surgery.onnx</div>
</div><!-- fragment --></li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_yolov8_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary file converted from the ONNX model can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For Detection, take Yolov8s as an example. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov8/config/ea_cvt_onnx_yolov8s_det.yaml</div>
</div><!-- fragment --></li>
<li>For Segmentation, take Yolov8s-seg as an example. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolov8/config/ea_cvt_onnx_yolov8s_seg.yaml</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_yolov8[n|s|m|l|x]_[det|seg]</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov*.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov8(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov8*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_yolov*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov8(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_yolov8*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolov8(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_yolov8*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolov8_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build the EVK binary as shown below. <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">        [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">          -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">            -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">              [*] Build Ambarella custom postprocess library with yolov8</div>
<div class="line">            [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build X86 Simulator binary with <em>make</em>. Refer to the CNNGen Doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build the x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;cvflow_cnngen_samples_package&gt;/library/eazyai/unit_test/build</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov8_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov8*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov8*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolov8*/onnx_yolov8*_cvt_summary.yaml \</div>
<div class="line">        -pwd ./out/onnx/demo_networks/onnx_yolov8*</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 1 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolov8*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolov8*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8*.bin \</div>
<div class="line">        -pn yolov8 -pl &lt;usr_path&gt;/yolov8/config/yolov8*.lua -dm 0 -lp &lt;usr_path&gt;/yolov8/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolov8_run_c_Inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274_mipi and cv22_walnut board are used. The <b>test_eazyai</b> is used for the following example, refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_deploy_c_library">4 EazyAI Inference C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_cpu_postproc">5 EazyAI Postprocess C Library</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_unit_test">6 EazyAI Unit Test</a></li>
<li><a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_live_application">7 EazyAI Live Application</a></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For the EVK board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov8_cnngen_conversion">2 CNNGen Conversion</a> for information on how to generate <code>onnx_yolov8[n|s|m|l|x]_[det|seg]_cavalry.bin</code>.</li>
<li>The <b>yolov8_[det|seg].lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of the EVK. If it does not exist, it can be found in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolov8_cnngen_conversion">2 CNNGen Conversion</a> for information on how to generate <b>onnx_yolov8*.json</b> and <b>onnx_yolov8*_ades.cmd</b>.</li>
</ol>
</dd></dl>
<p>Please follow below steps to run inference.</p>
<ul>
<li>Copy files to the secure digital (SD) card for the EVK test. For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/yolov8</div>
<div class="line">|--model</div>
<div class="line">|    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin</div>
<div class="line">|    &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|    label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|    bus.jpg</div>
<div class="line">|    bus.bin</div>
<div class="line">|</div>
<div class="line">|--out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Users can find <b>label_coco_80.txt</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. Users must keep the file path consistent during using.</li>
</ul>
</dd></dl>
</li>
<li>File mode:<ol type="1">
<li>For the X86 simulator, take yolov8s as an example:<ol type="a">
<li>Run ADES mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_det_parser/onnx_yolov8s_det.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov8s_det/onnx_yolov8s_det_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_image_bin|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov8_det_out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_seg_parser/onnx_yolov8s_seg.json \</div>
<div class="line">        --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov8s_seg/onnx_yolov8s_seg_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_image_bin|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov8_seg_out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov8 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_det_parser/onnx_yolov8s_det.json \</div>
<div class="line">        --lua_file yolov8_det.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov8s_det/onnx_yolov8s_det_ades.cmd</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n yolov8 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_seg_parser/onnx_yolov8s_seg.json \</div>
<div class="line">        --lua_file yolov8_seg.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt; --ades_cmd_file &lt;usr_path&gt;/&lt;chip&gt;_ades_onnx_yolov8s_seg/onnx_yolov8s_seg_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode.<ol type="i">
<li>The raw.bin is used as an input without the preprocess or postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_det_parser/onnx_yolov8s_det.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_image_bin|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov8_det_out</div>
<div class="line">build $ ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_seg_parser/onnx_yolov8s_seg.json \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_image_bin|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr_path&gt;/yolov8_seg_out</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the correct preprocess and postprocess. <div class="fragment"><div class="line">build $ ./build/test_eazyai -m 1 -d 0 -n yolov8 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_det_parser/onnx_yolov8s_det.json \</div>
<div class="line">        --lua_file yolov8_det.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
<div class="line">build $ ./build/test_eazyai -m 1 -d 1 -n yolov8 \</div>
<div class="line">        --model_path &lt;usr_path&gt;/out_onnx_yolov8s_seg_parser/onnx_yolov8s_seg.json \</div>
<div class="line">        --lua_file yolov8_seg.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr_path&gt;/dra_img|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path &lt;usr_path&gt;/label_coco_80.txt \</div>
<div class="line">        --output_dir &lt;usr_path&gt;</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For the EVK board, take yolov8s as an example:<ol type="a">
<li>Load Cavalry. <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run the following.<ol type="i">
<li>Dummy mode, to test CVflow® performance: <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin</div>
</div><!-- fragment --></li>
<li>The image is used as an input with preprocess and postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_det.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov8/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov8/out/</div>
<div class="line">board # test_eazyai -m 1 -d 1 -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_seg.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov8/in|t:jpg|c:rgb&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolov8/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as an input without the corrcet preprocess or postprocess. <div class="fragment"><div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov8/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov8/out/</div>
<div class="line">board # test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolov8/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolov8/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode using image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/yolov8/dra_img/bus.jpg</code>) in <code>/sdcard/yolov8/in</code>, and create <code>/sdcard/yolov8/out</code> as the output directory.</li>
<li>Option <b>&ndash;isrc"|d:vp"</b>: default preprocess is based on OpenCV; users can enable VProc if required with the option <b>"d:vp"</b>. The default value is CPU.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode:<ol type="1">
<li>Initialize the environment on the CV board. The cv22_walnut and imx274_mipi are used as examples. <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run the following commands, take yolov8s as an example.<ol type="a">
<li>Stream live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>): <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -r -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_det.lua \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 1 -r -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_seg.lua \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>Video output (VOUT) live mode (draw on VOUT high definition multimedia interface (HDMI®)): <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -r -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_det.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_det.lua \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 1 -r -n yolov8 \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolov8/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolov8s_seg.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolov8_seg.lua \</div>
<div class="line">        --label_path /sdcard/yolov8/labels/label_coco_80.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="i">
<li>For Segmentation, <b>OpenMP</b> is enabled by default, and "thread_num = 2," in lua can be set for the thread number of parallel processing.</li>
<li>If there is no display on the stream or the display is not fluent, check the following points.<ul>
<li>If the display is not fluency, use a larger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not large enough, it can be increased by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<b>1024 + resolution * (enc-dummy-latency + 5)</b>). For more details, refer to the <b>EazyAI Library application programming interface (API)-related content in the Linux software development kit (SDK) Doxygen documents</b>.</li>
</ul>
</li>
</ol>
</dd></dl>
<hr  />
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1><a class="anchor" id="sec_onnx_yolox"></a>
YOLOX</h1>
<p>YOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities. For more details, please refer to <a href="https://github.com/Megvii-BaseDetection/YOLOX">GitHub</a>.</p>
<p>The following contents demonstrate how to export YOLOX ONNX models from the public source project implemented with PyTorch, and how to run the ONNX models with the Ambarella CNNGen samples package.</p>
<dl class="section note"><dt>Note</dt><dd>If the current CNNGen samples package does not include this conversion step, contact the Ambarella support team for assistance.</dd></dl>
<h2><a class="anchor" id="onnx_yolox_export_onnx_model"></a>
1 Export ONNX Models</h2>
<p>The ONNX models are exported from the source project in GitHub with Apache-2.0 license. The steps are as below.</p>
<ol type="1">
<li>Download the source project from <a href="https://github.com/Megvii-BaseDetection/YOLOX">here</a>. The steps were tested with the following commit. <div class="fragment"><div class="line">build $ git reset --hard f778fdba8f083e6f1e8dd168fe7a8faa5faa8484</div>
</div><!-- fragment --></li>
<li>Go to the source project directory and install the required packages. <div class="fragment"><div class="line">build $ python3 -m pip install --user -r requirements.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>It's ok if there is an installing error related to the package "immutables".</dd></dl>
</li>
<li>Download weights of standard models and light models, and put them under the source project directory.<ol type="a">
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth">YOLOX-s</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth">YOLOX-m</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.pth">YOLOX-l</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth">YOLOX-x</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_darknet.pth">YOLOX-Darknet53</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_nano.pth">YOLOX-Nano</a></li>
<li><a href="https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth">YOLOX-Tiny</a></li>
</ol>
</li>
<li>Export ONNX models from standard models and light models. The reference is <a href="https://github.com/Megvii-BaseDetection/YOLOX/tree/main/demo/ONNXRuntime">this guide</a>.<ol type="a">
<li>Export YOLOX-s <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_s.onnx -n yolox-s -c yolox_s.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_s.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-m <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_m.onnx -n yolox-m -c yolox_m.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_m.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-l <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_l.onnx -n yolox-l -c yolox_l.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_s.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-x <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_x.onnx -n yolox-x -c yolox_x.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_s.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-Darknet53 <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_darknet.onnx -n yolox-darknet -c yolox_darknet.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_darknet.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-Nano <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_nano.onnx -n yolox-nano -c yolox_nano.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_nano.onnx</div>
</div><!-- fragment --></li>
<li>Export YOLOX-Tiny <div class="fragment"><div class="line">build $ python3 tools/export_onnx.py --output-<a class="codeRef" href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a> yolox_tiny.onnx -n yolox-tiny -c yolox_tiny.pth -o 11</div>
<div class="line">...</div>
<div class="line">generated simplified onnx model named yolox_tiny.onnx</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Do a graph surgery on the ONNX models to eliminate unused initializer, constantify shapes, eliminate deadend, and fold constants.<ol type="a">
<li>YOLOX-s <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_s.onnx -o yolox_s_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,640,640&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_s_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-m <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_m.onnx -o yolox_m_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,640,640&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_m_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-l <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_l.onnx -o yolox_l_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,640,640&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_l_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-x <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_x.onnx -o yolox_x_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,640,640&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_x_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-Darknet53 <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_darknet.onnx -o yolox_darknet_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,640,640&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_darknet_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-Tiny <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_tiny.onnx -o yolox_tiny_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_tiny_surgery.onnx</div>
</div><!-- fragment --></li>
<li>YOLOX-Nano <div class="fragment"><div class="line">build $ graph_surgery.py onnx -m yolox_nano.onnx -o yolox_nano_surgery.onnx -isrc <span class="stringliteral">&quot;i:images|is:1,3,416,416&quot;</span> -t CVFlow</div>
<div class="line">build $ onnx_print_graph_summary.py -p yolox_nano_surgery.onnx</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
<h2><a class="anchor" id="sub_sec_onnx_yolox_cnngen_conversion"></a>
2 CNNGen Conversion</h2>
<p>The Cavalry binary files converted from the ONNX models can be generated with the CNNGen sample package.</p>
<ol type="1">
<li>For YOLOX-s, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_s.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_s/</code>.</li>
<li>For YOLOX-m, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_m.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_m/</code>.</li>
<li>For YOLOX-l, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_l.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_l/</code>.</li>
<li>For YOLOX-x, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_x.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_x/</code>.</li>
<li>For YOLOX-Darknet53, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_darknet.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_darknet/</code>.</li>
<li>For YOLOX-Nano, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_nano.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_nano/</code>.</li>
<li>For YOLOX-Tiny, generate the Cavalry binary using the following commands. <div class="fragment"><div class="line">build $ eazyai_cvt -cy onnx/demo_networks/yolox/config/ea_cvt_onnx_yolox_tiny.yaml</div>
</div><!-- fragment --> The output is in <code>out/onnx/demo_networks/onnx_yolox_tiny/</code>.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Current default output data format is float32, and even some netowrk use FP32 as input. For CV7x, please use&#160;<code>-ac</code> in command for self-adaption, which will switch to FP16 as it does not support FP32.</li>
<li>The cnngen output folder is in <code>&lt;cvflow_cnngen_samples&gt;/out/onnx/demo_networks/onnx_yolox_*</code>.</li>
<li>For EVK, the cavalry binary <code>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_*.bin</code> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolox_(*)/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolox_*</code>.</li>
<li>For X86 simulator, model desc json file <b>onnx_yolox_*.json</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolox_(*)/out_&lt;build_target&gt;_parser/</code>. ades command <b>onnx_yolox_*_ades.cmd</b> is in the cnngen output folder <code>out/onnx/demo_networks/onnx_yolox_(*)/&lt;chip&gt;/&lt;chip&gt;_ades_onnx_yolox_*</code>.</li>
<li>Current DRA strategy is default, means use mixed fix8, fix16, and FP16(CVflow v3 only) to blance performance and accuracy.<ol type="a">
<li>For best performance, please use <code>-ds fx8</code> in command.</li>
<li>For best accuracy, please use <code>-ds fx16</code> or <code>-ds fp16</code> in command. The <code>-ds fp16</code> is only for CVflow V3.</li>
</ol>
</li>
<li>Please use <code>-cvb</code> to enable CVflowbackend convert flow. For CV3x, please enable CVflowbackend if GVP should be used.</li>
<li>The python scripts locate at <code>&lt;cvflow_cnngen_samples&gt;/library/cflite/eazyaitools/</code>. For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#sec_eazyai_python_tools">2 EazyAI Python Tools</a>.</li>
<li>For how to generate a new convert configuration yaml file, users can manually modify based on above files, also they can use the configuration generation tool in <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cfg">2.2 EazyAI Configuration Tool</a>.</li>
<li>Users can use quick dummy convert which is only for performance evaluation without above configuration file. For detail, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvt">2.3 EazyAI Convert Tool</a>.</li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sub_sec_onnx_yolox_build_unit_test"></a>
3 Build Unit Test</h2>
<ul>
<li>Build Unit Test for EVK <div class="fragment"><div class="line">build $ make menuconfig</div>
<div class="line">  [*] Ambarella <span class="keyword">package </span>Configuration ---&gt;</div>
<div class="line">    -*- Build Ambarella EAZYAI library ---&gt;</div>
<div class="line">      -*- Build eazyai library with OpenCV support</div>
<div class="line">      -*- Build Ambarella custom postprocess library ---&gt;</div>
<div class="line">        [*] Build Ambarella custom postprocess library with yolox</div>
<div class="line">     [*] Build EazyAi unit tests</div>
<div class="line">build $ make test_eazyai</div>
</div><!-- fragment --></li>
<li>Build Unit Test for X86 Simulator Refer to cnngen doxgen library EazyAI <a class="elRef" href="../../../library/d3/d0c/page_lib_eazyai_doc.html#eazyai_simulator">3 EazyAI Simulator</a> to build x86 binary. Then, the executable file <b>test_eazyai</b> can be found in <code>&lt;SDK&gt;/ambarella/packages/eazyai/unit_test/build/</code>.</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolox_run_python_inference"></a>
4 Run Python Inference</h2>
<p>For detailed script usage, please refer to <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_inf">2.4 EazyAI Inference Tool</a> and <a class="el" href="../../dd/de5/fs_eazyai.html#subsec_eazyai_python_tools_cvflow_inf">2.5 CVflow Simple Inference Tool</a>.</p>
<ul>
<li>Check if EVK is alive, and start CVflow engine for below Dummy and File mode with CVflow Chip <div class="fragment"><div class="line">build $ eazyai_video -ip 10.0.0.2</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For CV2x and CV5x, users need to run it at once to boot CVflow engine. But for CV7x, this command is not MUST have, users can use it check if EVK is alive.</li>
<li>Also it is not needed for the inferecne on Simulator and Original framework.</li>
</ul>
</dd></dl>
</li>
<li>Dummy Mode (Only For Chip) <div class="fragment"><div class="line">build $ eazyai_inf_simple_dummy -ip 10.0.0.2 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolox_*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolox_*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_*.bin</div>
</div><!-- fragment --></li>
<li>File Mode <div class="fragment"><div class="line">build $ eazyai_inf -p cvflow -ip 10.0.0.2 -cy out/onnx/demo_networks/onnx_yolox_*/onnx_yolox_*_cvt_summary.yaml \</div>
<div class="line">        -iy onnx/demo_networks/yolox/config/ea_inf_onnx_yolox_*.yaml -pwd ./out/onnx/demo_networks/onnx_yolox_*</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>Above is using CVflow, option <code>-ip ip_address</code> is needed to find the chip, for other model, users can remove it.</li>
<li>For Simulator, please useoption <code>-p ades</code> and <code>-p acinf</code>.</li>
<li>For Original Framework, please useoption <code>-p orig</code>.</li>
<li>Remove <code>-iy</code> to let this application run without postprocess.</li>
</ul>
</dd></dl>
</li>
<li>Live mode<ol type="1">
<li>Start CVflow engine and DSP video pipeline. Below demo used CV22 Walnut and imx274_mipi as the examples. <div class="fragment"><div class="line">build # eazyai_video -ip 10.0.0.2 -s1 1080p -h1 1080p</div>
</div><!-- fragment --></li>
<li>Run <div class="fragment"><div class="line">build $ eazyai_inf_simple_live -ip 10.0.0.2 -buf BUFFER1 -icf 0 \</div>
<div class="line">        -cb out/onnx/demo_networks/onnx_yolox_*/&lt;chip&gt;/&lt;chip&gt;_cavalry_onnx_yolox_*/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_*.bin \</div>
<div class="line">        -pn yolox -pl &lt;usr_path&gt;/yolox/config/yolox.lua -dm 0 -lp &lt;usr_path&gt;/yolox/config/label_coco_80.txt \</div>
<div class="line">        -dd STREAM1</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For Vout display , please use option <code>-dd HDMI</code>.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="sub_sec_onnx_yolox_run_c_inference"></a>
5 Run C Inference</h2>
<p>In the following examples, the camera module imx274 and CV22 board are used. The <b>test_eazyai</b> is used for the following example, please refer to the following referenced chapters for detailed usage of test_eazyai.</p><ol type="1">
<li><a class="el" href="../../d7/d53/fs_deployment.html#sec_deploy_applications">2 Applications</a></li>
<li>sec_ref_eazyai</li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>For EVK Board:<ol type="a">
<li>Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolox_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_*.bin</b>.</li>
<li>The <b>yolox.lua</b> is included in the path <code>/usr/share/ambarella/eazyai/lua</code> of EVK. If it does not exist, find it in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/lua</code>.</li>
</ol>
</li>
<li>For X86: Refer to <a class="el" href="../../d6/d99/fs_cnngen_onnx_demos.html#sub_sec_onnx_yolox_cnngen_conversion">2 CNNGen Conversion</a> for how to generate <b>onnx_yolox_*.json</b> and <b>onnx_yolox_*_ades.cmd</b>.</li>
</ol>
</dd></dl>
<p>Please follow below steps to run inference.</p>
<ul>
<li>Copy files to SD card for EVK test For example, place files on the SD card with the following structure. <div class="fragment"><div class="line">/sdcard/yolox</div>
<div class="line">|--model</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin</div>
<div class="line">|       &lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin</div>
<div class="line">|</div>
<div class="line">|--labels</div>
<div class="line">|       label_coco_80.txt</div>
<div class="line">|</div>
<div class="line">|--in</div>
<div class="line">|       bus.jpg</div>
<div class="line">|       bus.bin</div>
<div class="line">|       dog.jpg</div>
<div class="line">|       dog.bin</div>
<div class="line">|</div>
<div class="line">|__out</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ol type="1">
<li>Users can find <b>"label_coco_80.txt"</b> in <code>cvflow_cnngen_samples/library/eazyai/unit_test/resource/</code>.</li>
<li>This file saving method is only an example. The file can be placed freely according to the user's needs. users need to keep the file path consistent during use.</li>
</ol>
</dd></dl>
</li>
<li>File mode<ol type="1">
<li>For X86 Simulator:<ol type="a">
<li>Run Ades mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --model_path &lt;usr path&gt;/out_onnx_yolox_s_parser/onnx_yolox_s.json \</div>
<div class="line">        --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_s/onnx_yolox_s_ades.cmd \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_m_parser/onnx_yolox_m.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_m/onnx_yolox_m_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_l_parser/onnx_yolox_l.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_l/onnx_yolox_l_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_x_parser/onnx_yolox_x.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_x/onnx_yolox_x_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_darknet_parser/onnx_yolox_darknet.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_darknet/onnx_yolox_darknet_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_nano_parser/onnx_yolox_nano.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_nano/onnx_yolox_nano_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_tiny_parser/onnx_yolox_tiny.json \</div>
<div class="line">       --ades_cmd_file &lt;usr path&gt;/ades_onnx_yolox_tiny/onnx_yolox_tiny_ades.cmd \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_s_parser/onnx_yolox_s.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_s/onnx_yolox_s_ades.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_m_parser/onnx_yolox_m.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_m/onnx_yolox_m_ades.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_l_parser/onnx_yolox_l.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_l/onnx_yolox_l_ades.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_x_parser/onnx_yolox_x.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_x/onnx_yolox_x_ades.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_darknet_parser/onnx_yolox_darknet.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_darknet/onnx_yolox_darknet_ades.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_nano_parser/onnx_yolox_nano.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/&lt;chip&gt;_ades_onnx_yolox_nano/onnx_yolox_s_nano.cmd</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_tiny_parser/onnx_yolox_tiny.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/ --ades_cmd_file &lt;usr path&gt;/ades_onnx_yolox_tiny/onnx_yolox_tiny_ades.cmd</div>
</div><!-- fragment --></li>
</ol>
</li>
<li>Run Acinference mode<ol type="i">
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_s_parser/onnx_yolox_s.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_m_parser/onnx_yolox_m.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_l_parser/onnx_yolox_l.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_x_parser/onnx_yolox_x.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_darknet_parser/onnx_yolox_darknet.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_nano_parser/onnx_yolox_nano.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_tiny_parser/onnx_yolox_tiny.json \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:raw&quot;</span> \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_s_parser/onnx_yolox_s.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_m_parser/onnx_yolox_m.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_l_parser/onnx_yolox_l.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_x_parser/onnx_yolox_x.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_darknet_parser/onnx_yolox_darknet.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_nano_parser/onnx_yolox_nano.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">       --model_path &lt;usr path&gt;/out_onnx_yolox_tiny_parser/onnx_yolox_tiny.json \</div>
<div class="line">       --lua_file &lt;usr path&gt;/yolox.lua \</div>
<div class="line">       --isrc <span class="stringliteral">&quot;i:images=&lt;usr path&gt;/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">       --label_path &lt;usr path&gt;/label_coco_80.txt \</div>
<div class="line">       --output_dir &lt;usr path&gt;/out/</div>
</div><!-- fragment --></li>
</ol>
</li>
</ol>
</li>
<li>For EVK Board:<ol type="a">
<li>Load cavalry <div class="fragment"><div class="line">board # rmmod cavalry &amp;&amp; modprobe cavalry &amp;&amp; cavalry_load -f /lib/firmware/cavalry.bin -r</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>Only CV2x and CV5x need to boot up cavalry manually, for other chips, users do not need to run this command.</dd></dl>
</li>
<li>Run<ol type="i">
<li>Dummy mode for CVflow® performance test <div class="fragment"><div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin</div>
<div class="line">board # test_eazyai -m 2 --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin</div>
</div><!-- fragment --></li>
<li>The image is used as an input with the right preprocess and postprocess <div class="fragment"><div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # test_eazyai -m 1 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:jpg|c:bgr&quot;</span> \</div>
<div class="line">        --label_path /sdcard/yolox/labels/label_coco_80.txt \</div>
<div class="line">         --output_dir /sdcard/yolox/out/</div>
</div><!-- fragment --></li>
<li>The raw.bin is used as input without the preprocess and postprocess <div class="fragment"><div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
<div class="line">board # ./build/test_eazyai -m 1 -n to_file \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin \</div>
<div class="line">        --isrc <span class="stringliteral">&quot;i:images=/sdcard/yolox/in|t:raw&quot;</span> \</div>
<div class="line">        --output_dir /sdcard/yolox/out/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd><ul>
<li>For the file mode with image as input, place the test image (such as <code>cvflow_cnngen_samples/onnx/demo_networks/yolox/dra_img/bus.jpg</code>) in <code>/sdcard/yolox/in</code>, and create <code>/sdcard/yolox/out</code> as the output directory.</li>
<li>For specific parameter information, users can enter the command <b>test_eazyai</b>, and press <b>entry</b> to view.</li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>Live mode<ol type="1">
<li>Initialize the environment on the CV board. Use CV22 Walnut and imx274_mipi for examples <div class="fragment"><div class="line">board # eazyai_video.sh --stream_A 1080p --hdmi 1080p --enc_dummy_latency 4 --reallocate_mem overlay,0x01200000</div>
</div><!-- fragment --></li>
<li>Run<ol type="a">
<li>Streams live mode (draw on stream <code>rtsp://10.0.0.2/stream1</code>) <div class="fragment"><div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -s 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
</div><!-- fragment --></li>
<li>VOUT live mode (draw on VOUT HDMI) <div class="fragment"><div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_s.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_m.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_l.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_x.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_darknet.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_nano.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
<div class="line">board # test_eazyai -m 0 -d 0 -n yolox \</div>
<div class="line">        --<a class="codeRef" href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a> /sdcard/yolox/model/&lt;chip&gt;_cavalry&lt;version&gt;_onnx_yolox_tiny.bin \</div>
<div class="line">        --lua_file /usr/share/ambarella/eazyai/lua/yolox.lua \</div>
<div class="line">        --label_path /sdcard/yolox/label_coco_80.txt</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>-If there is no display on the stream or the display is not fluency, check the following two points.<ul>
<li>If the display is not fluency, use bigger value in <b>--enc_dummy_latency 4</b>, such as 7.</li>
<li>If the overlay buffer size is not enough, it can be added by changing the size in <b>--reallocate_mem overlay,0x04000000</b>.</li>
<li>The overlay buffer size taken by each feature in ea_display_feature_e is around (<code>1024 + resolution * (enc-dummy-latency + 5)</code>). For details, please refer to <b>EazyAI Library API related content in Linux SDK Doxygen documents</b>. </li>
</ul>
</dd></dl>
</li>
</ol>
</li>
</ol>
</li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div class="ttc" id="agroup__cflite-eazyaigen-layercompare_html_ga4d5bb0c360b13429f65cd327c8d0aa12"><div class="ttname"><a href="../../../library/da/dcb/group__cflite-eazyaigen-layercompare.html#ga4d5bb0c360b13429f65cd327c8d0aa12">model_path</a></div><div class="ttdeci">model_path</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga36536ba84124361916240b28721fe384"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga36536ba84124361916240b28721fe384">__init__</a></div><div class="ttdeci">def __init__(self, str work_dir, str data_name, abspth=True, log_level=logging.INFO)</div></div>
<div class="ttc" id="agroup__cflite-eazyaiinf-filemode_html_gab74e6bf80237ddc4109968cedc58c151"><div class="ttname"><a href="../../../library/de/d9a/group__cflite-eazyaiinf-filemode.html#gab74e6bf80237ddc4109968cedc58c151">name</a></div><div class="ttdeci">name</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_gac69e20380615374be0baa46ed46295b7"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#gac69e20380615374be0baa46ed46295b7">run</a></div><div class="ttdeci">tuple run(self)</div></div>
<div class="ttc" id="agroup__cflite-eazyaigen-dataprepare_html_ga8179f95715172cfcd3a44cd038a81a9f"><div class="ttname"><a href="../../../library/dc/dcb/group__cflite-eazyaigen-dataprepare.html#ga8179f95715172cfcd3a44cd038a81a9f">transforms</a></div><div class="ttdeci">transforms</div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
