<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.3"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Applications: Access Control Camera</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<link rel="search" href="../../search_opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="Applications"/>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
<link href="../../doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="../../doxygen-ambarella.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../Ambarella.png"/></td>
  <td id="projectalign">
   <div id="projectname">Applications<span id="projectnumber">&#160;1.0.0 @ 2024.07.10 14:12:44</span>
   </div>
   <div id="projectbrief">placeholder</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.3 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="../../menudata.js"></script>
<script type="text/javascript" src="../../menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('../../',true,true,'search.html','Search');
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('d2/d89/acc_ctrl_page.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<div><div class="header">
  <div class="headertitle"><div class="title">Access Control Camera </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="acc_ctrl_history"></a>
0. Revision History</h1>
<a class="anchor" id="acc_ctrl_history_table"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Updated Date </th><th align="left">Modification </th></tr>
<tr>
<td>20220815 </td><td>Initial Version </td></tr>
</table>
<hr  />
<h1><a class="anchor" id="acc_ctrl_ov"></a>
1. Overview</h1>
<p >This document introduces Ambarella access control applications and provides details of the reference design for the Ambarella CV2x system on chip (SoC). <br  />
</p>
<p >Users are encouraged to proceed sequentially through this guide (beginning with hardware setup for their particular chip), and become familiar with the system requirements, software, and features included in the software development kit (SDK).</p>
<p >Using the package contents and this document, customers can set up a demo to test a working procedure for their product. Because customer product requirements may vary, this document provides basic framework and sample codes as references for users to design their customized products.</p>
<p >The access control evaluation kit (EVK) is a reference design platform based on the Ambarella CV2x computer vision (CV) SoC. Designed for three-dimensional sensing with the structured-light module, it is capable of generating disparity images, depth images, and point cloud data for biometric identification.</p>
<p >Additionally, the access control EVK serves as a reference development platform on which Ambarella customers can build their own product / hardware designs and use as a software development / prototype platform. The final products possess unique requirements, ranging from (but not limited to) the field of view (FoV) of the lens used, the baseline and structured-light module design (thus, the operating range), the liveness / anti-spoofing algorithms based on depth information, and more.</p>
<dl class="section note"><dt>Note</dt><dd>This kit is not intended to provide the following:<ul>
<li>A turnkey solution</li>
<li>Demo applications for production purposes</li>
<li>Security camera / product application-level software or capabilities</li>
<li>Liveness algorithms, and more for production purposes</li>
</ul>
</dd></dl>
<a class="anchor" id="acc_ctrl_feature_table"></a>
<table class="doxtable">
<caption>Features of Access Control Demo</caption>
<tr>
<th>Feature </th><th align="center" width="150px">Demo Only </th><th align="center" width="150px">SDK Support </th></tr>
<tr>
<td>Generate RGB image </td><td align="center">YES </td><td align="center">YES </td></tr>
<tr>
<td>Generate IR image </td><td align="center">YES </td><td align="center">YES </td></tr>
<tr>
<td>Generate depth image </td><td align="center">YES </td><td align="center">NO </td></tr>
<tr>
<td>Generate point cloud data </td><td align="center">YES </td><td align="center">NO </td></tr>
<tr>
<td>Face detection / face recognition (FDFR) </td><td align="center">YES </td><td align="center">NO </td></tr>
<tr>
<td>Liveness detection </td><td align="center">YES </td><td align="center">NO </td></tr>
<tr>
<td>Detection range (cm) </td><td align="center">YES </td><td align="center">/ </td></tr>
</table>
<h2><a class="anchor" id="acc_ctrl_evk_pkg"></a>
1.1 EVK Package</h2>
<p >The CV series of the Flexible Linux Evaluation Kit includes the following hardware and accessories. If any of the following components are missing from the EVK package, contact the Ambarella support team for assistance. </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">SDK Components   </th><th class="markdownTableHeadNone">Descriptions    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">EVK boards   </td><td class="markdownTableBodyNone">The board armed with an Ambarella CV2x system on chip and a structured-light projector module    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Cables   </td><td class="markdownTableBodyNone">Micro USB cable    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">EVK Firmware   </td><td class="markdownTableBodyNone">The firmware in the folder <code>binary/</code> that is upgraded to the EVK board    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Install Program   </td><td class="markdownTableBodyNone">Tools in the folder <code>Tools/</code> used for evaluation, including for the AmbaUSB and the VLC player    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Unit test programs   </td><td class="markdownTableBodyNone">The unit test applications that enable users to evaluate features using commands from the command line interface   </td></tr>
</table>
<h2><a class="anchor" id="acc_ctrl_pc_req"></a>
1.2 PC Requirement</h2>
<p >A PC connected to the internet is required for downloading the SDK and / or upgrading the firmware. For further details on upgrading the firmware, refer to the quick_start_guide.</p>
<p >The PC must be equipped with the following: </p><table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Items   </th><th class="markdownTableHeadNone">Requirements    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">CPU   </td><td class="markdownTableBodyNone">Intel® i7 or higher grade    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Memory   </td><td class="markdownTableBodyNone">8 GB system memory or more    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Hard disk   </td><td class="markdownTableBodyNone">120 GB or more    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Video card   </td><td class="markdownTableBodyNone">3D hardware accelerator card required―100% DirectX 9.0c compatible. NVIDIA® GeForce GT520 or above to ensure the full frame rate of the HD playback    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Monitor   </td><td class="markdownTableBodyNone">24-inch LCD or larger with a digital visual interface (DVI) input. The Dell UltraSharp 2709W (VA panel) is preferred.    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Ethernet card   </td><td class="markdownTableBodyNone">1000 Mbps (adaptive)    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Operating system   </td><td class="markdownTableBodyNone">Windows 7/8 64-bit or Ubuntu 18.04 LTS 64-bit    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Virtual machine   </td><td class="markdownTableBodyNone">Oracle VirtualBox 5.0.32 or later   </td></tr>
</table>
<h2><a class="anchor" id="acc_ctrl_accuracy"></a>
1.3 Detection Range and Accuracy</h2>
<p >The detection range and accuracy of the structured-light module depends on the baseline, focal length, sensor pixel size, infra-red (IR) filter performance, structured-light strength, and pattern density, as well as the IR illumination interference.</p>
<p >For reference, CV25M Janus EVK uses a <b>4-cm baseline</b> that ensures a 0.3-1.5 m depth generation range and a liveness detection range indoors.</p>
<p >CV22 Vision+ EVK uses a <b>5-cm baseline</b> that allows for a 0.3-5 m depth generation range and a 0.3-1.5 m liveness detection range indoors.</p>
<p >When outdoors, sunlight exposure and restrictions from the structured-light strength and shutter time result in a narrow detection range of 0.3-1.0 m or greater. Additionally, depth information may be limited or non-existent if intense sunlight shines directly on the subject's face. Using increased structured-light strength can optimize performance, but because results can be limited by the environment, users must first define the scope of their product.</p>
<p >The following images are examples of both valid and invalid depth image information in the outdoor test environment.</p>
<div class="image">
<img src="../../acc_ctrl_valid_depth.png" alt=""/>
<div class="caption">
Figure 1-1. Valid Depth Image Information (Outdoor: 5000 LUX).</div></div>
<p> <br  />
 </p><div class="image">
<img src="../../acc_ctrl_invalid_depth.png" alt=""/>
<div class="caption">
Figure 1-2. Invalid Depth Image Information (Outdoor: Sunlight on Face).</div></div>
<p> <br  />
</p>
<h2><a class="anchor" id="acc_ctrl_depth_gen"></a>
1.4 Depth Generation</h2>
<p >The Ambarella SDK includes two depth generation algorithms. The first is Blade Runner, and the other is Fast Block Match. Blade Runner is a CPU intensive algorithm and its calculation relies only on CPU. Fast Block Match is a Vector processor (VP) intensive algorithm, and most of its calculation relies on VP.</p>
<p >The following table provides the performance of the depth map calculations, CPU usages, and VP usages at various resolutions. The value set in each cell presents the time cost for the depth map calculation, the CPU usage percentage, and VP usage percentage.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>EVK is facing a blank wall during the tests.</li>
<li>CV25M Janus uses the CV25S55M clock, and CV22 Vision+ uses the CV22S66 clock.</li>
<li>CV25M Janus uses the 270p and 1080p reference YUV, while CV22 Vision+ uses the 480x376 and 1280x1000 reference YUV.</li>
<li>CPU usage is measured in one-second intervals; it refers to average CPU usage within each second.</li>
<li>In typical use cases, a depth map is generated for the facial area only, which is much smaller than the entire frame size. Consequently, the depth map calculation time is reduced dramatically.</li>
</ul>
</dd></dl>
<a class="anchor" id="acc_ctrl_blade_runner"></a>
<table class="doxtable">
<caption>Blade Runner Depth Map Generation Time and Average CPU Usage.</caption>
<tr>
<th width="200px"></th><th align="center" width="200px">480x270 </th><th align="center" width="200px">1280x720 </th><th align="center" width="200px">480x376 </th><th align="center" width="200px">1280x1000 </th></tr>
<tr>
<th>CV25M Janus </th><td align="center">122 ms<br  />
 20%(CPU)<br  />
 0%(VP) </td><td align="center">678 ms<br  />
 65%(CPU)<br  />
 0%(VP) </td><td align="center">\ </td><td align="center">\ </td></tr>
<tr>
<th>CV22 Vision+ </th><td align="center">\ </td><td align="center">\ </td><td align="center">119 ms<br  />
 14%(CPU)<br  />
 0%(VP) </td><td align="center">820 ms<br  />
 60%(CPU)<br  />
 0%(VP) </td></tr>
</table>
<a class="anchor" id="acc_ctrl_fbm"></a>
<table class="doxtable">
<caption>Fast Block Match Depth Map Generation Time and Average CPU Usage.</caption>
<tr>
<th width="200px"></th><th align="center" width="200px">480x270 </th><th align="center" width="200px">1280x720 </th><th align="center" width="200px">480x376 </th><th align="center" width="200px">1280x1000 </th></tr>
<tr>
<th>CV25M Janus </th><td align="center">\ </td><td align="center">239 ms<br  />
 1%(CPU) 83%(VP) </td><td align="center">\ </td><td align="center">\ </td></tr>
<tr>
<th>CV22 Vision+ </th><td align="center">\ </td><td align="center">120 ms<br  />
 1%(CPU) 81.5%(VP) </td><td align="center">\ </td><td align="center">\ </td></tr>
</table>
<p >The low-resolution reference YUV must not be used in production, as it will yield a lower-quality depth map. The system must instead use the high-resolution image with the region of interest (ROI) for the depth map creation area.</p>
<h2><a class="anchor" id="acc_ctrl_liveness_det"></a>
1.5 Liveness Detection</h2>
<p >The table for liveness detection accuracy provides the performance of the liveness detection library. TP, TN, FP, and FN seen below stand for true positives, true negatives, false positives, and false negatives.</p>
<table class="doxtable">
<caption>Livenes Detection Accuracy - Version 1.0.0</caption>
<tr>
<th width="200px"></th><th align="center" width="300px">Formula </th><th align="center" width="300px">Result </th></tr>
<tr>
<th>Recall </th><td align="center">TP/(TP+FN) </td><td align="center">0.9444 </td></tr>
<tr>
<th>Precision </th><td align="center">TP/(TP+FP) </td><td align="center">0.9639 </td></tr>
<tr>
<th>Accuracy </th><td align="center">(TP+TN)/(TP+TN+FN+FP) </td><td align="center">0.9695 </td></tr>
</table>
<hr  />
<h1><a class="anchor" id="acc_ctrl_demo"></a>
2. Demo Application</h1>
<p >This chapter provides details on setting up the building environment and running the access control EVK demos.</p>
<h2><a class="anchor" id="acc_ctrl_bulid_image"></a>
2.1 Build Image for Access Control EVK</h2>
<p >If the user does not possess an SDK, skip this section and proceed directly to Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_initialize">2.2 Initialize the System and Video Pipeline</a>.</p>
<p >Refer to the <em>CV2x Linux SDK Release Notes</em> document for information relating to the Toolchain and other software.</p>
<p >To build an image for the access control EVK:</p><ul>
<li>Extract the latest access control SDK patch.</li>
<li>Refer to the readme.txt file for the complete build process.</li>
</ul>
<p >For further details, refer to <a class="elRef" href="../../../overview/d9/d98/ov_code_build.html">SDK Code Building</a>.</p>
<h2><a class="anchor" id="acc_ctrl_initialize"></a>
2.2 Initialize the System and Video Pipeline</h2>
<p >This section details the process for generating the demo scripts for the access control EVK boards.</p><ol type="1">
<li>Check the module ID on the access control EVK’s sensor module or acrylic mounting support (Note that this is not the ID found on the main board). Each sensor module has a unique ID, such as H1912060001 or H210615A001.</li>
<li>Verify that the configuration file, warp table, and reference data are ready; the default path is <code>/usr/share/ambarella/calib_mono</code>.<br  />
 The <code>board_config</code> file defines all user configuration settings for the access control demos. The <code>warp_&lt;resolution&gt;.bin</code> is the warp table, and the <code>ref_&lt;resolution&gt;.yuv</code> files are the structured-light reference data. <dl class="section note"><dt>Note</dt><dd>Users must back up the <code>calib_mono</code> folder before flashing a new firmware to the access control board. If there are any problems, contact the Ambarella support team for assistance.</dd></dl>
The <code>board_config</code> file can be generated and initialized by the <code>gen_board_config.sh</code> script. By changing the <code>CHANNEL_AMOUNT</code> and <code>FRAME_RATIO</code>, users can enable / disable the IR LED channel and adjust the frequency of the frames with the structured-light pattern. More details on the parameters in the <code>board_config</code> can be found in the Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_board_config">3.5 Customize the Board Configuration File</a>. <div class="fragment"><div class="line">board # gen_board_config.sh /usr/share/ambarella/calib_mono &lt;module ID&gt;</div>
</div><!-- fragment --></li>
<li>Run <code>quick_ipc_for_access_control.sh</code> to initialize the stream. The demo applications will generate under /tmp/run_ipc_folder.<br  />
 <b>Janus:</b><br  />
 &emsp;quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720<br  />
 <b>Vision+:</b><br  />
 &emsp;quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x1000<br  />
 <div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
<div class="line">===================================================================================</div>
<div class="line">Summary:</div>
<div class="line"> </div>
<div class="line">        Current board <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a25d22ecc7e656d2c59332072684e8766">name</a> is: [cv25m_janus]</div>
<div class="line">        Current sensor <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a25d22ecc7e656d2c59332072684e8766">name</a> is: [ar0237]</div>
<div class="line"> </div>
<div class="line">        Reallocate overlay memory to: [0x1000000]</div>
<div class="line"> </div>
<div class="line">        Channel num: [3]</div>
<div class="line">        Enable best_performance: [No]</div>
<div class="line"> </div>
<div class="line">        Canvas information:</div>
<div class="line">                canvas_id: [0], from SL  <a class="code hl_functionRef" href="../../../library/dc/d60/test__smartfb_8c.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a>   <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a>, resolution: [1920x1080]</div>
<div class="line">                canvas_id: [1], from RGB <a class="code hl_functionRef" href="../../../library/dc/d60/test__smartfb_8c.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a>   <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a>, resolution: [1920x1080]</div>
<div class="line">                canvas_id: [2], from IR  <a class="code hl_functionRef" href="../../../library/dc/d60/test__smartfb_8c.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a>   <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a>, resolution: [1920x1080]</div>
<div class="line">                canvas_id: [3], from SL  second <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a>, resolution: [480x272]</div>
<div class="line">                canvas_id: [4], from RGB third  <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a>, resolution: [1280x720]</div>
<div class="line"> </div>
<div class="line">        There are [stream_A], [stream_B], and [stream_C]:</div>
<div class="line">                stream_A <span class="keywordflow">for</span> [SL ], from canvas_id: [0], resolution: [1920x1080]</div>
<div class="line">                stream_B <span class="keywordflow">for</span> [RGB], from canvas_id: [1], resolution: [1920x1080]</div>
<div class="line">                stream_C <span class="keywordflow">for</span> [IR ], from canvas_id: [2], resolution: [1920x1080]</div>
<div class="line"> </div>
<div class="line">        In the current <span class="keywordflow">case</span>:</div>
<div class="line">                [shell script    : /tmp/run_ipc_folder/run_ipc.sh]</div>
<div class="line">                [lua             : /tmp/run_ipc_folder/run_ipc.lua]</div>
<div class="line"> </div>
<div class="line">        Run demo v1 script:</div>
<div class="line">                [Liveness script     : /tmp/run_ipc_folder/blade_runner_run_liveness_demo.sh]</div>
<div class="line">                [Depth capture script: /tmp/run_ipc_folder/blade_runner_run_depth_capture.sh]</div>
<div class="line">                [Face capture script : /tmp/run_ipc_folder/blade_runner_run_face_capture.sh]</div>
<div class="line"> </div>
<div class="line">        Run demo v2 script:</div>
<div class="line">                [Script         : /tmp/run_ipc_folder/sl_liveness_run_liveness_demo_v2.sh]</div>
<div class="line"> </div>
<div class="line">Warning: Please use the AC-DC power, USB power supply may not be enough to power the board !</div>
<div class="line">===================================================================================</div>
<div class="ttc" id="acJSON_8h_html_a25d22ecc7e656d2c59332072684e8766"><div class="ttname"><a href="../../../library/d1/d82/cJSON_8h.html#a25d22ecc7e656d2c59332072684e8766">name</a></div><div class="ttdeci">const char *const name</div></div>
<div class="ttc" id="acJSON_8h_html_aff2566f4c366b48d73479bef43ee4d2e"><div class="ttname"><a href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a></div><div class="ttdeci">char * buffer</div></div>
<div class="ttc" id="atest__smartfb_8c_html_a3c04138a5bfe5d72780bb7e82a18e627"><div class="ttname"><a href="../../../library/dc/d60/test__smartfb_8c.html#a3c04138a5bfe5d72780bb7e82a18e627">main</a></div><div class="ttdeci">int main(int argc, char **argv)</div></div>
</div><!-- fragment --></li>
<li>Using the VLC, open <code>rtsp://10.0.0.2/stream2</code> to get the RGB livestream.</li>
</ol>
<p >Ensure that the local PC and access control board are in the same network segment.</p>
<p >Stream 1 is used for the IR channel with the structured-light pattern, stream 2 is used for the RGB channel, and stream 3 is used for the IR channel with the IR LED.</p>
<dl class="section note"><dt>Note</dt><dd>If the structured-light pattern is not stable on stream 3, the power supply may be not suitable and / or sufficient.</dd></dl>
<h2><a class="anchor" id="acc_ctrl_run_demo"></a>
2.3 Run Demo Applications</h2>
<p >Users can run the demo applications after the board_config file is generated via the instructions provided in Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_initialize">2.2 Initialize the System and Video Pipeline</a>.</p>
<dl class="section note"><dt>Note</dt><dd>Each demo requires the stream to be initialized in advance. Ensure that <code>quick_ipc_for_access_control.sh</code> runs once after the reboot.</dd></dl>
<p>Demo shells also accept an option to select the reference YUV being used in the demo. A higher-resolution reference YUV provides a depth map with higher accuracy and resolution; However, it also increases the calculation time. <code>low_res</code> represents a low-resolution reference YUV, such as 480x270 and 480x376, and <code>high_res</code> represents a high-resolution 1080p reference YUV. By default, a 1080p reference YUV is selected in all demo applications. For example, <code>sl_run_liveness_demo_v2.sh low_res</code> enables the use of a low-resolution reference YUV in the liveness demo.</p>
<p >A low-resolution reference cannot be used in production because it results in a low-resolution depth map.</p>
<p >This chapter includes two demo applications:</p><ul>
<li><code>blade_runner_run_depth_capture.sh</code></li>
<li><code>sl_liveness_run_liveness_demo_v2.sh</code></li>
</ul>
<h3><a class="anchor" id="acc_ctrl_depth_cap_demo"></a>
2.3.1 Capture the Disparity, Depth, and Point Cloud Data</h3>
<p >Because the FoV of the access control sensors exceed the FoV of the structured-light projector, the valid FoV only covers approximately two-thirds of the full sensor FoV. Users can only obtain 1280x720 and 1280x1000 disparity or depth images from Janus and Vision+ respectively, if the 1080P YUV data is used as a reference frame. Users can obtain the focal length baseline and the center_x / center_y directly from the <code>calib_mono/warp_&lt;resolution&gt;.json</code> file.</p>
<p >In this example, the disparity is drawn on stream 2. </p><div class="fragment"><div class="line">board # /tmp/run_ipc_folder/blade_runner_run_depth_capture.sh</div>
<div class="line">Input <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a> type = 0</div>
<div class="line">Input <a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#aff2566f4c366b48d73479bef43ee4d2e">buffer</a> <span class="keywordtype">id</span> = 0</div>
<div class="line">Display on stream, <span class="keywordtype">id</span> = 0</div>
<div class="line">ref data resolution = 1920 <a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#gaf80df1bdae91e5f76236e6ed1110825d">x</a> 1080,pitch = 1920</div>
<div class="line">...</div>
<div class="line">...</div>
<div class="line">Frame <span class="keywordtype">id</span> = 0</div>
<div class="line">Frame <span class="keywordtype">id</span> = 1</div>
<div class="line">...</div>
<div class="ttc" id="agroup__IAV_html_gaf80df1bdae91e5f76236e6ed1110825d"><div class="ttname"><a href="../../../driver/df/dc0/group__IAV.html#gaf80df1bdae91e5f76236e6ed1110825d">x</a></div><div class="ttdeci">u32 x</div></div>
</div><!-- fragment --><div class="image">
<img src="../../acc_ctrl_depth_cap_demo.png" alt=""/>
<div class="caption">
Figure 2-1. Capturing the Disparity, Depth, and Point Cloud Data.</div></div>
<p> <br  />
 The following three files are generated:</p><ul>
<li><code>disparity_xxxxx_1frame_int16.4_xxxxx_2560.bin</code><br  />
 The file includes the disparity data: each pixel has 16 bits; the resolution is 1280x720 (Janus) or 1280x1000 (Vision+); the pitch=2560; fix point / signed / 16 bit / 4 fractional bits; and the data_format=(1, 1, 4, 0).<br  />
 Users may look up the disparity map file via the <code>Vooya</code> application on Linux. Please set the format as follows: color space = single channel, data container = single integer, and bit depth = 12 bit.</li>
<li><code>depth_xxxxx_1frame_fp32_xxxxxx_5120.bin</code><br  />
 The file includes the depth data: each pixel has a 32-bit floating point representing the distance to the lens center plane. The resolution is 1280x720 (Janus) or 1280x1000 (Vision+), the pitch=5120, and the data_format=(1, 2, 0, 7).<br  />
 Users may look up the disparity map file via the <code>Vooya</code> application on Linux. Set the format as follows: color space = single channel, data container = single float (32bit).</li>
<li><code>point_cloud_xxxxx_frame1_165327_points.pcd</code><br  />
 The file includes the pcd format-total of 165327 points; users can open the three-dimensional point clouds using the <code>pcl_viewer</code> on Ubuntu.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>The data format shown in the parentheses above (such as 1, 1, 4, 0) are the vectors that represent the corresponding output data format, and the meaning is as follows:<ul>
<li>1st argument: 0 is unsigned, 1 is signed</li>
<li>2nd argument: 0 refers to 8-bit, 1 is 16-bit, 2 is 32-bit</li>
<li>3rd argument: the exponent offset</li>
<li>4th argument: the number of bits reserved for the exponent field. 0 indicates a fixed point number</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="acc_ctrl_liveness_demo"></a>
2.3.2 Run the FDFR Liveness Test Demo</h3>
<ol type="1">
<li>Run the quick_ipc_for_access_control.sh script:<ul>
<li>With static structured light mode (the following command is optional if it has run in Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_initialize">2.2 Initialize the System and Video Pipeline</a>). <div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
</div><!-- fragment --></li>
<li>With dynamic structured light mode. <div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720 --enable_dynamic_mode</div>
</div><!-- fragment --></li>
</ul>
</li>
<li>Run the Demo:<ul>
<li>Run the demo to calculate the depth-of-face areas in the 1080p structured-light image. <div class="fragment"><div class="line">board # /tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh high_res</div>
</div><!-- fragment --></li>
<li>Run the demo to calculate the depth of the full image in the 480x272(Janus) or the 480x376(Vision+) structured-light image. <div class="fragment"><div class="line">board # /tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh low_res</div>
</div><!-- fragment --></li>
</ul>
</li>
<li>Register a new face in the database:<ul>
<li>On the keyboard, press <code>f</code> to initiate the program.</li>
<li>Select option <code>2</code> and enter a name. <div class="fragment"><div class="line">f</div>
<div class="line">Anything to <span class="keywordflow">do</span> on the face database?</div>
<div class="line">1. Show database information.</div>
<div class="line">2. Record a <span class="keyword">new</span> face.</div>
<div class="line">2</div>
<div class="line">Start recording 9 views of a person<span class="stringliteral">&#39;s face.</span></div>
<div class="line"><span class="stringliteral">Input name no more than 31 (&#39;</span>q<span class="stringliteral">&#39; to exit):</span></div>
<div class="line"><span class="stringliteral">Ambaman</span></div>
</div><!-- fragment --></li>
<li>Position the face in one of the nine orientations, and type <code>r</code> to repeat another eight times. <div class="image">
<img src="../../acc_ctrl_face_record.png" alt=""/>
<div class="caption">
Figure 2-2. Face Positions While Recording a Face Database.</div></div>
 <div class="fragment"><div class="line">Type <span class="charliteral">&#39;r&#39;</span> to record view 1 in 9 of Ambaman</div>
<div class="line">r</div>
<div class="line">…</div>
<div class="line">Type <span class="charliteral">&#39;r&#39;</span> to record view 9 in 9 of Ambaman</div>
<div class="line">r</div>
<div class="line">face database saved in /usr/local/bin/data/blade_runner_bin/database.bin.</div>
</div><!-- fragment --></li>
</ul>
</li>
<li>View the results on the livestream. The following three types of information will be shown in the results.<ul>
<li>Bounding boxes with different colors:<ul>
<li>Green: Live person</li>
<li>Red: Not a live person</li>
<li>Yellow: Invalid distance; no depth information</li>
</ul>
</li>
<li>Person’s name:<ul>
<li>Name of the person registered in the database</li>
</ul>
</li>
<li>Distance: <div class="fragment"><div class="line">[Notice]: det: 2</div>
<div class="line">[Notice]: ++++++++ Ambaman 0.897497, liveness 1, distance 0.422252m</div>
<div class="line">[Notice]: ++++++++ Ambaman 0.704975, liveness 0, distance 0.333818m</div>
</div><!-- fragment --> <br  />
 <div class="image">
<img src="../../acc_ctrl_face_depth.png" alt=""/>
<div class="caption">
Figure 2-3. Liveness Detection by Calculating the Depth-of-Face Areas.</div></div>
 <br  />
 <div class="image">
<img src="../../acc_ctrl_full_depth.png" alt=""/>
<div class="caption">
Figure 2-4. Liveness Detection by Calculating the Depth on the Full Structured-Light Image.</div></div>
 <br  />
</li>
</ul>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="acc_ctrl_soft_design"></a>
3. Software Design</h1>
<p >The access control EVK module can control the structured-light projector and the IR LED by generating an IR image with a structured-light pattern and an IR image with the IR LED light. The module can also simultaneously retrieve the RGB image. The IR image, with the structured-light pattern, can generate the depth image while the IR image, without the structured-light pattern, can provide face detection or recognition in “night” mode; alternatively, the RGB image can be used for face detection and recognition in “day” mode. Further, users can add an additional 940-nm IR-LED or a white LED for illumination adjustments.</p>
<h2><a class="anchor" id="acc_ctrl_sl_control"></a>
3.1 Structured-Light Control</h2>
<p >The data flow is configured by the image audio video (IAV) / sensor driver and the image digital signal processor (IDSP). The driver turns on the structured-light pattern when the sensor begins the exposure process. The typical use case does not require full frame rate IR data with the structured-light pattern, as the default sequence of structured-light pattern is 000001000001000001…. This turns on the structured light once every six frames.</p>
<div class="image">
<img src="../../acc_ctrl_sensor_sl.png" alt=""/>
<div class="caption">
Figure 3-1. Sensor Exposure and Structured Light.</div></div>
<p >IDSP splits the input data into two IR channels and one RGB channel. The video input (VIN) frames per second (fps) is equal to the sum of the fps for each of the three channels.</p>
<div class="image">
<img src="../../acc_ctrl_sensor_sl_control.png" alt=""/>
<div class="caption">
Figure 3-2. Structured Light and the RGB-IR Sensor Control.</div></div>
<h2><a class="anchor" id="acc_ctrl_data_flow"></a>
3.2 RGB and IR Data Flow for Streaming</h2>
<p >This section illustrates the data flow of the input IR and RGB channels.</p>
<p >First, the data moves to the main buffer of each channel, and then the valid structured-light data is cropped from channel 0.main. Depending on the hardware, the valid IR size and offset will vary. For example, Janus has an offset of (320,160) and a cropped size of (1280x720); Vision+ has an offset of (640, 0) and a cropped size of (2560, 2000). Check the setting files used by <code>test_encode</code>.</p>
<div class="image">
<img src="../../acc_ctrl_janus_data_flow.png" alt=""/>
<div class="caption">
Figure 3-3. Janus Data Flow of Three Channels.</div></div>
<p> <br  />
 </p><div class="image">
<img src="../../acc_ctrl_vision_plus_data_flow.png" alt=""/>
<div class="caption">
Figure 3-4. Vision+ Data Flow of Three Channels.</div></div>
<p> <br  />
</p>
<h2><a class="anchor" id="acc_ctrl_adv_feature"></a>
3.3 Advanced Features</h2>
<p >This section explains the advanced features of the access control product. </p>
<h3><a class="anchor" id="acc_ctrl_dyn_trigger"></a>
3.3.1 Dynamic Trigger of Structured Light</h3>
<p >Dynamic trigger mode is used to fully control the structured light by customer application. It includes controlling the light point of SL (Structured Light), adjusting the light duration of SL.</p>
<p >Dynamic trigger mode is enabled when the system enters the preview stage. This can be performed by setting the frame ratio of the SL channel to 0. The <code>FRAME_RATIO</code> value is defined in the board configuration file, and passed to the <code>test_encode</code> command via the <code>custom-chan-frame-ratio</code> option.</p>
<p >After dynamic trigger mode is enabled, the user gets the fully control to the SL. The SL control can be accessed by <code>IAV_IOC_CUSTOM_CMDS ioctl</code>.</p><ol type="1">
<li><code>flash_led_custom_trigger_once</code> enables SL on the next single frame.</li>
<li><code>flash_led_min_time_in_ms</code> sets the minimum SL light duration in miliseconds.</li>
<li><code>flash_led_line_offset</code> defines the SL light point in the line index. The range is [0, max line number].</li>
<li><code>flash_time_in_ms</code> configures the SL light duration in miliseconds.</li>
<li><code>ir_led_time_in_ms</code> configures the IR LED light duration in miliseconds.</li>
<li><code>flash_led_update_flag_bitmap</code> defines which parameters are updated in the IAV driver.</li>
</ol>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code hl_structRef" href="../../../library/d8/d78/structiav__custom__cfg.html">iav_custom_cfg</a> custom_cfg;</div>
<div class="line"><span class="keyword">struct </span>iav_custom_vin_led_ctrl *vin_led_ctrl = &amp;custom_cfg.<a class="code hl_variableRef" href="../../../library/d8/d78/structiav__custom__cfg.html#acdb1f1b09e03c35ac10076a199dbf643">arg</a>.vin_led_ctrl;</div>
<div class="line">memset(&amp;custom_cfg, 0, <span class="keyword">sizeof</span>(custom_cfg));</div>
<div class="line">vin_led_ctrl-&gt;flash_led_custom_trigger_once = 1;</div>
<div class="line">vin_led_ctrl-&gt;flash_led_min_time_in_ms = 5;</div>
<div class="line">vin_led_ctrl-&gt;flash_led_line_offset = 180;</div>
<div class="line">vin_led_ctrl-&gt;flash_time_in_ms = 10;</div>
<div class="line">vin_led_ctrl-&gt;ir_led_time_in_ms = 10;</div>
<div class="line">vin_led_ctrl-&gt;flash_led_update_flag_bitmap =</div>
<div class="line">    IAV_CUSTOM_LED_FLASH_LINE_OFFSET_UPDATE_FLAG |</div>
<div class="line">    IAV_CUSTOM_LED_FLASH_TRIGGER_ONCE_UPDATE_FLAG |</div>
<div class="line">    IAV_CUSTOM_LED_FLASH_MIN_TIME_UPDATE_FLAG |</div>
<div class="line">    IAV_CUSTOM_LED_FLASH_TIME_UPDATE_FLAG |</div>
<div class="line">    IAV_CUSTOM_LED_IR_FLOOD_FLASH_TIME_UPDATE_FLAG;</div>
<div class="line">custom_cfg.cmd_code = IAV_CUSTOMCMD_CFG_VIN_LED_RT_CTRL;</div>
<div class="line">ioctl(fd_iav, IAV_IOC_CUSTOM_CMDS, &amp;custom_cfg);</div>
<div class="ttc" id="astructiav__custom__cfg_html"><div class="ttname"><a href="../../../library/d8/d78/structiav__custom__cfg.html">iav_custom_cfg</a></div></div>
<div class="ttc" id="astructiav__custom__cfg_html_acdb1f1b09e03c35ac10076a199dbf643"><div class="ttname"><a href="../../../library/d8/d78/structiav__custom__cfg.html#acdb1f1b09e03c35ac10076a199dbf643">iav_custom_cfg::arg</a></div><div class="ttdeci">union iav_custom_cfg::@87 arg</div></div>
</div><!-- fragment --><h3><a class="anchor" id="acc_ctrl_ai_metering"></a>
3.3.2 AI Metering Mode</h3>
<p >AI metering mode allows the user to control the exposure based on a region of interest (ROI) input. The ROI input can be calculated using a different CV algorithm, such as face detection, person detection, and so on. By enabling this mode, the interested areas will always be well exposed and will not be interfered by background.</p>
<p >The Ambarella SDK provides an example of the AI Metering feature. The code below demonstrates the steps to run the AI metering feature on Janus. To run on Vision+, adjust rgb_sub_buf_out_res value to 1280x1000.</p>
<ol type="1">
<li>Initialize the system with AI Metering enabled: <div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720 --enable_dynamic_mode --enable_ai_metering</div>
</div><!-- fragment --></li>
<li>Run liveness demo application: <div class="fragment"><div class="line">board # /tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh</div>
</div><!-- fragment --></li>
</ol>
<h3><a class="anchor" id="acc_ctrl_depth_option"></a>
3.3.3 Depth Generation Method Selection</h3>
<p >The Ambarella SDK includes two depth generation methods: Blade Runner and Fast Block Match. Blade Runner runs on CPU only, while Fast Block Match mainly runs on VP. Due to their properties, they are proficient in VP-intensive and CPU-intensive systems.</p>
<p >Demo applications using Blade Runner:</p><ol type="1">
<li>test_blade_runner</li>
<li>test_sl_liveness_v2</li>
</ol>
<p >Demo applications using Fast Block Match:</p><ol type="1">
<li>test_sl_liveness_v2</li>
</ol>
<p >The Blade Runner algorithm is selected for all applications by default. In order to select the depth generation method in the <code>test_sl_liveness_v2</code> application, the <code>depth_algo_type</code> option will be used. A value of 0 refers to the Blade Runner method, and a value of 1 refers to Fast Block Match method. Further, the <code>fbm_config</code> option must be specified. Refer to Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_test_sl_liveness">4.5 Unit_test: test_sl_liveness_v2</a> for more details on these options.</p>
<p >The following configuration requires to be selected for FBM feature. </p><div class="fragment"><div class="line">[*] Ambarella Application Configuration  ---&gt;</div>
<div class="line">    [*]   Build Access Control Apps  ---&gt;</div>
<div class="line">        [*]   Build Structured Light Liveness V2 FBM Apps</div>
</div><!-- fragment --><p >The demos for using the Fast Block Match method on <code>test_sl_liveness_v2</code> can run with the commands below.</p>
<p >For Janus: </p><div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
<div class="line">board # /tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh fbm_enable</div>
</div><!-- fragment --><p> For Vision+: </p><div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720 --rgb_sub_buf_in_param 640,320,2560,1440</div>
<div class="line">board # /tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh fbm_enable</div>
</div><!-- fragment --><h2><a class="anchor" id="acc_ctrl_algo_data_flow"></a>
3.4 IR / RGB Data Flow for the Algorithm Library</h2>
<p >The digital signal processor (DSP) splits the IR / RGB data from one RGB-IR sensor.</p>
<p >The Ambarella IR process library can generate the depth and point cloud images from the IR structured-light pattern, which includes two main functions: (1) generating a disparity image, depth image, and point cloud data and (2) performing the liveness test. Users may refer to the documents in the SDK for additional details. </p><div class="fragment"><div class="line">build $ make sdk_doc</div>
<div class="line">make sdk_doc</div>
<div class="line">other_feature.h is formed.</div>
<div class="line">advanced_feature.h is formed.</div>
<div class="line">basic_feature.h is formed.</div>
<div class="line">[Succeeded] Generating document of feature_sets!</div>
<div class="line">[Succeeded] Generating document of bpi_app!</div>
<div class="line">amba_doc.h is generated.</div>
<div class="line">amba.doxygen is formed.</div>
<div class="line">[Succeeded] Generating document of amba!</div>
<div class="line">All documents are generated...</div>
</div><!-- fragment --><p >Refer to page_lib_blade_runner_doc for more details.</p>
<p >The demo <code>FDFR + liveness test</code> flow is shown below. The IR process library checks the depth information with the FD result for the liveness checking. <br  />
 </p><div class="image">
<img src="../../acc_ctrl_algo_data_flow.png" alt=""/>
<div class="caption">
Figure 3-5. IR / RGB Data Flow for the Algorithm Library.</div></div>
<h2><a class="anchor" id="acc_ctrl_board_config"></a>
3.5 Customize the Board Configuration File</h2>
<p >The <code>board_config</code> file is generated by the gen_board_config.sh shell, and specifies the system and hardware configuration data for each access control board. Each time the hardware calibration data is updated, the <code>board_config</code> file must be regenerated.</p>
<p >By default, the most commonly-used system configuration is selected in the <code>board_config</code> file. It uses a three-channel system for structured light, RGB, and IR LED, respectively. Additionally, the frame ratio for each channel is 5:20:5. The user is permitted to change the parameters within the <code>board_config</code> in order to customize the system. For example, <code>CHANNEL_AMOUNT</code> can be set to 2 for disabling the third IR LED channel. For more details, refer to the comments within the <code>board_config</code> file.</p>
<div class="fragment"><div class="line">CV2x # gen_board_config.sh /usr/share/ambarella/calib_mono H210615A001</div>
<div class="line">BOARD_CONFIG_VERSION=1.0.0</div>
<div class="line">MODULE_ID=1912060033 # Sensor module ID</div>
<div class="line"> </div>
<div class="line"># File Name and Path Configurations</div>
<div class="line">CALIB_MONO_PATH=/usr/share/ambarella/calib_mono</div>
<div class="line">CONFIG_FILE_PATH=/usr/share/ambarella/calib_mono/board_config</div>
<div class="line">CALIB_DATA_SHA256SUM_FILE=/usr/share/ambarella/calib_mono/calib_data_sha256.txt</div>
<div class="line">AAA_LUA_PATH=/usr/share/ambarella/idsp/product_id_authen_dn_switch_limit_shutter.lua</div>
<div class="line">DYNAMIC_MODE_AAA_LUA_PATH=/usr/share/ambarella/idsp/product_id_authen_dn_switch.lua</div>
<div class="line"> </div>
<div class="line"># User Configurations - Structured Light Control Settings</div>
<div class="line">CHANNEL_AMOUNT=3 # Configure the number of channels being used in the range of [2, 3].</div>
<div class="line">              # 2-channel mode includes RGB channel and Structured light channel.</div>
<div class="line">              # 3-channel mode additionaly enables IR LED channel.</div>
<div class="line"># [NOTE] Changes to FRAME_RATIO requires to update channels&#39; idsp_fps in IAV script luas accordingly.</div>
<div class="line">if [ $CHANNEL_AMOUNT -eq 3 ]; then</div>
<div class="line">FRAME_RATIO=1:4:1 # Frame ratio for each channel. Chan 2 frame ratio : Chan 1 frame ratio : Chan 0 frame ratio</div>
<div class="line">elif [ $CHANNEL_AMOUNT -eq 2 ]; then</div>
<div class="line">FRAME_RATIO=4:1 # Frame ratio for each channel. Chan 1 frame ratio : Chan 0 frame ratio</div>
<div class="line">else</div>
<div class="line">echo -e &quot;Unsupported Channel Number!&quot;</div>
<div class="line">fi</div>
<div class="line">STRUCTURED_LIGHT_FLASH_TIME=10 # Structured light flash time in miliseconds</div>
<div class="line">FLASH_START_OFFSET=0 # Adjust structured light flash start offset in lines for dynamic trigger mode. Range: [-(vin height), (vin height)];</div>
<div class="line">FLASH_START_MULTIPLIER=1 # Adjust structured light flash start line&#39;s multiplier.</div>
<div class="line"> </div>
<div class="line"># User Configurations - System and Physical Parameters</div>
<div class="line">BASELINE=0.04 # The distance between structured light and sensor in meter</div>
<div class="line">SL_MAIN_BUF_OUT_WIDTH=1920 # The width of main buffer output in structured light channel</div>
<div class="line">SL_MAIN_BUF_OUT_HEIGHT=1080 # The height of main buffer output in structured light channel</div>
<div class="line">SL_SUB_BUF_OUT_WIDTH=480 # The width of sub buffer output in structured light channel</div>
<div class="line">SL_SUB_BUF_OUT_HEIGHT=272 # The height of sub buffer output in structured light channel</div>
<div class="line">RGB_MAIN_BUF_OUT_WIDTH=1920 # The width of main buffer output in rgb channel</div>
<div class="line">RGB_MAIN_BUF_OUT_HEIGHT=1080 # The height of main buffer output in rgb channel</div>
<div class="line">RGB_SUB_BUF_OUT_WIDTH=1280 # The width of sub buffer output in rgb channel</div>
<div class="line">RGB_SUB_BUF_OUT_HEIGHT=720 # The height of sub buffer output in rgb channel</div>
<div class="line">IR_MAIN_BUF_OUT_WIDTH=1920 # The width of main buffer output in ir channel</div>
<div class="line">IR_MAIN_BUF_OUT_HEIGHT=1080 # The height of main buffer output in ir channel</div>
<div class="line">CROP_ROI_BASE_WIDTH=1920 # The width of crop source</div>
<div class="line">CROP_ROI_BASE_HEIGHT=1080 # The height of crop source</div>
<div class="line">CROP_ROI_MIN_OFFSET_X=320 # The minimum x offset of valid structured light pattern area</div>
<div class="line">CROP_ROI_MIN_OFFSET_Y=160 # The minimum y offset of valid structured light pattern area</div>
<div class="line">CROP_ROI_MAX_WIDTH=1280 # The maximum width of valid structured light pattern area</div>
<div class="line">CROP_ROI_MAX_HEIGHT=720 # The maximum height of valid structured light pattern area</div>
<div class="line"> </div>
<div class="line"># User Configurations - Blade Runner Parameter Settings</div>
<div class="line">CV_MODEL_DIR=/usr/local/bin/data/blade_runner_bin/</div>
<div class="line">DISP_SMOOTH_LOW_RES=8 # Disparity smooth value for using high resolution ref</div>
<div class="line">DISP_RANGE_LOW_RES=4 # Disparity range value for using high resolution ref</div>
<div class="line">DISP_SMOOTH_HIGH_RES=13 # Disparity smooth value for using low resolution ref</div>
<div class="line">DISP_RANGE_HIGH_RES=6 # Disparity range value for using low resolution ref</div>
<div class="line"> </div>
<div class="line"># Module Settings (Automatically Generated)</div>
<div class="line">REF_YUV_DIST=1.475 # The distance from the YUV being captured to sensor</div>
<div class="line">ROTATE_MODE=1 # Rotate operation depends on orientation of structured light and sensor</div>
<div class="line"> </div>
<div class="line"># Module settings for using low resolustion reference YUV (Automatically Generated)</div>
<div class="line">REF_YUV_PATH=/usr/share/ambarella/calib_mono/ref_b2_H1912060033_canvas2_480x272_NV12_1.475.yuv # Reference YUV file path</div>
<div class="line">REF_YUV_WIDTH=480 # The width of selected reference yuv</div>
<div class="line">REF_YUV_HEIGHT=272 # The height of selected reference yuv</div>
<div class="line">REF_YUV_PITCH=480 # The pitch of selected reference yuv</div>
<div class="line">FOCAL_LENGTH=306.75 # Focal length in pixels</div>
<div class="line">CENTER_X_COORD=239.25 # Coordinates of center X in pixels</div>
<div class="line">CENTER_Y_COORD=134.25 # Coordinates of cneter Y in pixels</div>
<div class="line">REF_YUV_OFFSET=0 # Reference YUV transition offset</div>
<div class="line">DISP_SMOOTH=8 # Disparity smooth value for selected ref</div>
<div class="line">DISP_RANGE=4 # Disparity range value for selected ref</div>
<div class="line">CROP_OFFSET_X=0 # X offset of crop area within reference yuv</div>
<div class="line">CROP_OFFSET_Y=0 # Y offset of crop area within reference yuv</div>
<div class="line">DEPTH_FACE_ONLY=&quot;&quot; # depth map for face only in liveness demo</div>
</div><!-- fragment --><h2><a class="anchor" id="acc_ctrl_quick_ipc_janus"></a>
3.6 Demo Preparation ― Quick IPC on CV25M Janus</h2>
<p >After the <code>board_config</code> is generated, <code>quick_ipc_for_access_control.sh</code> can be used to set up the IAV environment, including channels, canvases, and streams. It includes the commands <code>test_aaa_service</code> and <code>test_encode</code>.</p>
<h3><a class="anchor" id="acc_ctrl_quick_ipc_janus_options"></a>
3.6.1 Options</h3>
<table class="doxtable">
<caption>quick_ipc_for_access_control.sh for Janus Usages.</caption>
<tr>
<th>Short Options </th><th>Long Options </th><th>Usages </th></tr>
<tr>
<td colspan="3">Vin </td></tr>
<tr>
<td></td><td>&ndash;vin_mode </td><td>Specifies the high dynamic range (HDR) mode, such as "linear" and "2x". The default is "linear". </td></tr>
<tr>
<td></td><td>&ndash;vsrc_fps </td><td>Specifies the sensor frame rate; the default is 30. </td></tr>
<tr>
<td></td><td>&ndash;enable_dynamic_mode </td><td>Enables the dynamic mode; the default is disabled. No parameters are required. </td></tr>
<tr>
<td colspan="3">3A </td></tr>
<tr>
<td></td><td>&ndash;enable_ai_metering </td><td>Enables AI metering mode; the default is disabled. No parameters are required. It allows AE / AWB / AF (3A) to focus on face exposure. </td></tr>
<tr>
<td colspan="3">Source Buffer </td></tr>
<tr>
<td></td><td>&ndash;sl_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the SL channel. The default is "1920x1080". </td></tr>
<tr>
<td></td><td>&ndash;sl_sub_buf_in_param </td><td>Specifies the input crop parameters for the sub-buffer (second buffer) of the SL channel, which is cropped from the main buffer by the digital signal processor (DSP). The format of parameters is "offset_x, offset_y, width, and height". For example: "--sl_sub_buf_in_param 320,160,1280,720". </td></tr>
<tr>
<td></td><td>&ndash;sl_sub_buf_out_res </td><td>Specifies the output resolution for the sub-buffer (second buffer) of the SL channel, which must be smaller than the resolution that is defined in "sl_sub_buf_in_param" as DSP only supports the downscale. For example: "--sl_sub_buf_out_res 480x272". </td></tr>
<tr>
<td></td><td>&ndash;rgb_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the RGB channel. Normally, this buffer is for encoding to show the algorithm result. The default is "1920x1080". </td></tr>
<tr>
<td></td><td>&ndash;rgb_sub_buf_in_param </td><td>Specifies the input crop parameters for the sub-buffer (third buffer) of the RGB channel, which is cropped from the main buffer by the DSP. The format of parameters is "offset_x, offset_y, width, and height". For example: "--rgb_sub_buf_in_param 320,160,1280,720". The default is "320, 160, 1280, and 720". </td></tr>
<tr>
<td></td><td>&ndash;rgb_sub_buf_out_res </td><td>Specifies the output resolution for the sub-buffer (third buffer) of the RGB channel, which must be smaller than the resolution that is defined in "rgb_sub_buf_in_param" as DSP only supports the downscale. Normally, this buffer is for the face detection. The default is "1280x720". </td></tr>
<tr>
<td></td><td>&ndash;ir_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the IR channel. The default is "1920x1080". </td></tr>
<tr>
<td colspan="3">System </td></tr>
<tr>
<td></td><td>&ndash;realloc_mem </td><td>Reallocates the memory part size. It only supports "overlay" and "blur". Only one type can be reallocated at a time. For example: "--realloc_mem overlay,0x04009000" or "--realloc_mem blur, 0x04009000". </td></tr>
<tr>
<td></td><td>&ndash;best_performance </td><td>Disables unnecessary streams and the canvas buffer. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;customized_lua </td><td>Uses a customized Lua. For example: "--customized_lua /root/customized.lua". </td></tr>
<tr>
<td colspan="3">Debug </td></tr>
<tr>
<td>-h </td><td>&ndash;help </td><td>Prints help information. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;disable_summary_info </td><td>Does not print summary information. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;save_path </td><td>Saves the generated files to specify the directory. The default is "/tmp/run_ipc_folder". </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_quick_ipc_janus_config"></a>
3.6.2 Configurations</h3>
<p >Default with three channels </p><div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 1, from the RGB channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 2, from the IR channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 3, from the SL channel second buffer, the resolution is: [480x272]</li>
<li>Canvas 4, from the RGB channel third buffer, the resolution is: [1280x720]</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for SL, from canvas 0, the resolution is: [1920x1080] (the same as canvas)</li>
<li>Stream B for RGB, from canvas 1, the resolution is: [1920x1080] (the same as canvas)</li>
<li>Stream C for IR, from canvas 2, the resolution is: [1920x1080] (the same as canvas)</li>
<li>Stream D is reserved in the generated Lua file. It is not enabled by default. <br  />
</li>
</ul>
<p >Best performance with three channels </p><div class="fragment"><div class="line">quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720 --best_performance</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 1, from the RGB channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 2, from the IR channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 3, from the RGB channel third buffer, the resolution is : [1280x720]</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for RGB, from canvas 1, the resolution is: [1920x1080] (the same as canvas) <br  />
</li>
</ul>
<p >Two channels </p><div class="fragment"><div class="line">board # vi /usr/share/ambarella/calib_mono/board_config</div>
<div class="line">CHANNEL_AMOUNT=2</div>
<div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel main buffer , the resolution is: [1920x1080]</li>
<li>Canvas 1, from the RGB channel main buffer, the resolution is: [1920x1080]</li>
<li>Canvas 2, from the SL channel second buffer, the resolution is: [480x272]</li>
<li>Canvas 3, from the RGB third channel buffer, the resolution is: [1280x720]</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for SL, from canvas 0, the resolution is: [1920x1080] (the same as canvas)</li>
<li>Stream B for RGB, from canvas 1, the resolution is: [1920x1080] (the same as canvas)</li>
</ul>
<h2><a class="anchor" id="acc_ctrl_quick_ipc_vision_plus"></a>
3.7 Demo Preparation ― Quick IPC on CV22 Vision+</h2>
<p >After the <code>board_config</code> is generated, <code>quick_ipc_for_access_control.sh</code> can be used to set up the IAV environment, including channels, canvases, and streams. It includes the commands <code>test_aaa_service</code> and <code>test_encode</code>.</p>
<h3><a class="anchor" id="acc_ctrl_quick_ipc_vision_plus_options"></a>
3.7.1 Options</h3>
<table class="doxtable">
<caption>quick_ipc_for_access_control.sh for Vision+ Usages.</caption>
<tr>
<th>Short Options </th><th>Long Options </th><th>Usages </th></tr>
<tr>
<td colspan="3">Vin </td></tr>
<tr>
<td></td><td>&ndash;vin_mode </td><td>Specifies the HDR mode, such as "linear" or "2x". The default is "linear". </td></tr>
<tr>
<td></td><td>&ndash;enable_dynamic_mode </td><td>Enables dynamic mode. The default is disabled. No parameters are required. </td></tr>
<tr>
<td colspan="3">3A </td></tr>
<tr>
<td></td><td>&ndash;enable_ai_metering </td><td>Enables AI metering mode; the default is disabled. No parameters are required. It allows 3A to focus on face exposure. </td></tr>
<tr>
<td colspan="3">Source Buffer </td></tr>
<tr>
<td></td><td>&ndash;sl_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the SL channel. The default is "3840x2160". </td></tr>
<tr>
<td></td><td>&ndash;sl_sub_buf_in_param </td><td>Specifies the input crop parameters for the sub-buffer (second buffer) of the SL channel, which is cropped from the main buffer by the DSP. The format of parameters is "offset_x,offset_y,width,hight". For example: "--sl_sub_buf_in_param 320,160,1280, and 720". </td></tr>
<tr>
<td></td><td>&ndash;sl_sub_buf_out_res </td><td>Specifies the output resolution for the sub-buffer (second buffer) of the SL channel, which must be smaller than the resolution that is defined in "sl_sub_buf_in_param" as the DSP only supports the downscale. For example: "--sl_sub_buf_out_res 480x272". </td></tr>
<tr>
<td></td><td>&ndash;rgb_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the RGB channel. Normally, this buffer is for encoding to show the algorithm result. The default is "3840x2160". </td></tr>
<tr>
<td></td><td>&ndash;rgb_sub_buf_in_param </td><td>Specifies the input crop parameters for the sub-buffer (third buffer) of the RGB channel, which is cropped from the main buffer by the DSP. The format of parameters is "offset_x,offset_y,width,hight". For example: "--rgb_sub_buf_in_param 640,0,2560,2000". The default is "640, 0, 2560, and 2000". </td></tr>
<tr>
<td></td><td>&ndash;rgb_sub_buf_out_res </td><td>Specifies the output resolution for the sub-buffer (third buffer) of the RGB channel, which must be smaller than the resolution that is defined in "rgb_sub_buf_in_param" as the DSP only supports the downscale. Normally, this buffer is for face detection. The default is "1280x1000". </td></tr>
<tr>
<td></td><td>&ndash;ir_main_buf_out_res </td><td>Specifies the output resolution of the main buffer of the IR channel. The default is "3840x2160". </td></tr>
<tr>
<td colspan="3">System </td></tr>
<tr>
<td></td><td>&ndash;realloc_mem </td><td>Reallocates the memory part size. It only supports "overlay" and "blur". Only one type can be reallocated at a time. For example: "--realloc_mem overlay,0x04009000" or "--realloc_mem blur, 0x04009000". </td></tr>
<tr>
<td></td><td>&ndash;best_performance </td><td>Disables unnecessary streams and the canvas buffer. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;customized_lua </td><td>Uses a customized Lua. For example: "--customized_lua /root/customized.lua". </td></tr>
<tr>
<td colspan="3">Debug </td></tr>
<tr>
<td></td><td>&ndash;help </td><td>Prints help information. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;disable_summary_info </td><td>Does not print summary information. No parameters are required. </td></tr>
<tr>
<td></td><td>&ndash;save_path </td><td>Saves the generated files to specify the directory. The default is "/tmp/run_ipc_folder". </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_quick_ipc_vision_plus_config"></a>
3.7.2 Configurations</h3>
<p >Default with three channels </p><div class="fragment"><div class="line">quick_ipc_for_access_control.sh --rgb_sub_buf_out_res</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel third buffer, the resolution is [1920x1080]</li>
<li>For the stream encoded from canvas 0, the resolution is 1080p when the main buffer is 1080p. If the main buffer is less than 1080p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 1, from the RGB channel second buffer, the resolution is: [1920x1080]</li>
<li>For the stream encoded from canvas 1, the resolution is 1080p when the main buffer is 4K. If the main buffer is in (480p, 4K), the resolution will be set to 480p. If the main buffer is less than 480p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 2, from the IR channel second buffer, the resolution is [1920x1080]</li>
<li>For the stream encoded from canvas 2, the resolution is 1080p when the main buffer is 4K. If the main buffer is in (480p, 4K), the resolution will be set to 480p. If the main buffer is less than 480p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 3, from the SL channel second buffer, the resolution is [480x376]</li>
<li>Canvas 4, from the RGB channel third buffer, the resolution is [1280x1000]</li>
<li>Canvas 5, from the SL channel main buffer, the resolution is [3840x2160], disable_yuv_dram=1 in Lua</li>
<li>Canvas 6, from the RGB channel main buffer, the resolution is [3840x2160]</li>
<li>Canvas 7, from the IR channel main buffer, the resolution is [3840x2160], disable_yuv_dram=1 in Lua</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for SL, from canvas 0, the resolution is [1920x1080] (the same as canvas)</li>
<li>Stream B for RGB, from canvas 1, the resolution is [1920x1080] (the same as canvas)</li>
<li>Stream C for IR, from canvas 2, the resolution is [1920x1080] (the same as canvas)</li>
<li>Stream D is reserved in the generated Lua file. It is not enabled by default. <br  />
</li>
</ul>
<p >Best performance with three channels </p><div class="fragment"><div class="line">quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x1000 --best_performance</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel third buffer, the resolution is [1920x1080]</li>
<li>Canvas 1, from the RGB channel second buffer, the resolution is [1920x1080]</li>
<li>Canvas 2, from the IR channel second buffer, the resolution is [1920x1080]</li>
<li>For the stream encoded from canvas 0, 1, and 2, the resolution is 1080p when the main buffer is 4K. If the main buffer is in (480p, 4K), the resolution will be set to 480p. If the main buffer is less than 480p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 3, from the RGB third buffer, the resolution is [1280x1000]</li>
<li>Canvas 4, from the SL channel main buffer , the resolution is [3840x2160], disable_yuv_dram=1 in Lua</li>
<li>Canvas 5, from the RGB channel main buffer , the resolution is [3840x2160], disable_yuv_dram=1 in Lua</li>
<li>Canvas 6, from the IR channel main buffer , the resolution is [3840x2160], disable_yuv_dram=1 in Lua</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for RGB, from canvas 1, the resolution is [1920x1080] (the same as canvas) <br  />
</li>
</ul>
<p >Two channels </p><div class="fragment"><div class="line">board # vi /usr/share/ambarella/calib_mono/board_config</div>
<div class="line">CHANNEL_AMOUNT=2</div>
<div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x1000</div>
</div><!-- fragment --><p> <b>Canvas Information</b></p><ul>
<li>Canvas 0, from the SL channel third buffer, the resolution is [1920x1080]</li>
<li>For the stream encoded from canvas 0, the resolution is 1080p when the main buffer is 1080p. If the main buffer is less than 1080p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 1, from the RGB channel second buffer, the resolution is [1920x1080]</li>
<li>For the stream encoded from canvas 1, the resolution is 1080p when the main buffer is 4K. If the main buffer is in (480p, 4K), the resolution will be set to 480p. If the main buffer is less than 480p, the resolution will be set to be the same as the main buffer.</li>
<li>Canvas 2, from the SL channel second buffer, the resolution is [480x376]</li>
<li>Canvas 3, from the RGB channel third buffer, the resolution is [1280x1000]</li>
<li>Canvas 4, from the SL channel main buffer, the resolution is [3840x2160] disable_yuv_dram=1 in Lua</li>
<li>Canvas 5, from the RGB channel main buffer, the resolution is [3840x2160]</li>
</ul>
<p ><b>Stream Information</b></p><ul>
<li>Stream A for SL, from canvas 0, the resolution is [1920x1080] (the same as canvas)</li>
<li>Stream B for RGB, from canvas 1, the resolution is [1920x1080] (the same as canvas)</li>
</ul>
<h2><a class="anchor" id="acc_ctrl_sl_liveness_demo"></a>
3.8 Software Design of test_sl_liveness_v2</h2>
<p >This section provides details on the framework of <code>test_sl_liveness_v2</code> and how user customizations can be applied onto <code>test_sl_liveness_v2</code>. </p>
<h3><a class="anchor" id="acc_ctrl_sl_liveness_data_flow"></a>
3.8.1 Data Flow of the test_sl_liveness_v2 Demo</h3>
<p >The demonstration has im, fd, fr, ld, vl, and main threads.</p><ul>
<li>im: Image thread to query the colored YUV images and the structured-light images.</li>
<li>fd: Face detection thread to run the face detection algorithm.</li>
<li>fr: Face recognition thread to run the face recognition algorithm.</li>
<li>ld: Liveness detection thread to run the liveness detection with the Blade Runner and / or Fast Block Match algorithm.</li>
<li>vl: Visualize thread to draw the result on the stream.</li>
<li>main: The thread to initialize and use data to connect the other threads.</li>
</ul>
<div class="image">
<img src="../../acc_ctrl_sl_liveness_data_flow.png" alt=""/>
<div class="caption">
Figure 3-6. test_sl_liveness_v2 Multi-Thread Data Flow.</div></div>
<p >The data flow includes image data, face detection results, face recognition results, and liveness detection results; it begins with the image data, proceeding through each thread to generate more data along the way.</p>
<p >There are two types of image data queried by the im thread from the IAV driver: the colored YUV images and the structured-light images.</p><ul>
<li>Colored YUV images</li>
</ul>
<p >The colored YUV images are sampled when the structured light is off, and are used for face detection and face recognition in the fd and fr threads.</p><ul>
<li>Structured-light images</li>
</ul>
<p >The structured-light images are sampled when the structured light is on, and are used for liveness detection with the Blade Runner and / or Fast Block Match algorithm.</p>
<p >The image data follows the data flow path, so all threads can use the image data. After being generated by the im thread, the image data will return to the im thread and be released by the thread.</p>
<p >If the structured light is turned on dynamically, the face detection result returns to the im thread to control the query of the structured-light images. If any faces are detected, the im thread will turn on the light to query the structured-light images. If there is no face detected, the im thread will stop the querying process.</p>
<p >These threads can enable several frames of images in parallel, and handle the same frame of an image in order.</p><ul>
<li>Depending on the size of each queue and the performance of each algorithm, several frames may remain in the data flow.</li>
<li>The im thread will discard the old image data in the queue if the algorithm cannot manage it in a timely manner.</li>
<li>If the queue size is large, the number of frames to manage per second will be greater, so the delay from the generation of the image to the final detection result will be longer. Therefore, to reduce the delay in the demo, the queue size is limited and the new feeding of the fd thread will wait for the fr and id threads to complete their functions.</li>
</ul>
<h3><a class="anchor" id="acc_ctrl_sl_livenss_customize"></a>
3.8.2 Customize the Demo</h3>
<p >The source code of the demo is placed under <code>ambarella/app/access_control/sl_liveness_v2</code> in the SDK. The following examples demostrate how to customize the demo by changing the source code.</p><ol type="1">
<li>Replace the face detection module.<ul>
<li>Change the multi-task cascaded convolutional neural network (MTCNN)-related part in the <a class="el" href="../../d0/d5b/fd__thread_8h.html">fd_thread.h</a> and the fd_thread.c.<br  />
 <a class="el" href="../../d0/d5b/fd__thread_8h.html">fd_thread.h</a> <div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_s</a> {</div>
<div class="line">    <a class="code hl_struct" href="../../d8/d08/structmtcnn__s.html">mtcnn_t</a> <a class="code hl_variable" href="../../dc/d0c/structfd__thread__s.html#ad59f1a11c69d655448dba586c4d52b9d">mtcnn</a>;</div>
<div class="ttc" id="astructfd__thread__s_html"><div class="ttname"><a href="../../dc/d0c/structfd__thread__s.html">fd_thread_s</a></div><div class="ttdef"><b>Definition:</b> fd_thread.h:76</div></div>
<div class="ttc" id="astructfd__thread__s_html_ad59f1a11c69d655448dba586c4d52b9d"><div class="ttname"><a href="../../dc/d0c/structfd__thread__s.html#ad59f1a11c69d655448dba586c4d52b9d">fd_thread_s::mtcnn</a></div><div class="ttdeci">mtcnn_t mtcnn</div><div class="ttdef"><b>Definition:</b> fd_thread.h:78</div></div>
<div class="ttc" id="astructmtcnn__s_html"><div class="ttname"><a href="../../d8/d08/structmtcnn__s.html">mtcnn_s</a></div><div class="ttdef"><b>Definition:</b> mtcnn.h:59</div></div>
</div><!-- fragment --> fd_thread.c <div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="../../d9/df1/mtcnn_8h.html">mtcnn.h</a>&gt;</span></div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fd_net_init(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> fd_net_deinit(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <a class="code hl_typedefRef" href="../../../library/d5/df0/group__eazyai-angle-helper.html#ga2df9d3f13428a2486155ee1f330c1ed4">ea_tensor_t</a> *fd_net_input(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fd_net_update_input(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread, <a class="code hl_typedefRef" href="../../../library/d5/df0/group__eazyai-angle-helper.html#ga2df9d3f13428a2486155ee1f330c1ed4">ea_tensor_t</a> *input)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fd_net_forward(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fd_net_post_process(<a class="code hl_struct" href="../../dc/d0c/structfd__thread__s.html">fd_thread_t</a> *thread, <a class="code hl_struct" href="../../df/d78/structfd__result__s.html">fd_result_t</a> *result)</div>
<div class="ttc" id="agroup__eazyai-angle-helper_html_ga2df9d3f13428a2486155ee1f330c1ed4"><div class="ttname"><a href="../../../library/d5/df0/group__eazyai-angle-helper.html#ga2df9d3f13428a2486155ee1f330c1ed4">ea_tensor_t</a></div><div class="ttdeci">struct ea_tensor_s ea_tensor_t</div></div>
<div class="ttc" id="amtcnn_8h_html"><div class="ttname"><a href="../../d9/df1/mtcnn_8h.html">mtcnn.h</a></div></div>
<div class="ttc" id="astructfd__result__s_html"><div class="ttname"><a href="../../df/d78/structfd__result__s.html">fd_result_s</a></div><div class="ttdef"><b>Definition:</b> common.h:130</div></div>
</div><!-- fragment --></li>
<li>Change the image pre-processing in the test_sl_liveness.c. <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> feed_fd(<a class="code hl_struct" href="../../df/d88/structctx__s.html">ctx_t</a> *ctx)</div>
<div class="line">{</div>
<div class="line">    <a class="code hl_defineRef" href="../../../library/d4/d96/group__eazyai-log-helper.html#gabfe3786470deb087a8c51c3516fa0a3e">RVAL_OK</a>(<a class="code hl_functionRef" href="../../../library/d1/d67/group__eazyai-preprocess-details.html#ga565556c72e67622ff43979ef7eb4d01f">ea_cvt_color_resize</a>(fd_canvas_tensor, fd_input-&gt;image, <a class="code hl_variableRef" href="../../../library/d2/da6/group__eazyai-preprocess-helper.html#ggacf838ff025e07d2405d18f4d80494eb7a6e505ff310dfdc04463b8c03a9ee1429">EA_COLOR_YUV2RGB_NV12</a>, <a class="code hl_variableRef" href="../../../library/d9/d79/group__eazyai-tensor-helper.html#gga2e3f04280863e7c873d05a7eb73a8a55a54ee6b194dc5690fac2c2bdb3f3f5562">EA_VP</a>));</div>
<div class="ttc" id="agroup__eazyai-log-helper_html_gabfe3786470deb087a8c51c3516fa0a3e"><div class="ttname"><a href="../../../library/d4/d96/group__eazyai-log-helper.html#gabfe3786470deb087a8c51c3516fa0a3e">RVAL_OK</a></div><div class="ttdeci">#define RVAL_OK</div></div>
<div class="ttc" id="agroup__eazyai-preprocess-details_html_ga565556c72e67622ff43979ef7eb4d01f"><div class="ttname"><a href="../../../library/d1/d67/group__eazyai-preprocess-details.html#ga565556c72e67622ff43979ef7eb4d01f">ea_cvt_color_resize</a></div><div class="ttdeci">AMBA_API int ea_cvt_color_resize(ea_tensor_t *src, ea_tensor_t *dst, ea_color_convert_type_t type, ea_device_t device)</div></div>
<div class="ttc" id="agroup__eazyai-preprocess-helper_html_ggacf838ff025e07d2405d18f4d80494eb7a6e505ff310dfdc04463b8c03a9ee1429"><div class="ttname"><a href="../../../library/d2/da6/group__eazyai-preprocess-helper.html#ggacf838ff025e07d2405d18f4d80494eb7a6e505ff310dfdc04463b8c03a9ee1429">EA_COLOR_YUV2RGB_NV12</a></div><div class="ttdeci">EA_COLOR_YUV2RGB_NV12</div></div>
<div class="ttc" id="agroup__eazyai-tensor-helper_html_gga2e3f04280863e7c873d05a7eb73a8a55a54ee6b194dc5690fac2c2bdb3f3f5562"><div class="ttname"><a href="../../../library/d9/d79/group__eazyai-tensor-helper.html#gga2e3f04280863e7c873d05a7eb73a8a55a54ee6b194dc5690fac2c2bdb3f3f5562">EA_VP</a></div><div class="ttdeci">EA_VP</div></div>
<div class="ttc" id="astructctx__s_html"><div class="ttname"><a href="../../df/d88/structctx__s.html">ctx_s</a></div><div class="ttdef"><b>Definition:</b> main_thread.h:45</div></div>
</div><!-- fragment --></li>
<li>Add / remove parameters in <code><a class="el" href="../../da/d33/params_8h.html">params.h</a></code> and 'params.c'. <div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code hl_struct" href="../../d1/d83/structparams__s.html">params_s</a> {</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a160fe3ff3eeb5ad1d2670ac1d9794c36">fd_canvas_id</a>;</div>
<div class="line">    <a class="code hl_struct" href="../../d9/dea/structroi__s.html">roi_t</a> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a3cca0de3d2229618b0ecb5e138a603bf">fd_roi_in_canvas</a>;</div>
<div class="line">    <span class="keywordtype">float</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#aa07697b3de2037c04fe2486e9dae185e">fd_area_thresh</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#acc6460ba7f9b2bff53bb6ef9641e55c1">max_face_num</a>;</div>
<div class="ttc" id="astructparams__s_html"><div class="ttname"><a href="../../d1/d83/structparams__s.html">params_s</a></div><div class="ttdef"><b>Definition:</b> params.h:62</div></div>
<div class="ttc" id="astructparams__s_html_a160fe3ff3eeb5ad1d2670ac1d9794c36"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a160fe3ff3eeb5ad1d2670ac1d9794c36">params_s::fd_canvas_id</a></div><div class="ttdeci">int fd_canvas_id</div><div class="ttdef"><b>Definition:</b> params.h:75</div></div>
<div class="ttc" id="astructparams__s_html_a3cca0de3d2229618b0ecb5e138a603bf"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a3cca0de3d2229618b0ecb5e138a603bf">params_s::fd_roi_in_canvas</a></div><div class="ttdeci">roi_t fd_roi_in_canvas</div><div class="ttdef"><b>Definition:</b> params.h:76</div></div>
<div class="ttc" id="astructparams__s_html_aa07697b3de2037c04fe2486e9dae185e"><div class="ttname"><a href="../../d1/d83/structparams__s.html#aa07697b3de2037c04fe2486e9dae185e">params_s::fd_area_thresh</a></div><div class="ttdeci">float fd_area_thresh</div><div class="ttdef"><b>Definition:</b> params.h:77</div></div>
<div class="ttc" id="astructparams__s_html_acc6460ba7f9b2bff53bb6ef9641e55c1"><div class="ttname"><a href="../../d1/d83/structparams__s.html#acc6460ba7f9b2bff53bb6ef9641e55c1">params_s::max_face_num</a></div><div class="ttdeci">int max_face_num</div><div class="ttdef"><b>Definition:</b> params.h:78</div></div>
<div class="ttc" id="astructroi__s_html"><div class="ttname"><a href="../../d9/dea/structroi__s.html">roi_s</a></div><div class="ttdef"><b>Definition:</b> common.h:114</div></div>
</div><!-- fragment --></li>
</ul>
</li>
<li>Replace the face recognition module.<ul>
<li>Change the <code>Mobilefacenets</code> related part in the <code><a class="el" href="../../d3/d5d/fr__thread_8h.html">fr_thread.h</a></code> and the <code>fr_thread.c</code>.<br  />
 <a class="el" href="../../d3/d5d/fr__thread_8h.html">fr_thread.h</a> <div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_s</a> {</div>
<div class="line">    <a class="code hl_struct" href="../../d2/d58/structmobilefacenets__s.html">mobilefacenets_t</a> <a class="code hl_variable" href="../../d0/d45/structfr__thread__s.html#a6fb7c6c87f368b672418244398fcddcd">mobilefacenets</a>;</div>
<div class="ttc" id="astructfr__thread__s_html"><div class="ttname"><a href="../../d0/d45/structfr__thread__s.html">fr_thread_s</a></div><div class="ttdef"><b>Definition:</b> fr_thread.h:74</div></div>
<div class="ttc" id="astructfr__thread__s_html_a6fb7c6c87f368b672418244398fcddcd"><div class="ttname"><a href="../../d0/d45/structfr__thread__s.html#a6fb7c6c87f368b672418244398fcddcd">fr_thread_s::mobilefacenets</a></div><div class="ttdeci">mobilefacenets_t mobilefacenets</div><div class="ttdef"><b>Definition:</b> fr_thread.h:76</div></div>
<div class="ttc" id="astructmobilefacenets__s_html"><div class="ttname"><a href="../../d2/d58/structmobilefacenets__s.html">mobilefacenets_s</a></div><div class="ttdef"><b>Definition:</b> mobilefacenets.h:52</div></div>
</div><!-- fragment --> fr_thread.c <div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="../../d8/dab/mobilefacenets_8h.html">mobilefacenets.h</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#define FR_MODEL_BINARY_NAME          &quot;mobilefacenets_cavalry.bin&quot;</span></div>
<div class="line"><span class="preprocessor">#define FR_MAX_FEATURE_VECTOR_NUM    MOBILEFACENETS_MAX_OUT_NUM</span></div>
<div class="line"><span class="preprocessor">#define FR_FEATURE_VECTOR_SIZE        MOBILEFACENETS_FEATURE_VECTOR_SIZE</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fr_net_init(<a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> fr_net_deinit(<a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <a class="code hl_typedefRef" href="../../../library/d5/df0/group__eazyai-angle-helper.html#ga2df9d3f13428a2486155ee1f330c1ed4">ea_tensor_t</a> *fr_net_input(<a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fr_net_forward(<a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_t</a> *thread, <span class="keywordtype">int</span> batch_num)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> fr_net_post_process(<a class="code hl_struct" href="../../d0/d45/structfr__thread__s.html">fr_thread_t</a> *thread, fr_features_t *features)</div>
<div class="line"> </div>
<div class="line"><a class="code hl_typedef" href="../../d1/d74/calc_8h.html#af0856089bf0b524e9e75354ee7328cae">TFloat</a> fixed_dim[5 * 2] = {</div>
<div class="line">    30.2946, 51.6963,</div>
<div class="line">    65.5318, 51.6963,</div>
<div class="line">    48.0252, 71.7366,</div>
<div class="line">    33.5493, 92.3655,</div>
<div class="line">    62.7299, 92.3655};</div>
<div class="ttc" id="acalc_8h_html_af0856089bf0b524e9e75354ee7328cae"><div class="ttname"><a href="../../d1/d74/calc_8h.html#af0856089bf0b524e9e75354ee7328cae">TFloat</a></div><div class="ttdeci">float TFloat</div><div class="ttdef"><b>Definition:</b> calc.h:50</div></div>
<div class="ttc" id="amobilefacenets_8h_html"><div class="ttname"><a href="../../d8/dab/mobilefacenets_8h.html">mobilefacenets.h</a></div></div>
</div><!-- fragment --></li>
<li>Change the vector size in <a class="el" href="../../db/d35/mfn__face__db_8h.html">fr/mfn_face_db.h</a> if the new size is different. <div class="fragment"><div class="line"><span class="preprocessor">#define D_MFN_FACE_EV_LEN 128</span></div>
</div><!-- fragment --></li>
<li>Change the image pre-processing in the test_sl_liveness.c. <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> feed_fr_sl(<a class="code hl_struct" href="../../df/d88/structctx__s.html">ctx_t</a> *ctx)</div>
<div class="line">{</div>
<div class="line">    <a class="code hl_defineRef" href="../../../library/d4/d96/group__eazyai-log-helper.html#gabfe3786470deb087a8c51c3516fa0a3e">RVAL_OK</a>(<a class="code hl_functionRef" href="../../../library/d1/d67/group__eazyai-preprocess-details.html#ga565556c72e67622ff43979ef7eb4d01f">ea_cvt_color_resize</a>(fr_canvas_tensor, fr_input-&gt;image, <a class="code hl_variableRef" href="../../../library/d2/da6/group__eazyai-preprocess-helper.html#ggacf838ff025e07d2405d18f4d80494eb7a9587d53a90140ea1bba203e4b76c1f61">EA_COLOR_YUV2BGR_NV12</a>, <a class="code hl_variableRef" href="../../../library/d9/d79/group__eazyai-tensor-helper.html#gga2e3f04280863e7c873d05a7eb73a8a55a54ee6b194dc5690fac2c2bdb3f3f5562">EA_VP</a>));</div>
<div class="ttc" id="agroup__eazyai-preprocess-helper_html_ggacf838ff025e07d2405d18f4d80494eb7a9587d53a90140ea1bba203e4b76c1f61"><div class="ttname"><a href="../../../library/d2/da6/group__eazyai-preprocess-helper.html#ggacf838ff025e07d2405d18f4d80494eb7a9587d53a90140ea1bba203e4b76c1f61">EA_COLOR_YUV2BGR_NV12</a></div><div class="ttdeci">EA_COLOR_YUV2BGR_NV12</div></div>
</div><!-- fragment --></li>
</ul>
</li>
<li>Replace the liveness detection module.<ul>
<li>Add a new file, including depth generation and liveness detection, to the folder of <code>ambarella/app/access_control/sl_liveness_v2/liveness_detection</code>. For example: <code>blade_runner_algo.c</code></li>
<li>Implement the algorithm interface, such as <code>blade_runner_algo.c</code> <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> ld_new_depth_algo_init(<span class="keyword">const</span> <a class="code hl_struct" href="../../d0/d65/structld__thread__params__s.html">ld_thread_params_t</a> *params)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> ld_new_depth_algo_deinit(<span class="keywordtype">void</span>)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> ld_new_depth_algo_calc_full_depth(<a class="code hl_struct" href="../../d4/dd6/structld__thread__s.html">ld_thread_t</a> *thread)</div>
<div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> ld_new_depth_algo_calc_face_depth(<a class="code hl_struct" href="../../d4/dd6/structld__thread__s.html">ld_thread_t</a> *thread)</div>
<div class="line"><span class="keywordtype">int</span> register_new_depth_algo_ops(<a class="code hl_struct" href="../../d3/d33/structld__algo__op__s.html">ld_algo_op_t</a> *algo_ops)</div>
<div class="ttc" id="astructld__algo__op__s_html"><div class="ttname"><a href="../../d3/d33/structld__algo__op__s.html">ld_algo_op_s</a></div><div class="ttdef"><b>Definition:</b> ld_thread.h:105</div></div>
<div class="ttc" id="astructld__thread__params__s_html"><div class="ttname"><a href="../../d0/d65/structld__thread__params__s.html">ld_thread_params_s</a></div><div class="ttdef"><b>Definition:</b> ld_thread.h:71</div></div>
<div class="ttc" id="astructld__thread__s_html"><div class="ttname"><a href="../../d4/dd6/structld__thread__s.html">ld_thread_s</a></div><div class="ttdef"><b>Definition:</b> ld_thread.h:95</div></div>
</div><!-- fragment --></li>
<li>Register new liveness detection operations in ld_thread.c <div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> register_ld_ops(<a class="code hl_struct" href="../../d3/d33/structld__algo__op__s.html">ld_algo_op_t</a> *algo_ops, <span class="keywordtype">int</span> algo_type)</div>
</div><!-- fragment --></li>
<li>Add / remove parameters in <code><a class="el" href="../../da/d33/params_8h.html">params.h</a></code> and <code>params.c</code>. <div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code hl_struct" href="../../d1/d83/structparams__s.html">params_s</a> {</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#aca7bc02780c00358c2729ec56e7d59b5">ld_canvas_id</a>;</div>
<div class="line">    <a class="code hl_struct" href="../../d9/dea/structroi__s.html">roi_t</a> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a5607dbfb58e7718b48a0be57c207433f">ld_roi_in_canvas</a>;</div>
<div class="line">    <span class="keywordtype">float</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a81d9f165b1e496d7ee3acfc4dffe61f5">baseline</a>;</div>
<div class="line">    <span class="keywordtype">float</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a6f482a4b78802db8e529b2390b0bd243">ref_dist</a>;</div>
<div class="line">    <span class="keywordtype">char</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#aa69abf1db8095a32c8b40db176cea1b2">ref_yuv_path</a>[<a class="code hl_define" href="../../dc/d54/common_8h.html#ab099dbfd0da47c1ddf77058cbc61953c">MAX_PATH_STRLEN</a> + 1];</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a96f238c5d638ae07ead2520ba55cf667">ref_width</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a097183c30e367bf237e050800ceef39e">ref_height</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a38540c869ffa5ab7583b50820e7abbe4">ref_pitch</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a0bea6a6269d94779f89af5f91b122bec">depth_algo_type</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a0143778c6aca57ac707ab1b36d65f1c8">calc_full_depth</a>;</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#ab00430ddb7f1a0e79b976442c1f5aced">blade_runner_config</a>[7];</div>
<div class="line">    <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#a468aea09eaa706fc857d0b2a8d10803e">fbm_config</a>[10];</div>
<div class="line">    <span class="keywordtype">float</span> <a class="code hl_variable" href="../../d1/d83/structparams__s.html#af955e92d1b3937a4dcee0e1947870b8a">liveness_config</a>[7];</div>
<div class="line">}</div>
<div class="ttc" id="acommon_8h_html_ab099dbfd0da47c1ddf77058cbc61953c"><div class="ttname"><a href="../../dc/d54/common_8h.html#ab099dbfd0da47c1ddf77058cbc61953c">MAX_PATH_STRLEN</a></div><div class="ttdeci">#define MAX_PATH_STRLEN</div><div class="ttdef"><b>Definition:</b> common.h:45</div></div>
<div class="ttc" id="astructparams__s_html_a0143778c6aca57ac707ab1b36d65f1c8"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a0143778c6aca57ac707ab1b36d65f1c8">params_s::calc_full_depth</a></div><div class="ttdeci">int calc_full_depth</div><div class="ttdef"><b>Definition:</b> params.h:93</div></div>
<div class="ttc" id="astructparams__s_html_a097183c30e367bf237e050800ceef39e"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a097183c30e367bf237e050800ceef39e">params_s::ref_height</a></div><div class="ttdeci">int ref_height</div><div class="ttdef"><b>Definition:</b> params.h:90</div></div>
<div class="ttc" id="astructparams__s_html_a0bea6a6269d94779f89af5f91b122bec"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a0bea6a6269d94779f89af5f91b122bec">params_s::depth_algo_type</a></div><div class="ttdeci">int depth_algo_type</div><div class="ttdef"><b>Definition:</b> params.h:92</div></div>
<div class="ttc" id="astructparams__s_html_a38540c869ffa5ab7583b50820e7abbe4"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a38540c869ffa5ab7583b50820e7abbe4">params_s::ref_pitch</a></div><div class="ttdeci">int ref_pitch</div><div class="ttdef"><b>Definition:</b> params.h:91</div></div>
<div class="ttc" id="astructparams__s_html_a468aea09eaa706fc857d0b2a8d10803e"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a468aea09eaa706fc857d0b2a8d10803e">params_s::fbm_config</a></div><div class="ttdeci">int fbm_config[11]</div><div class="ttdef"><b>Definition:</b> params.h:99</div></div>
<div class="ttc" id="astructparams__s_html_a5607dbfb58e7718b48a0be57c207433f"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a5607dbfb58e7718b48a0be57c207433f">params_s::ld_roi_in_canvas</a></div><div class="ttdeci">roi_t ld_roi_in_canvas</div><div class="ttdef"><b>Definition:</b> params.h:85</div></div>
<div class="ttc" id="astructparams__s_html_a6f482a4b78802db8e529b2390b0bd243"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a6f482a4b78802db8e529b2390b0bd243">params_s::ref_dist</a></div><div class="ttdeci">float ref_dist</div><div class="ttdef"><b>Definition:</b> params.h:87</div></div>
<div class="ttc" id="astructparams__s_html_a81d9f165b1e496d7ee3acfc4dffe61f5"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a81d9f165b1e496d7ee3acfc4dffe61f5">params_s::baseline</a></div><div class="ttdeci">float baseline</div><div class="ttdef"><b>Definition:</b> params.h:86</div></div>
<div class="ttc" id="astructparams__s_html_a96f238c5d638ae07ead2520ba55cf667"><div class="ttname"><a href="../../d1/d83/structparams__s.html#a96f238c5d638ae07ead2520ba55cf667">params_s::ref_width</a></div><div class="ttdeci">int ref_width</div><div class="ttdef"><b>Definition:</b> params.h:89</div></div>
<div class="ttc" id="astructparams__s_html_aa69abf1db8095a32c8b40db176cea1b2"><div class="ttname"><a href="../../d1/d83/structparams__s.html#aa69abf1db8095a32c8b40db176cea1b2">params_s::ref_yuv_path</a></div><div class="ttdeci">char ref_yuv_path[MAX_PATH_STRLEN+1]</div><div class="ttdef"><b>Definition:</b> params.h:88</div></div>
<div class="ttc" id="astructparams__s_html_ab00430ddb7f1a0e79b976442c1f5aced"><div class="ttname"><a href="../../d1/d83/structparams__s.html#ab00430ddb7f1a0e79b976442c1f5aced">params_s::blade_runner_config</a></div><div class="ttdeci">int blade_runner_config[7]</div><div class="ttdef"><b>Definition:</b> params.h:96</div></div>
<div class="ttc" id="astructparams__s_html_aca7bc02780c00358c2729ec56e7d59b5"><div class="ttname"><a href="../../d1/d83/structparams__s.html#aca7bc02780c00358c2729ec56e7d59b5">params_s::ld_canvas_id</a></div><div class="ttdeci">int ld_canvas_id</div><div class="ttdef"><b>Definition:</b> params.h:84</div></div>
<div class="ttc" id="astructparams__s_html_af955e92d1b3937a4dcee0e1947870b8a"><div class="ttname"><a href="../../d1/d83/structparams__s.html#af955e92d1b3937a4dcee0e1947870b8a">params_s::liveness_config</a></div><div class="ttdeci">float liveness_config[7]</div><div class="ttdef"><b>Definition:</b> params.h:102</div></div>
</div><!-- fragment --></li>
</ul>
</li>
</ol>
<hr  />
<h1><a class="anchor" id="acc_ctrl_unit_test"></a>
4. Unit Test Usage</h1>
<p >The Ambarella IR process library includes three unit tests:</p><ul>
<li>test_blade_runner</li>
<li>test_sl_liveness_v2</li>
</ul>
<p >The <code>test_blade_runner</code> generates the disparity, depth, and point cloud, and performs the liveness test based on the specified ROI. Users can refer to this demo to understand this library.</p>
<p >The <code>test_sl_liveness_v2</code> runs the face detection algorithm MTCNN and the face recognition algorithm MobileFaceNet, along with the liveness test.</p>
<h2><a class="anchor" id="acc_ctrl_unit_test_cfg"></a>
4.1 Configuring the Parameters</h2>
<p >Before running the unit test demo, users must configure the following device-related parameters by converting them to meters:</p><ul>
<li>The real focal length (m) / sensor pixel size (m)</li>
<li>Baseline (m)</li>
<li>Center offset (x / y) (pixels)</li>
<li>Reference frame distance (m)</li>
</ul>
<p >This library uses the following focal length: real_focal_length (m) / sensor_pixel_size (m). Information generated by Ambarella’s calibration tools is located in the following file: <code>/usr/share/ambarella/calib_mono/fov1.json</code>.</p>
<p >Users can determine the reference frame distance from the file name. For example, if the reference YUV data name is <code>ref_b0_0_canvas0_1920x1080_0.8.yuv</code>, the reference frame distance is “0.8 m”.</p>
<p >If there is not a distance number in the filename, users can interpret the ID between H1912060001 ~ H1912060010 as “1.1 m” and “ID &gt;= H1912060011” as “0.8 m”. </p><dl class="section note"><dt>Note</dt><dd>Users can capture a new frame by following the steps for camera calibration in the Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_ref_cap">6. Reference Capture</a>.</dd></dl>
<p>The script file <code>/usr/local/bin/gen_board_config.sh</code> integrates these calculations. This script file provides an easy method to initialize parameters during the evaluation stage. Output parameters are generated when the <code>/usr/share/ambarella/calib_mono/board_config</code> file is created.</p>
<p >The following example demonstrates how to obtain parameters from a Janus calibration. (The Vision+ EVK follows the same process): </p><div class="fragment"><div class="line">board # cat /usr/share/ambarella/calib_mono/fov1.json</div>
<div class="line">{<span class="stringliteral">&quot;version&quot;</span>:1,<span class="stringliteral">&quot;generated-by&quot;</span>:<span class="stringliteral">&quot;CTB 4.6.29382&quot;</span>,<span class="stringliteral">&quot;time&quot;</span>:<span class="stringliteral">&quot;2019-Dec-09 18:41:43&quot;</span>,</div>
<div class="line"><span class="stringliteral">&quot;name&quot;</span>:<span class="stringliteral">&quot;fov1&quot;</span>,<span class="stringliteral">&quot;id&quot;</span>:<span class="stringliteral">&quot;1&quot;</span>,<span class="stringliteral">&quot;params&quot;</span>:{<span class="stringliteral">&quot;model&quot;</span>:<span class="stringliteral">&quot;pinhole&quot;</span>,<span class="stringliteral">&quot;ku&quot;</span>:817,<span class="stringliteral">&quot;kv&quot;</span>:817,</div>
<div class="line"><span class="stringliteral">&quot;u0&quot;</span>:959.5,<span class="stringliteral">&quot;v0&quot;</span>:517.89999999999998,<span class="stringliteral">&quot;w&quot;</span>:1920,<span class="stringliteral">&quot;h&quot;</span>:1080},<span class="stringliteral">&quot;position&quot;</span>:[0,0,0,0,-0,0],</div>
<div class="line"><span class="stringliteral">&quot;relative-to&quot;</span>: <span class="stringliteral">&quot;-&quot;</span>,<span class="stringliteral">&quot;src-params&quot;</span>:{<span class="stringliteral">&quot;model&quot;</span>:<span class="stringliteral">&quot;kannala-brandt&quot;</span>,<span class="stringliteral">&quot;ku&quot;</span>:950.7136530619706,</div>
<div class="line"><span class="stringliteral">&quot;kv&quot;</span>:950.7136530619706,<span class="stringliteral">&quot;u0&quot;</span>:940.94117891765529,<span class="stringliteral">&quot;v0&quot;</span>:523.62050365128425,</div>
<div class="line"><span class="stringliteral">&quot;w&quot;</span>:1920,<span class="stringliteral">&quot;h&quot;</span>:1080,<span class="stringliteral">&quot;lens-model&quot;</span>:<span class="stringliteral">&quot;brown-conrady&quot;</span>,<span class="stringliteral">&quot;dc&quot;</span>:[1.28873114001354741e-04,-5.74621130511462626e-04],</div>
<div class="line"><span class="stringliteral">&quot;kd&quot;</span>:[8.01211810077433889e-02,7.31375685226345364e-03,3.32111311321012974e-03]},</div>
<div class="line"><span class="stringliteral">&quot;src-params-uncertainty&quot;</span>:{<span class="stringliteral">&quot;model&quot;</span>:<span class="stringliteral">&quot;kannala-brandt&quot;</span>,<span class="stringliteral">&quot;ku&quot;</span>:0.096290835325604646,</div>
<div class="line"><span class="stringliteral">&quot;kv&quot;</span>:0.096290835325604646,<span class="stringliteral">&quot;u0&quot;</span>:0.07855489443337979,<span class="stringliteral">&quot;v0&quot;</span>:0.061336162934196442,</div>
<div class="line"><span class="stringliteral">&quot;w&quot;</span>:0,<span class="stringliteral">&quot;h&quot;</span>:0,<span class="stringliteral">&quot;lens-model&quot;</span>:<span class="stringliteral">&quot;brown-conrady&quot;</span>},</div>
<div class="line"><span class="stringliteral">&quot;src-orientation&quot;</span>:[0.00000000000000000e+00,0.00000000000000000e+00,0.00000000000000000e+00],</div>
<div class="line"><span class="stringliteral">&quot;src-position&quot;</span>:[0.00000000000000000e+00,0.00000000000000000e+00,0.00000000000000000e+00],</div>
<div class="line"><span class="stringliteral">&quot;fov&quot;</span>:[-1.11100949331228782e+00,-8.02715335971528554e-01,1.17702690362275231e+00,8.28458032121773269e-01],</div>
<div class="line"><span class="stringliteral">&quot;vsync&quot;</span>:0.00000000000000000e+00,<span class="stringliteral">&quot;fitting&quot;</span>:7.48930349946022034e-02,<span class="stringliteral">&quot;fitting_max&quot;</span>:4.19740349054336548e-01}#</div>
</div><!-- fragment --><p >For the pinhole model result in the example above, ku / kv is the focal_length, which is equal to the real_focal_length(m) / sensor_pixel_size(m); u0 is the center_x; and v0 is the center_y, which is the lens center coordinate on the image.</p>
<p >The following shows the resulting example parameters for the 1080P YUV data:</p><ul>
<li>focal_length = 817</li>
<li>baseline = 0.04</li>
<li>center_x = 959.5</li>
<li>center_y = 517.899</li>
</ul>
<p >The following lists the formula for converting the parameters (for a new resolution):</p><ul>
<li>scale_ratio = data_width_dst / data_width_src</li>
<li>focal_length_dst = focal_length_src * scale_ratio</li>
<li>baseline_dst = baseline_src</li>
<li>center_x_dst = (center_x_src - src_offset_x) * scale_ratio</li>
<li>center_y_dst = (center_y_src - src_offset_y) * scale_ratio</li>
</ul>
<h2><a class="anchor" id="acc_ctrl_test_encode"></a>
4.2 Unit_test: test_encode (New Parameters)</h2>
<p >Users can change the structured-light frame rate accordingly, depending on the algorithm speed. New options are added in test_encode. Users can refer to <code>test_encode</code> and review the contents of <code>quick_ipc_for_access_control.sh</code> and <code>/usr/share/ambarella/calib_mono/board_config</code>.</p>
<p >The new parameters for the access control platform in test_encode: </p><table class="doxtable">
<tr>
<th align="center">Parameters </th><th align="center">Features </th></tr>
<tr>
<td>custom-vinc-id </td><td>Specifies the VIN controller ID, which is 0 by default. </td></tr>
<tr>
<td>custom-flash-enable [0|1] </td><td>Specifies if this function is enabled. </td></tr>
<tr>
<td>custom-led-gpio </td><td>Specifies the output general purpose input / output (GPIO) to control the structured light. </td></tr>
<tr>
<td>custom-flash-time </td><td>Specifies the structured-light turn time for one frame. Set it to 5 for 5 ms. The current limit is 1 ms to 7 ms. <dl class="section note"><dt>Note</dt><dd>If the access control platform is turned on for too long, it will overheat, which can damage the structured-light projector. </dd></dl>
</td></tr>
<tr>
<td>custom-structured_light_chan [0~2] </td><td>Specifies the channel ID, which contains the structured-light pattern frames. </td></tr>
<tr>
<td>Custom-ir-led-chan [0~2] </td><td>Specifies the channel ID, which contains the IR LED pattern frames. </td></tr>
<tr>
<td>custom-chan-frame-ratio [0~10:0~10:0~10] </td><td>Specifies the frame ratio on each channel in the format of channel 2’s ratio, channel 1’s ratio, and channel 0’s ratio. If channel 2 is not used, the value can be left as 0. If one of the non-structured-light channels shares the same frames as the other non-structured-light channel, its value can be left as 0. <dl class="section note"><dt>Note</dt><dd>Update the idsp_fps for each channel in the IAV Lua script accordingly after changing this ratio value. </dd></dl>
</td></tr>
<tr>
<td>custom-ir-led-gpio </td><td>Specifies the custom configuration for IR LED GPIO. </td></tr>
<tr>
<td>custom-ir-led-mode [0|1|2] </td><td>Specifies the custom configuration for the IR LED work mode:<br  />
 0: off, 1: always on, 2: light-on interleave with structured light. </td></tr>
<tr>
<td>custom-ir-led-time </td><td>Specifies the IR LED enabled duration in ms on the non-structured-light frame. </td></tr>
<tr>
<td>custom-flash-latency-plus1 </td><td>Specifies the flash latency after EOF irq. 0: auto, others: (x-1) ms. </td></tr>
<tr>
<td>custom-flash-led-trigger-once </td><td>Manually triggers the custom configuration of the LED once, when the custom frame pattern = 0. </td></tr>
<tr>
<td>custom-flash-led-line-offset </td><td>Specifies the custom configuration of the LED flash line offset based on VIN height. </td></tr>
<tr>
<td>custom-update-flash-time [1~10] </td><td>Updates the custom configuration of the LED flash in ms. </td></tr>
<tr>
<td>custom-flash-min-time </td><td>Specifies the custom configuration of the LED flash minimum time. </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>Some physical parameters, such as <code>custom-led-gpio</code>, <code>custom-ir-led-gpio</code>, <code>custom-flash-time</code>, and <code>custom-ir-led-time</code>, have been defined in the device tree (DTS) files by default, however, these parameters can still be updated with the test_encode option on the fly.</dd></dl>
<h2><a class="anchor" id="acc_ctrl_test_aaa_service"></a>
4.3 Unit_test: test_aaa_service (New Parameters)</h2>
<table class="doxtable">
<caption>New Parameters of test_tuning for Access Control LED Control.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-p </td><td>&ndash;product_cfg </td><td>Specifies the product Lua path; the default Lua path is <code>/usr/share/ambarella/idsp/product_id_authen.lua</code>. The product Lua specifies the virtual type for each channel. </td></tr>
<tr>
<td>-m </td><td>&ndash;ai_metering </td><td>Enables the AI metering mode </td></tr>
</table>
<h2><a class="anchor" id="acc_ctrl_test_blade_runner"></a>
4.4 Unit_test: test_blade_runner</h2>
<p >The <code>test_blade_runner</code> includes the depth generation-associated functions.</p>
<h3><a class="anchor" id="acc_ctrl_tbr_intro"></a>
4.4.1 Introduction to test_blade_runner</h3>
<table class="doxtable">
<caption>test_blade_runner Usages.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-t </td><td>&ndash;source-type </td><td>Sets the live input data source type = canvas_buffer / pyramid_buffer,0=canvas_buffer, 1=pyramid_buffer. </td></tr>
<tr>
<td>-i </td><td>&ndash;source-id </td><td>Sets the live input data canvas / pyramid ID, canvas_buffer 0: Main / 1:Second / 2:Thrid / 3:Fourth / 4:Fifth; pyramid layer 0 / 1 / 2 / 3 / 4 / 5. </td></tr>
<tr>
<td>-s </td><td>&ndash;stream-id </td><td>Displays on the encode stream. Sets the stream ID, [0:1:2...] </td></tr>
<tr>
<td>-r </td><td>&ndash;ref-yuv </td><td>References the YUV file path. </td></tr>
<tr>
<td></td><td>&ndash;ref-info </td><td>References the YUV file width, height, and pitch. </td></tr>
<tr>
<td>-o </td><td>&ndash;ref-offset </td><td>References the YUV file offset; value ~ [-1920, 1920]. </td></tr>
<tr>
<td>-m </td><td>&ndash;smooth </td><td>Blade Runner disparity smoothness. The value must be &gt; = 1. </td></tr>
<tr>
<td>-g </td><td>&ndash;range </td><td>Blade Runner disparity data range. The value must be &gt; = 1. </td></tr>
<tr>
<td>-n </td><td>&ndash;min </td><td>Blade Runner min disparity. </td></tr>
<tr>
<td></td><td>&ndash;data-roi </td><td>ROI of the live YUV data. The format = x, y, w, h. </td></tr>
<tr>
<td>-f </td><td>&ndash;focal-length </td><td>Sets the focal length, focal_length = focal_length_real / sensor_pixel_size, and then crops / resizes the image from the 1080p input, focal_length = focal_length * resize_dst_w / crop_w. </td></tr>
<tr>
<td>-b </td><td>&ndash;baseline </td><td>Sets the baseline. The units are in meters. </td></tr>
<tr>
<td>-d </td><td>&ndash;ref-dist </td><td>Sets the ref_distance. The units are in meters. </td></tr>
<tr>
<td>-x </td><td>&ndash;cx </td><td>Sets the reference data center_x. It crops and resizes the image from the 1080p input, center_x = (center_x - crop_start_x) * resize_dst_w / crop_w. </td></tr>
<tr>
<td>-y </td><td>&ndash;cy </td><td>Sets the reference data center_y. It crops and resizes the image from the 1080p input, center_y = (center_y - crop_start_y) * resize_dst_h / crop_h. </td></tr>
<tr>
<td>-e </td><td>&ndash;rotate </td><td>Sets the rotate to 0, 1, 2. 0:no_rotate / 1:rotate_90_clockwise / 2:rotate_270_clockwise. </td></tr>
<tr>
<td>-M </td><td>&ndash;mode </td><td>Sets the mode, which can be 0 / 1 / 2 / 3. The default is 0. When facing the camera module, the coordinate (0,0) is the lens. If the structured-light projector is at the x-negative position: mode = 0/2 and rotate = 0. If the structured-light projector is located at the y-positive position: mode = 0/2 and rotate = 1. If the structured-light projector is at the x-positive position: mode = 1/3 and rotate = 0. If the structured-light projector is at the y-negative position: mode = 1/3 and rotate = 1. Compared to mode 0 / 1, mode 2 / 3 has better depth quality but introduces more noise for invalid pixels. </td></tr>
<tr>
<td></td><td>&ndash;flash-start-offset </td><td>Structured-light flash starts the offset in lines for the dynamic mode. The range is: [-(VIN height), (VIN height)]. </td></tr>
<tr>
<td></td><td>&ndash;flash-start-ratio </td><td>Structured-light flash starts the multiplier ratio for the dynamic mode. This value is equal to (VIN height) / (the height of structured-light main buffer). </td></tr>
<tr>
<td></td><td>&ndash;save-data-path </td><td>Saves the data path. </td></tr>
<tr>
<td></td><td>&ndash;save-dsi-num </td><td>Saves the disparity uint8 frame number. </td></tr>
<tr>
<td></td><td>&ndash;save-dsi16-num </td><td>Saves the disparity int16 frame number. </td></tr>
<tr>
<td></td><td>&ndash;save-depth-num </td><td>Saves the depth frame number. </td></tr>
<tr>
<td></td><td>&ndash;save-pcd-num </td><td>Saves the point cloud frame number. </td></tr>
<tr>
<td></td><td>&ndash;live-roi </td><td>Sets the liveness test ROI, which must be smaller than the disparity size. </td></tr>
<tr>
<td></td><td>&ndash;min-max-dist </td><td>Sets the liveness test face distance range. </td></tr>
<tr>
<td></td><td>&ndash;min-max-height </td><td>Sets the liveness test face height range. </td></tr>
<tr>
<td></td><td>&ndash;min-max-dsi-ratio </td><td>Sets the liveness test face ratio range. </td></tr>
<tr>
<td></td><td>&ndash;face-th </td><td>Sets the liveness test face threshold. </td></tr>
<tr>
<td></td><td>&ndash;depth-display </td><td>Draws the depth map on the overlay. 0=disable, 1=enable. The default is enabled. </td></tr>
<tr>
<td>-v </td><td>&ndash;verbose </td><td>Shows the verbose information. </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>-t and -i specify the YUV source buffer; in the default Lua configuration file, it must be canvas buffer 0 for 1920x1080.</li>
<li>-s specifies the stream ID. The result of disparity data will draw on that stream; the output disparity size must be less than the stream size.</li>
<li>-r sets the path of the reference YUV file; the typical file is the 1080P YUV data path. "--ref-info" describes the resolution of the YUV file. For the 1920x1080 resolution, the description is"--ref-info 1920,1080,1920". The resolution is the same as the input buffer specified by -t and -i.</li>
<li>-o specifies the offset of the input YUV data: 0 is for general cases. It can change the min-max range of the structured-light module based on the current reference frame. For example, if the current detection range is 0.2 m~1.0 m, set this value to a larger number, as 5 can adjust the detection range to 0.3 m~1.5 m.</li>
<li>-m sets the smoothness of the disparity data. “-m” must be greater than 1, and vary according to different YUV resolutions—a large resolution must have a larger value. Smaller values result in a greater sharpness disparity map.</li>
<li>-g sets the maximum 16-bit disparity, which must be greater than 1. The maximum output disparity value equals the value multiplied by 16.</li>
<li>-n sets the minimum 16-bit disparity value. The typical value is zero.</li>
<li>-e sets the rotation for input data, of input data, and for reference data. It could be 0, 1, or 2, with 0 for no rotate, 1 for rotate 90-degrees clockwise, 2 for rotate 270-degrees clockwise (90 counterclockwise).</li>
<li>-M sets the algorithm mode. It is reserved and set to 0.</li>
<li>&ndash;data-roi sets the ROI based on the YUV data; with parameter x_offset, y_offset, width, and height, the offset can set the start position on the reference YUV data. The width and height are the output disparity data resolution.</li>
<li>-f -b -d -x -y sets the focal length, baseline, reference frame distance, center_x, and center_y. For parameters, refer to Section <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_unit_test_cfg">4.1 Configuring the Parameters</a>.</li>
<li>&ndash;save-data-path specifies the path to the output data.</li>
<li>&ndash;save-dsi-num specifies the number of saved 8-bit disparity frames. The format is uint8, for any range of -g specified value, and its normalized range will always be 0-255. The resolution is the resolution of the disparity data.</li>
<li>&ndash;save-dsi16-num specifies the number of saved frames. The data format is 16-bit fixed-point data with the sign bit=1 and the expoffset bits = 4. Users can read the data by int16, then divide by 2^4=16; the maximum value can be specified by the -g option. After dividing by 16, if a -1 is returned, then the pixel is invalid and the resolution is the disparity data resolution.</li>
<li>&ndash;save-depth-num specifies the number of saved depth frames. The output data format is a 32-bit floating point, and the unit is one meter for each pixel. Users can read the data by the float pointer in C code, and the resolution is the disparity data resolution.</li>
<li>&ndash;save-pcd-num specifies the number of saved point cloud frames. The output is in the pcd file format, with a header and a binary, including x, y, and z coordinates in 32-bit floating point. The binary output data size without the header = total_valid_pixel * 3 * sizeof (float); users can install pcl_viewer in Linux, which reads and automatically shows the pcd file in a three-dimensional point cloud.</li>
<li>&ndash;live-roi specifies a region of interest for the liveness test. If the disparity data includes the full FoV of the image, the ROI can be a small region of the disparity data. If the disparity data is already the ROI of the human face, it can be close but not equal to the disparity data. The ROI for the liveness test must always be smaller than the disparity resolution to account for the black border of the disparity data. The border width is equal to the value in the -m parameter. If users want to calculate the disparity data for the face ROI based on the 1080P reference YUV, the command of &ndash;data-roi can be used to define the ROI region. After the ROI is assigned, the disparity calculation will be applied on the specified area only, which dramatically decreases the calculation time compared to generating the depth for the entire image. However, the output of the blade_runner depth image will always have a blank border at the edge of the image. In order to avoid these blank borders, expand the &ndash;data-roi first to calculate a larger image. After the depth image is generated, the blank borders then can be cropped from the output data. The border size is equal to the value specified by the -m option. Ambarella suggests expanding the ROI’s border (x - border_width, y - border_width, w + 2*border_width,h+2*border_width) by several pixels (equal to the value of -m) to calculate the disparity or depth, and then run the liveness test with a smaller resolution on the output disparity (without the black border).</li>
<li>&ndash;min-max-dist is used for the liveness test and sets the min / max distance for the liveness test. The face distance out of range returns -1 if the distance of the face is out of range.</li>
<li>&ndash;min-max-height is used for the liveness test and sets the min / max of the real face height. The typical range setting is 0.15 m-0.3 m.</li>
<li>&ndash;min-max-dsi-ratio is used for the liveness test and sets the min / max disparity ratio of the liveness ROI. If the valid pixels / total pixels are not within the specified range, an “out of range” message is returned.</li>
<li>&ndash;face-th is used for the liveness test and sets the threshold. A value of 0.999 is typical.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="acc_ctrl_tbr_demo"></a>
4.4.2 Capture the Disparity, Depth, and Point Cloud Data</h3>
<p >Because the FoV of the access control sensors exceeds the FoV of the structured-light projector, the valid FoV only covers approximately two-thirds of the full sensor FoV. Users can only obtain 1280x720 and 1280x1000 disparity or depth image from Janus and Vision+, respectively, if the 1080P YUV data is used as the reference frame. Users can obtain the focal length baseline and center_x / center_y directly from the <code>calib_mono/fov1.json</code> file.</p>
<p >The following examples show the sample commands for running <code>test_blade_runner</code>. Some hardware parameters are not universal to other boards. See the printouts of <code>/usr/local/bin/blade_runner_run_depth_capture.sh</code> for parameters dedicated to a personal board.</p>
<p >For CV25M Janus: </p><div class="fragment"><div class="line">board # test_blade_runner -t 0 -i 0 -s 0 -r /usr/share/ambarella/calib_mono/ref_b0_0_canvas0_1920x1080_0.8.yuv --ref-info 1920,1080,1920 -o 0 -m 11 -g 4 -n 0 -f 817 -<a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a> 0.04 -d 0.8 -<a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#gaf80df1bdae91e5f76236e6ed1110825d">x</a> 959.5 -<a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#ga11e444dd61d48abbdd34eed23186cb4b">y</a> 517.899 --data-roi 360,180,1280,720 -e 1 --save-dsi16-num 1 --save-depth-num 1 --save-pcd-num 1</div>
<div class="ttc" id="acJSON_8h_html_a1a175e87536301df98c805ac0636ad7c"><div class="ttname"><a href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a></div><div class="ttdeci">const cJSON *const b</div></div>
<div class="ttc" id="agroup__IAV_html_ga11e444dd61d48abbdd34eed23186cb4b"><div class="ttname"><a href="../../../driver/df/dc0/group__IAV.html#ga11e444dd61d48abbdd34eed23186cb4b">y</a></div><div class="ttdeci">u32 y</div></div>
</div><!-- fragment --><p> For CV22 Vision+: </p><div class="fragment"><div class="line">board # test_blade_runner -t 0 -i 0 -s 0 –r /usr/share/ambarella/calib_mono/ref_b0_H210615A001_canvas0_1920x1080_NV12_0.816.yuv --ref-info 1920,1080,1920 -o 21 -m 13 -g 4 -n 0 -M 2 -f 764 -<a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a> 0.05 -d 0.816 -<a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#gaf80df1bdae91e5f76236e6ed1110825d">x</a> 958 -<a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#ga11e444dd61d48abbdd34eed23186cb4b">y</a> 522 --data-roi 320,0,1280,1000 -e 1 --save-dsi16-num 1 --save-depth-num 1 --save-pcd-num 1</div>
</div><!-- fragment --><div class="image">
<img src="../../acc_ctrl_test_blade_runner.png" alt=""/>
<div class="caption">
Figure 4-1. Capturing the Disparity, Depth, and Point Cloud Data.</div></div>
<p> <br  />
 The following three files are generated:</p><ul>
<li><code>disparity_xxxxx_1frame_int16.4_xxxxx_2560.bin</code><br  />
 Includes the disparity data: Each pixel has 16 bits; the resolution is 1280x720 (Janus) or 1280x1000 (Vision+), the pitch=2560, fix point / signed / 16 bit / 4 fractional bits, and the data_format=(1, 1, 4, 0). <br  />
 Users may look up the disparity map file via the <code>Vooya</code> application in Linux. Set the format as follows: color space = single channel; data container = single integer; and bit depth = 12 bit.</li>
<li><code>depth_xxxxx_1frame_fp32_xxxxxx_5120.bin</code><br  />
 Includes the depth data: Each pixel has a 32-bit floating point representing the distance to the lens center plane. The resolution is 1280x720 (Janus) or 1280x1000 (Vision+); pitch=5120; and the data_format=(1, 2, 0, 7). Users may look up the disparity map file via the <code>Vooya</code> application on Linux. Set the format as follows: color space = single channel, data container = single float (32bit).</li>
<li><code>point_cloud_xxxxx_frame1_165327_points.pcd</code><br  />
 Includes the point cloud data (PCD) format: A total of 165327 points. Users can open the three-dimensional point clouds using pcl_viewer on Ubuntu.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>The data format shown in the parentheses above (such as 1, 1, 4, 0) is the vector to represent the corresponding output data format, and the meaning is as shown below.<ul>
<li>1st argument: 0 is unsigned, 1 is signed.</li>
<li>2nd argument: 0 refers to 8 bits, 1 is 16 bits, 2 is 32 bits.</li>
<li>3rd argument: The exponent offset.</li>
<li>4th argument: Number of bits reserved for the exponent field. 0 indicates a fixed point number.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="acc_ctrl_test_sl_liveness"></a>
4.5 Unit_test: test_sl_liveness_v2</h2>
<p >The <code>test_sl_liveness_v2</code> includes the face detection, face recognition, and liveness tests. Each algorithm is defined in an individual thread, which provides the modular design to the software. Moreover, some data queues are designed to connect these threads to a program flow.</p>
<h3><a class="anchor" id="acc_ctrl_tsll_v2_intro"></a>
4.5.1 Introduction of test_sl_liveness_v2</h3>
<table class="doxtable">
<caption>test_sl_liveness_v2 Usages.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-m </td><td>&ndash;run_mode </td><td>Run mode. 0=live, 1=dump, 2=replay. </td></tr>
<tr>
<td>-i </td><td>&ndash;input_dir </td><td>Path to the folder containing the model binaries (pnet*.bin, rnet.bin, onet.bin, mfacenet.bin), database binary (database.bin), etc. </td></tr>
<tr>
<td>-o </td><td>&ndash;output_dir </td><td>Path to the folder that will save the generated files </td></tr>
<tr>
<td>-n </td><td>&ndash;max_loop </td><td>Loop count to run. -1 means the endless loop. </td></tr>
<tr>
<td>-l </td><td>&ndash;log_level </td><td>Log level 0=None, 1=Error, 2=Notice, 3=Debug, 4=Verbose. </td></tr>
<tr>
<td>-t </td><td>&ndash;measure_time </td><td>Measure time flags in order of im, fd, fr, sl, vl, and main threads. For example, 1,0,0,0,0,0. </td></tr>
<tr>
<td>-b </td><td>&ndash;fd_canvas </td><td>RGB canvas parameters in order of id,roi_x,roi_y,roi_w,roi_h. The parameters roi_* are ROI in the canvas. </td></tr>
<tr>
<td>-p </td><td>&ndash;ld_canvas </td><td>IR structured-light canvas parameters in order of id,roi_x,roi_y,roi_w,roi_h. The parameters roi_* are ROI in the canvas. </td></tr>
<tr>
<td>-s </td><td>&ndash;stream </td><td>Stream parameters in order of id,roi_x,roi_y,roi_w,roi_h. The parameters roi_* are ROI in the stream. </td></tr>
<tr>
<td>-e </td><td>&ndash;max_face_num </td><td>Maximum face number to detect with the area priority. </td></tr>
<tr>
<td>-g </td><td>&ndash;fr_thres </td><td>FR threshold. </td></tr>
<tr>
<td></td><td>&ndash;fd_area_thresh </td><td>FD area threshold to multiply 96x112. </td></tr>
<tr>
<td></td><td>&ndash;sl_temperature </td><td>Sets the maximum temperature of structured light in Celsius (0~70). If the current temperature exceeds the maximum value, structured light will not be lit up in dynamic mode. </td></tr>
<tr>
<td></td><td>&ndash;sl_wait_ms </td><td>The time, in milliseconds, to wait after the structured light is enabled and until the IR pattern buffer is placed into the IDSP pipeline. </td></tr>
<tr>
<td></td><td>&ndash;ref_yuv_path </td><td>The reference YUV file path for the Blade Runner. </td></tr>
<tr>
<td></td><td>&ndash;ref_w_h_pitch </td><td>The shape of the reference YUV file in the order of width, height, and pitch. </td></tr>
<tr>
<td></td><td>&ndash;ref_dist </td><td>The distance of reference YUV captured. </td></tr>
<tr>
<td></td><td>&ndash;baseline </td><td>The baseline of sensor module. </td></tr>
<tr>
<td></td><td>&ndash;depth_algo_type </td><td>Select the depth algo, 0=blade_runner, 1=fbm. Default is 0. </td></tr>
<tr>
<td></td><td>&ndash;calc_full_depth </td><td>The flag to calculate the depth of the full image for liveness detection. </td></tr>
<tr>
<td></td><td>&ndash;blade_runner_config </td><td>The parameters for blade runner depth configuration in the order of rotate, mode, disp_range, disp_smooth,disp_min_value, ref_offset, verbose. </td></tr>
<tr>
<td></td><td>&ndash;fbm_config </td><td>The parameters for Fast Block Match configuration in the order of maxspecklesize,maxspecklediff, sad_win_size, sad_thresh, uniqueratio, texture_threshold, disparity_num, min_disparity, detect_near_depth, detect_far_depth. </td></tr>
<tr>
<td></td><td>&ndash;liveness_config </td><td>The parameters for blade runner liveness configuration in the order of min_face_dsi_ratio, max_face_dsi_ratio,min_face_3d_height, max_face_3d_height, min_face_distance, max_face_distance, face_threshold. </td></tr>
<tr>
<td></td><td>&ndash;font_scale </td><td>The font scale to draw the text with the OpenCV API. </td></tr>
<tr>
<td></td><td>&ndash;depth_display </td><td>Draw the depth map on the overlay, 0=disable, 1=enable. The default is enabled. </td></tr>
<tr>
<td></td><td>&ndash;door_control </td><td>Run /usr/local/bin/open_the_door.sh to open the door. Not yet implemented. </td></tr>
<tr>
<td></td><td>&ndash;video_record </td><td>Run /usr/local/bin/record_video.sh to record the video. Not yet implemented. </td></tr>
<tr>
<td></td><td>&ndash;access_led </td><td>The access LED configuration in order of GPIO ID, delay times in ms before the no-access to LED is on, and the timeout to close in ms. Not yet implemented. </td></tr>
<tr>
<td></td><td>&ndash;no_access_led </td><td>The "no access" LED configuration in order of GPIO ID, delay times in ms before access to LED is on, and the timeout to close in ms. Not yet implemented. </td></tr>
<tr>
<td></td><td>&ndash;access_sensitive </td><td>The access sensitive of status-holding threshold counted in frames to filter access events in order of active / inactive. Not yet implemented. </td></tr>
<tr>
<td></td><td>&ndash;ai_metering </td><td>Enables AI metering method and send face ROI to 3A application. 0=disable, 1=enable. The default is disabled. </td></tr>
<tr>
<td></td><td>&ndash;ini_config </td><td>Path to the ini configuration file. </td></tr>
<tr>
<td></td><td>&ndash;lens_config </td><td>The parameters for lens configuration in the order of focal_length, center_x, center_y. </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_tsll_v2_demo"></a>
4.5.2 Run the Face Detection, Face Recognition, and Liveness Test Demo</h3>
<p >The following examples show sample commands for running the <code>test_sl_liveness_v2</code>. Some hardware parameters are not universal to other boards. See the script <code>/tmp/run_ipc_folder/sl_run_liveness_demo_v2.sh</code> generated by <code>quick_ipc_for_access_control.sh</code> for parameters dedicated to the personal board.</p>
<p >For CV25M Janus: </p><div class="fragment"><div class="line">board # test_sl_liveness_v2 --ini_config /usr/share/ambarella/sl_liveness/liveness_v2_config.ini \</div>
<div class="line">    -<a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a> 4,0,0,1280,720 \</div>
<div class="line">    -p 0,320,160,1280,720 \</div>
<div class="line">    -s 1,320,160,1280,720 \</div>
<div class="line">    --ref_yuv_path /usr/share/ambarella/calib_mono/ref_b0_H1912060058_canvas0_1920x1080_NV12_1.524.yuv --ref_w_h_pitch 1920,1080,1920 \</div>
<div class="line">    --ref_dist 0.8 --baseline 0.04 --calc_full_depth 0 \</div>
<div class="line">    --lens_config 766,957,540 \</div>
<div class="line">    --blade_runner_config 1,0,6,13,0,23,0</div>
</div><!-- fragment --><p >For CV22 Vision+: </p><div class="fragment"><div class="line">board # test_sl_liveness_v2 --ini_config /usr/share/ambarella/sl_liveness/liveness_v2_config.ini \</div>
<div class="line">    -<a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a> 4,0,0,1280,1000 \</div>
<div class="line">    -p 0,320,0,1280,1000 \</div>
<div class="line">    -s 1,320,0,1280,1000 \</div>
<div class="line">    --ref_yuv_path /usr/share/ambarella/calib_mono/ref_b0_H210802A036_canvas0_1920x1080_NV12_0.798.yuv --ref_w_h_pitch 1920,1080,1920 \</div>
<div class="line">    --ref_dist 0.8 --baseline 0.04 --calc_full_depth 0 \</div>
<div class="line">    --lens_config 766,957,540 \</div>
<div class="line">    --blade_runner_config 1,0,6,13,0,23,0</div>
</div><!-- fragment --><h2><a class="anchor" id="acc_ctrl_test_calib_deviation"></a>
4.6 Unit_test: test_calibrate_deviation</h2>
<p >The <code>test_calibration_deviation</code> evaluates the dewarp result (<code>calib_mono/fov1.bin</code>). It analyzes a YUV file that contains a chess board object. The algorithm inside calculates whether or not the lines of the chess board are straight enough.</p>
<h3><a class="anchor" id="acc_ctrl_tcd_intro"></a>
4.6.1 Introduction to test_calibrate_deviation</h3>
<table class="doxtable">
<caption>test_calibrate_deviation Usages.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-f </td><td>&ndash;yuv-path </td><td>Specifies the YUV file's path. </td></tr>
<tr>
<td>-r </td><td>&ndash;yuv-info </td><td>Specifies the YUV file's width and height. The default is 1920x1080. </td></tr>
<tr>
<td>-c </td><td>&ndash;chess-board-info </td><td>Specifies the chess board's columns and rows. The default is 8x6. </td></tr>
<tr>
<td>-t </td><td>&ndash;point-dist-thres </td><td>Specifies the threshold of distance between the chess board corner point and the regression line in pixels. The default is 1.0. </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_tcd_demo"></a>
4.6.2 Run the Calibration Deviation to Check the Demo</h3>
<p >The following shows the command for the calibration deviation to check the demo. The chess board information must be specified depending on the chess board dimension captured in the YUV file. </p><div class="fragment"><div class="line">board # test_calibrate_deviation -f /tmp/chessboard.yuv -r 1920x1080 -c 8x6 -t 1</div>
</div><!-- fragment --><h2><a class="anchor" id="acc_ctrl_test_sl"></a>
4.7 Unit_test: test_structured_light</h2>
<p >The <code>test_structured_light</code> evaluates the quality and orientation of the structured-light projector. It scans the structured-light pattern image and calculates the coverage of the structured-light pattern in the provided ROI.</p>
<h3><a class="anchor" id="acc_ctrl_tsl_intro"></a>
4.7.1 Introduction to test_structured_light</h3>
<table class="doxtable">
<caption>test_structured_light Usages.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-f </td><td>&ndash;yuv-path </td><td>Specifies the YUV file's path. </td></tr>
<tr>
<td>-r </td><td>&ndash;yuv-info </td><td>Specifies the YUV file's width and height. The default is 1920x1080. </td></tr>
<tr>
<td>-d </td><td>&ndash;data-roi </td><td>Specifies the ROI area. The default is 0,0,1920,1080. </td></tr>
<tr>
<td>-w </td><td>&ndash;weber-contrast-thres </td><td>Specifies the Weber contrast threshold of the individual subareas (4x4 or 16x16 pixels). The default is 0.5. </td></tr>
<tr>
<td>-s </td><td>&ndash;sl-coverage-thres </td><td>Specifies the structured-light coverage threshold of the ROI area. The default is 0.99. </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_tsl_demo"></a>
4.7.2 Run the Structured Light Test to Check the Demo</h3>
<p >The following shows the command for the structured light test to check the demo. The test takes a YUV file as an input and evaluates the coverage of the structured-light pattern in a defined ROI area. </p><div class="fragment"><div class="line">board # test_structured_light -f /tmp/reference.yuv -r 1920x1080 -d 320,160,1280,720 -w 0.5 -s 0.9</div>
</div><!-- fragment --><h2><a class="anchor" id="acc_ctrl_test_ref_yuv"></a>
4.8 Unit_test: test_reference_yuv</h2>
<p >The <code>test_reference_yuv</code> is a tool that evaluates the quality of the reference YUV file taken in the calibration process. It receives a YUV input with a structured-light pattern and checks whether the pattern has sufficient contrast between the structured-light dots and the background.</p>
<h3><a class="anchor" id="acc_ctrl_try_intro"></a>
4.8.1 Introduction to test_reference_yuv</h3>
<table class="doxtable">
<caption>test_reference_yuv Usages.</caption>
<tr>
<th align="center" width="150px">Short Options </th><th align="center" width="200px">Long Options </th><th align="center">Usages </th></tr>
<tr>
<td>-f </td><td>&ndash;yuv-path </td><td>Specifies the YUV file's path. </td></tr>
<tr>
<td>-r </td><td>&ndash;yuv-info </td><td>Specifies the YUV file's width and height. The default is 1920x1080. </td></tr>
<tr>
<td>-a </td><td>&ndash;intensity-range </td><td>Specifies the average intensity range. The default is 0,255. </td></tr>
<tr>
<td>-c </td><td>&ndash;contrast-theshold </td><td>Specifies the contrast threshold. The default is 0. </td></tr>
</table>
<h3><a class="anchor" id="acc_ctrl_try_demo"></a>
4.8.2 Run the Reference YUV to Check the Demo</h3>
<p >The following shows the command for the reference YUV to check the demo. It calculates the intensity and contrast of the reference YUV file, and then evaluates the quality of the captured reference YUV. </p><div class="fragment"><div class="line">board # test_reference_yuv -f /tmp/reference.yuv -a 30,100 -c 500</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="acc_ctrl_len_calib"></a>
5. Lens Calibration</h1>
<h2><a class="anchor" id="acc_ctrl_mon_calib_tool"></a>
5.1 Mono Calibration Tool Version</h2>
<p >The table below shows the release history of the mono calibration tool. If the current calibration tool is not available, contact the Ambarella support team for the latest version.</p>
<table class="doxtable">
<caption>Mono Calibration Tool Version Table.</caption>
<tr>
<th align="center">Mono Calibration Tool Version </th><th align="center">Updated Date </th><th align="center">Modification </th></tr>
<tr>
<td>cv2x_sdk_2.5_mono_v231_20200713 </td><td>20200713 </td><td>Initial release of mono calibration </td></tr>
</table>
<h2><a class="anchor" id="acc_ctrl_calib_pkg_install"></a>
5.2 PC Environment Setup</h2>
<p >This section provides information for installing the AmbaCalib tool to calculate the warp table.</p><ol type="1">
<li>Prepare the PC with Ubuntu version 18.04 installed.</li>
<li>Download the latest mono calibration package.</li>
<li>Run the commands below to install the AmbaCalib tool. <div class="fragment"><div class="line">build # sudo dpkg -i ambacalib-&lt;VERSION&gt;.deb</div>
<div class="line">build # sudo apt install -f</div>
</div><!-- fragment --></li>
</ol>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>AmbaCalib enables the lens distortion parameters to be calculated and generated into the warp table in order to remove the distortion.</li>
<li>By default, NV12 YUV images with the resolution of 1920x1080 are selected. To specify another resolution, use the following parameter: -r 1920 1080.</li>
<li>Use the option <code>-i &lt;NAME&gt; &lt;imagelist&gt;</code> to specify the first master image list and the slave image list, respectively.</li>
<li>Use wildcard <code>%0xd</code> in the image list to select all images.</li>
<li>Use the option <code>-o Calibfolder</code> to override the output folder.</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="acc_ctrl_calib_board"></a>
5.3 Calibration Board Preparation</h2>
<p >Ambarella recommends that users print directly on a flat and rigid particle board or a medium-density fireboard to manufacture the calibration target. Because manufacturing tolerance can affect the dot spacing, a direct measurement of the actual board width and height must be performed. To perform this measurement, users must measure the space between the reference markers on the border of the grid, both in the vertical and horizontal directions. The illustration below shows an enlarged view of the top part of the circle target, with the reference marker positions highlighted.</p>
<p >After measuring the width and height, as demonstrated in the figure, the numbers (divided by horizontal / vertical grids) must be entered into the calibration initiation file. For example, if the width / height is 110.3 cm / 76.4 cm, the spacing is HSPACE = 110.3cm / 46, VSPACE = 76.4cm / 32 in .ini files (such as <code>calibtool_mono_pinhole.ini</code>).</p>
<p >[SECTION 3]<br  />
 &emsp; HSPACE = 0.023978<br  />
 &emsp; VSPACE = 0.023875<br  />
 [END]</p>
<dl class="section note"><dt>Note</dt><dd>The calibration target images and calibtool_mono_pinhole files can be found in the calibrate_targets folder and profiles folder respectively within the calibration tool package. The section number in the <code>calibtool_mono_pinhole.ini</code> file must match the calibrated target’s index.</dd></dl>
<div class="image">
<img src="../../acc_ctrl_calib_board.png" alt=""/>
<div class="caption">
Figure 5-1. Calibration Board.</div></div>
<h2><a class="anchor" id="acc_ctrl_gen_warp_table"></a>
5.4 Warp Table Generation</h2>
<p >This section provides instructions for capturing the RGB YUVs and calculating the warp table.</p><ol type="1">
<li>Generate the board_config for the calibration, followed by the device’s module number: <div class="fragment"><div class="line">board # gen_board_config.sh /usr/share/ambarella/calib_mono H1912060001</div>
</div><!-- fragment --></li>
<li>Run <code>quick_ipc_for_access_control.sh</code> followed by the board’s module ID to initialize the access control EVK board. Use VLC to open <code>rtsp://10.0.0.2/stream2</code> during the YUV capturing stage. <div class="fragment"><div class="line">board # quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</div>
</div><!-- fragment --></li>
<li>Insert an SD card and create a new folder with the name of the module ID. From this folder, run <code>calib_capture.sh H1912060001 0 16</code> to capture 17 YUV files. The data will be stored in the <code>mono_yuv</code> folder under the current path. Ensure that at least one rectangular tag appears in the FoV, and that the YUV images cover all regions of the full FoV. <div class="image">
<img src="../../acc_ctrl_calib_capture.png" alt=""/>
<div class="caption">
Figure 5-2. Capture the YUV File from the RGB Channel for Lens Calibration.</div></div>
</li>
<li>Copy the <code>mono_yuv</code> folder to the PC and run AmbaCalib. Ensure that <code>calibtool_mono_pinhole.ini</code> and <code>calibtool_mono_pinhole.cas</code> are placed under the mono_yuv folder. These two files can be found in the profile folder in the calibration tool package. Then, run the command (seen in the box on the following page) on the PC. The sensor name below (<code>warp_&lt;resolution&gt;</code>) must be renamed to end with the correct resolution; for example, <code>warp_1080p</code> for the Janus board, and <code>warp_2160p</code> for the Vision+ board. Additionally, the resolution below must match the YUV input; for example, 1920x1080 for the Janus board, and 3840x2160 for the Vision+ board. <div class="fragment"><div class="line">host $ mkdir -p calib_mono &amp;&amp; /opt/amba/bin/calibtool calibtool_mono_pinhole.ini \</div>
<div class="line">    -iwarp_&lt;resolution&gt; %d/mono_1920x1080_nv12_0000000000.yuv_canvas1_1920x1080_NV12.yuv \</div>
<div class="line">    -<a class="code hl_variableRef" href="../../../driver/df/dc0/group__IAV.html#gaf80df1bdae91e5f76236e6ed1110825d">x</a> warp_&lt;resolution&gt; -t calib -o calib_mono/</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>After the warp table is successfully generated, the fitting error parameters are printed out on the terminal. Ensure that the values are less than 0.2 sqm to achieve a favorable dewarp result. If the values exceed 0.2 sqm, the user must recapture the YUV files.</dd></dl>
</li>
<li>Add the <code>readme.txt</code> in the <code>calib_mono</code> folder to track the calibration data. <div class="fragment"><div class="line">host $ cat calib_mono/readme.txt</div>
<div class="line">This warp table is used <span class="keywordflow">for</span> access control EVK module H1912060001</div>
</div><!-- fragment --></li>
<li>Generate SHA256 for the warp table files. <div class="fragment"><div class="line">host $ sha256sum -<a class="code hl_variableRef" href="../../../library/d1/d82/cJSON_8h.html#a1a175e87536301df98c805ac0636ad7c">b</a> calib_mono/warp_* &gt;&gt; calib_mono/calib_data_sha256.txt</div>
</div><!-- fragment --></li>
<li>Copy the <code>calib_mono</code> folder to the access control board <code>/usr/share/ambarella</code> folder, and ensure that the <code>warp_&lt;resolution&gt;.bin</code> and <code>warp_&lt;resolution&gt;.json</code> files are located in <code>/usr/share/ambarella/calib_mono/</code>.</li>
<li>Reboot the system and run <code>quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</code>. Use VLC to open <code>rtsp://10.0.0.2/stream2</code>.</li>
<li>Place the camera so that it is facing the chessboard image, as shown below, and then capture the YUV image. <div class="fragment"><div class="line">board # test_yuvcap –b 1 –Y –F 2 –f /tmp/calib_dev –r 1</div>
</div><!-- fragment --> <div class="image">
<img src="../../acc_ctrl_calib_deviation_image.png" alt=""/>
<div class="caption">
Figure 5-3. Capture the YUV File from the RGB Channel for the Calibrate Deviation Check.</div></div>
</li>
<li>Evaluate the calibration deviation with <code>test_calibrate_deviation</code>. If the output deviation exceeds the threshold defined in the program option, a warning message will appear. <div class="fragment"><div class="line">board # test_calibrate_deviation –f /tmp/calib_dev* -c 8x6 -t 3</div>
<div class="line">    YUV file is /tmp/x_canvas1_1920x1080_NV12.yuv</div>
<div class="line">    Point distance threshold is set to 3.000000</div>
<div class="line">    Notice: calibrate deviation is acceptable. [max corner point deviation: 1.381636 pixels]</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>If a different chessboard image is used, change the grid dimension option <code>-c</code>.</dd></dl>
<hr  />
</li>
</ol>
<h1><a class="anchor" id="acc_ctrl_ref_cap"></a>
6. Reference Capture</h1>
<ol type="1">
<li><p class="startli">Run <code>quick_ipc_for_access_control.sh --rgb_sub_buf_out_res 1280x720</code> and then <code>ref_capture_init.sh</code>. After the reboot, the image will be tuned to best fit the current ambient light conditions, and the warp table in <code>calib_mono/warp_&lt;resolution&gt;.bin</code> will be enabled to remove the lens distortion.</p>
<p class="startli">To ensure that the YUV frame is captured with a structured-light pattern, open <code>rtsp://10.0.0.2/stream1</code> for the IR stream and <code>rtsp://10.0.0.2/stream2</code> for the RGB stream. Verify that the IR pattern is from stream 1. <code>ref_capture_init.sh</code> evaluates the current light environment and adjusts the structured-light flash time, shutter, and gain in order to find the best reference YUV under the current ambient light conditions. If the program cannot obtain the qualified reference YUV under the current light conditions, an error message will appear. <a class="anchor" id="acc_ctrl_ref_yuv_img"></a></p><div class="image">
<img src="../../acc_ctrl_ref_yuv.png" alt=""/>
<div class="caption">
Figure 6-1. Reference Data Quality.</div></div>
<p class="startli">Follow the instructions to change the ambient light. Further, ensure that there is no obstruction between the lens and the wall. For example, in <a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_ref_yuv_img">Figure 6-1</a>, (A) and (B) show quality images; however, (C) is unacceptable because the pattern is unclear.</p>
<p class="startli">Ensure that the setting is facing a white wall, the distance from the lens center to the wall is 0.8 meters, and that the sensor plane is parallel to the wall. Users must be able to see a cross on the center of stream 2. Draw a cross on the wall at the same height, and then place the sensor at 0.8 m (the same height with the cross and vertical to the wall). Finally, adjust the orientation to ensure that the cross in stream 2 is aligned with the cross on the wall.</p>
</li>
<li><p class="startli">Run the following command to capture the reference YUV file. The structured-light pattern coverage is checked. If its coverage is less than 85% of the cropped area, then the structured-light coverage may be insufficient or the orientation is incorrect. If either of these occur, the reference YUV file will not be captured. </p><div class="fragment"><div class="line">board # ref_capture.sh /usr/share/ambarella/calib_mono H1912060001 0.8</div>
</div><!-- fragment --><p> The first parameter is the path to the calib_mono folder, which is generated at the lens calibration stage (<a class="el" href="../../d2/d89/acc_ctrl_page.html#acc_ctrl_gen_warp_table">5.4 Warp Table Generation</a>); the second parameter is the module ID; and the third parameter is the distance from the lens’ center to the wall. Ambarella recommends that users utilize a laser rangefinder to measure the distance for the third parameter.</p>
<p class="startli">Two YUV files will be captured and saved into the <code>/usr/share/ambarella/calib_mono folder</code>. One is 480x272(Janus) / 480x376(Vision+) and the other is 1920x1080. Users can find the distance and module name in the YUV file name.</p>
</li>
<li>After the capture is completed, back up the <code>/usr/share/ambarella/calib_mono</code> folder for each setting. To create a configuration file for the access control boards, run <code>gen_board_config.sh</code>. <div class="fragment"><div class="line">board # gen_board_config.sh /usr/share/ambarella/calib_mono H1912060001</div>
</div><!-- fragment --> The first parameter is the path to the <code>/usr/share/ambarella/calib_mono</code> folder, and the second parameter is the access control board’s module ID. After running the command, the user configuration settings will be created in <code>/usr/share/ambarella/calib_mono/board_config</code>.</li>
<li>To verify the calibration and dewarp result, run <code>blade_runner_run_liveness_demo.sh</code>. Then open <code>rtsp://10.0.0.2/stream2</code> to check the depth map against the livestream.</li>
<li>Back up the calibration data and reference data, then copy <code>calib_mono</code> to <code>/usr/share/ambarella/</code> by FTP after the firmware downloads.</li>
</ol>
<hr  />
<h1><a class="anchor" id="acc_ctrl_faq"></a>
7. FAQ</h1>
<h2><a class="anchor" id="acc_ctrl_faq_q1"></a>
Question 1: How can frame rates of a structured light (SL) channel be increased?</h2>
<p ><b>Answer:</b> This can be achieved by changing VIN fps and the frame ratio on the SL channel.</p>
<p >VIN fps can be increased by the option of <code>vsrc_fps</code> in the <code>quick_ipc_for_access_control.sh</code>. It increases the total number of frames coming from the VIN sensor.</p>
<p >Frame ratio can be configured by the parameter of <code>FRAME_RATIO</code> in <code>/usr/share/ambarella/calib_mono/board_config</code> file. This parameter defines how VIN frames are dispatched onto each channels.</p>
<h2><a class="anchor" id="acc_ctrl_faq_q2"></a>
Question 2: What is dynamic mode? How is SL dynamic mode enabled?</h2>
<p ><b>Answer:</b> By default, the access control system starts from static mode. In this mode, the structured light is always enabled on predefined SL frames. The time sequence of frames outputting from each channel is static. The time when the SL projector is triggered is also static.</p>
<p >The option of <code>enable_dynamic_mode</code> for <code>quick_ipc_for_access_control.sh</code> allows the access control system to start in dynamic mode. At this time, there is no longer output on the SL channel,unless the SL-trigger-once event is enabled at the user application. This is useful when users would like to enable SL only when an event is occurring, such as a person-detect event or a motion-detect event.</p>
<h2><a class="anchor" id="acc_ctrl_faq_q3"></a>
Question 3: What are users required to take care of in the mechanical design?</h2>
<p ><b>Answer:</b> The vertical-cavity surface-emitting laser (VCSEL) projector can be easily overheat when it is enabled for extended periods of time. This requires heat dissipation design, such as a heat sink for the projector. The placement between VCSEL and the ensor should be steady and fixed. The displacement of the baseline will affect the depth generation result.</p>
<h2><a class="anchor" id="acc_ctrl_faq_q4"></a>
Question 4: How is the on-board SDIO WiFi set up?</h2>
<p ><b>Answer:</b> The WiFi setup can be divided into two steps:</p><ol type="1">
<li>Probe the WiFi driver. <div class="fragment"><div class="line">board # modprobe bcmdhd</div>
</div><!-- fragment --></li>
<li>Connect to the access point. <div class="fragment"><div class="line">board # wifi_setup.sh sta nl80211 &lt;ssid&gt; &lt;password&gt;</div>
</div><!-- fragment --> <dl class="section note"><dt>Note</dt><dd>The WiFi firmware is no longer included in the SDK starting from the revision 3.0.9. The <code>external_wifi_utility</code> patch must be applied in order to use the on-board SDIO WiFi.</dd></dl>
<hr  />
</li>
</ol>
<h1><a class="anchor" id="acc_ctrl_license"></a>
8. License</h1>
<p >Copyright (c) 2024 Ambarella International LP.</p>
<p >This file and its contents ("Software") are protected by intellectual property rights including, without limitation, U.S. and/or foreign copyrights. This Software is also the confidential and proprietary information of Ambarella International LP and its licensors. You may not use, reproduce, disclose, distribute, modify, or otherwise prepare derivative works of this Software or any portion thereof except pursuant to a signed license agreement or nondisclosure agreement with Ambarella International LP or its authorized affiliates. In the absence of such an agreement, you agree to promptly notify and return this Software to Ambarella International LP.</p>
<p >THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL AMBARELLA INTERNATIONAL LP OR ITS AFFILIATES BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; COMPUTER FAILURE OR MALFUNCTION; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.3 </li>
  </ul>
</div>
</body>
</html>
